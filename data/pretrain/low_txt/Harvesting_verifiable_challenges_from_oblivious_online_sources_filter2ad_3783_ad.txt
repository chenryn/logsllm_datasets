### Updating Policies Post-Deployment

After deploying policies, they can be updated alongside other security updates to the application. For instance, a new policy could be delivered with other security patches.

### Entry Lifetime and Frequency

**Entry Frequency:**
The rate at which new entries appear or old entries are modified is a critical parameter for feeds used in our system. This frequency determines how often fresh entries will be available. In the Popular data set, the average entry frequency was 16.5 per day, ranging from as high as 125 entries per day to just one entry over a week. For active sites in the Longtail data set, the average entry frequency was much lower at 4.77 per day, but it ranged up to 65.8 per day.

**Entry Lifetime:**
Another important parameter is the entry lifetime, which measures the duration from when an entry first appears in the feed until it is updated, deleted, or replaced by newer content. The average entry lifetime for a source determines how long we can expect to verify the source’s content. We measured lifetimes for entries that were removed during our data collection period. If an entry was posted before our data collection began, we used the posting date listed in the feed as the start of the entry’s lifetime.

- **Popular Data Set:**
  - We recorded the lifetimes of 11,779 entries from 102 sites.
  - The average lifetime was 38.5 hours.
  - Individual site averages ranged from 3.7 hours to 43 days (for sites with statistically significant data points).

- **Longtail Data Set:**
  - We recorded 2,404 entry lifetimes from 72 distinct sites.
  - The average lifetime was 118 hours.
  - Individual site averages ranged from 6.9 hours to 90 days (for sites with sufficient data).

### RSS Feed Characteristics

RSS feeds typically include a fixed number of entries, with older entries being replaced as new ones are added. The number of entries in an RSS feed limits the amount of content we can extract. We recorded the average number of entries present in each available feed:

- **Longtail Data Set:** Average of 16.3 entries.
- **Popular Data Set:** Average of 22.3 entries.

Since feed sizes are usually constant, there is an inverse relationship between the frequency of new entries and the average time entries remain in a feed, as shown in Figure 2. Points in the horizontal tail of the distribution tend to come from the Popular data set, indicating that popular sources are more likely to post several entries a day, leading to a short lifetime for each entry. Points in the vertical tail tend to come from the Longtail data set, indicating that those sites are more likely to post entries infrequently, leading to a longer lifetime for each entry.

### Policy Satisfaction

To test how well the RSS feeds in our data sets would serve as sources for harvested challenges, we modeled several simple policies and verifiability requirements and calculated the percentage of the sample period when these policies and requirements would be satisfied.

**General Principle:**
The more time that elapses between derivation and verification, the less precisely we need to know the freshness of the challenge. This mirrors the inverse relationship between entry lifetime and frequency depicted in Figure 2. Thus, we expect RSS feeds to meet the needs of many kinds of applications.

**Short Policy:**
- **Definition:** A deriver collects all entries from the source that are less than one hour old, and verifiers require at least one entry in the derivation to match the contents of the feed.
- **Satisfaction Calculation:** We tested this policy with two robustness standards requiring the policy to be verifiable a minimum of 6 hours and 12 hours after derivation.
- **Results:**
  - 13% of sources satisfied the 6-hour requirement at least 50% of the time.
  - 7% of sources satisfied the 12-hour requirement at least 50% of the time.

**Long Policy:**
- **Definition:** A deriver collects all entries from the source that are less than one day old, and verifiers require at least one entry in the derivation to match the contents of the feed.
- **Satisfaction Calculation:** We tested this policy with two robustness standards requiring the policy to be verifiable a minimum of 7 days and 14 days after derivation.
- **Results:**
  - 24% of sources satisfied the 7-day requirement at least 50% of the time.
  - 7% of sources satisfied the 14-day requirement at least 50% of the time.

**Conclusion:**
Not all sources are equally suitable for harvesting challenges; however, a significant fraction of sources were able to satisfy our model requirements at least half the time. Policy creators can achieve high robustness by combining data from multiple such sources. If satisfaction times were uniformly distributed and uncorrelated between sources, combining 10 sources with a suboptimal 50% satisfaction rate would reduce expected downtime to less than 90 seconds a day.

In practice, satisfaction times were correlated and not uniformly distributed, but policy creators will tend to select sources that are well-suited for their policies. Figures depicting the satisfaction of 7 selected sources modeling the “Short” and “Long” policies with respective verifiability requirements show that at least one or two sources satisfied the policies at all times during the period.

### Other Considerations

**Ensuring Freshness:**
To ensure the freshness of harvested challenges, a policy must prevent an attacker from predicting the future contents of sufficiently many of its sources. The predictability of an RSS feed depends heavily on the type of content being served. Content like wire service news headlines, which are widely carried by different sites, may be predictable minutes or hours before appearing on a particular source. Some sites, like the popular link aggregator Digg, allow members to influence the content that appears in their feeds by voting on candidate entries. An attacker could monitor changes in popularity during the voting process to predict future feed contents. Policy creators should avoid such feeds.

**Posting Times:**
Posting times for many sources in our data sets were strongly correlated with local daylight hours for the site. This effect is clearly visible in the first figure, where the large vertical shaded regions indicate times from 7 p.m. to 7 a.m., PST. If freshness on the order of hours is required, policy creators might select sources from around the world, such as major newspapers from each timezone.

**Security Considerations:**
We normally assume that an attacking deriver cannot sit between the verifier and the Internet sources and modify the verifier’s view of their contents. Even if this is not the case, our model can remain secure if the deriver is unable to corrupt enough sources to fool the verifier. The remaining danger can be mitigated by selecting feeds that are served using HTTPS.

### Related Work

Our work on harvesting challenges from the Internet touches on prior research in several areas. In this section, we describe relevant related work and provide context for our contributions.

**Deriving Randomness:**
The idea of extracting bits from a random source has been around for many years. Several works have shown how to extract randomness suitable for cryptographic applications from non-uniform sources of entropy, such as physical events on a local machine. Our problem is somewhat different, as we want Alice not only to be able to extract sufficient randomness for her own use but also to convince Bob that she derived it properly and freshly. Our primary challenges arise because the oblivious Internet sources we wish to use are unreliable, meaning the deriver and the verifier may not have the same view of these entropy sources. Our work focuses on the practical problems arising from this setting, simplifying the theoretical aspect of deriving uniform randomness by modeling our hash function as a random oracle.

**Proofs of Work and Client Puzzles:**
Dwork and Naor first proposed the idea of using proofs of computational puzzles for mitigating spam. Adam Back independently proposed and implemented a similar system known as Hashcash. Both systems fail to prevent the pre-computation of puzzles by an attacker, who can begin computation long before an attack is launched. Juels and Brainard observed that allowing arbitrary precomputation was problematic for protecting against DoS attacks, where an adversary might build up a collection of puzzle solutions over a long period and use them over a brief period to flood a server. They proposed a solution called “client puzzles,” where a challenger sends a fresh random challenge to a machine requesting a resource and demands that the machine do a proof of work relative to the fresh challenge. Since an attacker does not know the challenge ahead of time, he is forced to perform all his computation online. Several other papers subsequently proposed other types of client puzzle solutions.

One issue that arises from these systems is that a challenger must issue a random challenge, which is not possible for non-interactive applications such as email. By leveraging our tool, we can provide suitable fresh challenges in many of these settings. Waters et al. provide another motivation for our approach. In their system, a client spends a somewhat larger amount of time solving a puzzle that can be used as a proof of work for many different servers. Since each server does not have the opportunity to challenge the client individually, the system requires a common trusted source for random challenges. By adapting their system to utilize our tool, we can potentially eliminate the need for such servers.

Borisov examined the problem of deriving randomness from a community of peer-to-peer servers. Our approach is quite different, as we harvest challenges from oblivious Internet content providers and require less complex interaction to synthesize our challenges. Additionally, Borisov's work was initially inspired by our preliminary research.

Client puzzle solutions must be carefully designed to successfully mitigate attacks. Issues include the cost of verifying puzzles, the discrepancy between the computational resources of portable devices and standard processors, and the possibility that attackers will have control of large bot-nets. Many of the works cited above address these issues in specific settings. We stress that any system that uses our tool must carefully consider these issues in the context of the application it is protecting.

### Conclusions and Future Work

In this paper, we addressed the problem of harvesting challenges from oblivious online servers. This setting presented us with a challenging set of issues, as we not only had to consider the usual issues of security and robustness in our application but also deal with the unique problem that our Internet sources are unaware of their role in our system. We addressed this problem by creating a framework with which a party can harvest a challenge from several sources and (non-interactively) convince another party that it was formed correctly. Our framework allows an application designer to specify a flexible policy that can be tailored to specific needs. We identified multiple security contexts where our tool may be valuable, including remote storage auditing and P2P Sybil attack prevention.

We implemented our methods in a software tool named Combine. Combine is able to use RSS feeds, historical stock quotes, and explicit randomness servers as sources for harvesting random challenges. We provided experimental data supporting our framework’s practicality, and we built a proof-of-concept application, Postmark, that uses Combine to create an improved client puzzle system for email.

In the near future, we plan to apply these techniques to build auditing mechanisms for existing systems. We will start by constructing an auditing component for a remote storage service. We expect that this process will teach us more about the subtleties of using harvested challenges in a systems environment. From a broader perspective, we will continue to search for additional applications where harvested challenges can be used to verify claims of distributed systems.

### Acknowledgments

We would like to thank Dan Boneh, Ed Felten, Pat Lincoln, Chris Peikert, Amit Sahai, Shabsi Walfish, and our anonymous reviewers for useful comments and suggestions. This material is based upon work supported under a National Science Foundation Graduate Research Fellowship. Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the National Science Foundation.

### References

[1] Tuomas Aura, Pekka Nikander, and Jussipekka Leiwo. DOS-resistant authentication with client puzzles. In Security Protocols Workshop, pages 170–177, 2000.

[2] Adam Back. Hashcash – a denial of service counter-measure. http://www.hashcash.org/hashcash.pdf, 2002.

[3] Mihir Bellare and Sara K. Miner. A forward-secure digital signature scheme. In CRYPTO, pages 431–448, 1999.

[4] Mihir Bellare and Phillip Rogaway. Random oracles are practical: A paradigm for designing efficient protocols. In ACM Conference on Computer and Communications Security, pages 62–73, 1993.

[5] Nikita Borisov. Computational puzzles as sybil defenses. In Peer-to-Peer Computing, pages 171–176, 2006.

[6] Giovanni Di Crescenzo, Richard J. Lipton, and Shabsi Walfish. Perfectly secure password protocols in the bounded retrieval model. In TCC, pages 225–244, 2006.

[7] Drew Dean and Adam Stubblefield. Using client puzzles to protect TLS. In 10th Usenix Security Symposium, pages 1–8, 2001.

[8] Yevgeniy Dodis, Ariel Elbaz, Roberto Oliveira, and Ran Raz. Improved randomness extraction from two independent sources. In APPROX-RANDOM, pages 334–344, 2004.

[9] John R. Douceur. The sybil attack. In IPTPS, pages 251–260, 2002.

[10] Cynthia Dwork and Moni Naor. Pricing via processing or combatting junk mail. In CRYPTO, pages 139–147, 1992.

[11] J. Galvin, S. Murphy, S. Crocker, and N. Freed. Security Multiparts for MIME: Multipart/Signed and Multipart/Encrypted. RFC 1847 (Proposed Standard), October 1995.

[12] Ari Juels and John G. Brainard. Client puzzles: A cryptographic countermeasure against connection depletion attacks. In NDSS, 1999.

[13] Martijn Koster. A standard for robot exclusion. http://www.robotstxt.org/wc/norobots.html, 1994.

[14] R. Kotla, M. Dahlin, and L. Alvisi. Safestore: A durable and practical storage system. In USENIX Annual Technical Conference, 2007.

[15] National Solar Observatory/Sacramento Peak. Images and current data. http://nsosp.nso.edu/data/.

[16] USGS Earthquake Hazards Program. Latest earthquakes in the world - past 7 days. http://earthquake.usgs.gov/eqcenter/recenteqsww/Quakes/quakes_all.php.

[17] RSS 2.0 specification. http://blogs.law.harvard.edu/tech/rss, 2003.

[18] Technorati: About us. http://www.technorati.com/about/, 2007.

[19] XiaoFeng Wang and Michael K. Reiter. Defending against denial-of-service attacks with puzzle auction. In IEEE Symposium on Security and Privacy, pages 78–92, 2003.

[20] Brent Waters, Ari Juels, J. Alex Halderman, and Edward W. Felten. New client puzzle outsourcing techniques for DoS resistance. In ACM Conference on Computer and Communications Security, pages 246–256, 2004.