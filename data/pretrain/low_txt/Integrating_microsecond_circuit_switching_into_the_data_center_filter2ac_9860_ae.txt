### Duty Cycle Overhead and Line Rate

The duty cycle overhead is solely a function of the underlying line rate, not the aggregate link rate (e.g., 100 or 400 Gbps).

### Multiple-Ring Scaling

Another method to scale beyond 88 ports is to use multiple stacked rings, with each ring reusing the same wavelength channels. For instance, an 8×8 ring-selection Optical Circuit Switch (OCS) can enable the construction of a 704-port OCS (8 × 88 = 704). It is crucial that all inputs assigned to the same wavelength channel are connected to the same ring-selection OCS to avoid collisions within a particular ring. The ring-selection OCS is used only for input ports; output ports directly connect to the individual rings.

One drawback of a stacked ring architecture is the longer "week" lengths, which can be problematic for low-latency applications, necessitating a packet-switched network. While a single-ring architecture is fully non-blocking, the stacked-ring architecture is blocking, meaning not all input-output port mappings are possible. This challenge arises from reusing a finite number of wavelength channels across a larger number of switch ports. A potential solution is to use tunable lasers that can transmit on any wavelength channel, restoring the fully non-blocking property of the OCS, albeit at the cost of additional optical and algorithmic complexity.

In terms of scaling to higher port counts, our proposed approach may not directly apply to networks with clusters larger than 704 Top-of-Rack (ToR) switches. However, assuming 40 servers per ToR, this still scales to 25,000 servers in a single cluster.

### Integrated OCS Switching

It is also feasible to build an integrated scale-out OCS by interconnecting smaller OCS switches in a multi-stage topology on a single board, using waveguides instead of discrete fibers. This approach significantly reduces loss, as the couplers used to connect the switch to the fibers can be a significant source of loss. Multi-stage, integrated OCSes have been built [3], but they rely on slower 3D-MEMS technology.

### Synchronization Jitter

Figure 11 shows the synchronization jitter as seen by the OS in our Linux-based emulated ToR switches. The throughput of UDP traffic on the OCS (OCS-UDP) ranges from 5.73 to 8.43 Gbps, or within 4.6% of EPS-UDP. The main reasons for the discrepancy are duty cycle, NIC delay, the OS's delay in handling a softirq, and synchronization jitter (see discussion below).

TCP throughput on the OCS (OCS-TCP) ranges from 2.23 to 5.50 Gbps, or within 12.1% of EPS-TCP for circuit day durations. TCP throughput suffers from all the issues of UDP throughput, plus two additional ones: first, TCP traffic cannot use TCP Segmentation Offload (TSO) to offload, making the TCP/IP stack CPU-bound when handling the required 506 connections. Second, the observed 0.5% loss rate invokes congestion control, reducing throughput. However, TCP does show an upward trend in bandwidth with increasing duty cycle.

We suspect that the reduction in throughput in Mordia is due to the "long-tailed" behavior of the synchronization aspect of its control plane. To test this, we examined synchronization jitter. Synchronization packets are generated by the FPGA to minimize latency and jitter, but the OS can add jitter when receiving packets and scheduling softirqs. To measure this jitter, we set the day and night to 106 µs and 6 µs, respectively, and captured 1,922,507 synchronization packets across three random hosts. We computed the difference in timestamps between each packet and expected to see packets arriving with timestamp deltas of either 6±1 µs or 100±1 µs. We found that 99.31% of synchronization packets arrive at their expected times, 0.47% of packets arrive with timestamp deltas of zero, and 0.06% of packets arrive with timestamp deltas between 522 µs and 624 µs (see Figure 11). The remaining 0.16% of packets arrive between 7–99 µs. We also note that the 0.53% of synchronization packets that arrive at unexpected times is very close to our measured loss rate. Our attempts to detect bad synchronization events in the Qdisc did not change the loss rate measurably. Firmware changes in the NIC could be used to entirely avoid the need for these synchronization packets by directly measuring the link up/down events.

### Summary

Despite the non-real-time behavior inherent in emulating ToRs with commodity PCs, we achieve 95.4% of the bandwidth of a comparable EPS with UDP traffic and 87.9% of an EPS, sending non-TSO TCP traffic. We are encouraged by these results, which we consider to be lower bounds of what would be possible with more precise control over the ToR.

### Scalability

Supporting large-scale data centers requires an OCS that can scale to many ports. We briefly consider these scalability implications.

### Related Work

#### Optical Switching Technologies

Realistic optical switches that can be used in practice require limited overall insertion loss and crosstalk and must be compatible with commercial fiber optic transceivers. Subject to these constraints, the performance of a switch is characterized by the switch speed and port count. Optical switches based on electro-optic modulation or semiconductor amplification can provide nanosecond switching speeds, but intrinsic crosstalk and insertion loss limit their port count. Analog (3D) MEMS beam steering switches can have high port counts (e.g., 1000 [4]), but are limited in switching speed on the order of milliseconds. Digital MEMS tilt mirror devices are a "middle-ground." They have a lower port count than analog MEMS switches but have a switching speed on the order of a microsecond [9] and sufficiently low insertion loss to permit constructing larger port-count OCSes by composition.

#### Hotspot Schedulers

Mordia complements work such as Helios [8], c-Through [25], Flyways [13], and OSA [6], which explored the potential of deploying optical circuit switch technology in a data center environment. Such systems to date have all been examples of hotspot schedulers. A hotspot scheduler observes network traffic over time, detects hotspots, and then changes the network topology (e.g., optically [6, 8, 25] or wirelessly [13]) to allocate more network capacity to traffic matrix hotspots and maximize overall throughput. Instead of pursuing such a reactive policy, Mordia adopts a proactive scheduling approach, where multiple scheduling decisions are amortized over a single pass through the control plane.

#### Optical Burst Switching

Optical Burst Switching (OBS) [17, 21] is a research area exploring alternate ways of scheduling optical links through the Internet. Previous and current techniques require optical circuits to be set up manually on human timescales, resulting in low link utilization. OBS introduces statistical multiplexing, where a queue of packets with the same source and destination are assembled into a burst (a much larger packet) and sent through the network together. Like OBS, the Mordia architecture has a separate control plane and data plane.

#### TDMA

Time Division Multiple Access (TDMA) is often used in wireless networks to share the channel capacity among multiple senders and receivers. It is also used by some wired networks such as SONET, ITU-T G.hn "HomeGrid" LANs, and "FlexRay" automotive networks. Its applicability to data center packet-switched Ethernet networks was studied in [22], which found that much of the OS-based synchronization jitter can be eliminated by relying on in-NIC functionality such as 802.1Qbb Priority-based pause frames.

### Conclusions

In this paper, we presented the design and implementation of the Mordia OCS architecture and evaluated it on a 24-port prototype. A key contribution of this work is a control plane that supports an end-to-end reconfiguration time 2–3 orders of magnitude smaller than previous approaches, based on a novel circuit scheduling approach called Traffic Matrix Scheduling. While Mordia is only one piece in a larger effort, we are encouraged by this initial experience building an operational hardware/software network that supports microsecond switching.

### Acknowledgments

This work is primarily supported by the National Science Foundation CIAN ERC (EEC-0812072) and a Google Focused Research Award. Additional funding was provided by Ericsson, Microsoft, and the Multiscale Systems Center, one of six research centers funded under the Focus Center Research Program (FCRP), a Semiconductor Research Corporation program. We would like to thank our shepherd Arvind Krishnamurthy, and Alex Snoeren and Geoff Voelker for providing invaluable feedback to this work.

### References

[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity, Data Center Network Architecture. In Proceedings of ACM SIGCOMM, Aug. 2008.
[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: Dynamic Flow Scheduling for Data Center Networks. In Proceedings of 7th USENIX NSDI, Apr. 2010.
[3] W. Anderson, J. Jackel, G.-K. Chang, H. Dai, W. Xin, M. Goodman, C. Allyn, M. Alvarez, O. Clarke, A. Gottlieb, F. Kleytman, J. Morreale, V. Nichols, A. Tzathas, R. Vora, L. Mercer, H. Dardy, E. Renaud, L. Williard, J. Perreault, R. McFarland, and T. Gibbons. The MONET Project—A Final Report. IEEE Journal of Lightwave Technology, 18(12):1988–2009, Dec. 2000.
[4] D. Beaver, S. Kumar, H. C. Li, J. Sobel, and P. Vajgel. Finding a needle in Haystack: Facebook’s photo storage. In Proceedings of 9th USENIX OSDI, Oct. 2010.
[5] G. Birkhoff. Tres Observaciones Sobre el Algebra Lineal. Univ. Nac. Tucumán Rev. Ser. A, 5:147–151, 1946.
[6] K. Chen, A. Singla, A. Singh, K. Ramachandran, L. Xu, Y. Zhang, and X. Wen. OSA: An Optical Switching Architecture for Data Center Networks and Unprecedented Flexibility. In Proceedings of 9th USENIX NSDI, Apr. 2012.
[7] N. Farrington, G. Porter, Y. Fainman, G. Papen, and A. Vahdat. Hunting Mice with Microsecond Circuit Switches. In Proceedings of 11th ACM HotNets, 2012.
[8] N. Farrington, G. Porter, S. Radhakrishnan, H. H. Bazzaz, V. Subramanya, Y. Fainman, G. Papen, and A. Vahdat. Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers. In Proceedings of ACM SIGCOMM, Aug. 2010.
[9] J. E. Ford, V. A. Aksyuk, D. J. Bishop, and J. A. Walker. Wavelength Add-Drop Switching Using Tilting Micromirrors. IEEE Journal of Lightwave Technology, 17:904–911, 1999.
[10] Glimmerglass 80x80 MEMS Switch. http://www.glimmerglass.com/products/technology/.
[11] A. Goel, M. Kapralov, and S. Khanna. Perfect Matchings in O(nlogn) Time in Regular Bipartite Graphs. In Proceedings of 42nd ACM STOC, June 2010.
[12] Hadoop: Open source implementation of Map Reduce. http://hadoop.apache.org/.
[13] D. Halperin, S. Kandula, J. Padhye, P. Bahl, and D. Wetherall. Augmenting Data Center Networks with Multi-Gigabit Wireless Links. In Proceedings of ACM SIGCOMM, Aug. 2011.
[14] U. Hoelzle and L. A. Barroso. The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines. Morgan and Claypool Publishers, 2009.
[15] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford, S. Shenker, and J. Turner. OpenFlow: Enabling Innovation in Campus Networks. ACM Computer Communication Review, 38(2), Apr. 2008.
[16] R. N. Mysore, A. Pamporis, N. Farrington, N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat. PortLand: A Scalable, Fault-Tolerant Layer 2 Data Center Network Fabric. In Proceedings of ACM SIGCOMM, Aug. 2009.
[17] C. Qiao and M. Yoo. Optical Burst Switching (OBS) – A New Paradigm for an Optical Internet. Journal of High Speed Networks, 8(1):69–84, 1999.
[18] R. Sinkhorn. A Relationship Between Arbitrary Positive Matrices and Doubly Stochastic Matrices. The Annals of Mathematical Statistics, 35(2):876–879, 1964.
[19] T. A. Strasser and J. L. Wagener. Wavelength-Selective Switches for ROADM Applications. IEEE Journal of Selected Topics in Quantum Electronics, 16:1150–1157, 2010.
[20] Y. Tamir and G. L. Frazier. High-Performance Multi-Queue Buffers for VLSI Communication Switches. In Proceedings of 15th ACM ISCA, May 1988.
[21] J. S. Turner. Terabit Burst Switching. Journal of High Speed Networks, 8(1):3–16, 1999.
[22] B. C. Vattikonda, G. Porter, A. Vahdat, and A. C. Snoeren. Practical TDMA for Datacenter Ethernet. In Proceedings of ACM EuroSys, Apr. 2012.
[23] J. von Neumann. A certain zero-sum two-person game equivalent to the optimal assignment problem. Contributions to the Theory of Games, 2:5–12, 1953.
[24] M. Walraed-Sullivan, K. Marzullo, and A. Vahdat. Scalability vs. Fault Tolerance in Aspen Trees. Technical Report MSR-TR-2013-21, Microsoft Research, Feb 2013.
[25] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. S. E. Ng, M. Kozuch, and M. Ryan. c-Through: Part-time Optics in Data Centers. In Proceedings of ACM SIGCOMM, Aug. 2010.