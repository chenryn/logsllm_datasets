### Optimized Text

In the context of federated learning (FL), where participants can be customers, hospital patients, or other entities, we collaboratively train the FL model. The total number of users is denoted by \( N' \).

Recall from Section III-C2 that a model poisoning adversary must fully compromise the devices of targeted clients and persist in their systems for an extended period. This is because model poisoning attacks are online attacks (Section III-C2). In cross-silo FL, this implies that the adversary must break into large, well-protected corporations, such as banks, which are bound by strict contracts and maintain professional software stacks. Plausible cross-silo poisoning scenarios involve strong incentives (e.g., financial) and require multiple parties to collude, risking breach of contract, or one party to hack, thereby facing criminal liability. Given these stringent conditions, it is highly unlikely that an adversary could successfully infiltrate these silos. Therefore, we argue that model poisoning threats in cross-silo FL are impractical.

This contrasts with large-scale data breaches [46]–[48], which are typically short-lived and only capable of stealing information, not altering the underlying infrastructure. Consequently, we focus on the data poisoning threat in cross-silo FL. For worst-case analyses, we assume that each silo trains its model using all the data contributed by its users. If silos inspect and remove mislabeled data, clean-label data poisoning attacks [27], [54] should be considered; however, this study is left for future work. Note that data inspection is not feasible in cross-device FL, as client data is entirely local, making clean-label poisoning irrelevant in this setting.

We assume that each silo collects data from an equal number of users (\( N'/N \)). For data poisoning attacks (DPAs), we assume that \( M\% \) of the \( N' \) users are compromised and share poisoned data \( D_p \) (computed as described in Section IV-B2) with their parent silo. We further assume that \( |D_p| = 100 \times |D|_{\text{avg}} \) for each user. The compromised users can be distributed either uniformly across the silos or concentrated in a few silos. For example, with 50 silos and 50 compromised users, and each silo having a maximum of 50 users, in the uniform case, each silo receives data from one compromised user, while in the concentrated case, all 50 compromised users share their data with a single silo.

Figure 8 (Appendix E) illustrates the impact of the best DPAs in the concentrated case. The results show that cross-silo FL is highly robust to state-of-the-art DPAs. In the concentrated case, the large number of benign silos mitigates the poisoning impact of the few compromised silos. Similar results are observed in the uniform distribution case, as the large number of benign users in each silo mitigates the impact of the few compromised users.

**Takeaway V-D:** In production cross-silo FL, model poisoning attacks are not practical, and state-of-the-art data poisoning attacks have no significant impact, even with the Average AGR.

### Conclusions

In this work, we systematized the threat models of poisoning attacks on federated learning (FL), provided practical ranges for various parameters relevant to FL robustness, and designed a suite of untargeted model and data poisoning attacks on FL, including both existing and improved attacks. Using these attacks, we thoroughly evaluated the state-of-the-art defenses under production FL settings. Our findings indicate that the conclusions of previous FL robustness literature cannot be directly extended to production FL. We presented concrete takeaways from our evaluations to correct some established beliefs and highlighted the need to consider production FL environments in research on FL robustness.

We hope that our systematization of practical poisoning threat models will guide the community towards addressing practically significant research problems in FL robustness. For instance, one open problem is to obtain concrete theoretical robustness guarantees of existing defenses in production FL settings, where only a very small fraction of all clients is randomly selected in each FL round.

### Acknowledgements

This work was supported by DARPA and NIWC under contract HR00112190125, and by the NSF grants 1953786, 1739462, and 1553301. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes, notwithstanding any copyright notation thereon. The views, opinions, and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.

Authorized licensed use limited to: Tsinghua University. Downloaded on August 07, 2022 at 13:01:32 UTC from IEEE Xplore. Restrictions apply. 

---

### References

[1] “Federated learning: Collaborative machine learning without centralized training data,” https://ai.googleblog.com/2017/04/federated-learning-collaborative.html, 2017.
[2] D. Alistarh, Z. Allen-Zhu, and J. Li, “Byzantine stochastic gradient descent,” in NeurIPS, 2018.
[3] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to backdoor federated learning,” in AISTATS, 2020.
[4] M. Barreno, B. Nelson, and A. D. Joseph, “The security of machine learning,” Machine Learning, 2010.
[5] M. Baruch, B. Gilad, and Y. Goldberg, “A Little Is Enough: Circumventing Defenses For Distributed Learning,” in NeurIPS, 2019.
[6] J. Bernstein, J. Zhao, K. Azizzadenesheli, and A. Anandkumar, “signSGD with Majority Vote is Communication Efficient and Fault Tolerant,” in ICLR, 2018.
[7] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing federated learning through an adversarial lens,” in ICML, 2019.
[8] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector machines,” in ICML, 2012.
[9] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of adversarial machine learning,” Pattern Recognition, 2018.
[10] P. Blanchard, R. Guerraoui, J. Stainer et al., “Machine learning with adversaries: Byzantine tolerant gradient descent,” in NeurIPS, 2017.
[11] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecný, S. Mazzocchi, B. McMahan, T. V. Overveldt, D. Petrou, D. Ramage, and J. Roselander, “Towards federated learning at scale: System design,” in MLSys, 2019.
[12] M. H. Brendan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially private recurrent language models,” in ICLR, 2018.
[13] S. Caldas, P. Wu, T. Li, J. Konečný, H. B. McMahan, V. Smith, and A. Talwalkar, “LEAF: A benchmark for federated settings,” arXiv:1812.01097, 2018.
[14] X. Cao, J. Jia, and N. Z. Gong, “Provably Secure Federated Learning against Malicious Clients,” in AAAI, 2021.
[15] H. Chang, V. Shejwalkar, R. Shokri, and A. Houmansadr, “Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer,” arXiv:1912.11279, 2019.
[16] L. Chen, H. Wang, Z. Charles, and D. Papailiopoulos, “Draco: Byzantine-resilient distributed training via redundant gradients,” in ICML, 2018.
[17] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks on deep learning systems using data poisoning,” arXiv:1712.05526, 2017.
[18] G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik, “EMNIST: Extending MNIST to handwritten letters,” in IJCNN, 2017.
[19] “CS231n: Convolutional Neural Networks for Visual Recognition,” https://cs231n.github.io/optimization-2/#grad, 2021.
[20] D. Data and S. Diggavi, “Byzantine-resilient SGD in high dimensions on heterogeneous data,” arXiv:2005.07866, 2020.
[21] E.-M. El-Mhamdi, R. Guerraoui, A. Guirguis, and S. Rouault, “SGD: Decentralized Byzantine resilience,” arXiv:1905.03853, 2019.
[22] “Facebook has shut down 5.4 billion fake accounts this year,” https://www.cnn.com/2019/11/13/tech/facebook-fake-accounts/index.html, 2019.
[23] M. Fang, X. Cao, J. Jia, and N. Z. Gong, “Local Model Poisoning Attacks to Byzantine-Robust Federated Learning,” in USENIX, 2020.
[24] “Google Workshop on Federated Learning and Analytics,” https://docs.google.com/document/d/1dWzVeFLrPinonQMauxIo0oI-Vbvqup5cZzgdPXvu97Y/edit#heading=h.7dsxad3c3nf7, 2020.
[25] S. Fu, C. Xie, B. Li, and Q. Chen, “Attack-resistant federated learning with residual-based reweighting,” arXiv:1912.11464, 2019.
[26] C. Fung, C. J. Yoon, and I. Beschastnikh, “The limitations of federated learning in Sybil settings,” in RAID, 2020.
[27] M. Goldblum, D. Tsipras, C. Xie et al., “Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses,” arXiv:2012.10544, 2020.
[28] “Google Play Protect,” https://developers.google.com/android/play-protect, 2021.
[31] M. S. Jere, T. Farnan, and F. Koushanfar, “A taxonomy of attacks on federated learning,” IEEE Security & Privacy, 2020.
[32] P. Kairouz, H. B. McMahan, B. Avent et al., “Advances and open problems in federated learning,” arXiv:1912.04977, 2019.
[33] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon, “Federated learning: Strategies for improving communication efficiency,” NIPS Workshop on Private Multi-Party ML, 2016.
[34] A. Krizhevsky, “Learning multiple layers of features from tiny images,” University of Toronto, Tech. Rep., 2009.
[35] Y. LeCun, L. Bottou, Y. Bengio et al., “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, 1998.
[36] L. Li, W. Xu, T. Chen, G. B. Giannakis, and Q. Ling, “RSA: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets,” in AAAI, 2019.
[37] T. Li, S. Hu, A. Beirami, and V. Smith, “Ditto: Fair and robust federated learning through personalization,” in ICML, 2021.
[38] T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation for robust model fusion in federated learning,” in NeurIPS, 2020.
[39] H. Ludwig, N. Baracaldo, G. Thomas et al., “IBM Federated Learning: An Enterprise Framework White Paper v0.1,” arXiv:2007.10987, 2020.
[40] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas, “Communication-efficient learning of deep networks from decentralized data,” in AISTATS, 2017.
[41] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault, “The Hidden Vulnerability of Distributed Learning in Byzantium,” in ICML, 2018.
[42] T. Minka, “Estimating a Dirichlet distribution,” 2000.
[43] L. Muñoz-González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee, E. C. Lupu, and F. Roli, “Towards poisoning of deep learning algorithms with back-gradient optimization,” in AISec, 2017.
[44] L. Muñoz-González, B. Pfitzner, M. Russo, J. Carnerero-Cano, and E. C. Lupu, “Poisoning attacks with generative adversarial nets,” arXiv:1906.07773, 2019.
[45] A. Newell, R. Potharaju, L. Xiang, and C. Nita-Rotaru, “On the practicality of integrity attacks on document-level sentiment analysis,” in AISec, 2014.
[46] “Billion Passwords Stolen: Change All of Yours, Now!” https://www.nbcnews.com/tech/security/billion-passwords-stolen-change-all-yours-now-n174321, 2014.
[47] “Hackers Expose 8.4 Billion Passwords Post them Online in Possibly Largest Dump of Passwords Ever,” https://www.thegatewaypundit.com/2021/06/hackers-expose-8-4-billion-passwords-post-online-possibly-largest-dump-passwords-ever/, 2014.
[48] “26 million stolen passwords found online — see if you’re affected,” https://www.tomsguide.com/news/mystery-malware-info-stealer, 2021.
[49] M. Paulik, M. Seigel, H. Mason et al., “Federated Evaluation and Tuning for On-Device Personalization: System Design & Applications,” arXiv:2102.08503, 2021.
[50] K. Pillutla, S. M. Kakade, and Z. Harchaoui, “Robust aggregation for federated learning,” arXiv:1912.13445, 2019.
[51] “Acquire Valued Shoppers Challenge,” https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data, 2019.
[52] S. J. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Konečný, S. Kumar, and H. B. McMahan, “Adaptive Federated Optimization,” in ICLR, 2020.
[53] “SafetyNet Attestation API,” https://developer.android.com/training/safetynet/attestation, 2021.
[54] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, “Poison frogs! Targeted clean-label poisoning attacks on neural networks,” in NeurIPS, 2018.
[55] V. Shejwalkar and A. Houmansadr, “Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning,” in NDSS, 2021.
[56] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in ICLR, 2015.
[57] G. Sun, Y. Cong, J. Dong, Q. Wang, L. Lyu, and J. Liu, “Data poisoning attacks on federated machine learning,” IEEE IoT Journal, 2021.
[58] Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan, “Can you really backdoor federated learning?” NeurIPS FL Workshop, 2019.
[29] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D. Tygar, “Adversarial machine learning,” in AISec, 2011.
[59] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks against federated learning systems,” in ESORICS, 2020.
[30] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li, “Manipulating machine learning: Poisoning attacks and countermeasures against regression learning,” 39th IEEE Symposium on S&P, 2018.
[60] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,” in 40th IEEE Symposium on S&P, 2019.
[61] H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J.-Y. Sohn, K. Lee, and D. Papailiopoulos, “Attack of the tails: Yes, you really can backdoor federated learning,” in NeurIPS, 2020.
[62] “Utilization of FATE in Risk Management of Credit in Small and Micro Enterprises,” https://www.fedai.org/cases/utilization-of-fate-in-risk-management-of-credit-in-small-and-micro-enterprises/, 2019.
[63] C. Wu, X. Yang, S. Zhu, and P. Mitra, “Mitigating backdoor attacks in federated learning,” arXiv:2011.01767, 2020.
[64] H. Xiao, H. Xiao, and C. Eckert, “Adversarial label flips attack on support vector machines,” in ECAI, 2012.
[65] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli, “Support vector machines under adversarial label contamination,” Neurocomputing, 2015.
[66] C. Xie, M. Chen, P.-Y. Chen, and B. Li, “CRFL: Certifiably Robust Federated Learning against Backdoor Attacks,” in ICML, 2021.
[67] C. Xie, K. Huang, P.-Y. Chen, and B. Li, “DBA: Distributed backdoor attacks against federated learning,” in ICLR, 2019.
[68] C. Xie, O. Koyejo, and I. Gupta, “Generalized Byzantine-tolerant SGD,” arXiv:1802.10116, 2018.
[69] C. Yang, Q. Wu, H. Li, and Y. Chen, “Generative poisoning attack method against neural networks,” arXiv:1703.01340, 2017.
[70] D. Yin, Y. Chen, K. Ramchandran, and P. Bartlett, “Byzantine-robust distributed learning: Towards optimal statistical rates,” in ICML, 2018.
[71] T. Yu, E. Bagdasaryan, and V. Shmatikov, “Salvaging federated learning by local adaptation,” arXiv:2002.04758, 2020.

### Appendix

#### A. Related Work

1. **Targeted and Backdoor Attacks:**
   - **Targeted Attacks:** Section IV-A discusses all state-of-the-art untargeted attacks in detail. Below, we discuss existing works on targeted and backdoor attacks.
     - Targeted attacks [7], [58], [59] aim to make the global model misclassify a specific set of samples at test time. Bhagoji et al. [7] aimed to misclassify a single sample and proposed a model poisoning attack based on alternate minimization to make poisoned updates look similar to benign updates. Their attack, with a single attacker, can misclassify a single sample with 100% success against the non-robust Average AGR. Sun et al. [58] investigated the constrain-and-scale attack [3] with the aim to misclassify all samples of a few victim FL clients. Tolpegin et al. [23], [59] investigated targeted data poisoning attacks when compromised clients compute their updates by mislabeling the target samples.
   - **Backdoor Attacks:** Backdoor attacks [3], [61], [67] aim to make the global model produce incorrect outputs for inputs containing a specific trigger. These attacks are more subtle and can be harder to detect than targeted attacks.