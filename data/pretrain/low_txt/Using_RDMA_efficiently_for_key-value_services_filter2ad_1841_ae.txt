### Performance Comparison for Large Values
For large values, the performance of HERD, FaRM-em, and Pilaf-em-OPT is within 10% of each other.

### 5.4 Latency Comparison
Unlike FaRM-KV and Pilaf, HERD uses only one network round trip for any request. FaRM-KV and Pilaf use one round trip for PUT requests but require multiple round trips for GETs (except when FaRM-KV inlines values in the hash table). This results in higher GET latency for FaRM-KV and Pilaf compared to a single RDMA READ.

**Figure 11** compares the average latencies of the three systems under a read-intensive workload. The error bars indicate the 5th and 95th percentile latency. To understand how latency depends on throughput, we increase the server load by adding more clients.

### Throughput and Latency Analysis
- **Throughput (Mops) vs. PUT Percentage:**
  - **HERD, Pilaf-em-OPT, FaRM-em, FaRM-em-VAR, Apt-IB, Susitna-RoCE**
  - **Value Size: 0, 5, 10, 15, 20, 25, 30, 5%, 50%, 100%**

- **Throughput (Mops) vs. Value Size:**
  - **HERD, Pilaf-em-OPT, FaRM-em-VAR, FaRM-em, Apt-IB, Susitna-RoCE**
  - **Value Size: 0, 5, 10, 15, 20, 4, 8, 16, 32, 64, 128, 256, 512, 1024**

- **Latency (us) vs. Throughput (Mops):**
  - **HERD, Pilaf-em-OPT, FaRM-em, FaRM-em-VAR, Apt-IB**
  - **Throughput: 0, 2, 4, 6, 8, 10, 12, 0, 5, 10, 15, 20, 25**

- **Throughput (Mops) vs. Number of Client Processes:**
  - **HERD, WS = 4, HERD, WS = 16**
  - **Number of Client Processes: 0, 5, 10, 15, 20, 25, 0, 100, 200, 300, 400, 500**

### 5.5 Throughput as a Function of Server CPU Cores
**Figure 13** shows that there is a 4-5 Mops decrease with this change, but once made, the system can scale up to many thousands of clients while still outperforming an RDMA READ-based architecture. We expect the performance of the SEND/SEND architecture relative to WRITE-SEND to improve with the introduction of inlined RECVs in Connect-IB cards. This will reduce the load on RNICs by encapsulating the RECV payload in the RECV completion.

### 5.6 HERD CPU Usage
The primary drawback of not using READs in HERD is that GET operations require the server CPU to execute requests, in exchange for saving one cross-datacenter RTT. While it might seem that HERD’s CPU usage should be higher than Pilaf and FaRM-KV, in practice, these two systems also have significant sources of CPU usage that reduce the difference.

1. **Issuing Extra READs:** Issuing extra READs adds CPU overhead at the Pilaf and FaRM-KV clients. To issue the second READ, the clients must poll for the first READ to complete. HERD shifts this overhead to the server’s CPU, making more room for application processing at the clients.
2. **Handling PUT Requests:** Handling PUT requests requires CPU involvement at the server. Achieving low-latency PUTs requires dedicating server CPU cores that poll for incoming requests. The exact CPU use depends on the fraction of PUT throughput that the server is provisioned for, as this determines the CPU resources that must be allocated. For example, our experiments show that provisioning for 100% PUT throughput in Pilaf and FaRM-KV requires over 5 CPU cores.

**Figure 13** shows FaRM-em and Pilaf-em-OPT’s PUT throughput for 48-byte key-value items and different numbers of CPU cores at the server. Pilaf-em-OPT’s CPU usage is higher because it must post RECVs for new PUT requests, which is more expensive than FaRM-em’s request-region polling.

In **Figure 13**, we also plot HERD’s throughput for the same workload by varying the number of server CPU cores. HERD can deliver over 95% of its maximum throughput with 5 CPU cores. The modest gap to FaRM-em arises because the HERD server in this experiment handles hash table lookups and updates, whereas the emulated FaRM-KV handles only the network traffic.

We believe that HERD’s higher throughput and lower latency, along with the significant CPU utilization in Pilaf and FaRM-KV, justifies the architectural decision to involve the CPU on the GET path for small key-value items. For a 50% PUT workload, the moderate extra cost of adding a few more cores—or using the already-idle cycles on the cores—is likely worthwhile for many applications.

### 5.7 Resistance to Skew
To understand how HERD’s behavior is impacted by skew, we tested it with a workload where the keys are drawn from a Zipf distribution. HERD adapts well to skew, delivering its maximum performance even when the Zipf parameter is .99. HERD’s resistance to skew comes from two factors:

1. **Back-end MICA Architecture:** The back-end MICA architecture used in HERD performs well under skew. A skewed workload spread across several partitions produces little variation in the partitions’ load compared to the skew in the workload’s distribution. Under our Zipf-distributed workload with 6 partitions, the most loaded CPU core is only 50% more so than the least loaded core, even though the most popular key is over 105 times more popular than the average.
2. **Shared RNIC:** Because the CPU cores share the RNIC, the highly loaded cores benefit from the idle time provided by the less-used cores. **Figure 13** demonstrates this effect: with a uniform workload and using only a single core, HERD can deliver 6.3 Mops. When the system is configured to use 6 cores—the minimum required by HERD to deliver its peak throughput—the system delivers 4.32 Mops per core. The per-core performance reduction is not due to a CPU bottleneck but because the server processes saturate the PCIe PIO throughput. Therefore, even if the workload is skewed, there is ample CPU headroom on a given core to handle the extra requests.

**Figure 14** shows the per-core throughput of HERD for a skewed workload. The experimental configuration is: 48-byte items, read-intensive, skewed workload, 6 total CPU cores. The per-core throughput for a uniform workload is included for comparison.

### 6. Related Work
- **RDMA-based Key-Value Stores:** Other than Pilaf and FaRM, several projects have designed memcached-like systems over RDMA. Panda et al. [14] describe a memcached implementation using a hybrid of UD and RC transports. It uses SEND/RECV messages for all requests and avoids the overhead of UD transport (caused by a larger header size than RC) by actively switching connections between RC and UD. Although their cluster (ConnectX, 32 Gbps) is comparable to Susitna (ConnectX-3, 40 Gbps), their request rate is less than 1.5 Mops. Stuedi et al. [25] describe a SoftiWARP [28] based version of memcached targeting CPU savings in wimpy nodes with 10GbE.
- **Accelerating Systems with RDMA:** Several projects have used verbs to improve the performance of systems such as HBase, Hadoop RPC, PVFS [30, 13, 20]. Most of these use only SEND/RECV verbs as a fast alternative to socket-based communication. In a PVFS implementation over InfiniBand [30], read() and write() operations in the filesystem use both RDMA and SEND/RECV. They favor WRITEs over READs for the same reasons as in our work, suggesting that the performance gap has existed over several generations of InfiniBand hardware. There have been several versions of MPI over InfiniBand [16, 19]. MPICH2 uses RDMA writes for one-sided messaging: the server polls the head of a circular buffer that is written to by a client. HERD extends this messaging in a scalable fashion for all-to-all request-reply communication.
- **User-Level Networking:** Taken together, we believe that one conclusion to draw from the union of HERD, Pilaf, FaRM, and MICA [18] is that the biggest boost to throughput comes from bypassing the network stack and avoiding CPU interrupts, not necessarily from bypassing the CPU entirely. All four of these systems use mechanisms to allow user-level programs to directly receive requests or packets from the NIC: the user-level RDMA drivers for HERD, Pilaf, and FaRM, and the Intel DPDK library for MICA. As we discuss below, the throughput of these systems is similar, but the batching required by the DPDK-based systems confers a latency advantage to the hardware-supported InfiniBand systems. These lessons suggest profitable future work in making user-level classical Ethernet systems more portable, easier to use, and lower-latency. One ongoing effort is NIQ [10], an FPGA-based low-latency NIC which uses cacheline-sized PIOs (without any DMA) to transmit and receive small packets. Inlined WRITEs in RDMA use the same mechanism at the requester’s side.
- **General Key-Value Stores:** MICA [18] is a recent key-value system for classical Ethernet. It assigns exclusive partitions to server cores to minimize memory contention and exploits the NIC’s capability to steer requests to the responsible core [3]. A MICA server delivers 77 Mops with 4 dual-port, 10 Gbps PCIe 2.0 NICs, with 50 µs average latency (19.25 Gbps with one PCIe 2.0 card). This suggests that, comparing the state-of-the-art, classical Ethernet-based solutions can provide comparable throughput to RDMA-based solutions, although with much higher latency. RAMCloud [23] is a RAM-based, persistent key-value store that uses messaging verbs for low-latency communication.

### 7. Conclusion
This paper explored the options for implementing fast, low-latency key-value systems atop RDMA, arriving at an unexpected and novel combination that outperforms prior designs and uses fewer network round-trips. Our work shows that, contrary to widely held beliefs about engineering for RDMA, single-RTT designs with server CPU involvement can outperform the “optimization” of CPU-bypassing remote memory access when the RDMA approaches require multiple RTTs. These results contribute not just a practical artifact—the HERD low-latency, high-performance key-value cache—but an improved understanding of how to use RDMA to construct future DRAM-based storage services.

### Acknowledgements
We thank our shepherd Chuanxiong Guo and the anonymous reviewers for their feedback that helped improve the paper. We also thank Miguel Castro and Dushyanth Narayanan for discussing FaRM with us, Kirk Webb and Robert Ricci for getting us early access to the Apt cluster, and Hyeontaek Lim for his valuable insights. This work was supported by funding from the National Science Foundation under awards CNS-1314721 and CCF-0964474, and Intel via the Intel Science and Technology Center for Cloud Computing (ISTC-CC). The PRObE cluster [11] used for many experiments is supported in part by NSF awards CNS-1042537 and CNS-1042543 (PRObE).

### References
[1] Connect-IB: Architecture for Scalable High Performance Computing. URL http://www.mellanox.com/related-docs/applications/SB_Connect-IB.pdf.
[2] Intel DPDK: Data Plane Development Kit. URL http://dpdk.org.
[3] Intel 82599 10 Gigabit Ethernet Controller: Datasheet. URL http://www.intel.com/content/www/us/en/ethernet-controllers/82599-10-gbe-controller-datasheet.html.
[4] Redis: An Advanced Key-Value Store. URL http://redis.io.
[5] memcached: A Distributed Memory Object Caching System, 2011. URL http://memcached.org.
[6] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny. Workload Analysis of a Large-Scale Key-Value Store. In SIGMETRICS, 2012.
[7] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking Cloud Serving Systems with YCSB. In SoCC, 2010.
[8] A. Dragojevic, D. Narayanan, O. Hodson, and M. Castro. FaRM: Fast Remote Memory. In USENIX NSDI, 2014.
[9] B. Fan, D. G. Andersen, and M. Kaminsky. MemC3: Compact and Concurrent MemCache with Dumber Caching and Smarter Hashing. In USENIX NSDI, 2013.
[10] M. Flajslik and M. Rosenblum. Network Interface Design for Low Latency Request-Response Protocols. In USENIX ATC, 2013.
[11] G. Gibson, G. Grider, A. Jacobson, and W. Lloyd. PRObE: A Thousand-Node Experimental Cluster for Computer Systems Research.
[12] M. Herlihy, N. Shavit, and M. Tzafrir. Hopscotch Hashing. In DISC, 2008.
[13] J. Huang, X. Ouyang, J. Jose, M. W. ur Rahman, H. Wang, M. Luo, H. Subramoni, C. Murthy, and D. K. Panda. High-Performance Design of HBase with RDMA over InfiniBand. In IPDPS, 2012.
[14] J. Jose, H. Subramoni, K. C. Kandalla, M. W. ur Rahman, H. Wang, S. Narravula, and D. K. Panda. Scalable Memcached Design for InfiniBand Clusters Using Hybrid Transports. In CCGRID. IEEE, 2012.
[15] A. Kalia, D. G. Andersen, and M. Kaminsky. Using RDMA Efficiently for Key-Value Services. In Technical Report CMU-PDL-14-106, 2014.
[16] J. Li, J. Wu, and D. K. Panda. High Performance RDMA-Based MPI Implementation over InfiniBand. International Journal of Parallel Programming, 2004.
[17] H. Lim, B. Fan, D. G. Andersen, and M. Kaminsky. SILT: A Memory-efficient, High-performance Key-value Store. In SOSP, 2011.
[18] H. Lim, D. Han, D. G. Andersen, and M. Kaminsky. MICA: A Holistic Approach to Fast In-Memory Key-Value Storage. In USENIX NSDI, 2014.
[19] J. Liu, W. Jiang, P. Wyckoff, D. K. Panda, D. Ashton, D. Buntinas, W. Gropp, and B. Toonen. Design and Implementation of MPICH2 over InfiniBand with RDMA Support. In IPDPD, 2004.
[20] X. Lu, N. S. Islam, M. W. ur Rahman, J. Jose, H. Subramoni, H. Wang, and D. K. Panda. High-Performance Design of Hadoop RPC with RDMA over InfiniBand. In ICPP, 2013.
[21] C. Mitchell, Y. Geng, and J. Li. Using One-Sided RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store. In USENIX ATC, 2013.
[22] R. Nishtala, H. Fugal, S. Grimm, M. Kwiatkowski, H. Lee, H. C. Li, R. McElroy, M. Paleczny, D. Peek, P. Saab, D. Stafford, T. Tung, and V. Venkataramani. Scaling Memcache at Facebook. In USENIX NSDI, 2013.
[23] D. Ongaro, S. M. Rumble, R. Stutsman, J. Ousterhout, and M. Rosenblum. Fast Crash Recovery in RAMCloud. In SOSP, 2011.
[24] R. Pagh and F. F. Rodler. Cuckoo Hashing. J. Algorithms, 2004.
[25] P. Stuedi, A. Trivedi, and B. Metzler. Wimpy Nodes with 10GbE: Leveraging One-Sided Operations in Soft-RDMA to Boost Memcached. In USENIX ATC, 2012.
[26] S. Sur, A. Vishnu, H.-W. Jin, W. Huang, and D. K. Panda. Can Memory-Less Network Adapters Benefit Next-Generation InfiniBand Systems? In HOTI, 2005.
[27] S. Sur, M. J. Koop, L. Chai, and D. K. Panda. Performance Analysis and Evaluation of Mellanox ConnectX Infiniband Architecture with Multi-Core Platforms. In HOTI, 2007.
[28] A. Trivedi, B. Metzler, and P. Stuedi. A Case for RDMA in Clouds: Turning Supercomputer Networking into Commodity. In APSys, 2011.
[29] B. White, J. Lepreau, L. Stoller, R. Ricci, S. Guruprasad, M. Newbold, M. Hibler, C. Barb, and A. Joglekar. An Integrated Experimental Environment for Distributed Systems and Networks. In OSDI, 2002.
[30] J. Wu, P. Wyckoff, and D. K. Panda. PVFS over InfiniBand: Design and Performance Evaluation. In Ohio State University Tech Report, 2003.
[31] D. Zhou, B. Fan, H. Lim, M. Kaminsky, and D. G. Andersen. Scalable, High Performance Ethernet Forwarding with CuckooSwitch. In CoNEXT, 2013.