# 1st Workshop on Fault-Tolerance for HPC at Extreme Scale (FTXS 2010)

**Authors:**
John T. Daly and Nathan DeBardeleben  
Center for Exceptional Computing / Department of Defense, USA  
{john.daly@usdod.gov, nathan.debardeleben@usdod.gov}

**Conference:**
2010 IEEE/IFIP International Conference on Dependable Systems & Networks (DSN)

## 1. Introduction

The advent of many-core processors, alternative/heterogeneous accelerators, and new architectures presents the High-Performance Computing (HPC) community with a novel challenge: scaling in the number of processing elements, which surpasses the historical trend of scaling in processor frequencies. This increase in system complexity has significant implications for fault tolerance. 

Recent evidence suggests that traditional assumptions about HPC fault tolerance are no longer valid:
- Faults are increasingly multiple-point and interdependent rather than single-point and independent.
- Silent failures and silent data corruption are becoming more frequent and cannot be ignored.
- Stabilization time consumes a larger fraction of the useful system lifetime, with failure rates projected to exceed one per hour on the largest systems.
- Application interrupt rates are diverging from system failure rates.

This workshop aims to convene a diverse group of experts in HPC and fault tolerance to establish a research agenda for addressing the unique challenges posed by extreme scale and complexity. Innovation and discussion of non-traditional approaches are encouraged.

## 2. Inaugural Workshop

Assuming that hardware and software errors will be inevitable at extreme scale, this workshop will explore various aspects of fault tolerance, including but not limited to:

- **Quantitative Assessments:** Evaluating the cost in terms of power, performance, and resource impacts of fault-tolerant techniques such as checkpoint restart, which are redundant in space, time, or information.
- **Novel Techniques:** Developing and implementing new fault-tolerance methods and emerging hardware and software technologies that guard against silent data corruption (SDC) in memory, logic, and storage, and provide end-to-end data integrity for running applications.
- **Tradeoffs Analysis:** Studying the tradeoffs between hardware and software in error detection, failure prediction, error preemption, and recovery.
- **System Management:** Advancing monitoring, analysis, and control of highly complex systems.
- **Programming Models:** Creating highly scalable fault-tolerant programming models.
- **Metrics and Standards:** Establishing metrics and standards for measuring, improving, and enforcing the need for and effectiveness of fault tolerance.
- **Failure Modeling:** Developing scalable methods for reliability, availability, performability, and failure prediction in fault-tolerant HPC systems.
- **Byzantine Fault Tolerance:** Implementing scalable Byzantine fault tolerance and security measures to protect against single-fault and fail-silent violations.

## 3. Program Committee

The program committee for FTXS 2010 includes subject matter experts from industry, academia, and government. The members are:

- Greg Bronevetsky (Lawrence Livermore National Laboratory, USA)
- Franck Cappello (INRIA, France)
- Daniel Katz (University of Chicago, USA)
- Armando Fox (University of California, USA)
- Zbigniew Kalbarczyk (University of Illinois, USA)
- Yasunori Kimura (Fujitsu Laboratories, Japan)
- SÃ©bastien Monnet (University of Pierre and Marie Curie, France)
- Takashi Nanya (University of Tokyo, Japan)
- Nuno Neves (University of Lisbon, Portugal)
- Stephen Scott (Oak Ridge National Laboratory, USA)
- Marc Snir (University of Illinois, USA)
- Jon Stearley (Sandia National Laboratory, USA)
- Kishor Trivedi (Duke University, USA)

The program chairs for FTXS 2010 are John Daly and Nathan DeBardeleben from the Center for Exceptional Computing / Department of Defense, USA.

**Note:** U.S. Government work not protected by U.S. copyright.