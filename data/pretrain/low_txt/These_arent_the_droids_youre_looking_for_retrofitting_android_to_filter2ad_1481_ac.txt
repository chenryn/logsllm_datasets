### Consequences for Software Developers

Static analysis can be employed to identify code segments that are designed to transfer data using control flow, thereby exposing applications that attempt to subvert users' privacy policies. If developers are found to be actively circumventing AppFence’s exfiltration blocking controls, they may undermine their ability to use the traditional defense that they operate openly with the implicit consent of a user base that is willing to share information.

An application that is aware of AppFence can detect the presence of exfiltration blocking. For instance, an application could open two independent sockets, transmit tainted data over one and untainted data over the other, and have the server report back what it received. Similarly, shadow data might not be convincing enough to fool an application. Applications that detect the presence of privacy controls could refuse to provide desired functionality until the controls are deactivated.

### Test Methodology

The primary cost of imposing privacy controls on applications is the introduction of side effects that negatively impact the user experience. To evaluate our AppFence system, we developed a novel test methodology that automates the execution of applications and measures and characterizes the side effects introduced by the privacy controls. Our methodology addresses two main obstacles: reproducibility and detection of side effects. We use automated GUI testing and screenshot comparisons to tackle these issues.

We focus on user-visible side effects as the metric for evaluating AppFence because both shadowing and exfiltration blocking offer equivalent benefits when applied to the applications in our test bed. Since AppFence-unaware applications do not (to our knowledge) deliberately circumvent the information flow tracking used to block exfiltration, both privacy controls are equally effective on today's applications. We do not measure the performance impact of our privacy controls; the underlying information flow tracking provided by TaintDroid is fast enough to run applications in real-time with a modest slowdown (worst-case increase in CPU utilization of 14%).

### Implementation

To monitor and block network traffic, we modify both the Java and native code in the Android networking stack. When an application writes to a socket’s output stream, the buffer is sent to the `sendStream()` method within the `OSNetworkSystem` core library. We modified `sendStream` so that if the buffer is tainted by data that should not be sent to its intended destination, we drop the buffer. For SSL sockets, we capture write calls to the `SSLOutputStream` class.

To emulate airplane mode, we first return the error code `SOCKERR_TIMEOUT`, then block the next send with `SOCKERR_EPIPE`. If the application tries to open a new socket via `socket.connect()`, we return a `SocketException` with the error code `SOCKERR_ENETUNREACH`. Subsequent attempts to open sockets or send data will be allowed until we encounter tainted data bound for a forbidden destination.

To facilitate user configuration and testing, we separate the policy specification mechanism into a service (daemon) that can be configured automatically or by users. The privacy controls obtain their policies from this daemon and can be enabled globally or on a per-application basis.

AppFence relies on the open-source TaintDroid platform, which, at the time of our testing, did not fully support just-in-time (JIT) compilation. We initially implemented AppFence for Android version 2.1, which does not use JIT compilation. As of August 2011, Android 2.1 represented 15% of the Android installations accessing the Android Market. We did not encounter any compatibility issues running applications on Android 2.1.

### Automated Application Runs

Reproducibility is challenging because different runs of the same application may exercise different code paths. Variations in user inputs, timing, system state, and other factors can cause results to change. To minimize these variations, we built a test infrastructure that automates human usage patterns using the Android GUI testing system provided by the TEMA project. The test system supports a scripting language where user actions are expressed via high-level commands such as `TapObject`, `PressKey`, and `SelectFromMenu`.

We selected 50 applications to be scripted for our experiments (listed in Appendix B). Each application was scripted to perform its main tasks as we expected users to perform them. The average time to execute each test script, excluding installation, uninstallation, and cleanup, was 3.5 minutes, with an average of 24 script commands. We created a master test script that configures an Android device, enables the AppFence privacy controls for experimental configurations, or disables them for the baseline configuration, and then tests all applications.

### Detecting Changes in Behavior

Detecting whether side effects impact user-desired functionality requires consultation with a user. However, human evaluation can introduce bias and slow down the process. To reduce these constraints, we leverage the insight that side effects are likely easy to detect if the visual outputs of the baseline and experimental executions can be compared side by side. We captured a screenshot after every command in the test script and generated a web page with side-by-side screenshots from the baseline and experimental executions, along with a visual diff of the two images.

We also monitored the tainted data exposure across test runs and found that it is not deterministic. To mitigate the underlying sources of variation, we modified the Android framework to always return a fixed default location, rather than null, when no last known location is available. We examined the results of at least two test executions for every experimental configuration and used additional executions and manual log inspection to resolve inconsistent application behavior.

### Experiments

This section presents the experimental results of testing AppFence’s privacy controls on the 50 applications for which we generated test scripts (see Appendix B). We discuss the side effects resulting from the privacy controls and evaluate their impact on the user experience.

#### Experimental Configurations

We executed applications over eight different experimental configurations. The control configuration, without any privacy controls activated, represents how users run applications on Android today. In the shadowing configuration, sensitive data was replaced by shadow data. The remaining six configurations implemented some form of message blocking, three of which used overt blocking (simulating airplane mode) and three of which used covert blocking (pretending that blocked messages were actually sent).

One pair of exfiltration blocking configurations (one covert, one overt) blocked messages tainted by sensitive data regardless of the server to which they were destined. A pair of destination-specific exfiltration blocking configurations only blocked tainted messages if they were destined to known advertising & analytics (A&A) servers. Finally, to examine the benefits of exfiltration blocking over more naive approaches, a destination blacklisting pair blocked all traffic to known A&A servers, regardless of whether it was tainted by sensitive data or not.

We divided the possible side effects impacting the user experience into four categories based on severity: no side effect (none), advertisements no longer appeared (ads absent), the application still performed its primary purpose but failed to perform a less-important secondary function (less functional), or the application no longer fulfilled its primary purpose or crashed (broken).

#### Coarse-Grained Controls

Our first experiment examines the side effects of imposing privacy controls on all 12 data types simultaneously. This coarse-grained analysis allows us to identify the best applications for further examination. Our results are summarized in Table 5. Blocking tainted messages sent to known A&A servers has fewer side effects than approaches that block messages to other servers. However, even blocking just tainted messages to known A&A servers can cause disruption if applications fail to handle blocking gracefully. For example, after a connection to an A&A server failed, one application assumed the network was unavailable and abandoned all network access. Blocking all messages sent to A&A servers caused slightly more applications to break.

Closer inspection revealed that these applications send untainted communications to A&A servers upon launch, which may cause them to wait indefinitely for a response (covert mode) or receive a socket exception that is interpreted as network unavailability (overt mode). For all exfiltration blocking configurations, we found negligible differences in the occurrence of side effects caused by overt blocking versus covert blocking.

Blocking only A&A servers defends against behavioral advertising, which, despite its popularity, is likely the least pernicious threat to sensitive data. More nefarious applications can circumvent such blacklist approaches by proxying communications through their own servers. Preventing exfiltration of data through non-A&A servers requires one of our destination-agnostic approaches, i.e., using shadowing or exfiltration blocking of tainted messages to all destinations. Table 5 shows that overall, shadowing causes fewer and less severe side effects than exfiltration blocking.