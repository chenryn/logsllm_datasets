### References

1. **Systems Design and Implementation (OSDI 16).** USENIX Association, Savannah, GA, 249–264. [Link](https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gao)

2. **Peter X. Gao, Akshay Narayan, Gautam Kumar, Rachit Agarwal, Sylvia Ratnasamy, and Scott Shenker. 2015. pHost: Distributed Near-optimal Datacenter Transport over Commodity Network Fabric.** In Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies (CoNEXT '15). ACM, New York, NY, USA, Article 1, 12 pages. [DOI: 10.1145/2716281.2836086](https://doi.org/10.1145/2716281.2836086)

3. **Matthew P. Grosvenor, Malte Schwarzkopf, Ionel Gog, Robert N. M. Watson, Andrew W. Moore, Steven Hand, and Jon Crowcroft. 2015. Queues Don’t Matter When You Can JUMP Them!** In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 15). USENIX Association, Oakland, CA, 1–14. [Link](https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/grosvenor)

4. **Mark Handley, Costin Raiciu, Alexandru Agache, Andrei Voinescu, Andrew W. Moore, Gianni Antichik, and Marcin Mojcik. 2017. Re-architecting Datacenter Networks and Stacks for Low Latency and High Performance.** In Proceedings of the ACM SIGCOMM 2017 Conference (SIGCOMM '17). ACM, New York, NY, USA, 29–42.

5. **Chi-Yao Hong, Matthew Caesar, and P. Brighten Godfrey. 2012. Finishing Flows Quickly with Preemptive Scheduling.** In Proceedings of the ACM SIGCOMM 2012 Conference (SIGCOMM '12). ACM, New York, NY, USA, 127–138. [DOI: 10.1145/2342356.2342389](https://doi.org/10.1145/2342356.2342389)

6. **Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R. Dulloor, Jishen Zhao, and Steven Swanson. 2019. Basic Performance Measurements of the Intel Optane DC Persistent Memory Module.** CoRR abs/1903.05714 (2019), 1–61. [arXiv:1903.05714](http://arxiv.org/abs/1903.05714)

7. **Raj Jain, Dah Ming Chiu, and Hawe WR. 1984. A Quantitative Measure Of Fairness And Discrimination For Resource Allocation In Shared Computer Systems.** (September 1984), 37 pages.

8. **Dina Katabi, Mark Handley, and Charlie Rohrs. 2002. Congestion Control for High Bandwidth-Delay Product Networks.** In Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM '02). Association for Computing Machinery, New York, NY, USA, 89–102. [DOI: 10.1145/633025.633035](https://doi.org/10.1145/633025.633035)

9. **K. Katrinis, D. Syrivelis, D. Pnevmatikatos, G. Zervas, D. Theodoropoulos, I. Koutsopoulos, K. Hasharoni, D. Raho, C. Pinto, F. Espina, S. Lopez-Buedo, Q. Chen, M. Nemirovsky, D. Roca, H. Klos, and T. Berends. 2016. Rack-scale Disaggregated Cloud Data Centers: The dReDBox Project Vision.** In 2016 Design, Automation Test in Europe Conference Exhibition (DATE). IEEE, Dresden, Germany, 690–695.

10. **Changhoon Kim, Parag Bhide, Ed Doe, Hugh Holbrook, Anoop Ghanwani, Dan Daly, Mukesh Hira, and Bruce Davie. 2016. In-band Network Telemetry (INT).** [Link](https://p4.org/assets/INT-current-spec.pdf) (Accessed: 2020-01-13).

11. **Ana Klimovic, Christos Kozyrakis, Eno Thereska, Binu John, and Sanjeev Kumar. 2016. Flash Storage Disaggregation.** In Proceedings of the Eleventh European Conference on Computer Systems (EuroSys '16). Association for Computing Machinery, New York, NY, USA, Article 29, 15 pages. [DOI: 10.1145/2901318.2901337](https://doi.org/10.1145/2901318.2901337)

12. **Ana Klimovic, Heiner Litz, and Christos Kozyrakis. 2017. ReFlex: Remote Flash Local Flash.** In Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '17). Association for Computing Machinery, New York, NY, USA, 345–359. [DOI: 10.1145/3037697.3037732](https://doi.org/10.1145/3037697.3037732)

13. **Gautam Kumar, Srikanth Kandula, Peter Bodik, and Ishai Menache. 2013. Virtualizing Traffic Shapers for Practical Resource Allocation.** Presented as part of the 5th USENIX Workshop on Hot Topics in Cloud Computing. USENIX, San Jose, CA, 1–6. [Link](https://www.usenix.org/conference/hotcloud13/workshop-program/presentations/Kumar)

14. **C. Lee, C. Park, K. Jang, S. Moon, and D. Han. 2017. DX: Latency-Based Congestion Control for Datacenters.** IEEE/ACM Transactions on Networking 25, 1 (February 2017), 335–348. [DOI: 10.1109/TNET.2016.2587286](https://doi.org/10.1109/TNET.2016.2587286)

15. **Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang, Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, and et al. 2019. HPCC: High Precision Congestion Control.** In Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM '19). Association for Computing Machinery, New York, NY, USA, 44–58. [DOI: 10.1145/3341302.3342085](https://doi.org/10.1145/3341302.3342085)

16. **Youyou Lu, Jiwu Shu, Youmin Chen, and Tao Li. 2017. Octopus: an RDMA-enabled Distributed Persistent Memory File System.** In 2017 USENIX Annual Technical Conference (USENIX ATC 17). USENIX Association, Santa Clara, CA, 773–785. [Link](https://www.usenix.org/conference/atc17/technical-sessions/presentation/lu)

17. **Michael Marty, Marc de Kruijf, Jacob Adriaens, Christopher Alfeld, Sean Bauer, Carlo Contavalli, Michael Dalton, Nandita Dukkipati, William C. Evans, Steve Gribble, and et al. 2019. Snap: A Microkernel Approach to Host Networking.** In Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP '19). Association for Computing Machinery, New York, NY, USA, 399–413. [DOI: 10.1145/3341301.3359657](https://doi.org/10.1145/3341301.3359657)

18. **M. Mathis, J. Mahdavi, S. Floyd, and A. Romanow. 1996. TCP Selective Acknowledgment Options.** RFC 2018. RFC Editor.

19. **Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel, Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats. 2015. TIMELY: RTT-based Congestion Control for the Datacenter.** In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication (SIGCOMM '15). ACM, New York, NY, USA, 537–550. [DOI: 10.1145/2785956.2787510](https://doi.org/10.1145/2785956.2787510)

20. **IEEE Std. 2011. IEEE 802.11Qbb. Priority Based Flow Control.** (2011).

21. **Mohit P. Tahiliani, Vishal Misra, and K. K. Ramakrishnan. 2019. A Principled Look at the Utility of Feedback in Congestion Control.** In Proceedings of the 2019 Workshop on Buffer Sizing (BS '19). Association for Computing Machinery, New York, NY, USA, Article 8, 5 pages. [DOI: 10.1145/3375235.3375243](https://doi.org/10.1145/3375235.3375243)

22. **Jordan Tigani and Siddartha Naidu. 2014. Google BigQuery Analytics.** Wiley, Indianapolis, IN, USA.

23. **Balajee Vamanan, Jahangir Hasan, and T.N. Vijaykumar. 2012. Deadline-aware Datacenter TCP (D2TCP).** In Proceedings of the ACM SIGCOMM 2012 Conference (SIGCOMM '12). ACM, New York, NY, USA, 115–126. [DOI: 10.1145/2342356.2342388](https://doi.org/10.1145/2342356.2342388)

24. **Washington State Department of Transportation. 2020. What is a Roundabout?** [Link](https://www.wsdot.wa.gov/Safety/roundabouts/BasicFacts.htm) (Accessed: 2020-01-13).

25. **Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron. 2011. Better Never Than Late: Meeting Deadlines in Datacenter Networks.** In Proceedings of the ACM SIGCOMM 2011 Conference (SIGCOMM '11). ACM, New York, NY, USA, 50–61. [DOI: 10.1145/2018436.2018443](https://doi.org/10.1145/2018436.2018443)

26. **Jian Xu and Steven Swanson. 2016. NOVA: A Log-structured File System for Hybrid Volatile/Non-volatile Main Memories.** In 14th USENIX Conference on File and Storage Technologies (FAST 16). USENIX Association, Santa Clara, CA, 323–338. [Link](https://www.usenix.org/conference/fast16/technical-sessions/presentation/xu)

27. **Jian Yang, Joseph Izraelevitz, and Steven Swanson. 2019. Orion: A Distributed File System for Non-Volatile Main Memory and RDMA-Capable Networks.** In 17th USENIX Conference on File and Storage Technologies (FAST 19). USENIX Association, Boston, MA, 221–234. [Link](https://www.usenix.org/conference/fast19/presentation/yang)

28. **Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauly, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing.** Presented as part of the 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI 12). USENIX, San Jose, CA, 15–28. [Link](https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/zaharia)

29. **Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2010. Spark: Cluster Computing with Working Sets.** In Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing (HotCloud '10). USENIX Association, USA, 10.

30. **Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn, Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and Ming Zhang. 2015. Congestion Control for Large-Scale RDMA Deployments.** In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication (SIGCOMM '15). ACM, New York, NY, USA, 523–536. [DOI: 10.1145/2785956.2787484](https://doi.org/10.1145/2785956.2787484)

31. **Yibo Zhu, Monia Ghobadi, Vishal Misra, and Jitendra Padhye. 2016. ECN or Delay: Lessons Learnt from Analysis of DCQCN and TIMELY.** In Proceedings of the 12th International on Conference on Emerging Networking EXperiments and Technologies (CoNEXT '16). Association for Computing Machinery, New York, NY, USA, 313–327. [DOI: 10.1145/2999572.2999593](https://doi.org/10.1145/2999572.2999593)

32. **Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018. Homa: A Receiver-driven Low-latency Transport Protocol Using Network Priorities.** In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication (SIGCOMM '18). ACM, New York, NY, USA, 221–235. [DOI: 10.1145/3230543.3230564](https://doi.org/10.1145/3230543.3230564)

33. **Jacob Nelson, Brandon Holt, Brandon Myers, Preston Briggs, Luis Ceze, Simon Kahan, and Mark Oskin. 2015. Latency-Tolerant Software Distributed Shared Memory.** In 2015 USENIX Annual Technical Conference (USENIX ATC 15). USENIX Association, Santa Clara, CA, 291–305. [Link](https://www.usenix.org/conference/atc15/technical-session/presentation/nelson)

34. **John Ousterhout, Arjun Gopalan, Ashish Gupta, Ankita Kejriwal, Collin Lee, Behnam Montazeri, Diego Ongaro, Seo Jin Park, Henry Qin, Mendel Rosenblum, et al. 2015. The RAMCloud Storage System.** ACM Transactions on Computer Systems (TOCS) 33, 3 (2015), 7.

35. **Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah, and Hans Fugal. 2014. Fastpass: A Centralized “Zero-queue” Datacenter Network.** In Proceedings of the ACM SIGCOMM 2014 Conference (SIGCOMM '14). ACM, New York, NY, USA, 307–318. [DOI: 10.1145/2619239.2626309](https://doi.org/10.1145/2619239.2626309)

36. **Ahmed Saeed, Nandita Dukkipati, Vytautas Valancius, Vinh The Lam, Carlo Contavalli, and Amin Vahdat. 2017. Carousel: Scalable Traffic Shaping at End Hosts.** In Proceedings of the Conference of the ACM Special Interest Group on Data Communication (SIGCOMM '17). Association for Computing Machinery, New York, NY, USA, 404–417. [DOI: 10.1145/3098822.3098852](https://doi.org/10.1145/3098822.3098852)

37. **Ahmed Saeed, Yimeng Zhao, Nandita Dukkipati, Ellen Zegura, Mostafa Ammar, Khaled Harras, and Amin Vahdat. 2019. Eiffel: Efficient and Flexible Software Packet Scheduling.** In 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19). USENIX Association, Boston, MA, 17–32. [Link](https://www.usenix.org/conference/nsdi19/presentation/saeed)

38. **Yizhou Shan, Yutong Huang, Yilun Chen, and Yiying Zhang. 2018. LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation.** In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). USENIX Association, Carlsbad, CA, 69–87. [Link](https://www.usenix.org/conference/osdi18/presentation/shan)

39. **Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, and et al. 2015. Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network.** SIGCOMM Comput. Commun. Rev. 45, 4 (August 2015), 183–197. [DOI: 10.1145/2829988.2787508](https://doi.org/10.1145/2829988.2787508)

40. **Arjun Singhvi, Aditya Akella, Dan Gibson, Thomas F. Wenisch, Monica Wong-Chan, Sean Clark, Milo M. K. Martin, Moray McLaren, Prashant Chandra, Rob Cauble, Hassan M. G. Wassel, Behnam Montazeri, Simon L. Sabato, Joel Scherpelz, and Amin Vahdat. 2020. 1RMA: Re-envisioning Remote Memory Access for Multi-tenant Datacenters.** In Proceedings of the 2020 ACM Conference on Special Interest Group on Data Communication (SIGCOMM '20). ACM, New York, NY, USA, to appear.

41. **IEEE Std. 2010. IEEE 802.11Qau. Congestion Notification.** (2010).

### Appendices

#### A. Conversion Between Host and NIC Clocks
Some delay computations, such as NIC-Rx-queuing delay, require a combination of NIC hardware and host software clocks. Swift uses a simple linear-extrapolation approach to convert the incoming hardware clock into a host-clock value to compute such delays:

\[ \text{host\_clock} = \text{ratio} \times \text{nic\_clock} + \text{offset} \]

We update the ratio and offset periodically. The algorithm is straightforward: we read the nic_clock, then the host_clock, and also maintain the previous set of readings as last_nic_clock and last_host_clock. The ratio and offset can then be updated as follows:

\[ \text{ratio} = \frac{\text{host\_clock} - \text{last\_host\_clock}}{\text{nic\_clock} - \text{last\_nic\_clock}} \]
\[ \text{offset} = \text{host\_clock} - \text{ratio} \times \text{nic\_clock} \]

#### B. Packet Format
Figure 23 shows the format that Swift uses, which consumes 4 bytes to reflect back remote-side queuing delay. Additionally, 1 byte is used to echo back forward-side hop-count by computing the difference between the initial TTL and observed TTL on the incoming packet.

![Packet Format Changes for Swift](path_to_image.png)

#### C. Delays with Shared Rx/Tx Scheduling
Traditionally, links are bidirectional, meaning incoming traffic does not affect outgoing traffic on a link. Our deployment of Swift in Snap had a unique challenge since Rx and Tx scheduling is shared in Snap. While counter-intuitive, Swift's design addresses this by factoring local NIC queue buildup as part of endpoint congestion. The insight is taken from traffic roundabouts, which also have shared Rx and Tx scheduling; factoring local NIC-Rx delay emulates yield-at-entry as the right-of-way when viewed as a roundabout [53]. In other words, if a machine’s NIC-Rx queue builds up, it should prioritize clearing those packets before trying to inject more packets (through Tx) into the network.

#### D. Experiment with Target Delay
In our experience, a nice property of delay is that as low latency networking stacks advance to avoid interference from host CPUs, Swift continues to work well just with a simple knob of target delay. In Figure 24, we show results from a testbed implementation of Swift in a prototype stack where it is able to achieve near line-rate throughput (∼100Gbps) even at 15µs RTT.

![Achieved RTT vs. Target Delay, 160-flow Incast](path_to_image.png)

#### E. Delay or ECN
Zhu et al. [60] called out challenges in using delay as a congestion signal. We appreciate how some aspects of Swift's design ended up addressing these challenges.

- **The authors show that a fluid model of TIMELY does not have a unique fixed point because of reliance on gradient-based control.** Swift, instead, uses a target delay (vs. gradient-based control) and, in this way, does not suffer from multiple fixed points at convergence. The authors echo our experience and provide a version called, Patched TIMELY, that also gets rid of the delay-gradient.
  
- **The authors rightly note that delay lags behind ECN in that modern switches mark ECN at packet-egress while delay, implicitly, measures congestion when the packet arrives at the bottleneck switch.** As discussed in §3.2 and shown in experiments §5, Swift uses a low target delay and does not delay ACKs, and hence largely mitigates this concern. Testimony to this is our experience with Swift achieving low latency and losses at scale at Google.
  
- **The authors provide an important result (Theorem 6 in Reference [60]): purely relying on end-to-end delay measurements for congestion control can provide either fairness or fixed-delay but not both.** The reason is that if the delay is controlled to a fixed value, the algorithm is agnostic of the number of flows, making the system of equations inconsistent. Swift resolves this by not using the same target delay for all flows and instead scales the target delay as explained in §3.5. For example, flow-based scaling varies the target delay as a function of congestion window and converges to a single fixed point. As shown in Figure 21, this greatly improves fairness, especially under large-scale incast.
  
- **The authors in Reference [50] argue that end-to-end delay is an ambiguous signal in that a flow may traverse a wide variety of link speeds across a number of hops.** We believe while this can be a valid concern for Internet traffic, this concern is much less applicable to a datacenter CC like Swift, where paths are known and the link speeds do not vary as widely as in the Internet. Swift uses topology-based scaling to account for different hop-counts across flows.
  
- **In addition, delay has a few important operational advantages.** First, delay evolves naturally as networks become faster, and tuning it at scale is simpler than ECN, especially for production environments with multiple QoS classes—it has been shown that ECN is problematic in such scenarios, and sojourn time is more robust [8]. Second, delay has a multi-bit nature as a congestion signal; it provides visibility into the extent of congestion, unlike ECN, which only signals whether congestion exists or not. That said, we look forward to integrating multi-bit ECN signals like sojourn-time [8] and INT [29] in Swift’s framework.