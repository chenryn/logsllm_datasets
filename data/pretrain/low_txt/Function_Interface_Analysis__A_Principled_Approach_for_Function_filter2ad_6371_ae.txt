### Function Category Distribution

The following data represents the percentage of each function category, which is largely dependent on the language and program:

- 0.00
- 68.75
- 70.77
- 0.00
- 80.64
- 0.00
- 64.94
- 62.72
- 75.28
- 0.00
- 73.74
- 71.43
- 73.61
- 84.31
- 50.00
- 40.15
- 43.06
- 41.02
- 29.55
- 67.02
- 1851
- 228
- 1602
- 0
- 1916
- 2

### Analysis of Function Categories

As shown in the figure, the distribution of function categories varies significantly based on the programming language and the specific program. For most C and Fortran programs, direct calls contribute to the largest number of identified functions. This includes direct calls made within functions that are only indirectly reached. Some C programs, such as 445.gobmk, contain a large number of indirect functions. In many C++ programs, due to the abundance of virtual functions, there are generally more indirectly reached functions.

The fourth column presents the percentage of functions that are reached only by direct jumps (i.e., tail called). These functions are not uncommon in optimized binaries.

### Unreachable Functions

For some benchmarks, the percentage of unreachable functions is quite high, with an average of 17.7% and up to 40%. To verify these results, we selected a subset of these programs and used Pintools [27] to record the locations reached via calls or jumps. We found that none of these addresses corresponded to functions determined as unreachable by our technique.

Upon checking the source code, we found that the unreachable functions are mostly global (i.e., non-static) functions that are neither called directly nor have their addresses taken. Although they are not used, compilers do not generally remove them unless specific actions are taken during the build process to eliminate them. This is different from static functions, whose visibility is limited to the same compilation unit, and it is more common for unused static functions to be removed by default.

### Effectiveness of Interface Checking Techniques

Function interface checking is critical in pruning spurious functions from the identified candidate set. In this section, we evaluate the effectiveness of each checking mechanism independently. The results are presented in Fig. 13, showing only GCC -O2 compiled binaries for x86-32.

Each checking mechanism is independently effective in identifying a significant fraction of all spurious ("total pruned" in the figure) functions. However, no single mechanism can detect all spurious functions. It is through their combination that we can effectively reduce the number of spurious functions to a very low number. Note that for four of the binaries, no spurious functions are pruned because all the enumerated functions happen to be real functions.

### Analysis Runtime Performance

Our focus so far has been on accuracy, and we have not made any efforts to optimize runtime performance. Nevertheless, for completeness, we summarize the performance results we currently obtain.

Compared to machine learning-based approaches [7, 35], one of the advantages of our approach is that it does not require training, which is expensive. The results of our analysis, together with those from ByteWeight [7] and a neural network-based system [35], are summarized in Fig. 14. The numbers are based on our first dataset, and 10-fold cross-validation for machine learning systems.

The neural network-based system uses much less time for testing because it only identifies the bytes where functions start and end, without recovering the function body. In contrast, ByteWeight and our system follow the CFG to identify function ends, thereby recognizing the exact instructions belonging to the function and identifying physically non-contiguous parts of the function.

Currently, it takes about 40 seconds on average to analyze a binary in our test suite. Although this is already satisfactory for many cases, there are opportunities for improvement. For example, spurious functions can be immediately spotted if the entry basic block has violating behavior, thus avoiding the need to analyze the whole function. This is in contrast to our current naive implementation that performs complete analysis and checks.

### Case Studies

Since function recognition serves as an essential step for many techniques working on binaries, we analyze several representative applications. Our focus in this section is on binary instrumentation tasks that impose stringent requirements.

Many binary instrumentation techniques operate on individual functions as a unit [12, 32, 10]. These applications are sensitive to precision because a misidentified function could cause misbehavior or failure of the instrumented program. In this section, we analyze the applicability of our function identification system for these function-based instrumentation applications.

Since directly called functions are free of errors and unreachable functions are not relevant for correct functionality (as they will never be executed), imprecision could possibly originate from two sources: indirectly reachable functions and direct jump reached (tail called) functions. We analyze these two cases respectively.

For the first case, an address could be incorrectly identified as an (indirectly reachable) function start if our interface checking mechanism is insufficient. Although our comprehensive checking schemes are generally effective and can remove the vast majority of spurious function starts, such misses do occur. Fig. 15 shows one example. In this case, since all instructions access global memory and there are no stack or general-purpose register operations, our interface checking cannot identify [818c784, 818c8e4] as a spurious function.

Despite these imprecisions, one distinguishing feature of our system is that the real function which encloses the spurious one is always identified. In Fig. 15, [818ba30, 818c8e4] is also recovered. With this property, different measures could be taken for different instrumentations to cope with the imprecisions.

For RAD [12], no work is required at all because the instrumentation is resistant to such imprecisions. For more complicated instrumentations [32, 10], the overlapping functions could have their own instrumented version (which are disjoint), and an address translation scheme for indirect branches (commonly adopted by binary transformation systems [27, 45, 44, 37]) could be used. With this technique, an indirect call target is translated at runtime to point to its instrumented version before control transfer. Since the falsely identified function is never called at runtime, incorrect instrumentation will not be executed.

Our system may also falsely recognize intra-procedural jumps as direct tail calls (the second type of error). Essentially, this is equivalent to splitting the original function into two. However, we note that this will not introduce any correctness problems, as all executed instructions and exercised control flows have been well captured.

The above analysis indicates that our function recognition is effective and leaves limited error possibilities. The inaccuracies tend to either have no effect for function-based instrumentation correctness or can be easily managed. As a comparison, since machine learning-based approaches rely on code or byte patterns, false positives of function starts and ends are much more random and difficult to handle. Finally, as can be seen in these case studies, our system automatically classifies identified functions based on their reachability properties, enabling more flexible instrumentations.

### Discussion

#### Special Calling Conventions

Currently, our data flow checking technique is based on well-respected system ABIs and calling conventions and can be adapted to other architectures such as ARM [11]. Non-standard calling conventions, although not present in our tests, could be used in some cases, e.g., function calls within a single translation unit. To address this issue, a "self-checking" mechanism can be adopted.

Specifically, ABI violations can occur only in the context of direct calls and jumps. Since we do not apply interface checks for direct calls, ABI violations won't pose a problem in their context. That leaves direct jumps (i.e., direct tail calls) as the only problem case. We develop a self-checking mechanism in this case. Specifically, we can perform interface checks on a subset of directly called functions to determine whether the ABI is respected. If not, we identify a relaxed set of conventions that are respected in direct calls and apply these relaxed checks to tail call verification. (Note that verification of indirectly called functions can continue to rely on ABI.)

### Related Work

#### Function Recognition

Many tools recognize functions using call graph traversal and function prologue matching. Examples include CMU BAP [8], angr binary analysis platform [36], and the Dyninst instrumentation tool [22]. IDA [1] uses proprietary heuristics and a signature database for function boundary identification to assist disassembling. Its problems include underperformance for different compilers and platforms and the overhead of maintaining an up-to-date signature database.

Rosenblum et al. first proposed using machine learning for function start identification [31]. The precision and performance have been greatly improved by recent work from Bao et al. [7] and Shin et al. [35], due to the adoption of different machine learning techniques such as weighted prefix trees and neural networks. However, as discussed, machine learning requires a good training set and potentially subtle parameter tuning. Moreover, existing machine learning techniques have focused on surrounding code and may have difficulties grasping valuable global evidence or deeper semantics, factors that greatly benefit our analysis.

Nucleus [5] is a concurrent work also based on static analysis. Nucleus relies on control-flow analysis to infer inter-procedural edges and function starts. In contrast, our approach leverages both control-flow and data-flow properties for comprehensive function interface checking. We demonstrate that fine-grained static analysis [29] can recognize functions with much greater accuracy and has the potential to support demanding applications such as automated analysis and instrumentation.

#### Static Binary Analysis to Recover High-Level Constructs

Other than function boundaries, previous works also focus on recovering other high-level constructs, such as variables and types [6, 26, 3] or function signatures [17]. The more ambitious goal is to recover source code through decompilation [21, 18, 33]. However, many of these tools are either best-effort analyses designed for helping human audits or only tested with a much smaller corpus. We expect the precision of these downstream analyses to be improved with more accurately recognized functions.

### Conclusions

In this work, we present a static analysis-based approach for function boundary identification in stripped binary code. Compared with previous efforts that rely on matching code patterns, our approach is more principled by leveraging the function interface abstraction and implementation. By adopting a comprehensive checking mechanism that combines stack discipline, control flow, and data flow properties, our approach can substantially improve accuracy over the best previous systems that are either machine learning or static analysis-based. The deeper insights of identified functions provide further opportunities to reduce error rates and enable more flexible applications.

### References

[1] Hex rays. https://www.hex-rays.com/index.shtml.
[2] M. Abadi, M. Budiu, U. Erlingsson, and J. Ligatti. Control-flow integrity. In CCS, 2005.
[3] K. Anand, M. Smithson, K. Elwazeer, A. Kotha, J. Gruen, N. Giles, and R. Barua. A compiler-level intermediate representation based binary analysis and rewriting system. In ACM EuroSys, 2013.
[4] D. Andriesse, X. Chen, V. van der Veen, A. Slowinska, and H. Bos. An in-depth analysis of disassembly on full-scale x86/x64 binaries. In USENIX Security, 2016.
[5] D. Andriesse, A. Slowinska, and H. Bos. Compiler-agnostic function detection in binaries. In EuroS&P, 2017.
[6] G. Balakrishnan and T. Reps. WYSINWYX: What you see is not what you execute. ACM TOPLAS, 2010.
[7] T. Bao, J. Burket, M. Woo, R. Turner, and D. Brumley. Byteweight: Learning to recognize functions in binary code. In USENIX Security, 2014.
[8] D. Brumley, I. Jager, T. Avgerinos, and E. J. Schwartz. Bap: A binary analysis platform. In Computer aided verification, 2011.
[9] M. Chandramohan, Y. Xue, Z. Xu, Y. Liu, C. Y. Cho, and H. B. K. Tan. Bingo: Cross-architecture cross-os binary search. In FSE, 2016.
[10] X. Chen, A. Slowinska, D. Andriesse, H. Bos, and C. Giuffrida. Stackarmor: Comprehensive protection from stack-based memory error vulnerabilities for binaries. In NDSS, 2015.
[11] Y. Chen, D. Zhang, R. Wang, R. Qiao, A. Azab, L. Lu, H. Vijayakumar, and W. Shen. Norax: Enabling execute-only memory for COTS binaries on AArch64. In S&P, 2017.
[12] T. Chiueh and M. Prasad. A binary rewriting defense against stack-based overflows. In USENIX ATC, 2003.
[13] C. Cifuentes and M. Van Emmerik. Recovery of jump table case statements from binary code. In IEEE International Workshop on Program Comprehension, 1999.
[14] P. Cousot and R. Cousot. Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints. In POPL, 1977.
[15] Y. David, N. Partush, and E. Yahav. Statistical similarity of binaries. In PLDI, 2016.
[16] M. Egele, M. Woo, P. Chapman, and D. Brumley. Blanket execution: Dynamic similarity testing for program binaries and components. In USENIX Security, 2014.
[17] K. ElWazeer, K. Anand, A. Kotha, M. Smithson, and R. Barua. Scalable variable and data type detection in a binary rewriter. In PLDI, 2013.
[18] M. Emmerik and T. Waddington. Using a decompiler for real-world source recovery. In Working Conference on Reverse Engineering, 2004.
[19] H. Flake. Structural comparison of executable objects. In DIMVA, 2004.
[20] A. Fog. Calling conventions for different C++ compilers and operating systems, 2015.
[21] I. Guilfanov. Decompilers and beyond. Black Hat USA, 2008.
[22] L. C. Harris and B. P. Miller. Practical analysis of stripped binary code. ACM SIGARCH Computer Architecture News, 2005.
[23] N. Hasabnis, R. Qiao, and R. Sekar. Checking correctness of code generator architecture specifications. In CGO, 2015.
[24] N. Hasabnis and R. Sekar. Extracting instruction semantics via symbolic execution of code generators. In FSE, 2016.
[25] N. Hasabnis and R. Sekar. Lifting assembly to intermediate representation: A novel approach leveraging compilers. In ASPLOS, 2016.
[26] J. Lee, T. Avgerinos, and D. Brumley. TIE: Principled reverse engineering of types in binary programs. In NDSS, 2011.
[27] C. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S. Wallace, V. J. Reddi, and K. Hazelwood. Pin: Building customized program analysis tools with dynamic instrumentation. In PLDI, 2005.
[28] X. Meng and B. P. Miller. Binary code is not easy. In ISSTA, 2016.
[29] R. Qiao and R. Sekar. Effective function recovery for COTS binaries using interface verification. Technical report, Secure Systems Lab, Stony Brook University, 2016.
[30] R. Qiao, M. Zhang, and R. Sekar. A principled approach for ROP defense. In ACSAC, 2015.
[31] N. E. Rosenblum, X. Zhu, B. P. Miller, and K. Hunt. Learning to analyze binary computer code. In AAAI, 2008.
[32] P. Saxena, R. Sekar, and V. Puranik. Efficient fine-grained binary instrumentation with applications to taint-tracking. In CGO, 2008.
[33] E. J. Schwartz, J. Lee, M. Woo, and D. Brumley. Native x86 decompilation using semantics-preserving structural analysis and iterative control-flow structuring. In Usenix Security, 2013.
[34] B. Schwarz, S. Debray, and G. Andrews. Disassembly of executable code revisited. In Working Conference on Reverse Engineering, 2002.
[35] E. C. R. Shin, D. Song, and R. Moazzezi. Recognizing functions in binaries with neural networks. In USENIX Security, 2015.
[36] Y. Shoshitaishvili, R. Wang, C. Salls, N. Stephens, M. Polino, A. Dutcher, J. Grosen, S. Feng, C. Hauser, C. Kruegel, and G. Vigna. (State of) the art of war: Offensive techniques in binary analysis. In IEEE S&P, 2016.
[37] M. Smithson, K. ElWazeer, K. Anand, A. Kotha, and R. Barua. Static information: Overcoming the tradeoff between coverage and correctness. In WCRE, 2013.
[38] D. Song, D. Brumley, H. Yin, J. Caballero, I. Jager, M. G. Kang, Z. Liang, J. Newsome, P. Poosankam, and P. Saxena. BitBlaze: A new approach to computer security via binary analysis. In International Conference on Information Systems Security. Keynote paper., 2008.
[39] V. van der Veen, D. Andriesse, E. Göktas, B. Gras, L. Sambuc, A. Slowinska, H. Bos, and C. Giuffrida. Practical context-sensitive CFI. In CCS, 2015.
[40] V. van der Veen, E. Göktas, M. Contag, A. Pawlowski, X. Chen, S. Rawat, H. Bos, T. Holz, E. Athanasopoulos, and C. Giuffrida. A tough call: Mitigating advanced code-reuse attacks at the binary level. In IEEE S&P, 2016.
[41] S. Wang, P. Wang, and D. Wu. Reassembleable disassembling. In USENIX Security, 2015.
[42] C. Zhang, T. Wei, Z. Chen, L. Duan, L. Szekeres, S. McCamant, D. Song, and W. Zou. Practical control flow integrity and randomization for binary executables. In IEEE S&P, 2013.
[43] M. Zhang. Static Binary Instrumentation with Applications to COTS Software Security. PhD thesis, Stony Brook University, 2015.
[44] M. Zhang, R. Qiao, N. Hasabnis, and R. Sekar. A platform for secure static binary instrumentation. In VEE, 2014.
[45] M. Zhang and R. Sekar. Control flow integrity for COTS binaries. In USENIX Security, 2013.
[46] M. Zhang and R. Sekar. Control flow and code integrity for COTS binaries: An effective defense against real-world ROP attacks. In ACSAC, 2015.