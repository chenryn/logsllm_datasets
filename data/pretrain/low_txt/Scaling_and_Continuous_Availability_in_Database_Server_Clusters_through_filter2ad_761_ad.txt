### Figures and Descriptions

**Figure 8: Fail-over onto Warm DMV Backup with 1% Query-Execution Warm-up**

- **(a) Throughput**
- **(b) Latency**

**Figure 9: Failover Effect for Alternate Backup Warmup Scheme Using Page ID Transfers from an Active Slave**

- The active slave transfers page IDs to the backup every 100 transactions, while the backup touches these pages.
- Performance in this case is comparable to periodic query execution, enabling seamless failure handling.

- **(a) Throughput**
- **(b) Latency**

**Figure 7: Fail-over onto Cold Up-to-Date DMV Backup**

- **(a) Throughput**
- **(b) Latency**

- The drop in throughput is significant due to the need to warm up the entire database cache on the cold backup. It takes more than one minute to recover.

- **(a) Throughput**
- **(b) Latency**

### Related Work

A variety of solutions exist for replicating relational databases, aiming to provide both scaling and strong consistency. These range from industry-established solutions like Oracle RAC [3] and IBM DB2 HADR suite [1], to research and open-source prototypes such as MySQL Cluster [17], C-JDBC [9, 10], Postgres-R [14], and Ganymed [20].

- **Industry Solutions**: Provide high availability and good scalability but are costly and often require specialized hardware, such as Shared Network Disk [3].
- **MySQL Cluster [17]**: Offers fast in-memory replicated storage with lazy logging of updates, similar to our system prototype. However, it uses traditional two-phase locking for concurrency control, which can stall readers accessing data being modified. In contrast, our solution resolves read/write conflicts optimistically, avoiding thread blocking.
- **Research Prototypes**: Use commodity software and hardware but have limited scaling for moderately heavy write workloads [5, 10] due to coarse-grained concurrency control. Some sacrifice failure transparency and data availability by introducing single points of failure [20]. Even solutions that offer transparent failover and data availability [5] do so through complex protocols. Our solution provides transparent scalability and fast, transparent failover, with a minimal scheduler node that allows for extremely fast reconfiguration in single node fail-stop scenarios.

### Previous Work in Primary-Backup Replication

Most previous work in primary-backup replication [24] has followed a "passive backup as a hot-standby" approach, where the backup mirrors the primary's updates. These solutions either enforce fully synchronous application of updates or do not enforce strict consistency, although the backup maintains a copy of the primary's database. The backup is either idle during normal operation [24] or executes a different set of applications/tasks. In contrast, our replicated cluster uses backups for seamless failover while a large set of active slaves execute read-only transactions with strong consistency guarantees.

### Recent Efforts in Database Concurrency Control and Replication

Recent efforts towards integrating fine-grained concurrency control and replication techniques use snapshot isolation [11, 23, 20] to minimize consistency maintenance overheads. These solutions depend on support for multiversioning within each database replica. Our solution dynamically creates the required versions across distributed replicas.

### Introduction of Lightweight Reconfiguration Techniques

In this paper, we introduce novel lightweight reconfiguration techniques for the database tier in dynamic content web sites. Our solution is based on an in-memory replication algorithm called Dynamic Multiversioning, which provides transparent scaling with strong consistency and ease of reconfiguration.

- **Dynamic Multiversioning**: Exploits naturally arising versions across asynchronous database replicas to offer high concurrency. We integrate the replication process with database concurrency control, avoiding duplication of database functionality in the scheduler and the copy-on-write overheads associated with standalone database multiversioning.
- **Version-Aware Scheduler Algorithm**: Distributes transactions requesting different version numbers across different nodes, keeping aborts due to version conflicts at negligible rates.

### Evaluation

Our evaluation shows that our system is flexible and efficient. While a primary replica is always needed in our in-memory tier, a set of active slaves can be adaptively and transparently expanded to accommodate faults. We scaled a web site using an InnoDB on-disk database backend by factors of 14.6, 17.6, and 6.5 for TPC-W browsing, shopping, and ordering mixes, respectively, when interposing our intermediate in-memory tier with 9 replicas. Additionally, our in-memory tier can incorporate a spare backup after a fault without any noticeable impact on performance due to reconfiguration.

### References

[1] IBM DB2 High Availability and Disaster Recovery. <http://www.ibm.com/db2/>

[2] MySQL Database Server. <http://www.mysql.com/>

[3] Oracle Real Application Clusters 10g. <http://www.oracle.com/technology/products/database/clustering/>

[4] Amza, C., Cecchet, E., Chanda, A., Cox, A., Elnikety, S., Gil, R., Marguerite, J., Rajamani, K., & Zwaenepoel, W. (2002). Specification and implementation of dynamic web site benchmarks. In 5th IEEE Workshop on Workload Characterization.

[5] Amza, C., Cox, A., & Zwaenepoel, W. (2003). Conflict-aware scheduling for dynamic content applications. Proceedings of the Fifth USENIX Symposium on Internet Technologies and Systems, 71–84.

[6] Amza, C., Cox, A., & Zwaenepoel, W. (2003). Distributed versioning: Consistent replication for scaling back-end databases of dynamic content web sites. ACM/IFIP/Usenix International Middleware Conference.

[7] The Apache Software Foundation. <http://www.apache.org/>

[8] Bernstein, P., Hadzilacos, V., & Goodman, N. (1987). Concurrency Control and Recovery in Database Systems. Addison-Wesley.

[9] Cecchet, E., Marguerite, J., & Zwaenepoel, W. (2004). C-jdbc: Flexible database clustering middleware. Proceedings of the USENIX 2004 Annual Technical Conference.

[10] Cecchet, E., Marguerite, J., & Zwaenepoel, W. (2004). RAIDb: Redundant array of inexpensive databases. IEEE/ACM International Symposium on Parallel and Distributed Applications (ISPA’04).

[11] Elnikety, S., Pedone, F., & Zwaenepoel, W. (2004). Generalized snapshot isolation and a prefix-consistent implementation. Technical Report IC/2004/21, EPFL.

[12] IBM. (2003). High availability with DB2 UDB and Steeleye Lifekeeper. IBM Center for Advanced Studies Conference (CASCON): Technology Showcase, Toronto, Canada.

[13] Kemme, B., & Alonso, G. (2000). A New Approach to Developing and Implementing Eager Database Replication Protocols. ACM Transactions on Data Base Systems, 25, 333–379.

[14] Kemme, B., & Alonso, G. (2000). Don’t be lazy, be consistent: Postgres-R, a new way to implement database replication. The VLDB Journal, 134–143.

[15] Lowell, D., & Chen, P. (1997). Free transactions with Rio Vista. Proceedings of the 16th ACM Symposium on Operating Systems Principles.

[16] Manassiev, K., Mihailescu, M., & Amza, C. (2006). Exploiting distributed version concurrency in a transactional memory cluster. PPOPP, 198–208.

[17] MySQL Cluster. <http://www.mysql.com/products/database/cluster/>

[18] Patino-Martinez, M., Jimenez-Peris, R., Kemme, B., & Alonso, G. (2000). Scalable replication in database clusters. DISC ’00: Proceedings of the 14th International Conference on Distributed Computing, 315–329. Springer-Verlag.

[19] PHP Hypertext Preprocessor. <http://www.php.net>

[20] Plattner, C., & Alonso, G. (2004). Ganymed: Scalable replication for transactional web applications. Proceedings of the 5th ACM/IFIP/USENIX International Middleware Conference, Toronto, Canada, October 18-22.

[21] Soundararajan, G., Amza, C., & Goel, A. (2006). Database replication policies for dynamic content applications. EuroSys’06: Proceedings of the EuroSys 2006 Conference, 89–102. ACM.

[22] Transaction Processing Council. <http://www.tpc.org/>

[23] Wu, S., & Kemme, B. (2005). Postgres-r(si): Combining replica control with concurrency control based on snapshot isolation. Proceedings of the 21st International Conference on Data Engineering.

[24] Zhou, Y., Chen, P., & Li, K. (1999). Fast cluster failover using virtual memory-mapped communication. Proc. of the Int’l Conference on Supercomputing.

---

This revised text aims to be more clear, coherent, and professional, with improved organization and readability.