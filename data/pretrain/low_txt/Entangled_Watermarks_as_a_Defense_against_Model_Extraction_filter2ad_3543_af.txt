### Deep Neural Networks (DNNs) and Selective Layer Entanglement

While it may not be necessary to apply the Soft Nearest Neighbors Loss (SNNL) to all hidden layers, our experiments in Figure 20 demonstrate that applying SNNL to only the last few layers can provide the desired guarantees. Additionally, we observed a slight increase in model utility when not all layers are entangled. A detailed exploration of how to optimally select these layers is an area for future research.

### Scalability and Future Research Directions

As discussed in §4.3.4, EWE (Entangled Watermark Embedding) faces challenges in balancing model performance and watermark robustness when applied to deeper architectures and more complex datasets. The results on CIFAR-100 indicate that further work is needed to scale the current method to larger datasets. Figure 24 (in Appendix A.3) shows that the performance of EWE is affected by the number of classes, likely due to the increased complexity of the representation space. This suggests that the next step should involve investigating the design of triggers to control the cluster of watermarked data, and the similarity structures and orientation of the representation space to choose source and target classes more effectively.

Another potential improvement is the use of m-to-n watermarking. In this study, we focused on 1-to-1 watermarking, where one class of data is watermarked and entangled with another class. However, if the watermarked model behaves significantly differently from a clean model, the model owner could choose to watermark m classes of data and entangle them with n other classes, following a similar verification process as described in §4.3.1.

### Conclusions

We introduced Entangled Watermark Embedding (EWE), which forces the model to entangle representations for legitimate task data and watermarks. Our mechanism formulates a new loss function involving the Soft Nearest Neighbors Loss, which, when minimized, increases entanglement. Through evaluations on vision and audio tasks, we demonstrated that EWE is robust against model extraction attacks, piracy attacks, anomaly detection, transfer learning, and efforts to mitigate backdoor (poisoning) attacks. These benefits are achieved while preserving watermarking accuracy, with a nominal loss in classification accuracy and a 1.5-2x increase in computational overhead. Scaling EWE to complex tasks without significant accuracy loss remains an open problem.

### Acknowledgments

The authors would like to thank Carrie Gates for her guidance. This research was funded by CIFAR, DARPA GARD, Microsoft, and NSERC. VC was partially supported by the Landweber Fellowship.

### References

[1] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. In 27th USENIX Security Symposium (USENIX Security 18). USENIX Association, August 2018.

[2] Ibrahim M Alabdulmohsin, Xin Gao, and Xiangliang Zhang. Adding robustness to support vector machines against adversarial reverse engineering. In Proceedings of the 23rd ACM International Conference on Information and Knowledge Management. ACM, 2014.

[3] Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan Picek. CSI NN: Reverse engineering of neural network architectures through electromagnetic side channel. In 28th USENIX Security Symposium (USENIX Security 19). USENIX Association, August 2019.

[4] Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, and Jörg Sander. LOF: Identifying density-based local outliers. SIGMOD Rec., 29(2):93–104, May 2000.

[5] Varun Chandrasekaran, Kamalika Chaudhuri, Irene Giacomelli, Somesh Jha, and Songbai Yan. Model extraction and active learning. CoRR, abs/1811.02054, 2018.

[6] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In Proceedings of the 13th ACM Workshop on Artificial Intelligence and Security. ACM, 2020.

[7] Keunwoo Choi, Deokjin Joo, and Juho Kim. Kapre: On-GPU audio preprocessing layers for a quick implementation of deep neural network models with Keras. In Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning. ICML, 2017.

[8] Jacson Rodrigues Correia-Silva, Rodrigo F Berriel, Claudine Badue, Alberto F de Souza, and Thiago Oliveira-Santos. Copycat CNN: Stealing knowledge by persuading confession with random non-labeled data. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2018.

[9] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based on centered alignment. Journal of Machine Learning Research, 13(Mar):795–828, 2012.

[10] Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. DeepSigns: A Generic Watermarking Framework for IP Protection of Deep Learning Models. arXiv e-prints, page arXiv:1804.00750, Apr 2018.

[11] Whitfield Diffie and Martin E. Hellman. New directions in cryptography, 1976.

[12] Nicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. Analyzing and Improving Representations with the Soft Nearest Neighbor Loss. arXiv e-prints, page arXiv:1902.01889, Feb 2019.

[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

[14] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples. arXiv e-prints, December 2014.

[15] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv e-prints, page arXiv:1708.06733, August 2017.

[16] Alon Halevy, Peter Norvig, and Fernando Pereira. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24(2):8–12, 2009.

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016.

[18] Xuedong Huang, James Baker, and Raj Reddy. A historical perspective of speech recognition. Communications of the ACM, 57(1):94–103, 2014.

[19] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. High-Fidelity Extraction of Neural Network Models. arXiv e-prints, page arXiv:1909.01838, Sep 2019.

[20] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning. arXiv e-prints, Apr 2018.

[21] Ian Jolliffe. Principal Component Analysis. Springer, 2002.

[22] A. B. Kahng, J. Lach, W. H. Mangione-Smith, S. Mantik, I. L. Markov, M. Potkonjak, P. Tucker, H. Wang, and G. Wolfe. Watermarking techniques for intellectual property protection. In Proceedings of the 35th Annual Design Automation Conference, DAC ’98, New York, NY, USA, 1998. Association for Computing Machinery.

[23] Eamonn Keogh and Abdullah Mueen. Curse of Dimensionality, pages 314–315. Springer, Boston, MA, 2017.

[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 3rd International Conference on Learning Representations ICLR 2015, 2015.

[25] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of Neural Network Representations Revisited. The 36th International Conference on Machine Learning, 2019.

[26] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

[27] Alexey Kurakin, J. Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. 5th International Conference on Learning Representations, 2017.

[28] Y. LeCun and C. Cortes. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.

[29] T. Lee, B. Edwards, I. Molloy, and D. Su. Defending against neural network model stealing attacks using deceptive perturbations. In 2019 IEEE Security and Privacy Workshops (SPW), pages 43–49, 2019.

[30] F. Liu, K. M. Ting, and Z. Zhou. Isolation forest. In 8th IEEE International Conference on Data Mining, 2008.

[31] K. Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In 21st International Symposium on Research in Attacks, Intrusions, and Defenses, 2018.

[32] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641–647. ACM, 2005.

[33] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, 2018.

[34] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. UMAP: Uniform Manifold Approximation and Projection. The Journal of Open Source Software, 3(29):861, 2018.

[35] Smitha Milli, L. Schmidt, A. Dragan, and M. Hardt. Model reconstruction from model explanations. Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.

[36] A. Mogelmose, M. M. Trivedi, and T. B. Moeslund. Vision-based traffic sign detection and analysis for intelligent driver assistance systems: Perspectives and survey. IEEE Transactions on Intelligent Transportation Systems, 13(4):1484–1497, 2012.

[37] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.

[38] Yuki Nagai, Y. Uchida, S. Sakazawa, and Shin’ichi Satoh. Digital watermarking for deep neural networks. International Journal of Multimedia Information Retrieval, 7:3–16, 2018.

[39] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Ng. Reading digits in natural images with unsupervised feature learning. 24th International Conference on Neural Information Processing Systems, 2011.

[40] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff Nets: Stealing functionality of black-box models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.

[41] Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish K. Shevade, and Vinod Ganapathy. A framework for the extraction of deep neural networks by leveraging public data. CoRR, abs/1905.09165, 2019.

[42] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345–1359, 2010.

[43] Nicolas Papernot, P. McDaniel, Ian J. Goodfellow, S. Jha, Z. Y. Celik, and A. Swami. Practical black-box attacks against machine learning. ACM Asia Conference on Computer and Communications Security, 2017.

[44] Nicolas Papernot, P. McDaniel, S. Jha, Matt Fredrikson, Z. Y. Celik, and A. Swami. The limitations of deep learning in adversarial settings. 1st IEEE European Symposium on Security and Privacy, 2016.

[45] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. arXiv e-prints, page arXiv:1605.07277, May 2016.

[46] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-propagating errors. Nature, 323:533–536, 1986.

[47] R. Salakhutdinov and Geoffrey E. Hinton. Learning a nonlinear embedding by preserving class neighborhood structure. In 11th International Conference on Artificial Intelligence and Statistics, 2007.

[48] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In IEEE International Joint Conference on Neural Networks, pages 1453–1460, 2011.

[49] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. ACL, 2019.

[50] Christian Szegedy, W. Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, Ian J. Goodfellow, and R. Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2014.

[51] Florian Tramèr, F. Zhang, A. Juels, M. Reiter, and T. Ristenpart. Stealing machine learning models via prediction APIs. In USENIX Security Symposium, 2016.

[52] Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aäron van den Oord. Adversarial risk and the dangers of evaluating against weak attacks. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 5032–5041. PMLR, 2018.

[53] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, B. Viswanath, H. Zheng, and B. Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. 2019 IEEE Symposium on Security and Privacy (SP), pages 707–723, 2019.

[54] Pete Warden. Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition. arXiv e-prints, page arXiv:1804.03209, Apr 2018.

[55] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv e-prints, page arXiv:1708.07747, Aug 2017.

[56] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, M. P. Stoecklin, H. Huang, and I. Molloy. Protecting intellectual property of deep neural networks with watermarking. In Proceedings of the 2018 on Asia Conference on Computer and Communications Security, 2018.

### Appendix

#### A.1 Fine-tuning the Hyperparameters of EWE

Next, we delve into the details of each hyperparameter of EWE and perform an ablation study.

**Temperature**: Temperature is a hyperparameter introduced by Frosst et al. [12]. It controls the importance of distances between points: at low temperatures, small distances are more significant, while at high temperatures, large distances matter more. In our experiments, we...