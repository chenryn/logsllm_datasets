### References

1. **Calo, M.** (2019). Analyzing Federated Learning through an Adversarial Lens. In *International Conference on Machine Learning*, pages 634–643.

2. **Biggio, B., Nelson, B., and Laskov, P.** (2012). Poisoning Attacks against Support Vector Machines. In *Proceedings of the 29th International Conference on Machine Learning*.

3. **Blanchard, P., Guerraoui, R., Stainer, J., et al.** (2017). Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent. In *Advances in Neural Information Processing Systems*, pages 119–129.

4. **Caldas, S., Wu, P., Li, T., Konečný, J., McMahan, H. B., Smith, V., and Talwalkar, A.** (2018). LEAF: A Benchmark for Federated Settings. *arXiv preprint arXiv:1812.01097*.

5. **Chang, H., Shejwalkar, V., Shokri, R., and Houmansadr, A.** (2019). Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer. *arXiv preprint arXiv:1912.11279*.

6. **Charikar, M., Steinhardt, J., and Valiant, G.** (2017). Learning from Untrusted Data. In *Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing*, pages 47–60. ACM.

7. **Chen, L., Wang, H., Charles, Z., and Papailiopoulos, D.** (2018). Draco: Byzantine-Resilient Distributed Training via Redundant Gradients. In *International Conference on Machine Learning*, pages 903–912.

8. **Cohen, G., Afshar, S., Tapson, J., and Van Schaik, A.** (2017). EMNIST: Extending MNIST to Handwritten Letters. In *2017 International Joint Conference on Neural Networks (IJCNN)*, pages 2921–2926. IEEE.

9. **Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y.** (2012). Large Scale Distributed Deep Networks. *Advances in Neural Information Processing Systems*.

10. **Diakonikolas, I., Kamath, G., Kane, D., Li, J., Steinhardt, J., and Stewart, A.** (2019). SEVER: A Robust Meta-Algorithm for Stochastic Optimization. In *International Conference on Machine Learning*, pages 1596–1606.

11. **Diakonikolas, I., Kamath, G., Kane, D. M., Li, J., Moitra, A., and Stewart, A.** (2017). Being Robust (in High Dimensions) Can Be Practical. In *Proceedings of the 34th International Conference on Machine Learning-Volume 70*.

12. **Fang, M., Cao, X., Jia, J., and Gong, N. Z.** (2020). Local Model Poisoning Attacks to Byzantine-Robust Federated Learning. In *29th USENIX Security Symposium (USENIX Security 20)*, Boston, MA. USENIX Association.

13. **Ghosh, A., Hong, J., Yin, D., and Ramchandran, K.** (2019). Robust Federated Learning in a Heterogeneous Environment. *arXiv preprint arXiv:1906.06629*.

14. **He, L., Karimireddy, S. P., and Jaggi, M.** (2020). Byzantine-Robust Learning on Heterogeneous Datasets via Resampling. *arXiv preprint arXiv:2006.09365*.

15. **Jagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., and Li, B.** (2018). Manipulating Machine Learning: Poisoning Attacks and Countermeasures Against Regression Learning. *39th IEEE Symposium on Security and Privacy*.

16. **Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.** (2019). Advances and Open Problems in Federated Learning. *arXiv preprint arXiv:1912.04977*.

17. **Konečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., and Bacon, D.** (2016). Federated Learning: Strategies for Improving Communication Efficiency. *arXiv preprint arXiv:1610.05492*.

18. **Krizhevsky, A. and Hinton, G.** (2009). Learning Multiple Layers of Features from Tiny Images.

19. **Krizhevsky, A., Sutskever, I., and Hinton, G. E.** (2012). ImageNet Classification with Deep Convolutional Neural Networks. In *Advances in Neural Information Processing Systems*.

20. **Lai, K., Rao, A., and Vempala, S.** (2016). Agnostic Estimation of Mean and Covariance. In *2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)*.

21. **LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et al.** (1998). Gradient-Based Learning Applied to Document Recognition. *Proceedings of the IEEE*, 86(11):2278–2324.

22. **Li, J. Z.** (2018). Principled Approaches to Robust Machine Learning and Beyond. PhD thesis, Massachusetts Institute of Technology.

23. **Mahloujifar, S., Mahmoody, M., and Mohammed, A.** (2019). Universal Multi-Party Poisoning Attacks. In *Proceedings of the 36th International Conference on Machine Learning*, volume 97, pages 4274–4283.

24. **McMahan, H. B., Moore, E., Ramage, D., Hampson, S., and Aguera y Arcas, B.** (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. *Proceedings of the 20th International Conference on Artificial Intelligence and Statistics*.

25. **El Mhamdi, E. M., Guerraoui, R., and Rouault, S.** (2018). The Hidden Vulnerability of Distributed Learning in Byzantium. In *International Conference on Machine Learning*, pages 3518–3527.

26. **Muñoz-González, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E. C., and Roli, F.** (2017). Towards Poisoning of Deep Learning Algorithms with Back-Gradient Optimization. In *Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security*, pages 27–38. ACM.

27. **Muñoz-González, L., Co, K. T., and Lupu, E. C.** (2019). Byzantine-Robust Federated Machine Learning through Adaptive Model Averaging. *arXiv preprint arXiv:1909.05125*.

28. **Simonyan, K. and Zisserman, A.** (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. *arXiv preprint arXiv:1409.1556*.

29. **Sun, Z., Kairouz, P., Suresh, A. T., and McMahan, H. B.** (2019). Can You Really Backdoor Federated Learning? *arXiv preprint arXiv:1911.07963*.

30. **Tran, B., Li, J., and Madry, A.** (2018). Spectral Signatures in Backdoor Attacks. In *Advances in Neural Information Processing Systems*, pages 8000–8010.

31. **Xie, C., Koyejo, O., and Gupta, I.** (2018). Generalized Byzantine-Tolerant SGD. *arXiv preprint arXiv:1802.10116*.

32. **Xie, C., Koyejo, S., and Gupta, I.** (2019). Fall of Empires: Breaking Byzantine-Tolerant SGD by Inner Product Manipulation. *arXiv preprint arXiv:1903.03936*.

33. **Yin, D., Chen, Y., Ramchandran, K., and Bartlett, P.** (2018). Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates. In *Proceedings of the 35th International Conference on Machine Learning*.

### Appendix

#### A. Additional Experimental Results

- **Figure 6a**: Shows the results for the attack impacts of LIE, Fang, and our model poisoning attacks on the FEMNIST and MNIST datasets, when the percentage of malicious clients is varied from 5% to 24%. Detailed comparison is provided in Section VI-D2.
  
- **Table VII**: Presents the results for the empirical robustness of three state-of-the-art robust AGRs (Aggregation Rules) when coupled with the recently proposed resampling mechanism [19]. We note that similar to our DnC, resampling reduces the robustness of existing AGRs.

- **Figure 7**: Illustrates the impact of the three perturbation vectors introduced in Section IV-A on emulated (light bars) and actual FL settings. Detailed discussion is provided in Section VI-C2.

#### B. Proof of Lemma 1

The outlier detection guarantees of our robust AGR DnC depend on Lemma 1, which is a standard result in spectral-methods-based outlier detection [36], [28], [15], [11]. For completeness, we provide its proof motivated from [28], [36].

**Lemma 1**: Consider two distributions \( B \) and \( M \) with means \( \mu_B \) and \( \mu_M \), and covariances \( \Sigma_B \) and \( \Sigma_M \leq \sigma^2 I \). Let \( U = (1 - \epsilon)B + \epsilon M \) be a mixture of samples from \( B \) and \( M \). The following holds for any unit vector \( u \) due to Chebyshev's inequality:
\[ \Pr_{X \sim B} \left[ | \langle X - \mu_B, u \rangle | > t \right] \leq \frac{\sigma^2}{t^2} \]
\[ \Pr_{X \sim M} \left[ | \langle X - \mu_M, u \rangle | > t \right] \leq \frac{\sigma^2}{t^2} \]

Let \( \Delta = \mu_B - \mu_M \) and \( v \) be the top right singular eigenvector of \( U \). Ideally, the unit vector \( u \) should be a scaled version of \( \Delta \), which will maximally separate the samples from \( B \) and \( M \). But as argued in [36], any \( u \) with sufficient correlation with \( \Delta \) suffices, leading to:

**Lemma 2**: Let \( \alpha > 0 \) such that \( | \langle u, \Delta \rangle | > \alpha \sigma \sqrt{\epsilon} \). Then there exists \( t > 0 \) such that:
\[ \Pr_{X \sim B} \left[ | \langle X - \mu_B, u \rangle | > t \right] < \frac{\alpha^2}{\sigma \sqrt{\epsilon}} \]
\[ \Pr_{X \sim M} \left[ | \langle X - \mu_M, u \rangle | > t \right] < \frac{(\alpha - 1)^2}{\sigma \sqrt{\epsilon}} \]

**Lemma 3**: Under the assumptions of Lemma 1, we have:
\[ \langle \Delta, v \rangle^2 \geq 2 \sigma^2 \epsilon \]

**Proof**:
- **Covariance of \( U \)**:
  \[ \Sigma_U = (1 - \epsilon) \Sigma_B + \epsilon \Sigma_M + \epsilon (1 - \epsilon) \Delta \Delta^T \]
  \[ \Rightarrow \Sigma_U \succeq \epsilon (1 - \epsilon) \Delta \Delta^T \]
  \[ \Rightarrow \| \Sigma_U \|_2 \geq \epsilon (1 - \epsilon) \| \Delta \|^2 \]

- **Bounding \( \langle \Delta, v \rangle \)**:
  \[ \epsilon (1 - \epsilon) \| \Delta \|^2 \leq v^T \Sigma_U v \]
  \[ = (1 - \epsilon) v^T \Sigma_B v + \epsilon v^T \Sigma_M v + \epsilon (1 - \epsilon) \langle \Delta, v \rangle^2 \]
  \[ \leq \sigma^2 + \epsilon (1 - \epsilon) \langle \Delta, v \rangle^2 \]

  Assuming \( \sigma^2 \leq \epsilon \| \Delta \|^2 \):
  \[ \langle \Delta, v \rangle^2 \geq \left( 1 - \frac{1}{6(1 - \epsilon)} \right) \| \Delta \|^2 \geq \frac{2}{3} \| \Delta \|^2 \geq 2 \sigma^2 \epsilon \]

Finally, combining the results of Lemmas 2 and 3 gives us the result of Lemma 1.