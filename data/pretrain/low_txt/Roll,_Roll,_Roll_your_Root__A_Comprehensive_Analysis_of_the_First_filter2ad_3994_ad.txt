### 4.2.3 The User’s Perspective

From the above analysis, it is not possible to fully assess the actual impact on end users. During our measurements, 175 RIPE Atlas probes (representing approximately 1% of all vantage points) relied exclusively on resolvers that were issuing invalid responses (set B in Table 6). These probes were unable to receive valid responses at some point after the rollover. However, more than 70% of these probes experienced issues for only an hour or less. Additionally, 166 probes could rely on at least one other resolver to serve their queries and were thus not affected by the failing resolver.

Other research [36] indicates that users often switch to public DNS providers when they encounter issues with their ISP's resolver. To investigate this, we analyzed whether any vantage points switched to the public resolvers provided by Google, Cloudflare, or OpenDNS. We found only two such vantage points. One of these was using the resolver of the Irish ISP EIR, which experienced a well-publicized DNS outage [37] during the rollover. The DNS community speculated that this outage was caused by EIR’s resolvers failing validation. Using RIPE Atlas measurements, we identified the IP addresses of EIR’s resolvers and counted the number of DNSKEY queries sent to A/J Root per day (see Fig. 10). Starting from October 12th, the number of queries increased, reaching a peak one day after the rollover and returning to normal levels after three days. Given that RIPE Atlas probes actively switched resolvers at the same time, this strongly suggests that the EIR outage was indeed caused by validation errors. It is also noteworthy that the number of DNSKEY queries from EIR rose again after the removal of KSK-2010. This increase is discussed further in Section 4.3.2.

**Key Takeaways During the Roll:**
- We observed few resolvers with serious problems.
- Where such problems occurred, they were promptly resolved by operators.
- Less than 0.01% of the resolvers we monitored during the rollover experienced issues that lasted beyond our observation window.

### 4.3 After the Roll

We now discuss what happened after the rollover, from the point when all resolvers should have a DNSKEY RRset signed by KSK-2017, to the removal of KSK-2010 from the root zone.

#### 4.3.1 Revocation of KSK-2010

As discussed in Section 3.2, the Root Sentinel standard (RFC 8509) was published too late to be useful for the actual rollover. However, we can study the revocation of KSK-2010 with resolvers that adopted this protocol. Using all RIPE Atlas probes, we sent out Root Sentinel queries from August 2018 to August 2019. Fig. 11 shows the Root Sentinel signals observed over this period. The number of resolvers supporting Root Sentinel queries steadily increased to 2,419 resolvers in 720 ASs by mid-August 2019. This is encouraging given the early stage of deployment of the protocol. After the revocation of the old key (event V), the number of resolvers with KSK-2010 dropped to almost zero, while the number of resolvers with KSK-2017 continued to increase. Interestingly, about 20 resolvers continued to signal having KSK-2010 in their trust anchor store, suggesting either a manually configured trust anchor or a failure in their RFC 5011 processing. From mid-June 2019, KSK-2010 made a surprising comeback. We will explain this further in Section 4.3.4.

To provide a broader perspective, we also used Luminati to measure a total of 52,378 resolvers serving 589,928 exit nodes across 210 countries and 7,867 ASs over a period of 14 days starting from March 28th, 2019. We selected resolvers on which we could test all four combinations of Root Sentinel queries (cf. Table 3). This left us with 21,563 resolvers, to which 385,520 exit nodes sent queries at least once. We further categorized these resolvers into those that support Root Sentinel queries and those that do not. The vast majority—21,056 (97.63%) resolvers from 5,311 ASs—do not support RFC 8509, covering 330,891 (85.8%) exit nodes. Only 468 (2.2%) resolvers from 164 ASs support Root Sentinel queries and have only KSK-2017; these resolvers cover 33,266 (8.6%) exit nodes, indicating that a few large ASs, including Telenor (Norway), Bezeq (Israel), and Meo (South Africa), support RFC 8509 queries. Additionally, 39 resolvers (0.19%) still signal that they have KSK-2010 configured.

Comparing our observations through RIPE Atlas and Luminati, Fig. 12 shows the top 9 ASs with resolvers supporting RFC 8509 in our RIPE Atlas measurements. We found that 43 resolvers from AS2119 (Telenor), 10 from AS16276 (OVH), 10 from AS6830 (Liberty Global), and 2 from AS7922 (Comcast) were observed in the same state through both RIPE Atlas and Luminati. Fig. 12 also shows a surprising increase in KSK-2010 from June 2019, which we will explain in Section 4.3.4.

#### 4.3.2 Increase in DNSKEY Queries

As mentioned at the end of Section 4.2, we observed an increase in DNSKEY queries from certain resolvers at various stages of the roll. We analyze this phenomenon in more detail here, especially because of the sharp increase in queries after the revocation of KSK-2010, to the extent that at some point up to 10% of traffic to the root consisted of DNSKEY queries.

We start by analyzing the total amount of DNSKEY queries to the root. DNSSEC validators must regularly verify their locally configured trust anchors against the zone’s published DNSKEY records. In other words, validators periodically issue DNSKEY queries for the root zone. Due to the retry behavior of implementations, a validator with an out-of-date trust anchor is likely to send more than the normal amount of DNSKEY queries. This behavior was already observed in 2009—before the root zone was signed—during a KSK rollover for an in-addr.arpa zone operated by RIPE. The group investigating that incident called it “rollover and die” [38].

Just after the root KSK rollover on October 11th, 2018, root name servers observed an increase in DNSKEY queries. Fig. 13 shows the query rate for A/J Root. The increase was gradual, ramping up over the course of two days as the DNSKEY RRset timed out from resolver caches. Pre-rollover, the rate was around 15 million queries per day. Post-rollover, it increased five-fold to 75 million (①). An even more dramatic increase occurred when KSK-2010 was revoked (Event V in Fig. 2). Immediately after the revocation, A/J Root saw a sudden spike in DNSKEY queries (②), jumping from 75 million to over 200 million queries per day within 24 hours. The DNSKEY query rate continued to climb over the following weeks and months, exceeding one billion per day in March 2019 (③). At this point, DNSKEY queries comprised 7% of the total traffic received at A/J Root. The final phase of the rollover saw KSK-2010 removed from the root zone on March 22nd, 2019. Surprisingly, the DNSKEY query rate dropped dramatically immediately after KSK-2010 was removed. As Fig. 13 shows (④), the rate dropped and slowly crept back up to post-rollover levels as seen in October, November, and December 2018.

Fig. 13 only shows data for A/J Root. To confirm similar increases at other root servers, we used the RSSAC002 data (see Section 3.1). The RSSAC002 data does not have a dataset specifically identifying DNSKEY queries, but we can infer the presence of such queries by examining the response size dataset. Fig. 14 shows the percentage of responses between 1232–1472 bytes as solid lines. The dashed lines—marked A* and J*—are actual A/J Root traffic and show a strong correlation. Not all root servers saw the same increase in queries, but we currently lack sufficient information to explain this.

A deeper inspection of the A/J Root traffic shows vastly differing DNSKEY query patterns on a per-AS basis. Fig. 15 shows the average of multiple ASs whose DNSKEY queries exhibit distinct patterns at different times throughout the rollover. Some ASs expressed a systemic trend of increased DNSKEY queries post-rollover and even higher rates post-revocation (ASs-A). Other ASs only exhibited an increase in DNSKEY queries after the removal of KSK-2010 (ASs-B). Similarly, some ASs showed increased rates post-rollover until revocation (ASs-D) and again after removal (ASs-C). To better profile these resolvers, we issued version.bind queries to IP addresses expressing the various behaviors. While the response rate was low (4.3% of ±18K resolvers), the majority returned older versions of BIND (45% BIND 9.9.x, 34% BIND 9.8.x, and 13% BIND 9.10.x).

**Explaining the Increase in DNSKEY Queries:**
To find the cause of the increased query rates, we studied traffic coming from individual, high-volume sources. Outreach efforts at a global DNS scale are challenging, but we were able to contact multiple operators willing to help diagnose the DNSKEY query increase. One operator (a large French cloud hoster) stated that their servers were running BIND 9.8.2 on CentOS 6.7 and the logs contained large numbers of validation errors. Another set of sources identified as sending excessive DNSKEY queries to the root came from 8 addresses in a single subnet at a large midwestern university. Their staff quickly identified a DNS lab exercise that had been left running inside virtual machines (VMs). After shutting down the VMs, we confirmed that the excess DNSKEY traffic had stopped. From the university’s class instructions, we hypothesized that the DNSKEY query spikes were the result of ISC’s BIND software running in a specific state:
- The DNSSEC managed keys did not contain KSK-2017 but did contain KSK-2010.
- The dnssec-enable flag was set to false.
- The dnssec-validation flag was unset, leaving it in its default state of yes.

To verify this hypothesis, we performed experiments to test for bugs related to BIND’s behavior in the absence of a valid trust anchor. We set up a BIND 9.11.5-P4 resolver (the oldest supported release at the time), configuring it as per the university’s class instructions. We also ensured that BIND’s managed keys file contained only KSK-2010. Then, we ran 20 experiments in which we started a fresh copy of BIND configured as specified above. In each run, we sent ten sets of queries to BIND for test domains in seven TLDs at 30-second intervals, recording DNSKEY queries sent by the resolver, along with timestamps. Fig. 16 shows the results. Each experiment start time was normalized to zero and overlaid in Fig. 17, showing highly variable query patterns in each run (note experiments 7, 13, and 17).

Both plots show wide variations in the behavior of the resolver under test. At times, it behaves as expected, sending only a few DNSKEY queries after initializing. At other times, the resolver seems stuck in a state where every incoming request causes the resolver to send out a flurry of DNSKEY queries.

From the analysis of events V and VI, and the corresponding DNSKEY loads seen at the root (Fig. 13 and Fig. 14), we conclude that there are likely two different bugs causing the increase in queries. One bug is likely the cause of the increase in DNSKEY queries shortly after the rollover (event IV) and after KSK-2010 is removed (event VI). Another bug is likely the cause of the extreme query loads seen in Fig. 14, when KSK-2010 was present but with the revoke bit set. We have reached out to the developers of BIND to confirm our hypotheses but have not received any feedback as of September 13th, 2019.

What remains unclear is why operators have not noticed this broken resolver behavior, as we expect these resolvers to return SERVFAIL errors to every query. We speculate that only one resolver in a group is failing, with an alternate succeeding on behalf of their clients. This behavior is a well-known fact from other work [39]. To facilitate reproducibility, we published experiment configurations and scripts in a public GitHub repository [40].

**Increased Response Size:**
Another potential risk during the rollover is the increased response size. This is discussed in the next section.