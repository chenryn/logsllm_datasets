### Data Placement and Directory Service Implementation

We have full control over the placement of user data and updates. The Directory Service (DS) is implemented on the same Cassandra nodes, leveraging the underlying data store infrastructure. This distributed DS spans all servers to avoid potential bottlenecks.

### SPAR Middleware for Cassandra

The SPAR middleware for Cassandra is written using a Thrift interface. We will describe its functionality in the context of a canonical operation in Statusnet: retrieving the last 20 updates (tweets) for a given user. The middleware performs the following three operations:

1. **Directory Service Query**: Randomly selects a DS node and requests the location of the master replica of the user's data using the `get` primitive in Cassandra.
2. **Data Retrieval**: Connects to the node hosting the master replica and performs a `get-slice` operation to retrieve the update IDs of the 20 most recent status updates.
3. **Content Fetching**: Executes a `multi-get` to fetch the content of these updates and returns it to Statusnet.

### Performance Evaluation

In our evaluation, we compare the performance of Statusnet with the SPAR instantiation on Cassandra against the standard Cassandra with random partitioning. Our primary questions are:
- What impact does SPAR have on response time compared to random partitioning?
- How much does SPAR reduce network traffic?

#### Experimental Setup

To answer these questions, we conducted the following experiments:
- We randomly selected 40,000 users from the Twitter dataset and issued requests to retrieve the last 20 status updates at rates of 100, 200, 400, and 800 requests per second.
- These requests are not primitive `get/set` operations but application-level requests, simulating a user retrieving status updates from their friends.

#### Response Time Analysis

**Figure 10** shows the response times for SPAR and vanilla Cassandra with random partitioning. SPAR reduces the average response time by 77% at 400 requests per second. More importantly, SPAR can support 800 requests per second with the 99th percentile response time below 100 ms. In contrast, Cassandra with random partitioning can only handle about 200 requests per second with the same quality of service.

**Why SPAR Outperforms Random Partitioning:**
- **Reduced Network I/O**: Cassandra is affected by the delay of the worst-performing server due to heavy inter-server traffic for remote reads. SPAR, by design, ensures that all relevant data is local, eliminating the need for remote reads.
- **Improved Memory Hit Ratio**: Non-random partitioning in SPAR leads to a higher memory hit ratio. When a user's data is read, it also brings in the data of their friends, which are likely to be on the same server. This results in better cache utilization and reduced disk I/O.

#### Network Load Analysis

**Figure 11** depicts the aggregate network activity under various request rates. For random partitioning, requests are spread across multiple nodes, significantly increasing network load. SPAR reduces network traffic by a factor of 8 compared to vanilla Cassandra (for 400 requests per second).

### Evaluation with MySQL

We also evaluated SPAR with MySQL, a traditional RDBMS. The key question is whether SPAR can scale an OSN application using MySQL, allowing developers to continue using familiar RDBMS frameworks without worrying about scaling issues.

#### Experimental Setup

- **MySQL Version and Schema**: We used MySQL version 5.5 with the SQL schema provided by Statusnet, including tables for users, social graph, updates, and user-specific update lists.
- **Stress Testing**: We used Tsung and two servers to emulate the activity of thousands of concurrent users, generating both read and write requests.

#### Comparison to Full Replication

- **Full Replication**: Loading the entire Twitter dataset on all 16 servers resulted in poor performance, with average 95th percentile response times of 113 ms for 16 requests per second, 151 ms for 160 requests per second, and 245 ms for 320 requests per second.
- **SPAR with MySQL**: The cluster can serve more than 2,500 requests per second with a 99th percentile response time below 150 ms, demonstrating that SPAR can handle Twitter-scale read loads with a small cluster of commodity machines.

#### Adding Writes

- **Insertions**: We introduced insertions at a rate of 16 updates per second (1 update per machine). Grouping inserts and controlling the rate allowed us to achieve a 95th percentile response time below 260 ms for 50 read requests per second and below 380 ms for 200 read requests per second, with a median response time of around 2 ms.
- **Scalability**: Performance improves as more machines are added.

### Related Work

- **Scaling Out**: Cloud providers like Amazon EC2 and Google AppEngine offer transparent scaling for stateless applications. SPAR addresses the challenge of scaling the backend when data is not independent.
- **Key-Value Stores**: While popular for scalability, Key-Value stores often rely on random partitioning, leading to poor performance for OSN workloads. SPAR minimizes network I/O by keeping relevant data local.
- **Distributed File Systems and Databases**: Systems like Ficus, Coda, and Farsite replicate files for high availability. Distributed RDBMS systems like MySQL Cluster and Bayou provide eventual consistency. SPAR maintains data locally via replication, suitable for OSNs where data is frequently fetched from multiple servers.

### Conclusions

Scaling OSNs is challenging due to the interconnected nature of user data. SPAR addresses this by partitioning the social graph and replicating data at the user level, ensuring local data semantics. This approach enables transparent scaling, increases throughput, and reduces network traffic. We validated SPAR using real datasets from three different OSNs and demonstrated significant gains in throughput and reduced network traffic on both MySQL and Cassandra.

### Acknowledgments

We thank the anonymous reviewers and our shepherd Yin Zhang for their valuable feedback. Special thanks to Evan Weaver, Zografoula Vagena, and Ravi Sundaram for their early comments and feedback.

### References

[1] Facebook’s memcached multiget hole: More machines != more capacity.  
[2] Friendster lost lead due to failure to scale.  
[3] Notes from scaling mysql up or out.  
[4] Rightscale.  
[5] Status net.  
[6] Tsung: Distributed load testing tool.  
[7] Twitter architecture.  
[8] A. Adya, W. J. Bolosky, M. Castro, G. Cermak, R. Chaiken, J. R. Douceur, Jon, J. Howell, J. R. Lorch, M. Theimer, and R. P. Wattenhofer. Farsite: Federated, available, and reliable storage for an incompletely trusted environment. In OSDI 02.  
[9] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. H. Katz, A. Konwinski, G. Lee, D. A. Patterson, A. Rabkin, I. Stoica, and M. Zaharia. Above the clouds: A berkeley view of cloud computing. Technical Report UCB/EECS-2009-28.  
[10] S. Arora, S. Rao, and U. Vazirani. Expander flows, geometric embeddings and graph partitioning. J. ACM, 56(2):1–37, 2009.  
[11] F. Benevenuto, T. Rodrigues, M. Cha, and V. Almeida. Characterizing user behavior in online social networks. In Proc. of IMC ’09, pages 49–62, New York, NY, USA, 2009. ACM.  
[12] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks. J.STAT.MECH., page P10008, 2008.  
[13] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels. Dynamo: amazon’s highly available key-value store. SIGOPS Oper. Syst. Rev., 41(6):205–220, 2007.  
[14] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman & Co., New York, NY, USA, 1979.  
[15] R. G. Guy, J. S. Heidemann, W. Mak, T. W. Page, Jr., G. J. Popek, and D. Rothmeier. Implementation of the Ficus replicated file system. In USENIX Conference Proceedings, 1990.  
[16] J. Hamilton. Geo-replication at facebook.  
[17] J. Hamilton. Scaling linkedin.  
[18] HighScalability.com. Why are facebook, digg and twitter so hard to scale?  
[19] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM J. Sci. Comput., 20(1):359–392, 1998.  
[20] H. Kwak, Y. Choi, Y.-H. Eom, H. Jeong, and S. Moon. Mining communities in networks: a solution for consistency and its evaluation. In ACM IMC ’09.  
[21] H. Kwak, C. Lee, H. Park, and S. Moon. What is twitter, a social network or a news media? 2010.  
[22] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: Densification and shrinking diameters. ACM Transactions on KDD, 1:1, 2007.  
[23] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney. Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters. CoRR, abs/0810.1355, 2008.  
[24] N. Media. Growth of twitter.  
[25] A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel, and B. Bhattacharjee. Measurement and analysis of online social networks. In ACM IMC ’07.  
[26] M. Newman and J. Park. Why social networks are different from other types of networks. Phys. Rev. E, 68:036122, 2003.  
[27] M. E. J. Newman. Modularity and community structure in networks. PNAS, 103:8577, 2006.  
[28] J. M. Pujol, V. Erramilli, and P. Rodriguez. Divide and conquer: Partitioning online social networks. http://arxiv.org/abs/0905.4918v1, 2009.  
[29] J. M. Pujol, G. Siganos, V. Erramilli, and P. Rodriguez. Scaling online social networks without pains. In Proc of NETDB, 2009.  
[30] J. Rothschild. High performance at massive scale - lessons learned at facebook.  
[31] M. Satyanarayanan. Coda: A highly available file system for a distributed workstation environment. IEEE Transactions on Computers, 39:447–459, 1990.  
[32] F. Schneider, A. Feldmann, B. Krishnamurthy, and W. Willinger. Understanding online social network usage from a network perspective. In IMC ’09.  
[33] D. B. Terry, M. M. Theimer, K. Petersen, A. J. Demers, M. J. Spreitzer, and C. H. Hauser. Managing update conflicts in bayou, a weakly connected replicated storage system. In ACM SOSP ’95.  
[34] B. Viswanath, A. Mislove, M. Cha, and K. P. Gummadi. On the evolution of user interaction in facebook. In Proc of WOSN’09.