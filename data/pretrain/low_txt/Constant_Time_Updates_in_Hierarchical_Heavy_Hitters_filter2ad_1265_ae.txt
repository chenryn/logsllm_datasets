### Optimized Text

The expression is less than \(2Z_{1-\frac{\delta}{8}}\).

We denote by \(K\) the set of packets that may affect \(C_q|P\). Since the expression for \(C_q|P\) is not monotonic, we split \(K\) into two sets: \(K^+\) and \(K^-\). The set \(K^+\) contains packets that positively affect \(C_q|P\), while \(K^-\) contains packets that negatively affect it.

Similarly, we define \(\{Y_K\}\) as Poisson random variables representing the number of packets from \(K\) in each bin. We do not know the exact number of bins that affect the sum, but we are certain that there are no more than \(N\) balls. We define the random variable \(Y_K^+\) to represent the number of packets from \(K\) that fall into the corresponding bins and have a positive impact on \(C_q|P\). By invoking Lemma 6.3 on \(Y_K^+\), we get:

\[
\Pr\left(\left|Y_K^+ - \mathbb{E}[Y_K^+]\right| \geq Z_{1-\frac{\delta}{8}}\sqrt{N V} + \frac{N V}{4}\right) \leq \frac{\delta}{2}
\]

Applying Lemma 6.3 to both \(Y_K^+\) and \(Y_K^-\), we obtain:

\[
\Pr\left(C_q|P \geq C_q|P \right) \leq \Pr\left(\left|Y_K^+ - \mathbb{E}[Y_K^+]\right| \geq Z_{1-\frac{\delta}{8}}\sqrt{N V} + \frac{N V}{4}\right) + \Pr\left(\left|Y_K^- - \mathbb{E}[Y_K^-]\right| \geq Z_{1-\frac{\delta}{8}}\sqrt{N V} + \frac{N V}{4}\right)
\]

\[
\Pr\left(C_q|P \geq C_q|P \right) \leq \frac{\delta}{2} + \frac{\delta}{2} = \delta
\]

This completes the proof.

### Proof
The proof follows from Theorem 6.13 in one dimension or Theorem 6.17 in two dimensions, which guarantees that in both cases:

\[
\Pr\left(C_q|P \geq C_q|P \right) \leq 1 - \delta
\]

### 6.3 RHHH Properties Analysis
Finally, we can prove the main result of our analysis. It establishes that if the number of packets is sufficiently large, RHHH is correct.

**Theorem 6.19.** If \(N > \psi\), then RHHH solves the \((\delta, \epsilon, \theta)\)-Approximate Hierarchical Heavy Hitters problem.

**Proof.** The theorem is proved by combining Lemma 6.8 and Corollary 6.18.

Note that \(\psi \approx Z_{1-\frac{\delta s}{2}}^{-2}\) contains the parameter \(V\). When the minimal measurement interval is known in advance, the parameter \(V\) can be set to ensure correctness at the end of the measurement. For short measurements, we may need to use \(V = H\), while longer measurements justify using \(V \gg H\) to achieve better performance. With modern line speeds and emerging transmission technologies, this speedup capability is crucial because faster lines deliver more packets in a given amount of time, justifying a larger value of \(V\) for the same measurement interval.

For completeness, we prove the following:

**Theorem 6.20.** RHHH’s update complexity is \(O(1)\).

**Proof.** Observe Algorithm 1. For each update, we randomize a number between 0 and \(V-1\), which can be done in \(O(1)\). If the number is smaller than \(H\), we also update a Space Saving instance, which can be done in \(O(1)\) [34].

**Theorem 6.21.** The space complexity of RHHH is \(O\left(\frac{H}{\epsilon_a}\right)\) table entries.

**Proof.** RHHH utilizes \(H\) separate instances of Space Saving, each using \(\frac{1}{\epsilon_a}\) table entries. There are no other significant data structures.

### 7 DISCUSSION
This work focuses on realizing hierarchical heavy hitters measurement in virtual network devices. Existing HHH algorithms are too slow to cope with current improvements in network technology. Therefore, we define a probabilistic relaxation of the problem and introduce a matching randomized algorithm called RHHH. Our algorithm leverages the massive traffic in modern networks to perform simpler update operations. Intuitively, the algorithm replaces the traditional approach of computing all prefixes for each incoming packet by sampling (if \(V > H\)) and then choosing one random prefix to be updated. While similar convergence guarantees can be derived for the simpler approach of updating all prefixes for each sampled packet, our solution has the clear advantage of processing elements in \(O(1)\) worst-case time.

We evaluated RHHH on four real Internet packet traces, consisting of over 1 billion packets each, and achieved a speedup of up to 62 times compared to previous works. Additionally, we showed that the solution quality of RHHH is comparable to that of previous work. RHHH performs updates in constant time, an asymptotic improvement from previous works whose complexity is proportional to the hierarchy’s size. This is especially important in the two-dimensional case as well as for IPv6 traffic that requires larger hierarchies.

Finally, we integrated RHHH into a DPDK-enabled Open vSwitch and evaluated its performance as well as the alternative algorithms. We provided a dataplane implementation where HHH measurement is performed as part of the per-packet routing tasks. In a dataplane implementation, RHHH is capable of handling up to 13.8 Mpps, 4% less than an unmodified DPDK OVS (that does not perform HHH measurement). We showed a throughput improvement of 2.5 times compared to the fastest dataplane implementations of previous works.

Alternatively, we evaluated a distributed implementation where RHHH is realized in a virtual machine that can be deployed in the cloud, and the virtual switch only sends the sampled traffic to RHHH. Our distributed implementation can process up to 12.3 Mpps. It is less intrusive to the switch and offers greater flexibility in virtual machine placement. Most importantly, our distributed implementation is capable of analyzing data from multiple network devices.

Notice the performance improvement gap between our direct implementation—62 times, compared to the performance improvement when running over OVS—2.5 times. In the case of the OVS experiments, we were running over a 10 Gbps link and were bound by that line speed—the throughput obtained by our implementation was only 4% lower than the unmodified OVS baseline (that does nothing). In contrast, previous works were clearly bounded by their computational overhead. Thus, one can anticipate that once we deploy the OVS implementation on faster links or in a setting that combines traffic from multiple links, the performance boost compared to previous work will be closer to the improvement we obtained in the direct implementation.

A downside of RHHH is that it requires some minimal number of packets to converge to the desired formal accuracy guarantees. In practice, this is a minor limitation as busy links deliver many millions of packets every second. For example, in the settings reported in Section 4.1, RHHH requires up to 100 million packets to fully converge, yet even after as little as 8 million packets, the error reduces to around 1%. With a modern switch that can serve 10 million packets per second, this translates into a 10-second delay for complete convergence and around 1% error after 1 second. As line rates continue to improve, these delays would become even shorter accordingly. The code used in this work is open-sourced [4].

**Acknowledgments.** We thank Ori Rottenstreich for his insightful comments and Ohad Eytan for helping with the code release. We would also like to thank the anonymous reviewers and our shepherd, Michael Mitzenmacher, for helping us improve this work.

This work was partially funded by the Israeli Science Foundation grant #1505/16 and the Technion-HPI research school. Marcelo Caggiani Luizelli is supported by the research fellowship program funded by CNPq (201798/2015-8).

### REFERENCES
[1] Intel DPDK, http://dpdk.org/.
[2] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Matus, Rong Pan, Navindra Yadav, and George Varghese. 2014. CONGA: Distributed Congestion-aware Load Balancing for Datacenters. In ACM SIGCOMM. 503–514.
[3] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal Near-optimal Datacenter Transport. ACM SIGCOMM (2013), 435–446.
[4] Ran Ben Basat. RHHH code. Available: https://github.com/ranbenbasat/RHHH.
[5] Ran Ben-Basat, Gil Einziger, Roy Friedman, and Yaron Kassner. 2016. Heavy Hitters in Streams and Sliding Windows. In IEEE INFOCOM.
[6] Ran Ben-Basat, Gil Einziger, Roy Friedman, and Yaron Kassner. 2017. Optimal elephant flow detection. In IEEE INFOCOM.
[7] Ran Ben-Basat, Gil Einziger, Roy Friedman, and Yaron Kassner. 2017. Randomized Admission Policy for Efficient Top-k and Frequency Estimation. In IEEE INFOCOM.
[8] Theophilus Benson, Ashok Anand, Aditya Akella, and Ming Zhang. 2011. MicroTE: Fine Grained Traffic Engineering for Data Centers. In ACM CoNEXT.
[9] Moses Charikar, Kevin Chen, and Martin Farach-Colton. 2002. Finding Frequent Items in Data Streams. In EATCS ICALP. 693–703.
[10] Graham Cormode and Marios Hadjieleftheriou. 2008. Finding Frequent Items in Data Streams. VLDB 1, 2 (Aug. 2008), 1530–1541.
[11] Graham Cormode and Marios Hadjieleftheriou. 2010. Methods for Finding Frequent Items in Data Streams. J. VLDB 19, 1 (2010), 3–20.
[12] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. 2003. Finding Hierarchical Heavy Hitters in Data Streams. In VLDB. 464–475.
[13] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. 2004. Diamond in the Rough: Finding Hierarchical Heavy Hitters in Multi-dimensional Data. In SIGMOD. 155–166.
[14] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. 2008. Finding Hierarchical Heavy Hitters in Streaming Data. ACM Trans. Knowl. Discov. Data 1, 4 (Feb. 2008), 2:1–2:48.
[15] Graham Cormode and S. Muthukrishnan. 2005. An Improved Data Stream Summary: The Count-min Sketch and Its Applications. J. Algorithms (2005), 18.
[16] Andrew R. Curtis, Jeffrey C. Mogul, Jean Tourrilhes, Praveen Yalagandula, Puneet Sharma, and Sujata Banerjee. 2011. DevoFlow: Scaling Flow Management for High-performance Networks. In ACM SIGCOMM. 254–265.
[17] Erik D. Demaine, Alejandro López-Ortiz, and J. Ian Munro. 2002. Frequency Estimation of Internet Packet Streams with Limited Space. In EATCS ESA.
[18] Gil Einziger and Roy Friedman. 2014. TinyLFU: A Highly Efficient Cache Admission Policy. In Euromicro PDP. 146–153.
[19] Gil Einziger and Roy Friedman. 2016. Counting with TinyTable: Every Bit Counts!. In ACM ICDCN.
[20] Gil Einziger, Marcelo Caggiani Luizelli, and Erez Waisbard. 2017. Constant Time Weighted Frequency Estimation for Virtual Network Functionalities. In IEEE ICCCN.
[21] Paul Emmerich, Sebastian Gallenmüller, Daniel Raumer, Florian Wohlfart, and Georg Carle. 2015. MoonGen: A Scriptable High-Speed Packet Generator. In ACM IMC. 275–287.
[22] Pedro Garcia-Teodoro, Jesus E. Diaz-Verdejo, Gabriel Macia-Fernandez, and E. Vazquez. 2009. Anomaly-Based Network Intrusion Detection: Techniques, Systems and Challenges. Computers and Security (2009), 18–28.
[23] John Hershberger, Nisheeth Shrivastava, Subhash Suri, and Csaba D. Tóth. 2005. Space Complexity of Hierarchical Heavy Hitters in Multi-dimensional Data Streams. In ACM PODS. 338–347.
[24] Paul Hick. CAIDA Anonymized 2013 Internet Trace, equinix-sanjose 2013-12-19 13:00-13:05 UTC, Direction B.
[25] Paul Hick. CAIDA Anonymized 2014 Internet Trace, equinix-sanjose 2013-06-19 13:00-13:05 UTC, Direction A.
[26] Paul Hick. CAIDA Anonymized 2015 Internet Trace, equinix-chicago 2015-12-17 13:00-13:05 UTC, Direction A.
[27] Paul Hick. CAIDA Anonymized 2016 Internet Trace, equinix-chicago 2016-02-18 13:00-13:05 UTC, Direction B.
[28] Lavanya Jose and Minlan Yu. 2011. Online Measurement of Large Traffic Aggregates on Commodity Switches. In USENIX Hot-ICE.
[29] Abdul Kabbani, Mohammad Alizadeh, Masato Yasuda, Rong Pan, and Balaji Prabhakar. 2010. AF-QCN: Approximate Fairness with Quantized Congestion Notification for Multi-tenanted Data Centers. In IEEE HOTI. 58–65.
[30] Richard M. Karp, Scott Shenker, and Christos H. Papadimitriou. 2003. A Simple Algorithm for Finding Frequent Elements in Streams and Bags. ACM Transactions Database Systems 28, 1 (March 2003).
[31] Yuan Lin and Hongyan Liu. 2007. Separator: Sifting Hierarchical Heavy Hitters Accurately from Data Streams. In ADMA (ADMA). 170–182.
[32] Nishad Manerikar and Themis Palpanas. 2009. Frequent Items in Streaming Data: An Experimental Evaluation of the State-of-the-art. Data Knowl. Eng. (2009), 415–430.
[33] Gurmeet Singh Manku and Rajeev Motwani. 2002. Approximate Frequency Counts over Data Streams. In VLDB.
[34] Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. 2005. Efficient Computation of Frequent and Top-k Elements in Data Streams. In ICDT.
[35] M. Mitzenmacher, T. Steinke, and J. Thaler. 2012. Hierarchical Heavy Hitters with the Space Saving Algorithm. In Proceedings of the Meeting on Algorithm Engineering & Experiments (ALENEX). 160–174.
[36] Michael Mitzenmacher and Eli Upfal. 2005. Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cambridge University Press, New York, NY, USA.
[37] S. Muthukrishnan. 2005. Data Streams: Algorithms and Applications. Foundations and Trends in Theoretical Computer Science 1, 2 (2005), 117–236.
[38] V.V. Patil and H.V. Kulkarni. 2012. Comparison of confidence intervals for the Poisson mean: Some new aspects. REVSTAT Statistical Journal 10, 2 (June 2012), 211–227.
[39] Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno Rajahalme, Jesse Gross, Alex Wang, Joe Stringer, Pravin Shelar, Keith Amidon, and Martin Casado. 2015. The Design and Implementation of Open vSwitch. In USENIX NSDI. 117–130.
[40] Neil C. Schwertman and Ricardo A. Martinez. 1994. Approximate poisson confidence limits. Communications in Statistics - Theory and Methods 23, 5 (1994), 1507–1529.
[41] Vyas Sekar, Nick Duffield, Oliver Spatscheck, Jacobus van der Merwe, and Hui Zhang. 2006. LADS: Large-scale Automated DDOS Detection System. In USENIX ATEC. 16–16.
[42] Vibhaalakshmi Sivaraman, Srinivas Narayana, Ori Rottenstreich, S. Muthukrishnan, and Jennifer Rexford. 2017. Heavy-Hitter Detection Entirely in the Data Plane. In Proceedings of the Symposium on SDN Research (ACM SOSR). 164–176.
[43] P. Truong and F. Guillemin. 2009. Identification of heavyweight address prefix pairs in IP traffic. In ITC. 1–8.
[44] David P. Woodruff. 2016. New Algorithms for Heavy Hitters in Data Streams (Invited Talk). In ICDT.
[45] L. Ying, R. Srikant, and X. Kang. 2015. The Power of Slightly More than One Sample in Randomized Load Balancing. In IEEE INFOCOM. 1131–1139.
[46] Yin Zhang, Sumeet Singh, Subhabrata Sen, Nick Duffield, and Carsten Lund. 2004. Online Identification of Hierarchical Heavy Hitters: Algorithms, Evaluation, and Applications. In ACM IMC. 101–114.