### Feature Groups and Model Characteristics

1. **Ease of Integration**: The model allows for the seamless addition and combination of feature groups as weak learners.
2. **Versatility**: It can be used for both classification and regression tasks.
3. **Interpretability**: The model is interpretable, with strong mathematical justifications for its predictive power, and it provides the ability to compute and analyze global feature importance.
4. **Performance**: As demonstrated in §6.3, the model outperforms other classical machine learning methods such as Random Forest (RF) and \( k \)-Nearest Neighbors (KNN), which have been proposed for 3G/4G signal strength and bandwidth prediction problems [20, 34, 54, 60].

### Seq2Seq ML Models

Initially designed for natural language processing and machine translation, Seq2Seq learning has become widely used for solving various high-dimensional time series prediction problems [49, 50, 61]. Unlike standard Long Short-Term Memory (LSTM) models [35], Seq2Seq allows for the modeling of an arbitrary length of the predicted output sequence, enabling predictions over a longer horizon.

Formally, let \( X_t = \{x_1, x_2, \ldots, x_t\} \) be a sequence of input features known a priori at time \( t \), where each \( x_t \) is a feature vector. Let \( Y_t = \{y_1, y_2, \ldots, y_k\} \) be a sequence of \( k \) outputs to be predicted. In our case, \( Y_t \) represents a sequence of future throughput values to be predicted over the next \( k \) time slots. The time slots are defined based on the specific prediction problem (e.g., seconds for short-term prediction or minutes/hours for long-term prediction).

In our design of the Seq2Seq ML models (see Figure 15 for an illustration), we incorporate an encoder-decoder architecture using an LSTM-type network. Our models can handle different feature groups represented as sequences of high-dimensional inputs.

### Performance Evaluation

Using the proposed Lumos5G framework for 5G throughput prediction, we evaluate the performance of GDBT and Seq2Seq models with different feature groups and their combinations. We also compare our models with several other analytical and ML models proposed in the literature for 3G/4G signal strength and throughput prediction.

#### 6.1 Evaluation Framework

We begin by presenting the model setups and evaluation metrics used in our evaluation framework.

**Model Setups for GDBT & Seq2Seq:**
- **Hyperparameter Tuning**: We perform grid search to tune hyperparameters for both Seq2Seq and GDBT models using throughput traces from a new area not part of the training or testing data. Although the models were robust to multiple hyperparameter values, we selected the set that provided the best performance.
- **GDBT Models**: We use a gradient boosting regressor (and classifier) with 8000 estimators, bounded by a depth of size 8, and a learning rate of 0.01.
- **Seq2Seq Models**: We use a two-layer LSTM Encoder-Decoder architecture with 128 hidden units. We run Seq2Seq experiments for 2000 epochs with a batch size of 256. The input and output sequence lengths are set to 20. To obtain classification results, we associate the predicted throughput with a throughput class during post-processing.
- **Data Splitting**: For both GDBT and Seq2Seq, we randomly split our datasets using a 70/30 ratio for training and testing, respectively. The loss function is mean-squared-error (MSE). All experiments are run on a single machine with an Intel Core i7-6850K (12-core) CPU and 2× NVIDIA TITAN V GPUs. Training times vary depending on the dataset size, with Seq2Seq taking 6 to 44 hours and GDBT taking 10-30 minutes.

**Evaluation Metrics:**
- **Regression**: We use Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).
- **Classification**: We consider the weighted average F1 score as the main metric. Additionally, we use recall to evaluate the low-throughput class (i.e., below 300 Mbps). Recall is defined as True Positives / (True Positives + False Negatives). Misclassifying low-throughput as high-throughput can lead to more significant Quality of Experience (QoE) degradation, such as video stalls, compared to misclassifying high-throughput as low (e.g., only video quality degradation without a stall). Therefore, we prefer a high recall value for the low-throughput class.

#### 6.2 Results and Observations

Table 7 shows the classification results for both GDBT and Seq2Seq models under different feature groupings, while Table 8 shows the regression results. Datasets collected from three areas under stationary+walking (4-way Intersection & Airport) and stationary+walking+driving (1300m Loop) mobility scenarios are used for training and testing. We also build a model by combining data from all areas with known 5G panel locations into a single dataset, referred to as Global. For GDBT, the prediction is based on current feature values, whereas for Seq2Seq, recent feature history values (i.e., a sequence of feature values) are used for prediction.

The classification results in Table 7 contain two values in each cell: the weighted average F1-score and recall of the low-throughput class [0, 300) Mbps. For the 1300m Loop, no results are reported for T+M and T+M+C due to unreliable 5G panel location information. Table 8 shows the regression results for all areas, and Figure 16 displays sample regression prediction plots for the L+M+C feature group on the Global dataset using GDBT and Seq2Seq, with ± 200 Mbps error bounds shaded.

**Key Observations:**
- Both Seq2Seq and GDBT achieve good overall prediction results, especially when feature group combinations include additional UE-side features beyond geolocation.
- Location-based feature groups alone are generally inadequate for high prediction accuracy, especially under high mobility.
- By combining additional features from mobility and/or connection-related feature groups, the weighted average F1 scores for both GDBT and Seq2Seq throughput class predictions are consistently above 0.89, except for one L+M result for GDBT at the Loop area.
- The Seq2Seq model produces slightly better prediction results than GDBT, likely due to:
  - The use of a sequence of past feature values, which indicates the benefits of incorporating historical data.
  - The stronger representation power of LSTM-based general-purpose encoder-decoders [37, 59] compared to GDBT.
- This is best demonstrated in the regression results shown in Table 8, where Seq2Seq generally has lower MAEs and RMSEs.

**Transferability Analysis:**
- Comparing feature groups (L+M vs. T+M and L+M+C vs. T+M+C), we see that prediction results using tower-based (T*) features, which are location-agnostic, match those using location-based (L*) features.
- A key advantage of using T-based feature groups is that ML models trained on one area may be transferable to another if both share similar environments. This is demonstrated in the results, showing the potential for model transferability.

### Conclusion

Our evaluation demonstrates that both GDBT and Seq2Seq models, particularly when combined with additional UE-side features, can achieve high prediction accuracy for 5G throughput. The Seq2Seq model, with its ability to incorporate historical data and stronger representation power, generally outperforms GDBT. The results also highlight the potential for model transferability using tower-based feature groups.