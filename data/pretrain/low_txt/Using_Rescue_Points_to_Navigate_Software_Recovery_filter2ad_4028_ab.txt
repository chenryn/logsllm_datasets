### 3.2 Rescue Point Selection and Error Virtualization

Functions that return pointers require a more thorough inspection of the underlying data structures to determine the values of their return types, beyond simply checking for `NULL`. Empirical evidence suggests that C programs often use integer return types as failure indicators. After filtering functions, we obtain the final set of candidate rescue points. The candidate rescue-point graph is then used to identify potential rescue points.

The function where the fault occurred serves as the root of the rescue graph. For each node in this graph, we replay the input that caused the failure and attempt error virtualization at each node. With the protection mechanism in place, faults are "caught" by the application monitor, and the program state is rolled back to the rescue point. The rescue point then examines the rescue graph to determine which value to force as a return, derived from the return values of the rescue-point function.

Using the example in Figure 2, when an error is detected in the function `bad()`, we first extract the call stack, which includes functions `bar()` and `foo()`. Assuming the rescue graph contains all functions found in the call stack, we initiate error virtualization with the root node, `bad()`. We iterate through the rescue graph, attempting error virtualization on nodes `bad()` and `bar()`, until we reach the function `foo()`, which recovers program execution without side effects.

If there is no overlap between the rescue and function call graphs, alternative recovery mechanisms can be employed. One option is to recover to a programmer-annotated point. Another is to iterate through the call stack of the vulnerable function (potentially all the way to `main`) until a suitable rescue point is found, i.e., one that does not crash the application using the heuristics described in previous work [20].

### 3.3 Fault Detection Monitors

We treat the fault-detection component as a black box, which only needs to notify the fault monitor of the occurrence of a fault. In addition to standard operating system error handling (e.g., illegal memory dereferences), we use additional mechanisms for detecting memory errors. Several fault detection components, such as ProPolice [9] and TaintCheck [12], can detect memory errors, while others, like those in [1, 11], detect violations of underlying security policies.

For our system, we use two previously developed fault detection components [19, 20] that offer trade-offs between performance overhead and the range of faults they can detect. While we assume source-code availability, future work will address the applicability to commercial off-the-shelf (COTS) software.

### 3.4 Decision: Hypothesis Testing

Once a candidate rescue point is selected, the system proceeds to the patch testing and analysis phase. The rescue-enabled version of the application is restarted and supplied with the input that caused the fault (or the N most recent inputs if the specific one cannot be easily identified, where N is a configurable parameter). If the application crashes, a new fix is created using the next available candidate rescue point, and the testing and analysis phase is repeated.

Initially, we focus on failures with a one-to-one correspondence between inputs and failures, rather than those caused by a combination of inputs. However, many of the latter type of failures are addressed by our system because the last input (and code leading to the failure) will be recognized as problematic and handled as discussed.

If the fix does not introduce any faults that cause the application to crash, the application is examined for semantic bugs using a set of user-supplied tests. These tests provide some level of confidence in the semantic correctness of the generated patch. For example, an online vendor could run tests to ensure that client orders can be submitted and processed by the system.

### 4. Discussion

One of the most critical concerns in recovering from software faults and vulnerability exploits is ensuring the consistency and correctness of program data and state. This issue is present in most recovery efforts. The presence of rescue points, whether derived automatically or with programmer assistance, can alleviate most concerns about unpredictable execution paths but cannot completely eliminate them. In this section, we examine the advantages and disadvantages of fault recovery in general and error virtualization in particular.

#### Fault Response

A fault recovery mechanism must evaluate and choose a response from a wide array of options. When encountering a fault, a system can choose from the following: crash [9], crash and be restarted by a monitor [4, 5], return arbitrary values [16, 17], change the environment and replay [14], slice off the functionality [19, 20], or jump to a safe (rescue) point and force an error. Previous approaches focused on crash-based methods, assuming no acceptable alternative. More recent work [17, 20] has shown that alternative reactive techniques can work well in practice. We elect to use the last approach of recovering execution to safe points and forcing errors. Early experimentation has shown that this choice works extremely well, even at the machine instruction level [21].

However, there is a fundamental problem in choosing a particular response. Since the high-level behavior of any system cannot be algorithmically determined, the system must avoid cases where the response would take execution down a semantically incorrect path. An example of this is skipping a check in `sshd` that would allow an unauthenticated user to gain access to the system. We posit that through the use of rescue points, we can minimize (though not eliminate) the uncertainty that a program will go down an unexpected execution path. This is because we use as recovery points positions in the program known to propagate errors. For higher assurance, programmers can provide annotations to specify which parts of the code should be used for recovery and which should not be circumvented.

#### Programming with Error Virtualization

In this paper, we focus on fully automated techniques for every aspect of our system. However, it would be imprudent to dismiss the use of programmer assistance in program recovery. Programmers can design software with error virtualization in mind, assigning specific locations in the code as rescue points that propagate faults gracefully. Programmer insight is difficult to replicate with automated techniques, especially in code cleanup and efficiency. We envision that programming with error virtualization will be easier than dealing with language-specific constructs, such as exception handling, as attention can be focused on a few select program points.

#### Applicability to Safe Languages

A pressing question when discussing techniques to protect legacy applications written in unsafe languages is: Can we avoid the problems by using a safe dialect? Having appropriate language constructs for handling errors (exceptions) solves some issues but is far from a panacea, especially for large, evolving systems where complexity makes it difficult to cover all corner cases. Our approach can be applied to such systems by creating a map between the finite set of existing error handling capabilities and the infinite set of future add-on capabilities. Systems often start with good intentions to cover error cases, but as features are added, the complexity of examining all cases becomes prohibitively large.

#### Availability

One of the principal goals of our work is to reduce system downtime and increase service availability in the face of failures and attacks. We anticipate that the reduction in downtime will be significant; error virtualization relies on fault detection monitors detecting faults, finding appropriate rescue points, creating a patch, and inserting the patch into the running application. This process requires some downtime, but the cost is amortized as it is incurred once per detected vulnerability. Combining our approach with techniques such as micro-rebooting [4, 5] is a topic for future research.

#### Dealing with Non-Server Type Applications

The success of our system in recovering program execution can be partly explained by the characteristics of the types of applications we examine. As articulated in [17], server-type applications tend to have short error propagation distances, and forcing errors in one request has little impact on future requests. While server applications might have an inherent advantage in propagating errors, we believe that most applications are written with some error handling capabilities. Correctly identifying these rescue points should translate our approach to a wide range of applications, although applications that rely on the integrity of their computation might be better off using an alternative strategy.

### 5. Conclusions

We have outlined error virtualization using rescue points, a new software self-healing technique for detecting, tolerating, and recovering from software faults in server applications. Our approach leverages existing quality assurance testing to generate known bad inputs to an application, creating a call graph of functions and their return values as potential rescue points. We then use target systems to detect software faults caused by attacks exploiting software vulnerabilities and obtain a resulting call stack. This is matched with the potential set of rescue points by rolling back and repeating execution with the fault to determine which rescue point can be used for recovery. Our system dynamically patches the running production application to self-checkpoint at the rescue point and, if a fault occurs, roll back to the checkpoint and return a known return value used by the application's built-in error handling mechanisms to recover from the fault.

Future work includes demonstrating the effectiveness of our technique using a battery of real and synthetic attacks and failures and evaluating its performance impact on applications.

### 6. Acknowledgements

We thank the anonymous reviewers and our shepherd Diego Zamboni for their insightful comments and suggestions on the paper. This material is based on research sponsored by Air Force Research Laboratory under agreement number FA8750-06-2-0221 and by NSF Grant 06-27473. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon.

### References

[1] M. Abadi, M. Budiu, U. Erlingsson, and J. Ligatti. Control flow integrity. In CCS '05: Proceedings of the 12th ACM conference on Computer and communications security, pages 340–353, New York, NY, USA, 2005. ACM Press.

[2] B. Buck and J. K. Hollingsworth. An API for runtime code patching. The International Journal of High Performance Computing Applications, 14(4):317–329, Winter 2000.

[3] C. Cadar and D. R. Engler. Execution generated test cases: How to make systems code crash itself. In P. Godefroid, editor, SPIN, volume 3639 of Lecture Notes in Computer Science, pages 2–23. Springer, 2005.

[4] G. Candea and A. Fox. Recursive Restartability: Turning the Reboot Sledgehammer into a Scalpel. In Proceedings of the 8th Workshop on Hot Topics in Operating Systems (HotOS), pages 125–132, May 2001.

[5] G. Candea and A. Fox. Crash-only software. In Proceedings of the 9th Workshop on Hot Topics in Operating Systems, May 2003.

[6] S. Chandra. An Evaluation of the Recovery-related Properties of Software Faults. PhD thesis, University of Michigan, 2000.

[7] S. Chandra and P. M. Chen. Wither Generic Recovery from Application Faults? A Fault Study using Open-Source Software. In Proceedings of the International Conference on Dependable Systems and Networks / Symposium on Fault-Tolerant Computing (FTCS), June 2000.

[8] B. Demsky and M. C. Rinard. Automatic Detection and Repair of Errors in Data Structures. In Proceedings of the 18th Annual ACM SIGPLAN Conference on Object Oriented Programming, Systems, Languages, and Applications, October 2003.

[9] J. Etoh. GCC extension for protecting applications from stack-smashing attacks. http://www.trl.ibm.com/projects/security/ssp/, June 2000.

[10] P. Godefroid, N. Klarlund, and K. Sen. Dart: directed automated random testing. In PLDI '05: Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation, pages 213–223, New York, NY, USA, 2005. ACM Press.

[11] V. Kiriansky, D. Bruening, and S. Amarasinghe. Secure Execution Via Program Shepherding. In Proceedings of the 11th USENIX Security Symposium, pages 191–205, August 2002.

[12] J. Newsome, D. Brumley, and D. Song. Vulnerability-specific execution filtering for exploit prevention on commodity software. In Proceedings of the 13th Annual Network and Distributed Systems Security Symposium, 2006. http://www.cs.cmu.edu/~dbrumley/.

[13] S. Osman, D. Subhraveti, G. Su, and J. Nieh. The design and implementation of Zap: A system for migrating computing environments. In Proceedings of the 5th USENIX Symposium on Operating Systems Design and Implementation (OSDI), pages 361–376, December 2002.

[14] F. Qin, J. Tucek, J. Sundaresan, and Y. Zhou. Rx: treating bugs as allergies - a safe method to survive software failures. In A. Herbert and K. P. Birman, editors, SOSP, pages 235–248. ACM, 2005.

[15] M. Rinard. Acceptability-oriented computing, 2003.

[16] M. Rinard, C. Cadar, D. Dumitran, D. Roy, and T. Leu. A Dynamic Technique for Eliminating Buffer Overflow Vulnerabilities (and Other Memory Errors). In Proceedings 20th Annual Computer Security Applications Conference (ACSAC), December 2004.

[17] M. Rinard, C. Cadar, D. Dumitran, D. Roy, T. Leu, and J. W Beebee. Enhancing Server Availability and Security Through Failure-Oblivious Computing. In Proceedings 6th Symposium on Operating Systems Design and Implementation (OSDI), December 2004.

[18] R. Sengupta, O. J. D., F. D. J., K. D. S., Springer, S. P. L., N.-S. H. S., H. M. A., M. R. J., and J. C. Software Fault Tolerance for Low-to-Moderate Radiation Environments. In ASP Conf. Ser., Vol. 238, Astronomical Data Analysis Software and Systems X, 2001.

[19] S. Sidiroglou, Y. Giovanidis, and A. Keromytis. A Dynamic Mechanism for Recovery from Buffer Overflow Attacks. In Proceedings of the 8th Information Security Conference (ISC), September 2005.

[20] S. Sidiroglou, M. E. Locasto, S. W. Boyd, and A. D. Keromytis. Building a reactive immune system for software services. In Proceedings of the USENIX Technical Conference, April 2005.

[21] N. Wang, M. Fertig, and S. Patel. Y-Branches: When You Come to a Fork in the Road, Take It. In Proceedings of the 12th International Conference on Parallel Architectures and Compilation Techniques, September 2003.

Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 02:54:28 UTC from IEEE Xplore. Restrictions apply.
2007 IEEE Symposium on Security and Privacy (SP'07) 0-7695-2848-1/07 $20.00 © 2007