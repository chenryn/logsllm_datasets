### Program Chair's Message
**Author: Peng Ning**

It has been an honor to serve as the Program Chair for ASPLOS 2014. I am both inspired and humbled by the breadth and quality of the work submitted, as well as the dedication of the organizing team to ensuring a successful conference.

The overarching goal of a conference is to advance the field. I was fortunate that recent Program Chairs of ASPLOS had led highly successful conferences, and I largely followed their lead. However, I identified two specific goals for additional efforts: (1) to further enhance ASPLOS's reputation as a broad, multidisciplinary conference, and (2) to continue raising the bar for quality and fairness in our review process. This document outlines the processes I implemented, summarizes the program, and acknowledges those who made this program possible.

#### Scope of ASPLOS

The call for papers emphasized that ASPLOS is a broad, multidisciplinary conference open to new and non-traditional systems-related topics. To reach a wider audience, I reached out to "non-traditional" SIGs and other organizations to promote the call. One tangible outcome is that ASPLOS is "in-cooperation" with SIGBED this year. I hope our collaboration with SIGBED will strengthen in the coming years.

The call also specified that papers should emphasize the synergy of at least two ASPLOS disciplines (broadly defined), and the submission form explicitly asked authors to identify these areas.

We received 217 submissions, a 12% increase from the previous year and a new record. Approximately 147 papers identified architecture as one of their areas, 89 identified programming languages or compilers, and 94 identified operating systems. Many papers also identified other broad areas, including verification, graphics, big data, networks, cloud computing, mobile computing, embedded systems, software engineering, and more.

For specific research topics, there was a wide diversity. The following were listed by more than 20 submissions: power/energy/thermal management (34 papers), parallel architecture (31), heterogeneous architectures and accelerators (28), caches (27), high-performance computing (26), OS scheduling and resource management (26), compiler optimization (23), software reliability (23), virtualization (22), parallel programming languages (21), and programming models (21). The accepted papers reflect a similarly diverse set of topics.

In summary, ASPLOS continues to fulfill its vision as a multidisciplinary conference, attracting a broad range of systems-related researchers.

#### The Review Process: Setting the Stage

I was fortunate to have an excellent Program Committee (PC) of 35 members and an External Review Committee (ERC) of 69 members. An additional 43 reviewers contributed to the process.

**Review Process Overview:**
I used a two-phase process similar to recent ASPLOS conferences. Unlike ASPLOS'13 but like other ASPLOS conferences, I obtained phase 2 reviews for only a subset of papers. The two review phases were followed by an author response period, an intense two-week online discussion period, and a one-day in-person PC meeting. ERC reviewers were required to participate in all parts of this process except for the PC meeting.

**Aids for Reviewer Assignments:**
I assigned all reviews. To help find the best reviewers, I used the following aids new to ASPLOS:
1. The submission instructions allowed unlimited pages for the bibliography and required all citations to include all authors (i.e., no et al.). This approach, borrowed from NSF, makes it easier to identify potential reviewers from related work.
2. I asked reviewers to suggest other reviewers as part of their review, a practice borrowed from ISCA'13.
3. I allowed authors to (optionally) suggest reviewers.

I required all reviewers to write their assigned reviews themselves. Consulting with others for small aspects was allowed if it brought clear added value (after checking with me for conflicts), but the assigned reviewers needed to write the bulk of the review and provide their own scores.

**System Enhancements for Conflicts:**
I used double-blind reviewing and handled conflicts of interest using community norms. Sandhya Dwarkadas managed the 20 papers with which I had a conflict using the same process described here. I requested Eddie Kohler, the author of the HotCRP reviewing software, to add a new mechanism to handle PC chair conflicts. This mechanism allows the PC chair to yield (most) chair privileges to a different "manager" for a given paper and allows reviewers to email paper-specific issues to the (anonymous) paper manager. This system:
1. Significantly streamlined handling the review process for Sandhya.
2. Better hid sensitive information from me than alternatives I had previously used.
3. Better hid the conflict from assigned reviewers, preventing inadvertent guessing of author identity.

I am grateful to Eddie for implementing such an intrusive change at very short notice.

**Tone:**
I consistently set a tone that encouraged reviewers to focus on moving the field forward while being constructive and fair to all authors. I emphasized that there was no target acceptance rate and that all worthy papers would be accepted. Accepting many papers implies multitrack sessions and potentially reduced interaction. To mitigate these downsides, the first day of the conference features lightning presentations, and the second day features a poster session for all papers, a practice borrowed from MICRO'12. Although I used numeric scores in the review forms, decisions were made based on the review texts.

#### Phase 1 Reviews

I assigned 2 PC and 1 ERC reviewer for each paper in phase 1. I received virtually all reviews within two days of the completion deadline (I learned to nag). Unlike ASPLOS'13 (but like other conferences), I did not move all papers to phase 2. An important benefit of a two-phase review process is to relieve reviewers of the burden of reviewing clearly weak papers. On the other hand, a process that rejects papers based on just three reviews risks inadequately informed decisions and inadequate author feedback. I put significant effort into balancing these opposing demands.

Papers with at least one accept score for overall merit moved to phase 2 by default. Papers with all reject scores and high confidence levels were candidates to not move to phase 2. I highlighted the remaining 38 papers (no accepts, but some undecided and/or low confidence scores) for online discussion among reviewers, using the HotCRP comments system, which provides a persistent record. Many reviewers voluntarily discussed many other papers as well.

In parallel, I read most reviews and every review of every paper that was a candidate for terminating at phase 1 or under discussion. For each review that did not provide adequate justification for a low score (e.g., a subjective claim that the paper was incremental without adequate citations of prior work), I used a HotCRP comment to request the reviewer to update their review or risk the paper moving to phase 2. I ensured all score changes were accompanied by justifications in the review text and not just an easy reaction to adjust to the majority. I used the HotCRP color tags feature extensively to help reviewers and myself distinguish different paper categories and track online discussion results.

After this process, all papers with at least one accept or undecided score proceeded to phase 2. A few papers with all reject scores also moved to phase 2 if the review text did not justify the reject scores or had low confidence scores (when in doubt, I took the authors’ perspective and moved the paper to phase 2). 30% of the papers did not move to phase 2.

#### Phase 2 Reviews

The phase 1 discussions and reviewer suggestions were very helpful in assigning reviewers for phase 2. All phase 2 papers had at least 5 total reviews, with at least 3 from the PC. The maximum total number of reviews for a PC or ERC member was 19 or 7, respectively.

Midway through phase 2, I used a tool by Andrew Myers to calibrate excessive negativity or exuberance among reviewers. I sent each reviewer their estimated bias score and an anonymized list of all reviewers’ scores. A positive (negative) bias indicated a tendency to give scores that are higher (lower) than others who reviewed the same paper. I believe (hope) this calibration motivated some reflection as reviewers moved to the critical decision-making stages.

The phase 2 reviews were due three days before the author response period. I wanted to ensure high-quality reviews to facilitate responses focused on substantive issues. Since time was limited, I distributed the quality assurance task among all reviewers. The last reviewer to submit a review for a paper was asked to perform a review sufficiency check (RSC) to ensure that (1) the reviews provided sufficient information to make an informed decision for the paper and (2) the reviews provided sufficient feedback to the authors. If the paper passed the RSC, the reviewer colored the paper purple (using HotCRP color tags); otherwise, they noted the problems.

The RSC mechanism significantly enhanced my ability to find problem papers and focus my attention on them. Many reviews were updated, and we identified 18 papers for additional reviews. When the author response started, all reviews from phase 2 had already been received (I got good at nagging). All post-phase 2 (post-RSC) reviews had been requested, and some were already received! The authors were notified that all pending reviews were late requests and they wouldn’t be penalized for not responding to them. In the end, we received a total of 902 reviews.

#### Online Discussion After Author Response

This was the most critical phase of the review process. The goal for the online discussion phase was to triage papers into preliminary accept (tagged green, to be presented quickly at the PC meeting), preliminary reject (tagged red, not to be presented at the PC meeting), and discuss (tagged yellow, and the focus of most of the PC meeting).

Each paper was assigned a lead from the PC or ERC to initiate discussion among its reviewers (tagging the paper purple) and lead it towards a consensus (unanimous agreement) for green (accept) or red (reject). If significant discussion did not lead to a unanimous agreement, the paper was marked yellow (discuss). For these, the discussion was expected to reconcile as many differences as possible.