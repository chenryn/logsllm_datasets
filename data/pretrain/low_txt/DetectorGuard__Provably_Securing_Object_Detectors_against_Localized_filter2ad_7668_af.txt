### Effect of Window Size

We investigate the impact of varying window sizes in the second sub-figure of Figure 7. The figure illustrates a trade-off between provable robustness and clean performance. As the window size increases, each window captures more information from the input, leading to improved clean performance metrics (AP and 1-FAR). However, larger window sizes also increase the number of windows affected by a small adversarial patch, thereby reducing provable robustness. In our default configuration, we set the window size to 8 to achieve a low FAR and good CR.

### Effect of DBSCAN Parameters

We also analyze the effect of DBSCAN parameters in the DetCluster(¬∑) function. DBSCAN has two key parameters: ùúñ and min_points. A point is labeled as a core point if there are at least min_points within a distance ùúñ. All core points and their neighbors form clusters. The right two sub-figures in Figure 7 show the impact of varying ùúñ and min_points. Increasing either parameter makes it more difficult to form clusters, which improves clean performance due to fewer detected clusters and fewer false alerts. However, this results in a drop in provable robustness (CR) because fewer clusters are detected in the worst-case objectness map.

## Discussion

In this section, we discuss future work directions and potential extensions of DetectorGuard.

### 6.1 Future Work

#### Robust Object Detection without Abstention

In this paper, DetectorGuard is designed for attack detection: when no attack is detected, the model uses conventional object detectors; when an attack is detected, the model alerts and abstains from making predictions. This defense is useful in scenarios like autonomous vehicles, where control can be returned to the driver upon detecting an attack. However, the most desirable form of robustness is to make correct predictions without any abstention. Extending DetectorGuard to achieve robust object detection without abstention is an interesting direction for future research.

#### Better Robust Image Classifier

DetectorGuard leverages the key principles introduced in Section 2.4 to co-design the provably robust image classifier and Objectness Predictor. However, the limitations of the adapted robust image classifier still affect DetectorGuard's performance. Although we can optimize for a few Clean Error 2 and tolerate a potentially high Clean Error 3, as discussed in Section 3.4, a high Clean Error 3 limits the certified recall of DetectorGuard in adversarial settings. We note that DetectorGuard is a general framework compatible with any conventional object detector and provably robust image classifier. We expect that advancements in robust image classification will enhance DetectorGuard's performance.

#### Extension to Video Setting

This paper focuses on object detection in single-frame settings. Extending DetectorGuard to multiple-frame video settings is an intriguing area for future work. Temporal information could be beneficial for robustness. Additionally, performing the defense on a subset of frames could reduce computational overhead and minimize false alerts in clean settings.

### 6.2 Defense Extension

In this paper, we propose DetectorGuard as a provably robust defense against patch hiding or false-negative (FN) attacks. Here, we discuss how to extend DetectorGuard to defend against false-positive (FP) attacks. FP attacks aim to introduce incorrect bounding boxes in the detector's predictions to increase FP. These attacks can be treated as misclassification problems, and a robust auxiliary image classifier can re-classify the detected bounding boxes to mitigate them. If the auxiliary classifier predicts a different label, we consider it an FP attack and can correct or filter out the FP boxes.

The pseudocode for using an auxiliary classifier against FP attacks is provided in Algorithm 3. The algorithm re-classifies each detected bounding box in D as label ùëô‚Ä≤ (Line 5). Bounding boxes with non-background labels are added to ÀÜD (Line 7). Finally, the algorithm returns the filtered detection ÀÜD, and we can replace D in Line 8 of Algorithm 1 with ÀÜD to extend the original DetectorGuard design. When the patch is not present in the FP box or occupies only a small portion, the auxiliary classifier is likely to correctly predict "background" since there are few corrupted pixels. When the patch occupies a large portion, the object detector may correctly locate the adversarial patch but predict a wrong label, which is acceptable as the class label for patches is undefined. Therefore, Algorithm 3 provides a strong empirical defense against FP attacks.

## Related Work

### 7.1 Adversarial Patch Attacks

#### Image Classification

Unlike most adversarial examples that introduce global perturbations with an ùêøùëù-norm constraint, localized adversarial patch attacks allow arbitrary perturbations within a restricted region. Brown et al. [4] introduced the first adversarial patch attack against image classifiers. Subsequent works have explored various localized attacks against image classifiers under different threat models [21, 27, 28].

#### Object Detection

Localized patch attacks against object detection have also gained attention. Liu et al. [30] proposed DPatch, the first patch attack against object detectors in the digital domain. Lu et al. [31], Chen et al. [7], Eykholt et al. [14], and Zhao et al. [66] proposed physical attacks against object detectors for traffic sign recognition. Thys et al. [50] used a rigid physical patch to evade human detection, while Xu et al. [61] and Wu et al. [57] generated non-rigid perturbations on T-shirts to evade detection.

### 7.3 Other Adversarial Example Attacks and Defenses

#### Image Classification

Attacks and defenses for classic ùêøùëù-bounded adversarial examples have been extensively studied [6, 16, 48]. Many empirical defenses [32, 34, 35, 40, 62] were proposed but later found vulnerable to adaptive attackers [1, 5, 51]. This led to the development of certified defenses that are robust to any attacker considered in the threat model [10, 17, 22, 37, 41, 47, 56]. For more details, we refer readers to survey papers [39, 63].

#### Object Detection

Global perturbations against object detectors were first studied by Xie et al. [60] and followed by researchers in various applications [54, 55]. Defending against global ùêøùëù perturbations is challenging. Zhang et al. [64] used adversarial training (AT) to improve empirical model robustness, while Chiang et al. [8] proposed randomized median smoothing (RMS) for building certifiably robust object detectors. Both defenses suffer from poor clean performance, whereas DetectorGuard's clean performance is close to state-of-the-art object detectors. On PASCAL VOC, AT incurs a ~26% clean AP drop, while DetectorGuard incurs a <1% drop. On MS COCO, both AT and RMS have a clean AP drop larger than 10%, while DetectorGuard's drop is smaller than 1%.

It is possible to extend our robust objectness predictor design and objectness explaining strategy to mitigate attacks that use global perturbations with a bounded ùêø‚àû norm, given a robust image classifier against ùêø‚àû perturbations. This is a future work direction.

### 7.2 Defenses against Adversarial Patches

#### Image Classification

Digital Watermark (DW) [18] and Local Gradient Smoothing (LGS) [38] were early heuristic defenses against adversarial patch attacks but are vulnerable to adaptive attackers. Certified defenses [9, 24, 33, 36, 58, 59, 65] provide strong provable robustness guarantees. Notably, PatchGuard [58] introduces small receptive fields and secure aggregation, achieving state-of-the-art defense performance for image classification. In contrast, DetectorGuard adapts robust image classifiers for the more challenging task of robust object detection.

#### Object Detection

Securing object detection is less studied due to its complexity. Saha et al. [46] demonstrated that YOLOv2 [43] was vulnerable to adversarial patches and proposed a new training loss to limit the usage of context information. To our knowledge, this is the only prior attempt to secure object detectors from patch attacks. However, this defense is heuristic and lacks provable robustness. Moreover, it is specific to YOLOv2 and may not generalize to other detectors. In contrast, DetectorGuard provides provable robustness against any patch hiding attack within our threat model and is compatible with state-of-the-art object detectors.

## Conclusion

In this paper, we introduce DetectorGuard, the first general framework for building provably robust object detectors against patch hiding attacks. DetectorGuard adapts robust image classifiers for robust object detection using an objectness explaining strategy. Our evaluation on PASCAL VOC, MS COCO, and KITTI datasets shows that DetectorGuard achieves the first provable robustness against any patch hiding attacker within the threat model and maintains high clean performance comparable to state-of-the-art detectors.

## Acknowledgments

We thank Gagandeep Singh for shepherding the paper and anonymous reviewers at CCS 2021 for their valuable feedback. We also thank Vikash Sehwag, Shawn Shan, Sihui Dai, Alexander Valtchanov, Ruiheng Chang, Jiachen Sun, and researchers at Intel Labs for helpful discussions and insightful comments. This work was supported in part by the National Science Foundation under grants CNS-1553437 and CNS-1704105, the ARL‚Äôs Army Artificial Intelligence Innovation Institute (A2I2), the Office of Naval Research Young Investigator Award, Schmidt DataX award, and Princeton E-ffiliates Award.

## References

[1] Anish Athalye, Nicholas Carlini, and David A. Wagner. 2018. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. In Proceedings of the 35th International Conference on Machine Learning (ICML). 274‚Äì283.
[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. 2020. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv preprint arXiv:2004.10934 (2020).
[3] Wieland Brendel and Matthias Bethge. 2019. Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet. In 7th International Conference on Learning Representations (ICLR).
[4] Tom B. Brown, Dandelion Man√©, Aurko Roy, Mart√≠n Abadi, and Justin Gilmer. 2017. Adversarial patch. In Advances in neural information processing systems workshops (NeurIPS Workshops).
[5] Nicholas Carlini and David A. Wagner. 2017. Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (AISec@CCS). 3‚Äì14.
[6] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In 2017 IEEE Symposium on Security and Privacy (S&P). 39‚Äì57.
[7] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau. 2018. Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 52‚Äì68.
[8] Ping-yeh Chiang, Michael Curry, Ahmed Abdelkader, Aounon Kumar, John Dickerson, and Tom Goldstein. 2020. Detection as Regression: Certified Object Detection with Median Smoothing. In Advances in Neural Information Processing Systems (NeurIPS) 2020, Vol. 33.
[9] Ping-Yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studor, and Tom Goldstein. 2020. Certified defenses for adversarial patches. In 8th International Conference on Learning Representations (ICLR).
[10] Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. 2019. Certified Adversarial Robustness via Randomized Smoothing. In Proceedings of the 36th International Conference on Machine Learning (ICML). 1310‚Äì1320.
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). 248‚Äì255.
[12] Martin Ester, Hans-Peter Kriegel, J√∂rg Sander, Xiaowei Xu, et al. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise.. In Kdd, Vol. 96. 226‚Äì231.
[13] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. 2010. The Pascal Visual Object Classes (VOC) Challenge. International Journal of Computer Vision 88, 2 (2010), 303‚Äì338.
[14] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tram√®r, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Robust Physical-World Attacks on Deep Learning Models. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).