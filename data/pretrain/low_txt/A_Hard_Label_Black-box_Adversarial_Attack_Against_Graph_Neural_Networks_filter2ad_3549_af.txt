### 6.3 Discussion

The defense results presented in Sections 6.1.2 and 6.2.2 indicate that our adversarial attack remains effective even when detection or prevention measures are in place. For example, on the NCI1 dataset, the best detection performance is achieved with a budget of 0.20 using GUNet and our attack to train the detector. However, the false negative rate (FNR) and false positive rate (FPR) remain at 0.25 and 0.20, respectively, even when the detector has full knowledge of our attack. In real-world scenarios, detecting adversarial graphs is even more challenging because the detector often lacks complete information about the true attack.

Low-rank based defenses can mitigate the impact of adversarial graphs to some extent, but this comes at the cost of reduced testing performance on clean graphs. For instance, on the NCI1 dataset, up to 40% of adversarial graphs cannot be prevented even after removing 95% of the smallest singular values.

The proposed detector is a data-level defense strategy that aims to block detected adversarial graphs before they query the target model. It has two key limitations: (i) it is heuristic and (ii) it requires a substantial number of adversarial graphs for training, with detection performance heavily dependent on the quality of the training dataset. Specifically, the structural differences between adversarial and normal graphs should be significant for effective detection.

In contrast, the low-rank based defense is a model-level strategy that equips the target GNN model with the ability to remove the smallest singular values, thereby allowing accurate predictions even on adversarially perturbed graphs. There are several potential ways to empirically strengthen our defense:
- Locating vulnerable regions of graphs based on feedback from our attacks.
- Designing an attack-aware graph partitioning algorithm, as the current method is generic and does not exploit specific settings of adversarial attacks.
- Implementing adversarial training [8, 20, 23], which involves training a robust GNN model by introducing white-box adversarial attacks and playing a min-max game during training. We did not adopt this method due to the lack of existing white-box attacks against the considered GNN models.

Another direction for future work is to provide certified robustness [4, 22, 47] for GNN models against adversarial structural perturbations. We will also explore defenses against hard-label black-box adversarial attacks in future work.

### 7 Related Work

Existing studies have demonstrated that Graph Neural Networks (GNNs) are vulnerable to adversarial attacks [5, 6, 33, 46, 48, 61], which can deceive a GNN into producing incorrect labels for specific target graphs (in graph classification tasks) or target nodes (in node classification tasks). These attacks can be categorized into training-time poisoning attacks [30, 46, 53, 63, 64] and testing-time adversarial attacks [7, 9, 27, 32, 43, 48]. This paper focuses on testing-time adversarial attacks against classification tasks.

#### Adversarial Attacks Against Node Classification

Most existing adversarial attacks target GNN models for node classification. These attacks can be divided into two categories: optimization-based methods [32, 42, 44, 52] and heuristic-based methods that use greedy algorithms [9, 50] or reinforcement learning (RL) [13, 41].

- **Optimization-Based Methods:** The attacker formulates the attack as an optimization problem and solves it using techniques such as gradient descent. For example, Xu et al. [54] developed a CW-type loss function and used projected gradient descent to minimize the loss.
- **Heuristic-Based Methods:** The attacker uses a greedy approach, defining an objective function and traversing all candidate components (e.g., edges or nodes) to add perturbations. The process is repeated until an adversarial graph is found or the perturbation budget is exceeded. Chen et al. [9] proposed an adversarial attack on GCN, which selects the edge with the maximal absolute link gradient and adds it as a perturbation in each iteration.

#### Adversarial Attacks Against Graph Classification

Fewer attacks target graph classification tasks [13, 33, 43]. For example, Ma et al. [33] proposed an RL-based adversarial attack on GNNs, which constructs the attack by rewiring the target graph. Tang et al. [43] performed an attack against Hierarchical Graph Pooling (HGP) neural networks using a greedy-based method. Unlike these white-box or grey-box attacks, we study the most challenging hard-label and black-box attacks against graph classification. Our attack is both time and query efficient, achieving a high attack success rate with small perturbations.

### 8 Conclusion

We propose a black-box adversarial attack to fool GNNs for graph classification tasks in the hard-label setting. We formulate the adversarial attack as an optimization problem, which is intractable in its original form. We then relax the problem and design a sign stochastic gradient descent algorithm to solve it with convergence guarantees. Additionally, we propose two algorithms—coarse-grained searching and query-efficient gradient computation—to reduce the number of queries during the attack. We conduct our attack against three representative GNN models on real-world datasets from different fields. The experimental results show that our attack is more effective and efficient compared to state-of-the-art attacks. Furthermore, we propose two defense methods: one to detect adversarial graphs and the other to prevent their generation. The evaluation results show that our attack remains effective, highlighting the need for advanced defenses in future work.

### Acknowledgments

We would like to thank our shepherd Pin-Yu Chen and the anonymous reviewers for their valuable comments. This work is supported in part by the National Key R&D Program of China under Grant 2018YFB1800304, NSFC under Grants 62132011, 61625203, and 61832013, U.S. ONR under Grant N00014-18-2893, and BNRist under Grant BNR2020 RC01013. Qi Li and Mingwei Xu are the corresponding authors of this paper.

### References

[1] Pedro HC Avelar, Anderson R Tavares, Thiago LT da Silveira, Clíudio R Jung, and Luís C Lamb. 2020. Superpixel image classification with graph attention networks. In 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI). IEEE, 203–209.

[2] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. 2018. signSGD: Compressed optimisation for non-convex problems. In International Conference on Machine Learning. PMLR, 560–569.

[3] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment 2008, 10 (2008), P10008.

[4] Aleksandar Bojchevski, Johannes Klicpera, and Stephan Günnemann. 2020. Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In International Conference on Machine Learning. PMLR, 1003–1013.

[5] Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui, Wenwu Zhu, and Junzhou Huang. 2020. A Restricted Black-Box Adversarial Framework Towards Attacking Graph Embedding Models. In AAAI. 3389–3396.

[6] Jinyin Chen, Yixian Chen, Haibin Zheng, Shijing Shen, Shanqing Yu, Dan Zhang, and Qi Xuan. 2020. MGA: Momentum Gradient Attack on Network. arXiv preprint arXiv:2002.11320 (2020).

[7] Jinyin Chen, Xiang Lin, Ziqiang Shi, and Yi Liu. 2020. Link prediction adversarial attack via iterative gradient attack. IEEE Transactions on Computational Social Systems 7, 4 (2020), 1081–1094.

[8] Jinyin Chen, Xiang Lin, Hui Xiong, Yangyang Wu, Haibin Zheng, and Qi Xuan. 2020. Smoothing Adversarial Training for GNN. IEEE Transactions on Computational Social Systems (2020).

[9] Jinyin Chen, Yangyang Wu, Xuanheng Xu, Yixian Chen, Haibin Zheng, and Qi Xuan. 2018. Fast gradient attack on network embedding. arXiv preprint arXiv:1809.02797 (2018).

[10] Zhengdao Chen, Xiang Li, and Joan Bruna. 2017. Supervised community detection with line graph neural networks. arXiv preprint arXiv:1705.08415 (2017).

[11] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. 2018. Query-efficient hard-label black-box attack: An optimization-based approach. arXiv preprint arXiv:1807.04457 (2018).

[12] Minhao Cheng, Simranjit Singh, Patrick Chen, Pin-Yu Chen, Sijia Liu, and Cho-Jui Hsieh. 2019. Sign-opt: A query-efficient hard-label adversarial attack. arXiv preprint arXiv:1909.10773 (2019).

[13] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. 2018. Adversarial attack on graph structured data. arXiv preprint arXiv:1806.02371 (2018).

[14] Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. 2020. All you need is low (rank) defending against adversarial attacks on graphs. In Proceedings of the 13th International Conference on Web Search and Data Mining. 169–177.

[15] Hongyang Gao and Shuiwang Ji. 2019. Graph U-Nets. arXiv preprint arXiv:1905.05178 (2019).

[16] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212 (2017).

[17] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems. 1024–1034.

[18] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017).

[19] Xinlei He, Jinyuan Jia, Michael Backes, Neil Zhenqiang Gong, and Yang Zhang. 2020. Stealing Links from Graph Neural Networks. arXiv preprint arXiv:2005.02131 (2020).

[20] Weibo Hu, Chuan Chen, Yaomin Chang, Zibin Zheng, and Yunfei Du. 2021. Robust graph convolutional networks with directional graph adversarial training. Applied Intelligence (2021), 1–15.

[21] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. 2019. Strategies for Pre-training Graph Neural Networks. arXiv preprint arXiv:1905.12265 (2019).

[22] Hongwei Jin, Zhan Shi, Venkata Jaya Shankar Ashish Peruri, and Xinhua Zhang. 2020. Certified Robustness of Graph Convolution Networks for Graph Classification under Topological Attacks. Advances in Neural Information Processing Systems 33 (2020).

[23] Hongwei Jin and Xinhua Zhang. 2021. Robust Training of Graph Convolutional Networks via Latent Perturbation. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part III. Springer International Publishing, 394–411.

[24] George Karypis and Vipin Kumar. 1998. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM Journal on Scientific Computing 20, 1 (1998), 359–392.

[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).

[26] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention graph pooling. arXiv preprint arXiv:1904.08082 (2019).

[27] Wanyu Lin, Shengxiang Ji, and Baochun Li. 2020. Adversarial Attacks on Link Prediction Algorithms Based on Graph Neural Networks. In Proceedings of the 15th ACM Asia Conference on Computer and Communications Security. 370–380.

[28] Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. 2018. signSGD via zeroth-order oracle. In International Conference on Learning Representations.

[29] Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. 2018. Zeroth-order stochastic variance reduction for nonconvex optimization. Advances in Neural Information Processing Systems 31 (2018), 3727–3737.

[30] Xuanqing Liu, Si Si, Xiaojin Zhu, Yang Li, and Cho-Jui Hsieh. 2019. A unified framework for data poisoning attack to graph-based semi-supervised learning. arXiv preprint arXiv:1910.14147 (2019).

[31] Guixiang Ma, Nesreen K Ahmed, Theodore L Willke, Dipanjan Sengupta, Michael W Cole, Nicholas B Turk-Browne, and Philip S Yu. 2019. Deep graph similarity learning for brain data analysis. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2743–2751.

[32] Jiaqi Ma, Shuangrui Ding, and Qiaozhu Mei. 2020. Black-box adversarial attacks on graph neural networks with limited node access. arXiv preprint arXiv:2006.05057 (2020).

[33] Yao Ma, Suhang Wang, Tyler Derr, Lingfei Wu, and Jiliang Tang. 2019. Attacking graph convolutional networks via rewiring. arXiv preprint arXiv:1906.03750 (2019).

[34] Thibault Maho, Teddy Furon, and Erwan Le Merrer. 2021. SurFree: a fast surrogate-free black-box attack. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10430–10439.

[35] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. 2020. TUDataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020). arXiv:2007.08663 www.graphlearning.io

[36] Yurii Nesterov and Vladimir Spokoiny. 2017. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics 17, 2 (2017), 527–566.

[37] Kaspar Riesen and Horst Bunke. 2008. IAM graph database repository for graph-based pattern recognition and machine learning. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR). Springer, 287–297.

[38] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. DropEdge: Towards deep graph convolutional networks on node classification. In ICLR. https://openreview.net/forum?id=HklgnnHKwB

[39] S. K. Nayar, S. A. Nene, and H. Murase. 1996. Columbia Object Image Library. http://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php

[40] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. 2011. Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research 12, 9 (2011), 2539–2561.

[41] Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar. 2019. Node injection attacks on graphs via reinforcement learning. arXiv preprint arXiv:1909.06543 (2019).

[42] Tsubasa Takahashi. 2019. Indirect Adversarial Attacks via Poisoning Neighbors for Graph Convolutional Networks. In 2019 IEEE International Conference on Big Data (Big Data). IEEE, 1395–1400.

[43] Haoteng Tang, Guixiang Ma, Yurong Chen, Lei Guo, Wei Wang, Bo Zeng, and Liang Zhan. 2020. Adversarial Attack on Hierarchical Graph Pooling Neural Networks. arXiv preprint arXiv:2005.11560 (2020).

[44] Yunzhe Tian, Jiqiang Liu, Endong Tong, Wenjia Niu, Liang Chang, Qi Alfred Chen, Gang Li, and Wei Wang. 2021. Towards Revealing Parallel Adversarial Attack on Politician Socialnet of Graph Structure. Security and Communication Networks 2021 (2021), 1–15.