### Fault-Catch Coverage and Test Results

We recorded the faults in all appropriate catch blocks, demonstrating that our methodology can drive the application through all its responses to these faults, thereby achieving good test coverage. However, the NIC DOWN fault often causes latent errors. When this fault was injected into the six vulnerable try blocks, only two catches were covered. We re-ran our tests for NIC DOWN in fault-not-cancel mode, which allowed us to cover catch 3. We also tried fault-reinject mode, but it did not affect our results for NIC DOWN.

### Coverage Data

Figure 2 summarizes the |ei|/|fi| values for each catch block graphically, and Table 4 provides our aggregate coverage metrics for the tested code. Our (fraction of) covered catches metric is the most stringent, highlighting that about half of the catches have not been fully tested. The other two metrics take into account the amount of coverage of the partially covered catches. In this experiment, we obtained slightly higher values for overall fault-catch coverage than average fault-catch coverage, as the former effectively weighs the individual catch average ratios by the number of associated faults, and our lowest percentage coverage occurred on a catch with only two faults.

| Mode           | Average Covered Catches | Overall Fault-Catch Coverage |
|----------------|-------------------------|-------------------------------|
| fault-cancel   | 84.3%                   | 85.2%                         |
| fault-not-cancel | 87.1%                   | 88.9%                         |

**Table 4: Aggregated Report of Coverage**

Our data show that we can inject faults, instrument programs to measure fault-catch coverage, and achieve significant levels of fault-catch coverage for Mufﬁn. For faults that do not produce latent errors, we achieved 100% fault-catch coverage, indicating that our techniques are valuable. We were less successful with faults that produce latent errors, covering four out of the seven NIC DOWN/catch combinations in fault-not-cancel mode. While these coverage results are valuable in guiding testers to the fault-recovery codes that are not fully tested, it is crucial to improve our coverage for faults resulting in latent errors.

### Related Work

Researchers in the dependability and software engineering communities have extensively studied program coverage and fault coverage. Given the limited space, we will focus on comparing our work with previous research on fault injection using program-coverage metrics. An understanding of probabilistic fault coverage [9], its relationship to system dependability [13], and fault-injection [3] is essential to understand the context of our work. Our program-coverage metrics are most similar to those used in dataflow testing [26]. These references have been discussed in Section 2.1.

Our fault-injection experiments most closely resemble those measuring responses to errors using traditional program-coverage metrics. Tsai et al. [34] placed breakpoints at key program points along known execution paths and injected faults at each point, such as corrupting a value in a register. Their work differs from ours in its goal, the kinds of faults injected, and their definition of coverage. The primary goal of their approach was to increase fault activations and fault coverage, not to increase program coverage. They injected hardware-centric faults, primarily affecting program state, not communication with the operating system or I/O hardware. They used a basic-block definition of program coverage, rather than measuring coverage of a program-level construct such as a catch block. Bieman et al. [7] explored an alternative approach where a fault is injected by violating a set of pre- or post-conditions in the code, which are required to be expressed explicitly in the program by the programmer. This approach used branch coverage, a program-coverage metric.

In the terminology of Hamlet’s summary paper reconciling traditional program-coverage metrics and probabilistic fault analysis [16], our work can be classified as a probabilistic input sequence generator, exploring the low-frequency inputs to a program. Using the terminology presented by Tang and Hecht [32], which surveyed the entire software dependability process, our method can be classified as a stress-test, because it generates unlikely inputs to the program.

### Conclusions

We have posed what we believe to be a new challenge in the field of techniques for developing highly available systems: determining whether all the fault-recovery code in a Web services application has been exercised on an appropriate set of faults. We have presented our fault-catch coverage metric, which formalizes what it means to meet this challenge successfully, and have shown that it is possible to instrument programs to collect coverage information at runtime. Our metric combines ideas of testing software in response to injected faults, developed by the dependability community, with ideas of testing for coverage of specific program constructs, developed by the software engineering community.

We have also developed an API that allows the program being tested to direct a fault-injection engine and have extended Mendosus to respond to this API. We have described compiler analyses that can be applied to Java source or bytecodes to instrument codes to direct fault injection to produce high fault-catch coverage.

Our preliminary case study results with Mufﬁn indicate that our approach is highly effective for faults that do not create latent errors (i.e., 100% coverage), and somewhat effective for faults that do (i.e., covering 4 of the 7 NIC DOWN/catch combinations). Next, we plan to enhance our approach to achieve better coverage in the presence of latent errors and to study issues of testing distributed applications.

### References

[1] The Eclipse IDE. See http://www.eclipse.org/.

[2] The Mufﬁn world wide web filtering system. See http://muffin.doit.org/.

[3] J. Arlat, A. Costes, Y. Crouzet, J.-C. Laprie, and D. Powell. Fault injection and dependability evaluation of fault-tolerant systems. IEEE Transactions on Computers, 42(8):913–923, Aug. 1993.

[4] K. Arnold and J. Gosling. The Java Programming Language, Second Edition. Addison-Wesley, 1997.

[5] M. Arnold and P. F. Sweeney. Approximating the calling context tree via sampling. Technical Report RC 21789, IBM T.J. Watson Research Center, July 2000.

[6] D. Bacon and P. Sweeney. Fast static analysis of C++ virtual functions calls. In Proceedings of ACM SIGPLAN Conference on Object-oriented Programing Systems, Languages and Applications (OOPSLA’96), pages 324–341, Oct. 1996.

[7] J. Bieman, D. Dreilinger, and L. Lin. Using fault injection to increase software test coverage. In Proc. 7th Int. Symp. on Software Reliability Engineering (ISSRE’96), pages 166–74. IEEE Computer Society Press, 1996.

[8] R. V. Binder. Testing Object-oriented Systems. Addison Wesley, 1999.

[9] W. G. Bouricius, W. C. Carter, and P. Schneider. Reliability modeling techniques for self-repairing computer systems. In Proceedings of the 24th National Conference of the ACM, pages 295–309, March 1969.

[10] M. Cukier, R. Chandra, D. Henke, J. Pistole, and W. H. Sanders. Fault injection based on a partial view of the global state of a distributed system. In Symposium on Reliable Distributed Systems, pages 168–177, 1999.

[11] S. Dawson, F. Jahanian, and T. Mitton. ORCHESTRA: A Fault Injection Environment for Distributed Systems. In Proc. 26th Int. Symp. on Fault Tolerant Computing (FTCS-26), pages 404–414, Sendai, Japan, June 1996.

[12] J. Dean, D. Grove, and C. Chambers. Optimization of object-oriented programs using static class hierarchy. In Proceedings of 9th European Conference on Object-oriented Programming (ECOOP’95), pages 77–101, 1995.

[13] J. B. Dugan and K. S. Trivedi. Coverage modeling for dependability analysis of fault-tolerant systems. IEEE Transactions on Computers, 38(6):775–787, June 1989.

[14] C. Fu, R. P. Martin, K. Nagaraja, T. D. Nguyen, B. G. Ryder, and D. Wonnacott. Compiler-directed program-fault coverage for highly available Java internet services. Technical Report DCS-TR-518, Department of Computer Science, Rutgers University, Jan. 2003.

[15] D. Grove and C. Chambers. A framework for call graph construction algorithms. ACM Transactions on Programming Languages and Systems (TOPLAS), 23(6), 2001.

[16] D. Hamlet. Foundations of software testing: dependability theory. In Proceedings of the 2nd ACM SIGSOFT Symposium on Foundations of Software Engineering, pages 128–139. ACM Press, 1994.

[17] S. Han, K. Shin, and H. Rosenberg. DOCTOR: An Integrated Software Fault Injection Environment for Distributed Real-Time Systems. In Int. Computer Performance and Dependability Symp. (IPDS’95), pages 204–213, Erlangen, Germany, Apr. 1995.

[18] H. Hecht and P. Crane. Rare conditions and their effect on software failures. In Proceedings of the Annual Reliability and Maintainability Symposium, pages 334–337, Anaheim, CA, Jan. 1994.

[19] M. Kalyanakrishnam, Z. Kalbarczyk, and R. Iyer. Failure Data Analysis of a LAN of Windows NT Based Computers. In Proceedings of the 18th Symposium on Reliable and Distributed Systems (SRDS ’99), 1999.

[20] G. A. Kanawati, N. A. Kanawati, and J. A. Abraham. FERRARI: A Tool for the Validation of System Dependability Properties. In Proc. 22nd Int. Symp. on Fault Tolerant Computing (FTCS-22), pages 336–344, Boston, Massachusetts, 1992. IEEE Computer Society Press.

[21] X. Li, R. P. Martin, K. Nagaraja, T. D. Nguyen, and B. Zhang. Mendosus: A SAN-Based Fault-Injection Test-Bed for the Construction of Highly Available Network Services. In Proceedings of the 1st Workshop on Novel Uses of System Area Networks (SAN-1), Cambridge, MA, Jan. 2002.

[22] D. Liang, M. Pennings, and M. Harrold. Extending and evaluating flow-insensitive and context-insensitive points-to analyses for Java. In Proceedings of the 2001 ACM SIGPLAN - SIGSOFT Workshop on Program Analysis for Software Tools and Engineering, pages 73–79, June 2001.

[23] B. Marick. The Craft of Software Testing, Subsystem Testing Including Object-based and Object-oriented Testing. Prentice-Hall, 1995.

[24] A. Milanova, A. Rountev, and B. G. Ryder. Parameterized object sensitivity for points-to and side-effect analysis. In Proceedings of the International Symposium on Software Testing and Analysis, pages 1–11, 2002.

[25] G. J. Myers. The Art of Software Testing. John Wiley and Sons, 1979.

[26] S. Rapps and E. Weyuker. Selecting software test data using data flow information. IEEE Transactions on Software Engineering, SE-11(4):367–375, Apr. 1985.

[27] A. Rountev, A. Milanova, and B. G. Ryder. Points-to analysis for Java using annotated constraints. In Proceedings of the Conference on Object-oriented Programming, Languages, Systems and Applications, pages 43–55, 2001.

[28] Z. Segall, D. Vrsalovic, D. Siewiorek, D. Yaskin, J. Kownacki, J. Barton, D. Rancey, A. Robinson, and T. Lin. FIAT — Fault Injection based Automated Testing environment. In Proc. 18th Int. Symp. on Fault-Tolerant Computing (FTCS-18), pages 102–107, Tokyo, Japan, 1988. IEEE Computer Society Press.

[29] R. Sethi. Programming Languages, Concepts and Constructs, 2nd Edition. Addison Wesley, 1996.

[30] Sun-Microsystems. Java development kit 1.2. See http://java.sun.com/products/jdk/1.2/docs/api/.

[31] N. Talagala and D. Patterson. An Analysis of Error Behaviour in a Large Storage System. In Proceedings of the Annual IEEE Workshop on Fault Tolerance in Parallel and Distributed Systems, April 1999.

[32] D. Tang and H. Hecht. An approach to measuring and assessing dependability for critical software systems. In Proceedings of the Eighth International Symposium on Software Reliability Engineering, pages 192–202, Albuquerque, NM, Nov. 1997.

[33] F. Tip and J. Palsberg. Scalable propagation-based call graph construction algorithms. In Proceedings of the Conference on Object-oriented Programming, Languages, Systems and Applications, pages 281–293, Oct. 2000.

[34] T. Tsai, M. Hsueh, H. Zhao, Z. Kalbarczyk, and R. Iyer. Stress-based and path-based fault injection. IEEE Transactions on Computers, 48(11):1183–1201, Nov. 1999.

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021 at 07:06:14 UTC from IEEE Xplore. Restrictions apply.