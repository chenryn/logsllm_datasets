### Framework for Evaluating Adversarial Attacks on Virtual Personal Speech Systems (VPSes)

This document outlines a framework to evaluate existing works in the adversarial space against Virtual Personal Speech Systems (VPSes). We aim to identify unique contributions, open problems, and future research directions. The attack landscape for VPSes is distinct and more complex compared to that of image-based systems due to the unique characteristics of audio data.

#### 1. Introduction
The field of adversarial attacks has seen significant advancements, particularly in the context of image recognition. However, the domain of VPSes presents a different set of challenges and opportunities. This complexity arises from the nature of audio signals, which are continuous and time-variant, and the real-time processing requirements of speech recognition systems.

#### 2. Literature Review
We review key studies and findings in the adversarial space against VPSes:

- **Audio Adversarial Examples**:
  - [6] N. Carlini and D. Wagner, "Audio adversarial examples: Targeted attacks on speech-to-text," in 2018 IEEE Security and Privacy Workshops (SPW), pp. 1–7.
  - [7] H. Abdullah, W. Garcia, C. Peeters, P. Traynor, K. Butler, and J. Wilson, "Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems," Proceedings of the 2019 Network and Distributed System Security Symposium (NDSS).

- **Psychoacoustics and Audio Watermarking**:
  - [8] Y. Lin and W. H. Abdulla, "Principles of Psychoacoustics," in Audio Watermark, Springer, 2015, pp. 15–49.

- **Commercial VPSes**:
  - [9] "Cloud Speech-to-Text," Last accessed in 2019, available at https://cloud.google.com/speech-to-text/.
  - [10] "Amazon Lex," Last accessed in 2019, available at https://aws.amazon.com/lex/.

- **NSA and Surveillance**:
  - [12] "The Computers are Listening, Part 2," Accessed in 2018, available at https://theintercept.com/2015/06/nsa-transcription-american-phone-calls/.
  - [13] "The Listening Computers, Part 1," Accessed in 2018, available at https://theintercept.com/2015/05/nsa-speech-recognition-snowden-searchable-text/.

- **Speech Recognition and Processing**:
  - [18] J. Sohn, N. S. Kim, and W. Sung, "A statistical model-based voice activity detection," IEEE Signal Processing Letters, vol. 6, no. 1, pp. 1–3, 1999.
  - [19] S. Sigurdsson, K. B. Petersen, and T. Lehn-Schiøler, "Mel Frequency Cepstral Coefficients: An Evaluation of Robustness of MP3 Encoded Music," in ISMIR, 2006, pp. 286–289.

- **Explainable AI and Sequence Modeling**:
  - [22] D. Gunning, "Explainable artificial intelligence (XAI)," Defense Advanced Research Projects Agency (DARPA), nd Web, 2017.
  - [23] A. Hannun, "Sequence modeling with CTC," Distill, vol. 2, no. 11, p. e8, 2017.

- **Adversarial Machine Learning**:
  - [30] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D. Tygar, "Adversarial Machine Learning," in Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence, ser. AISec '11, New York, NY, USA: ACM, 2011, pp. 43–58.
  - [31] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman, "Towards the science of security and privacy in machine learning," arXiv preprint arXiv:1611.03814, 2016.

- **Evasion and Poisoning Attacks**:
  - [32] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. D. Tygar, "ANTIDOTE: Understanding and Defending against Poisoning of Anomaly Detectors," in Proceedings of the 9th ACM SIGCOMM conference on Internet measurement, ACM, 2009, pp. 1–14.
  - [33] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, and F. Roli, "Evasion attacks against machine learning at test time," in Joint European conference on machine learning and knowledge discovery in databases, Springer, 2013, pp. 387–402.

- **Transferability and Black-Box Attacks**:
  - [46] N. Papernot, P. McDaniel, and I. Goodfellow, "Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples," arXiv preprint arXiv:1605.07277, 2016.
  - [47] F. Tramèr, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, "The Space of Transferable Adversarial Examples," arXiv preprint arXiv:1704.03453, 2017.

- **Privacy and Security in VPSes**:
  - [56] J. Lau, B. Zimmerman, and F. Schaub, "Alexa, are you listening? privacy perceptions, concerns and privacy-seeking behaviors with smart speakers," Proceedings of the ACM on Human-Computer Interaction, vol. 2, no. CSCW, pp. 1–31, 2018.
  - [57] W. Diao, X. Liu, Z. Zhou, and K. Zhang, "Your voice assistant is mine: How to abuse speakers to steal information and control your phone," in Proceedings of the 4th ACM Workshop on Security and Privacy in Smartphones & Mobile Devices, 2014, pp. 63–74.

- **Physical and Acoustic Attacks**:
  - [75] L. Schönherr, K. Kohls, S. Zeiler, T. Holz, and D. Kolossa, "Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding," The Internet Society, 2019.
  - [79] M. Alzantot, B. Balaji, and M. B. Srivastava, "Did you hear that? Adversarial Examples Against Automatic Speech Recognition," in Neural Information Processing Systems Workshop on Machine Deception 2017, vol. abs/1801.00554, Neural Information Processing Systems, 2017.

#### 3. Unique Contributions
- **Psychoacoustic Hiding**: Techniques that exploit human auditory perception to create adversarial examples that are imperceptible to humans but can fool VPSes.
- **Real-Time Attacks**: Development of methods for generating and deploying adversarial attacks in real-time, posing significant challenges to VPSes.
- **Cross-Platform Transferability**: Studies on the transferability of adversarial examples across different VPS platforms and models.

#### 4. Open Problems
- **Robustness and Detection**: Enhancing the robustness of VPSes against adversarial attacks and developing effective detection mechanisms.
- **User Privacy**: Addressing privacy concerns related to the use of VPSes and protecting user data from adversarial attacks.
- **Real-World Deployment**: Ensuring the practicality and effectiveness of adversarial defenses in real-world scenarios.

#### 5. Future Research Directions
- **Advanced Defense Mechanisms**: Exploring new defense strategies such as adversarial training, input transformation, and anomaly detection.
- **Human-AI Collaboration**: Investigating ways to leverage human feedback and interaction to improve the robustness of VPSes.
- **Regulatory and Ethical Considerations**: Developing guidelines and regulations to ensure the ethical use of VPSes and protect users from adversarial threats.

By systematically evaluating the existing literature and identifying key areas for improvement, this framework aims to guide future research and development in the field of adversarial attacks on VPSes.