### Reviews and Rating Strategies

**Review Generation:**
Reviews are often provided by developers, but they can also write their own. Consistent with previous studies (e.g., [38, 42, 51, 52, 60, 61, 63–65, 74, 76, 96, 97, 99]), several participants admitted to reusing common linguistic patterns and copying reviews across similar products. Our quantitative study confirmed this finding. Most participants claimed to write short reviews, which is also reflected in our gold standard fraud data. Previous work (e.g., [38, 49, 51, 52, 60, 65]) observed this and attributed it to the fraudster's lack of experience with the product. However, we also found evidence of ASO workers who post much longer reviews, suggesting that fraud evasion may be a factor.

**Rating Choice Strategies:**
All 18 interview participants admitted to writing mostly 4 or 5-star reviews unless they receive specific instructions from the developers. Eight participants (P3, P7, P8, P9, P10, P11, P14, P18) mentioned receiving instructions on the ratio of review ratings from the developers. For example, P12 stated, "developers request us to write a few 4, 3, and even some 1-star reviews." When no specific instructions are given, participants maintain their own ratios. For instance:
- P5, P16, P17: 10% 4-star, 90% 5-star
- P2, P10, P18: 20% 4-star, 80% 5-star
- P1, P9: 30% 4-star, 70% 5-star
- P13: 40% 4-star, 60% 5-star
- P6, P11, P14: No specific ratio
- P4, P12: Only 5-star reviews

Three participants (P6, P11, P14) also employ strategies to post lower ratings to avoid detection. For example, P6 said, "if the average rating is too high, we add a few 3 or 4-star reviews to make it look more natural."

### Account Management and Evasion Techniques

**Profile Pictures:**
Seven participants added profile pictures from various sources (e.g., Google search, Google Plus, pixabay.com) to make accounts appear more authentic. P9 described, "After using a fake name generator to create the account name, we search the name on Google Plus and choose a profile, then select a random person from the list of followers and use their image for the account profile." In contrast, P10 stated, "We use no picture as the picture defines your demographics. Buyers do not want this now."

**Account Creation vs. Purchase:**
Six participants (P1, P3, P7, P10, P13, P16) create new accounts periodically, ranging from daily (P10) to monthly (P16). P2 and P9 create new accounts when they need more for a job, especially if the job requires accounts from a specific geographic region. P5 and P18 create new accounts when Google deletes some of their existing ones. P15 creates new accounts when the job requires more reviews than they can provide. Five participants (P1, P5, P13, P17) purchase new accounts. P1 claimed to have purchased over 10,000 accounts, while P13 claimed to have purchased 47,000. Two participants (P1 and P3) age their new accounts (1-2 months) before using them to post reviews.

### Validation and Efficacy of ASO

**Validation of Quantitative Study:**
Collecting ground truth fraud data attributed to the workers who created it is challenging. We believe any process to obtain such data should involve the workers. To validate the accounts claimed to be controlled by the 39 workers, we used co-review graphs. Figure 13 shows co-review graphs built over the accounts claimed to be controlled by F13 and F32, where edge width is proportional to the number of apps reviewed in common by the endpoint accounts.

**Efficacy of ASO:**
To investigate the efficacy of the ASO strategies, we examined:
1. The number of active and inactive accounts controlled by each worker.
2. The impact of their ASO campaigns.

Figure 15 shows the number of active and inactive accounts (i.e., Google returns a 404 error) controlled by the 39 workers. Of the 1,164 accounts, 120 were inactive (10.30%) in May 2019. Qualitative study participants stated they never abandon accounts unless closed by Google or if Google filters all their reviews. This reveals diverse success among the 39 ASO workers in keeping their accounts active long-term.

### Disruption Strategies and Recommendations

**VP1: Proactive Fraud Monitoring:**
Recruiting WhatsApp/Facebook groups should aggressively accept new collaborators. These communication channels are easy to infiltrate, so we recommend proactively detecting campaigns and flagging apps likely to receive fraudulent reviews and suspicious accounts engaged in posting fraud.

**VP2: Device Fingerprinting:**
Device models and their per-country popularity can be used to detect reviews written from accounts claiming to be from a country where the posting device is not popular. This vulnerability could also be used by ASO workers to blend in with normal users by mimicking the distribution of devices observed in Google Play.

**VP3: 1-to-1 Review-To-Device:**
Our interviews and experiments revealed that a user can download an application once and review it multiple times using different accounts logged into the same device (up to 5, claimed by P8). We suggest enforcing that a device can be used to post only one review per downloaded app.

**VP4: Organic Fraud Detection:**
Use account activity levels to differentiate organic from inorganic (sockpuppet) accounts. Organic ASO workers are likely to use their devices continuously, while sockpuppet accounts may experience inactive interludes. Account activity includes the number of apps interacted with, duration of interactions, and the number of other Google services subscribed to.

**VP5: Monitor Review Feedback:**
An account should be able to upvote or downvote a review only if it has installed the respective app on at least one device. This is currently not enforced by Google Play. Fraud attribution can also be used to discount upvotes from accounts known to be controlled by the same ASO worker.

**VP6: Verify App Install and Retention:**
Develop protocols to verify that an app has been or is still installed on the device before accepting a user review. We studied the impact of a worker on each app. The impact (IA) of an ASO worker (W) for an app (A) is the change in A’s rating during W’s active interval. Specifically, IA = Rf - Ri, where Ri is A’s initial average rating, and Rf is A’s final average rating after W’s last review. Figure 16 shows the distribution of impact values, revealing diverse abilities of these workers. Some workers, like F7, F12, and F21, had only positive impacts, while most had mixed impacts.

**VP7: Account Validation and Re-validation:**
The cellular provider used during account validation can detect inconsistencies with the claimed profile (e.g., location) of the user account. Several ASO workers use SIM cards of others to validate their accounts. Peer-opinion sites could ask users to re-validate their accounts at random login times, especially if their validating SIM cards have been used for other accounts.

**VP8: App Usage:**
Most ASO workers suggest using apps before reviewing them and keeping them installed for a while to mimic genuine behaviors. Features extracted from per-app waiting times, interaction modes, and post-review behaviors can be used to pinpoint sockpuppet and organic fraud accounts. Mandating wait times to post reviews will impact the number of apps an ASO worker can target at a time.

**VP9: Mislead ASO Workers Through Fraud Attribution:**
SIM cards can help attribute sockpuppet accounts to the ASO workers who control them. Account-to-ASO worker attribution can reduce the worker's ability to adjust to detection. Peer-opinion sites could show removed fake positive reviews only to the accounts used to post them, the other accounts suspected of being controlled by the same worker, and the account of the app developer.

**VP10: Once a Cheater, Always a Cheater:**
Our studies provide evidence that developers rehire ASO workers for the same and other apps they develop. We recommend monitoring overlapping accounts that review sets of apps by the same developer and red-flagging fraud developers early on.

### Conclusion

The varied capabilities, behaviors, and evasion strategies exhibited by the participants suggest that fraud detection solutions should be comprehensive. While some participants fit the mold of assumptions made in previous work, we present claims and evidence of evolution, perhaps fueled by the competitive nature of the market.