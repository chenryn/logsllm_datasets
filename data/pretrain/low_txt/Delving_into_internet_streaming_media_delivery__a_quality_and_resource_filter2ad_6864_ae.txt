### Stream Thinning and Quality Degradation

Stream thinning operates similarly to stream switching. To quantify the quality degradation caused by stream thinning, we define two key metrics:

1. **Thinning Duration**: The time interval from the "thinning" command to either the "un-thinning" command or the "stop playing" command. This metric reflects the duration of quality degradation experienced by the user.
2. **Thinning Interval**: The time between two consecutive stream thinning events, which indicates the frequency of such degradations.

Figures 15(a) and 15(b) illustrate the thinning duration and interval for video sessions longer than 30 seconds in home and business user workloads, respectively. As shown in Figure 15(a), over 70% of the thinning durations are less than 30 seconds. Figure 15(b) reveals that a majority (70% in home user workload and 82% in business user workload) of thinning intervals exceed 30 seconds.

### Bandwidth and Stream Cancellation

When bandwidth is insufficient to transmit the key frame of a video stream, the client may send a TEARDOWN command to cancel the video stream, and the server will then send only the audio. If the bandwidth improves, the client can re-establish the connection and request the video stream again.

### Summary of Internet Streaming Quality

Our extensive trace analysis and real experiments reveal that Fast Cache does not support rate adaptation in practice. In a streaming session with Fast Cache enabled, the client never requests a stream switch after the initial selection, even if the bandwidth changes. Thinning and video cancellation are also disabled. Consequently, when the bandwidth drops below the encoding rate, Fast Cache behaves like pseudo-streaming: the player stops to buffer data, plays for about five seconds (the play-out buffer size), and repeats this cycle. This configuration can lead to worse streaming quality compared to normal TCP streaming, especially during prolonged network congestion. Figure 16 shows that the rebuffing duration for Fast Cache is significantly longer than for normal TCP streaming in home user workloads due to the inability to switch to a lower rate stream.

Figures 17(a) and 17(b) present the cumulative distribution function (CDF) of playback duration for TCP-based video streaming sessions longer than 30 seconds in home and business user workloads. The three curves in each figure represent all sessions, sessions without quality degradations, and sessions with quality degradations (including rebuffing, stream switch, stream thinning, and video cancellation). For longer sessions, quality degradation is more likely. For instance, in the business user workload, 88% of sessions with quality degradations last more than 100 seconds, while only 58% of sessions without degradations last that long. Table 5 provides a breakdown of sessions with and without quality degradations for TCP-based video streaming sessions longer than 30 and 300 seconds in home and business user workloads. Quality degradation is less frequent in home user workloads, possibly due to shorter playback durations. For sessions longer than 30 seconds, 13%â€“40% still experience quality degradation, and this percentage increases for sessions longer than 300 seconds.

### Discussion: Coordinating Caching and Rate Adaptation

Fast Cache and rate adaptation are common techniques used to enhance streaming media experiences. Fast Cache buffers media data at a higher rate than the encoding rate to absorb network jitter, while rate adaptation switches to a lower bit rate stream during congestion. Both techniques have their advantages and limitations. Fast Cache can increase server load and generate extra traffic, while rate adaptation introduces non-trivial latency due to small play-out buffers.

**Coordinated Streaming** combines the benefits of both techniques. It sets upper and lower bounds on the client's play-out buffer. The upper bound prevents aggressive buffering, while the lower bound eliminates switch latency. When a session starts, the server transmits data at the highest possible rate until the lower bound is reached. Playback begins, and the client continues to buffer data at the highest rate until the buffer reaches its upper bound. With a full buffer, the client buffers data at the encoding rate. During network congestion, if the buffer drops below the lower bound, the client switches to a lower rate stream. The new stream should be chosen such that it can maintain normal playback and fill the buffer to its upper bound. When bandwidth improves, the client can switch to a higher rate stream.

We conducted an ideal experiment to validate this scheme. Setting the lower bound to 5 seconds and the upper bound to 30 seconds, we found that Coordinated Streaming significantly reduces rebuffing and over-supplied traffic. Figure 18(a) shows that the rebuffing ratio for Coordinated Streaming is close to zero, and Figure 18(b) indicates a 77% reduction in over-supplied traffic compared to Fast Cache. Figure 18(c) demonstrates that the switch handoff latency is nearly zero, and the number of stream switches is 33.4% of that in normal TCP-based streaming.

### Related Work

Previous studies have analyzed Internet streaming traffic in various environments. Li et al. [20] characterized Web-stored streaming media, while Mena et al. [21] and Wang et al. [29] studied Real audio and video traffic. Almeida et al. [10] and Chesire et al. [13] examined client session durations and object popularities. Cherkasova et al. [12] characterized enterprise media workloads. Yu et al. [31] studied user behavior in large-scale VOD systems. Padhye et al. [23] and Costa et al. [15] analyzed client interactivity in educational and entertainment media sites. Live streaming workloads have also been studied, but few have focused on the mechanisms, quality, and resource utilization of streaming media delivery. Chung et al. [14] and Nichols et al. [22] conducted experimental studies on RealNetworks and Windows streaming media, and Wang et al. [28] proposed a model for TCP-based streaming. Our study analyzes the delivery quality and resource utilization of streaming techniques based on a large-scale Internet streaming media workload.

### Conclusion

In this study, we collected a 12-day streaming media workload from a large ISP, including live and on-demand streaming for both audio and video. We characterized the streaming traffic requested by different user communities and hosting services. We analyzed commonly used techniques such as protocol rollover, Fast Streaming, MBR, and rate adaptation. Our analysis shows that these techniques often over-utilize CPU and bandwidth resources. A coordination mechanism combining Fast Streaming and rate adaptation is proposed to effectively utilize server and Internet resources. Trace-driven simulations demonstrate the effectiveness of this approach.

### Acknowledgments

We thank the anonymous referees for their valuable comments. William Bynum and Matti Hiltunen provided helpful suggestions on an early draft. This work is partially supported by the National Science Foundation under grants CNS-0405909 and CNS-0509054/0509061.

### References

[References remain the same as in the original text.]