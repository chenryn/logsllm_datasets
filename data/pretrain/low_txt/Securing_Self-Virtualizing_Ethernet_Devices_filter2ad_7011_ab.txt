### 3.4 Attacking VMs via Flow Control

Direct device assignment allows malicious guests to launch attacks on the Ethernet network using well-known Layer 2 vulnerabilities [14, 17, 47, 56, 73, 75]. Even with virtualization-aware switching extensions such as the Virtual Edge Port Aggregator (VEPA) [30, 31], all guests with direct access to the VFs of the same PF share the same physical link to the edge switch. The edge switch allocates processing resources per link.

Since both 802.3x and 802.1Qbb perform flow control at the link level, and the link is shared among VMs, any flow control manipulation by a single VM affects the PF and all associated VFs. This means that a malicious VM can control the bandwidth and latency of all VMs sharing the same adapter. The malicious VM can pause all traffic on the link by sending 802.3x pause frames or stop a specific traffic class by sending 802.1Qbb pause frames. To halt all traffic on a 10 Gbps Ethernet link, an attacker needs to transmit pause frames at a rate of 300 frames/second, which equates to approximately 155 Kbps of bandwidth. The attacker can fully control the bandwidth and latency of all tenant VMs with minimal resources and without cooperation from the host or other guest VMs.

### 4. Attack Evaluation

#### 4.1 Experimental Setup

We set up a laboratory environment to perform and evaluate the flow-control attack described in the previous section. Our setup includes a Dell PowerEdge R420 server with dual sockets, each containing six cores, powered by Intel Xeon E5-2420 CPUs running at 1.90 GHz. The chipset is the Intel C600 series, and the server has 16 GB of memory. We use an SRIOV-capable Intel NIC (10GbE 82599 or 1GbE I350) installed in PCIe generation 3 slots, with two VFs enabled.

The KVM Hypervisor [50] and Ubuntu Server 13.10 with a 3.11.0 x86_64 kernel are used for the host, guest VMs, and the client. Each guest VM is configured with 2 GB of memory, two virtual CPUs, and one VF directly assigned to it. The client and host machines are identical servers connected to the same dedicated switch, as shown in Figure 3.

To ensure consistent results, the server’s BIOS profile is optimized for performance, all power optimizations are disabled, and Non-Uniform Memory Access (NUMA) is enabled. The guest virtual CPUs are pinned to the cores on the same NUMA node as the Intel PF, and the host allocates memory from the same NUMA node.

For our 1GbE environment, we use an Intel Ethernet I350-T2 network interface connected to a Dell PowerConnect 6224P 1Gb Ethernet switch. For our 10GbE environment, we use an Intel 82599 10 Gigabit TN network interface connected to an HP 5900AF 10Gb Ethernet switch.

The host and client use their distribution's default drivers with default configuration settings. Guest VMs use version 2.14.2 of the ixgbevf driver for the Intel 10G 82599 Ethernet controller virtual function and the default igbvf version 2.0.2-k for the Intel 1G I350 Ethernet controller virtual function. IEEE 802.3x Ethernet flow control is enabled on switch ports, and the Ethernet Maximal Transfer Unit (MTU) is set to 1500 bytes on all Ethernet switches and network interfaces.

#### 4.2 Benchmark Methodology

We conduct performance evaluations according to the methodology in RFC 2544 [25]. For throughput tests, we use an Ethernet frame size of 1518 bytes and measure maximal throughput without packet loss. Each throughput test runs for at least 60 seconds, and we take the average of five test cycles. For latency tests, we use 64-byte and 1024-byte messages. Each latency test runs for at least 120 seconds, and we measure the average of at least 15 test cycles. (While RFC 2544 dictates running 20 cycles, we obtained plausible results after 15 cycles, so we decided to reduce the test runtime by running each test only 15 cycles.)

**Benchmark Tools:**
- **iperf [3]:** Used for measuring throughput with the TCP stream test.
- **netperf [45]:** Used for measuring latency with the TCP RR test.

The iperf and netperf clients run on the client machine, while the servers run on VM1. We measure the bandwidth and latency from the client to VM1.

**Traffic Generators:**
- **tcpdump [44]:** Captures traffic.
- **tcpreplay [5]:** Sends previously captured and modified frames at the desired rate.

**Testbed Scheme:**
- **Figure 3:** Shows the testbed scheme, consisting of two identical servers, one acting as the client and the other as the host with an SRIOV-capable NIC. Two VFs are configured on the host’s SRIOV PF, with VF1 assigned to guest VM1 and VF2 to guest VM2. The client and host are connected to the same Ethernet switch. Traffic is generated between VM1 and the client using iperf and netperf, while VM2 acts as the attacking VM.

#### 4.3 Flow-Control Attack Implementation

We use tcpreplay [5] to send specially crafted 802.3x pause frames at the desired rate from the malicious VM2. When the switch receives a pause frame from VM2, it inhibits transmission of any traffic on the link between the switch and the PF, including the traffic between the client and VM1, for a certain number of pause time quanta. By sending pause frames from VM2, we can manipulate the bandwidth and latency of the traffic between VM1 and the client. The value of the pause time for each pause frame is 0xFFFF pause quanta units. Knowing the link speed, we can calculate the pause frame rate and impose precise bandwidth limits and latency delays on VM1. The results of the attack in both 1GbE and 10GbE environments are presented in Section 4.4.

**Note:** We use 802.3x pause frames for simplicity, but PFC frames could also be used. PFC uses the same flow control mechanism and has the same MAC control frame format. The only difference is the addition of seven pause time fields in PFC, which are padded in 802.3x frames.

#### 4.4 Attack Results

**Figures 4 and 5** show the results of the pause frame attack on victim throughput in the 1GbE and 10GbE environments, respectively. **Figures 4a and 5a** depict the victim (VM1) throughput under periodic attack by VM2. Every 10 seconds, VM2 transmits pause frames for 10 seconds at 30 frames/second (Figure 4a) and at 300 frames/second (Figure 5a). These figures clearly show that VM2 can gain complete control over VM1's throughput, stopping traffic on the link for ten seconds starting from the tenth second.

**Figure 6** shows the results of the pause frame attack on victim latency in the 10GbE environment. **Figure 6a** depicts the victim latency under the same periodic attack. In this test, we use 64B and 1024B messages. For better visualization, we lowered the attack rate to 150 pause frames/second. **Figure 6a** shows that the attacker can increase victim latency to 250% by running the attack at a rate of only 150 frames/second.

**Victim Throughput:**
- **Figures 4b and 5b** display the throughput of VM1 as a function of the rate of pause frames sent by VM2. From **Figure 4b**, we see that VM2 can pause all traffic on the 1GbE link with almost no effort, by sending pause frames at a rate of 30 frames/second. For the 10GbE link, VM2 needs to increase its rate to 300 frames/second. These results confirm the calculations in Table 1. **Figures 7a and 7b** confirm that the measured victim throughput is exactly as predicted, indicating that it is easily and completely controlled by the attacker.

These tests demonstrate that a malicious VM can precisely control the throughput of other VMs using the pause frame attack. Furthermore, the attack requires minimal effort from the attacker and is difficult to detect amid the high volume of network traffic. To halt all transmissions on a 10GbE link, the attacker only needs to send 64B pause frames at 300 frames/second, which is approximately 0.002% of the 14.88 million frames/second maximum frame rate for 10GbE. Detecting such an attack can be challenging, especially on a busy high-speed link like 10GbE or 40GbE.

**Victim Latency:**
- **Figure 6b** shows the victim’s latency as a function of the attacker’s pause frame rate. In this test, we measure the latency of 64-byte and 1024-byte messages. The figures for both 64B and 1024B messages indicate that the attacker can significantly increase latency with a relatively low frame rate.