### Evaluation of Adversarial Attacks on Link Prediction Heuristics

We evaluate the performance of several well-known link prediction heuristics, including PAGERANK [6], SimRank [17], and WLK [28]. For each heuristic, we report the average AUC (Area Under the Curve) over five runs, both for clean graph data and attacked graph data.

#### Methodology
To ensure a fair comparison, we trained SEAL using only graph structure features, as described in [34]. Specifically, the node information matrix contains only structural node labels. We then applied adversarial attacks using our efficient design, OGSP (Optimized Graph Structure Perturbation).

#### Results
Table 7 summarizes the AUC values for different heuristics under both clean and attacked conditions. The results show that the performance of most existing heuristics is even worse than random guessing (AUC < 0.5) when subjected to our attacks. This indicates that our attacks can be effectively transferred to several existing heuristics.

| **Heuristic** | **Clean Graph AUC** | **Attacked Graph AUC** |
|---------------|---------------------|------------------------|
| SEAL          | 0.902               | 0.002                  |
| CN            | 0.843               | 0.139                  |
| Jaccard       | 0.943               | 0.000                  |
| PA            | 0.947               | 0.000                  |
| AA            | 0.910               | 0.178                  |
| RA            | 0.346               | 0.000                  |
| Katz          | 0.827               | 0.000                  |
| PAGERANK      | 0.591               | 0.000                  |
| SimRank       | 0.246               | 0.000                  |
| WLK           | 0.780               | 0.000                  |

#### Adversary Capability Analysis
To analyze the performance of our attack with respect to the adversary's capability, we conducted four sets of experiments for each dataset, varying the number of nodes the adversary can manipulate. Specifically, we set the number of manipulable nodes as 25%, 50%, 75%, and 100% of the entire node set. The node set \( V_s \) was selected randomly for each run.

Figure 5 illustrates the link prediction margin for GGSP (Generalized Graph Structure Perturbation) versus OGSP. As the size of \( V_s \) increases, the performance degradation becomes more pronounced.

#### Dataset-Specific Observations
- **USAir**: The mounted attacks perform extremely well, indicating high transferability.
- **NS**: Similar to USAir, the attacks are highly effective.
- **Celegans**: The attacks also show significant performance degradation.
- **PB**: The attacks are less effective compared to other datasets, possibly due to the unique characteristics of this network.

#### Defense Against Attacks
The mounted attacks violate the fundamental assumptions used for link predictions, such as the Υ-decaying theory and the link formation mechanism. Complete elimination of these attacks is challenging. One potential defense strategy is adversarial training, which forces the model to assign similar outputs to both clean and perturbed data. However, this approach is left for future work.

### Related Work

#### Adversarial Attacks on Link Prediction
- **Zhou et al.** [18] investigated attacks on several heuristics for link prediction, categorizing them based on the maximum hop of neighbors needed to calculate similarity scores.
- **Chen et al.** [10] proposed an iterative gradient attack against GAE-based link prediction. In contrast, our attacks are more general and perform well across multiple heuristics.

#### Neural Networks for Link Prediction
- **Weisfeiler-Lehman Neural Machine (WLNM)** [33] was the first attempt to use DNNs for link prediction by encoding subgraphs into fixed-size adjacency matrices.
- **SEAL** [34] is a state-of-the-art framework for link prediction using graph neural networks, which has been shown to unify a wide range of existing heuristics through the Υ-decaying theory.

#### Adversarial Attacks on Machine Learning
- **Huang et al.** [16] categorized attacks into causative (poisoning the training dataset) and evasion (crafting adversarial examples to evade pre-trained models).
- Our work focuses on evasion attacks that aim to mislead the SEAL framework while preserving unnoticeability.

### Concluding Remarks
We have demonstrated that adversarial attacks on graphs can significantly degrade the performance of the SEAL framework for link prediction. These attacks can achieve high success rates and reduce the model's performance to below random guessing levels. By incorporating the Υ-decaying theory and the link formation mechanism, our attacks are generated efficiently and effectively. Extensive experiments show that the attacks can be transferred to several existing heuristics, highlighting the need for robust defenses in graph-based machine learning systems.

### References
[1] Robert Ackland et al. 2005. Mapping the US Political Blogosphere: Are Conservative Bloggers More Prominent? BlogTalk Downunder 2005 Conference, Sydney.

[2] Lada A Adamic and Eytan Adar. 2003. Friends and Neighbors on the Web. Social Networks 25, 3 (2003), 211–230.

[3] Mohammad Al Hasan, Vineet Chaoji, Saeed Salem, and Mohammed Zaki. 2006. Link Prediction using Supervised Learning. SDM06: Workshop on Link Analysis, Counter-Terrorism and Security.

[4] Albert-László Barabási and Réka Albert. 1999. Emergence of Scaling in Random Networks. Science 286, 5439 (1999), 509–512.

[5] Vladimir Batagelj and Andrej Mrvar. 2006. http://vlado.fmf.unilj.si/pub/networks/data/

[6] Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks and ISDN Systems 30, 1-7 (1998), 107–117.

[7] Emrah Budur, Seungmin Lee, and Vein S Kong. 2015. Structural Analysis of Criminal Network and Predicting Hidden Links using Machine Learning. arXiv preprint arXiv:1507.05739 (2015).

[8] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. Proc. IEEE Symposium on Security and Privacy (S&P 2017).

[9] Nicholas Carlini and David Wagner. 2018. Audio Adversarial Examples: Targeted Attacks on Speech-to-Text. arXiv preprint arXiv:1801.01944 (2018).

[10] Jinjin Chen, Ziqiang Shi, Yangyang Wu, Xuanheng Xu, and Haibin Zheng. 2018. Link Prediction Adversarial Attack. arXiv preprint arXiv:1803.06373 (2018).

[11] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. 2018. EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples. Proc. AAAI Conference on Artificial Intelligence (AAAI 2018).

[12] Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto Perdisci, Manos Antonakakis, and Nikolaos Vasiloglou. 2017. Practical Attacks against Graph-Based Clustering. Proc. ACM SIGSAC Conference on Computer and Communications Security (CCS 2017).

[13] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. 2018. Adversarial Attack on Graph Structured Data. Proc. International Conference on Machine Learning (ICML 2018).

[14] Yuxiao Dong, Jie Tang, Sen Wu, Jilei Tian, Nitesh V Chawla, Jinghai Rao, and Huanhuan Cao. 2012. Link Prediction and Recommendation across Heterogeneous Social Networks. Proc. IEEE International Conference on Data Mining (ICDM 2012). 181–190.

[15] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2017. Adversarial Examples for Malware Detection. Proc. European Symposium on Research in Computer Security (ESORICS 2017). Springer.

[16] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and JD Tygar. 2011. Adversarial Machine Learning. Proc. ACM Workshop on Security and Artificial Intelligence.

[17] Glen Jeh and Jennifer Widom. 2002. SimRank: A Measure of Structural-Context Similarity. Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2002). 538–543.

[18] Harini Kannan, Alexey Kurakin, and Ian Goodfellow. 2018. Adversarial Logit Pairing. arXiv preprint arXiv:1803.06373 (2018).

[19] Leo Katz. 1953. A New Status Index Derived from Sociometric Analysis. Psychometrika 18, 1 (1953), 39–43.

[20] David Liben-Nowell and Jon Kleinberg. 2007. The Link-Prediction Problem for Social Networks. Journal of the American Society for Information Science and Technology 58, 7 (2007), 1019–1031.

[21] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017. Delving into Transferable Adversarial Examples and Black-Box Attacks. Proc. International Conference on Learning Representation (ICLR 2017).

[22] Linyuan Lü and Tao Zhou. 2011. Link Prediction in Complex Networks: A Survey. Physica A: Statistical Mechanics and Its Applications 390, 6 (2011), 1150–1170.

[23] Yacin Nadji, Manos Antonakakis, Roberto Perdisci, and Wenke Lee. 2013. Connected Colors: Unveiling the Structure of Criminal Networks. Proc. International Workshop on Recent Advances in Intrusion Detection. Springer.

[24] Mark EJ Newman. 2006. Finding Community Structure in Networks using the Eigenvectors of Matrices. Physical Review E 74, 3 (2006), 036104.

[25] Joshua O’Madadhain, Jon Hutchins, and Padhraic Smyth. 2005. Prediction and Ranking Algorithms for Event-Based Network Data. ACM SIGKDD Explorations Newsletter 7, 2 (2005), 23–30.

[26] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. arXiv preprint arXiv:1605.07277 (2016).

[27] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks against Machine Learning. Proc. ACM on Asia Conference on Computer and Communications Security (AsiaCCS 2017). ACM, 506–519.

[28] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. 2011. Weisfeiler-Lehman Graph Kernels. Journal of Machine Learning Research 12, Sep (2011), 2539–2561.

[29] Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2017. The Space of Transferable Adversarial Examples. arXiv preprint arXiv:1704.03453 (2017).

[30] Binghui Wang and Neil Zhenqiang Gong. 2019. Attacking Graph-based Classification via Manipulating the Graph Structure. Proc. ACM SIGSAC Conference on Computer and Communications Security (CCS 2019).

[31] Hao Wang, Xingjian Shi, and Dit-Yan Yeung. 2017. Relational Deep Learning: A Deep Latent Variable Model for Link Prediction. Proc. AAAI Conference on Artificial Intelligence.

[32] Duncan J Watts and Steven H Strogatz. 1998. Collective Dynamics of “Small-World” Networks. Nature 393, 6684 (1998), 440.

[33] Muhan Zhang and Yixin Chen. 2017. Weisfeiler-Lehman Neural Machine for Link Prediction. Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2017).

[34] Muhan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neural Networks. Proc. International Conference on Neural Information Processing Systems (NeurIPS 2018).

[35] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An End-to-End Deep Learning Architecture for Graph Classification. Proc. AAAI Conference on Artificial Intelligence (AAAI 2018).

[36] Tao Zhou, Linyuan Lü, and Yi-Cheng Zhang. 2009. Predicting Missing Links via Local Information. The European Physical Journal B 71, 4 (2009), 623–630.

[37] Daniel Zugner, Amir Akbarnejad, and Stephan Gunnemann. 2018. Adversarial Attacks on Neural Networks for Graph Data. Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2018).

[38] Daniel Zugner and Stephan Gunnemann. 2019. Adversarial Attacks on Graph Neural Networks via Meta Learning. Proc. International Conference on Learning Representation (ICLR 2019).