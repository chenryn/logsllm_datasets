### 4.3 Analytical Methodology

We examine two sets of data for our study. For intra-data center reliability, we use seven years of service-level event (SEV) data collected from our SEV database (cf. §4.2). For inter-data center reliability, we use eighteen months of data on fiber repairs collected between October 2016 and April 2018. Below, we describe the analysis for each data source.

#### 4.3.1 Intra-Data Center Networks

For intra-data center reliability, we analyze network incidents in three aspects: (1) root cause, (2) network device types, and (3) network architecture (cluster-based design versus data center fabric).

**Root Cause Analysis:**
The root causes are chosen by the engineers who authored the corresponding SEV reports. The root cause category, listed in Table 2, is a mandatory field in our SEV authoring workflow.

**Device Type Classification:**
To classify network incidents by the type of offending devices, we leverage the naming convention enforced at Facebook, where each network device is named with a unique, machine-understandable string prefixed with the device type. For example, every rack switch has a name prefixed with "rsw." By parsing the prefix of the offending device's name, we can classify the SEVs based on device types.

**Network Architecture Classification:**
Recall from Figure 1, CSA and CSW belong to classic cluster-based networks, while ESW, SSW, and FSW devices are part of the data center fabric.

#### 4.3.2 Inter-Data Center Networks (Backbone)

For inter-data center reliability, we study the reliability of end-to-end fiber links (§3.2) based on repair tickets from fiber vendors whose links form Facebook’s backbone networks connecting the data centers. Facebook has extensive monitoring systems that check the health of every fiber link, as unavailability of these links could significantly affect ingress traffic or disconnect a data center from the rest of our infrastructure.

When a vendor starts repairing a link (when the link is down) or performing maintenance, Facebook is notified via email. The email includes structured information such as the logical IDs of the fiber link, the physical location of the affected fiber circuits, the starting time of the repair/maintenance, and the estimated duration. Similarly, when the vendor completes the repair/maintenance, they send an email for confirmation. These emails are automatically parsed and stored in a database for later analysis. We examine eighteen months of repair data from this database, ranging from October 2016 to April 2018. From this data, we measure the mean time to incident (MTTI) and mean time to repair (MTTR) of fiber links.

#### 4.3.3 Limitations and Conflating Factors

Conducting a longitudinal study of failures at a company of Facebook’s scale presents several challenges. Our study has limitations and conflating factors:

- **Absolute vs. Relative Number of Failures:** Due to legal reasons, we cannot report the absolute number of failures. Instead, we report failure rates using a fixed baseline when the trend of the absolute failures aids our discussion.
- **Logged vs. Unlogged Failures:** Our intra-data center network study relies on SEVs reported by employees. While Facebook fosters a culture of opening SEVs for all incidents affecting production, we cannot guarantee our incident dataset is exhaustive.
- **Technology Changes Over Time:** Switch hardware consists of a variety of devices sourced and assembled from multiple vendors. We do not account for these factors in our study. Instead, we analyze trends by switch type when a switch’s architecture significantly deviates from others.
- **Switch Maturity:** Switch architectures vary in their lifecycle, from newly-introduced switches to those ready for retirement. We differentiate between a switch’s maturity in Facebook’s fleet.

Throughout our analysis, we state when a factor not under our control may affect our conclusions.

### 5 Intra-Data Center Reliability

In this section, we study the reliability of data center networks. We analyze network incidents within Facebook data centers over the course of seven years, from 2011 to 2018, comprising thousands of real-world events. A network incident occurs when the root cause of a SEV relates to a network device. We analyze root causes (§5.1), incident rate (§5.2), incident severity (§5.3), incident distribution (§5.4), incidents by network design (§5.5), and switch reliability (§5.6).

#### 5.1 Root Causes

- **Maintenance Failure:** Maintenance failures contribute the most documented incidents.
- **Human Errors:** There are twice as many human errors as hardware errors.

Table 2 lists network incident root causes. If a SEV has multiple root causes, it is counted toward multiple categories. If a SEV has no root causes, it is classified as undetermined. Human classification of root causes implies SEVs can be misclassified [53, 64]. While the rest of our analysis does not depend on the accuracy of root cause classification, examining the types of root causes is instructive.

We find that 29% of network incidents have undetermined root causes. These SEVs typically correspond to transient and isolated incidents where engineers only reported the symptoms. Wu et al. noted a similar fraction of unknown issues (23%, [75], Table 1), while Turner et al. had a smaller, but higher-fidelity, set of data (5%, [74], Table 5).

Maintenance failures contribute the most documented root causes (17%). This suggests that despite efforts to automate and standardize maintenance procedures, maintenance failures will still occur and lead to disruptions. Therefore, it is important to build mechanisms for quickly and reliably routing around faulty devices or devices under maintenance.

Hardware failures represent 13% of the root causes, while human-induced software issues—bugs and misconfigurations—occur at nearly double the rate of those caused by hardware failures. Prior studies, such as Turner et al. [74] and Wu et al. [75], observe incident rates within 7% of ours, suggesting hardware incidents remain a fundamental issue. Misconfiguration causes as many incidents as faulty hardware, something outside of operator control. This corroborates the findings of prior work that report misconfiguration as a large source of network failures in data centers [14, 15, 33, 47, 49, 62, 75], and shows the potential for emulation, verification, and more extensive automated repair approaches to reduce the number of incidents [11–13, 25, 26, 28, 45, 47].

Compared to prior work, we corroborate the lower rate of configuration-related incidents in Turner et al. [74], Table 5 (9%), but contrast with Wu et al. [75], Table 1, who found configuration incidents dominate (38%). We suspect network operators play a large role in determining how configuration causes network incidents. At Facebook, for example, all configuration changes require code review and are typically tested on a small number of switches before being deployed to the fleet. These practices may contribute to the lower misconfiguration incident rate we observe compared to Wu et al.

A mix of accidents and capacity planning makes up the last 15% of incidents (cf. Table 2). This highlights the many sources of entropy in large-scale production data center networks. Designing network devices to tolerate all these issues in practice is prohibitively difficult (if not impossible). Therefore, one reliability engineering principle is to prepare for the unexpected in large-scale data center networks.

Figure 2 further illustrates the root cause breakdown across different types of network devices. Note that the major root cause categories, including undetermined, maintenance, configuration, hardware, and bugs, have relatively even representation across all types of network devices. Some root cause categories are represented unequally among devices due to the small number of incidents, leading to too small a population to be statistically meaningful. For example, ESWs, which represent a smaller population size compared to SSWs, FSWs, and RSWs, do not have SEVs with a "bug" root cause. In fact, ESWs run the same Facebook Open Switching System (FBOSS)-based software stack [5, 69] as SSWs and FSWs, which do have network incidents caused by bugs.

#### 5.2 Incident Rate

- **Higher Bandwidth Devices:** Higher incident rates on higher bandwidth devices.
- **Fabric Devices:** Lower incident rates on fabric devices.
- **Better Repair Practices:** Better repair practices lower incident rates.

The reliability of data center networks is determined by the reliability of each interconnected network device. To measure the frequency of incidents related to each device type, we define the incident rate of a device type as \( r = \frac{i}{n} \), where \( i \) denotes the number of incidents caused by this type of network device and \( n \) is the number of active devices of that type (the population). Note that the incident rate could be larger than 1.0, meaning that each device of the target type caused more than one network incident on average. Figure 3 shows the incident rate of each type of network device in Facebook’s data centers over the seven-year span. From Figure 3, we draw four key observations.

First, network devices with higher bisection bandwidth (e.g., Cores and CSAs in Figure 1) generally have higher incident rates, in comparison to devices with lower bisection bandwidth (e.g., RSWs). Intuitively, devices with higher bisection bandwidth tend to affect a larger number of connected devices and are thus correlated with more widespread impact when these types of devices fail. The annual incidence rate for ESWs, SSWs, FSWs, RSWs, and CSWs in 2017 is less than 1%.

These reliability characteristics largely influence our fault-tolerant data center network design. For example, we currently provision eight Cores in each data center, which allows us to tolerate one unavailable Core (e.g., if it must be removed from operation for maintenance) without any impact on the data center network. Note that nearly all of the Cores and CSAs are third-party vendor switches. In principle, if we do not have direct control of the proprietary software on these devices, the network design and implementation must take this into consideration. For example, it may be more challenging to diagnose, debug, and repair switches that rely on firmware whose source code is unavailable. In these cases, it may make sense to increase the redundancy of switches in case some must be removed for repair by a vendor.

Second, devices in the fabric network design (including ESWs, SSWs, and FSWs) have much lower incident rates compared to those devices in the cluster design (CSAs and CSWs). There are two differences between the fabric network devices and the cluster devices: (a) the devices deployed in the data center fabric are built from commodity chips [4, 5], while devices in the cluster network are purchased from third-party vendors, and (b) the fabric network employs automated remediation software to handle common sources of failures [65].

Third, the fact that fabric-based devices are less frequently associated with failures verifies that fabric-based data center networks, equipped with automated failover and remediation, are more resilient to device failures. Specifically, we can see excessive CSA-related incidents during 2013 and 2014, where the number of incidents exceeds the number of CSAs (with the incident rate as high as 1.7× and 1.5×, respectively). These high incidence rates were motivation to transition from the cluster network to the fabric network.

Fourth, the CSA-related incident rate decreased in 2015, while the Core-related incident rate has generally increased from pre-2015 levels. This trend can be attributed to three causes: (1) the decreasing size of the CSA population, (2) the decreased development of their software as they were being replaced by newer fabric networks, and (3) new repair practices that were adopted around the time. For example, prior to 2014, network device repairs were often performed without draining the traffic on their links. This meant that in the worst case, when things went wrong, maintenance could affect a large volume of traffic. Draining devices prior to maintenance provided a simple but effective means to limit the likelihood of repair affecting production traffic. In addition, the data center fabric, with its more resilient design and higher path diversity, has been proven to be more amenable to automated remediation.

Network devices with higher bisection bandwidth have a higher likelihood of affecting service-level software, and network devices built from commodity chips have much lower incident rates compared to devices from third-party vendors due to the ease of integration with automated failover and remediation software.

#### 5.3 Incident Severity

- **Core Devices:** Core devices have the most incidents, but low severity.
- **Fabric Devices:** Fewest incidents on fabric devices.
- **Lowest Severity on Fabric Devices:** Not all incidents are created equal. Facebook classifies incidents based on their severity, with SEV1 being the most severe and SEV3 being the least severe.