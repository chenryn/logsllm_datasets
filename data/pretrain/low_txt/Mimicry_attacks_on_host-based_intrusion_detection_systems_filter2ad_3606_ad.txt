### Zero Mismatches in the Database

Our experiments demonstrated that we could transform the original attack sequence into a modified variant that would not trigger a single mismatch, yet still achieve a similarly harmful effect. Notably, there was no need to exploit the fact that the pH Intrusion Detection System (IDS) allows for occasional mismatches without triggering an alarm. Our attack would be successful regardless of the pH locality frame count threshold setting, making our results even more significant.

### Summary of Findings

Our experiments indicate that sophisticated attackers can evade the pH IDS. We were surprised by the success of the mimicry attack in converting the autowux script into one that avoided detection. Initially, we were concerned that the unusual system call sequence in the payload might limit our ability to modify the script. However, it appears that the database of normal system call sequences is rich enough to provide the attacker with considerable power.

### Limitations of the Experimental Methodology

We acknowledge several significant limitations in our experimental methodology:

1. **Lack of Comprehensive Testing**: We did not compile the stealthy sequence into a modified exploit script or test it against a machine protected by pH.
2. **Assumptions on Exploit Modification**: We assumed that modifications to the autowux exploit sequence would not affect the success of the attack. A more convincing example would have been one where the attack did not require such modifications.
3. **Limited Scope**: We tested only one exploit script (autowux), one vulnerable application (wuftpd), one operating system (Redhat Linux), one system configuration (default Redhat 5.0 installation), and one IDS (pH). This scope is sufficient to establish the presence of a risk but does not provide enough data to assess the magnitude of the risk or how differences in operating systems or configurations might affect it.
4. **Practicality of the Attack**: We did not evaluate the practicality of the attack, including the effort and knowledge required from an attacker.
5. **Predictability and Diversity**: We did not empirically test how effectively one can predict the target host's configuration and IDS normal database, nor did we measure whether database diversity is a significant barrier to the attack.
6. **Vulnerability Coverage**: We did not estimate what percentage of vulnerabilities would allow the attacker to mount a mimicry attack and inject enough foreign code to execute the entire stealthy sequence.
7. **Evolving Threats**: Attacks often improve over time, so it may be too soon to draw definitive conclusions about the level of risk associated with mimicry attacks.

### Validity of the Test Results

Because pH uses lookahead pairs, stide is more restrictive than pH. However, since our modified sequence was accepted by stide, we can expect it to be accepted by pH as well. Using stide makes our experiment even more meaningful, as it indicates that stide-based IDSs will also be vulnerable to mimicry attacks.

### Related Work

Recent research has explored the security of host-based anomaly detection systems against sophisticated, adaptive adversaries. Wagner and Dean briefly sketched the idea of mimicry attacks [25, x6]. Gi(n), Jha, and Miller elaborated on this by outlining a metric for susceptibility to evasion attacks based on attack automata [6, x4.5]. Somayaji suggested that evading the pH IDS might be possible in principle but difficult in practice, providing a brief example [22, x7.5]. None of these papers developed these ideas in depth or examined their implications, but they set the stage for future research.

More recently, Tan, Killourhy, and Maxion provided a thorough treatment of the issue [23], showing how attackers can render host-based IDSs blind to their attacks. In follow-up work, Tan, McHugh, and Killourhy refined the technique and provided further experimental confirmation of the risk [24]. Their methods differ from ours, but their results align with our findings.

### Discussion

Several lessons emerge from these experiments:

1. **Designing Resilient IDSs**: Where possible, intrusion detection systems should be designed to resist mimicry attacks and other stealthy behavior from sophisticated attackers. Specific recommendations include:
   - Monitoring not only attempted system calls but also failed calls and error codes.
   - Monitoring and predicting arguments passed to system calls.
   - Minimizing and precisely defining the database of normal behavior to reduce the attacker's freedom.

2. **Security Analysis in Future Research**: All future published work proposing new IDS designs should include a detailed analysis of the proposal’s security against evasion attacks. Even if complete countermeasures are not possible, evaluating the risks is worthwhile.

3. **Public Release of Implementations**: IDS designers should publicly release full implementations of their designs to enable independent security analysis. We were unable to examine several proposed intrusion detection techniques due to a lack of access to reference implementations.

### Conclusions

We have shown how attackers may be able to evade detection in host-based anomaly intrusion detection systems and presented initial evidence that some IDSs may be vulnerable. While the seriousness of mimicry attacks in practice is unclear, the lesson is that protecting against today’s attacks is not enough; we must also defend against tomorrow’s attacks, considering that future attackers may adapt to current protection measures. More attention should be paid in the intrusion detection community to security against adaptive attackers, and we hope this will stimulate further research in this area.

### Acknowledgements

We thank Umesh Shankar, Anil Somayaji, and the anonymous reviewers for their insightful comments on an earlier draft of this paper. We are also indebted to Somayaji for making the pH source code publicly available, which made this research possible.

### References

[1] M. Chung, N. Puketza, R.A. Olsson, B. Mukherjee, "Simulating Concurrent Intrusions for Testing Intrusion Detection Systems: Parallelizing Intrusions," National Information Systems Security Conference, pp. 173-183, 1995.

[2] S. Forrest, S.A. Hofmeyr, A. Somayaji, T.A. Longstaff, "A Sense of Self for Unix Processes," 1996 IEEE Symposium on Security & Privacy.

[3] S. Forrest, A.S. Perelson, L. Allen, R. Cherukuri, "Self-Nonself Discrimination in a Computer," 1994 IEEE Symposium on Security & Privacy.

[4] A.K. Ghosh, A. Schwartzbard, M. Schatz, "Learning Program Behavior Profiles for Intrusion Detection," 1st USENIX Workshop on Intrusion Detection & Networking Monitoring, 1999.

[5] A.K. Ghosh, A. Schwartzbard, M. Schatz, "Using Program Behavior Profiles for Intrusion Detection," 3rd SANS Workshop on Intrusion Detection & Response, 1999.

[6] J.T. Gi(n), S. Jha, B.P. Miller, "Detecting Manipulated Remote Call Streams," 11th USENIX Security Symposium, 2002.

[7] M. Handley, C. Kreibich, V. Paxson, "Network Intrusion Detection: Evasion, Traffic Normalization, and End-to-End Protocol Semantics," 10th USENIX Security Symposium, 2001.

[8] S. Hofmeyr, S. Forrest, A. Somayaji, "Intrusion Detection Using Sequences of System Calls," Journal of Computer Security, vol. 6, pp. 151-180, 1998.

[9] G.J. Holzmann, Design and Validation of Computer Protocols, Prentice-Hall, 1990.

[10] G.J. Holzmann, "The Model Checker Spin," IEEE Trans. on Software Engineering, Special issue on Formal Methods in Software Practice, May 1997.

[11] J.E. Hopcroft, J.D. Ullman, Introduction to Automata Theory, Languages, and Computation, Addison-Wesley, 1979.

[12] T. Lane, C.E. Brodley, "Sequence Matching and Learning in Anomaly Detection for Computer Security," AAAI Workshop: AI Approaches to Fraud Detection and Risk Management, pp. 49-49, 1997.

[13] T. Lane, C.E. Brodley, "Temporal Sequence Learning and Data Reduction for Anomaly Detection," ACM Trans. Information & System Security, vol. 2, no. 3, pp. 295-331, 1999.

[14] W. Lee, S.J. Stolfo, "Data Mining Approaches for Intrusion Detection," 7th USENIX Security Symposium, 1998.

[15] W. Lee, S.J. Stolfo, K. Mok, "A Data Mining Framework for Building Intrusion Detection Models," 263.

[16] K.L. McMillan, Symbolic Model Checking, Kluwer Academic Publishers, 1993.

[17] C. Michael, A. Ghosh, "Using Finite Automata to Mine Execution Data for Intrusion Detection: A Preliminary Report," RAID 2000, LNCS 1907, pp. 66-79, 2000.

[18] V. Paxson, "Bro: A System for Detecting Network Intruders in Real-Time," Computer Networks, 31(23-24), pp. 2435-2463, 14 Dec. 1999.

[19] T.H. Ptacek, T.N. Newsham, "Insertion, Evasion, and Denial of Service: Eluding Network Intrusion Detection," Secure Networks, Jan. 1998.

[20] F. Schneider, "Enforceable Security Policies," ACM Transactions on Information & System Security, vol. 3, no. 1, pp. 30-50, Feb. 2000.

[21] A. Somayaji, S. Forrest, "Automated Response Using System-Call Delays," 9th Usenix Security Symposium, pp. 185-197, 2000.

[22] A.B. Somayaji, "Operating System Stability and Security through Process Homeostasis," Ph.D. dissertation, Univ. New Mexico, Jul. 2002.

[23] K.M.C. Tan, K.S. Killourhy, R.A. Maxion, "Undermining an Anomaly-Based Intrusion Detection System Using Common Exploits," to appear at RAID 2002, 16-18 Oct. 2002.

[24] K. Tan, J. McHugh, K. Killourhy, "Hiding Intrusions: From the Abnormal to the Normal and Beyond," to appear at 5th Information Hiding Workshop, 7-9 Oct. 2002.

[25] D. Wagner, D. Dean, "Intrusion Detection via Static Analysis," IEEE Symposium on Security & Privacy, 2001.

[26] C. Warrender, S. Forrest, B. Pearlmutter, "Detecting Intrusions Using System Calls: Alternative Data Models," 1999 IEEE Symposium on Security & Privacy.

[27] A. Wespi, M. Dacier, H. Debar, "Intrusion Detection Using Variable-Length Audit Trail Patterns," RAID 2000, LNCS 1907, pp. 110-129, 2000.

### Appendix: Extensions: How to Model More Sophisticated IDSs

Forrest et al. have proposed counting the total number of mismatched length-6 subtraces and only triggering an alarm if the total mismatch count exceeds a certain threshold, say 7 mismatches. This gives the intruder an extra degree of freedom, as the exploit code can cause a few mismatches as long as they are not too numerous.

We can extend our finite-state model to account for this degree of freedom by adding an extra dimension to the state space, counting the number of mismatches so far. Thus, the state space becomes \(Q' = (Q \times \{0, 1, \ldots, 6\}) \cup \{\text{Alarm}\}\), and each non-alarm state is a pair \((q, m)\) of a state \(q \in Q\) from the old model and a count \(m \in \{0, \ldots, 6\}\) of the number of mismatches seen so far. For each non-alarm transition \(q \rightarrow q'\) in the old model, we introduce transitions \((q, m) \rightarrow (q', m)\) for each \(m = 0, 1, \ldots, 6\). Also, for each alarm transition \((s_0, \ldots, s_4) \rightarrow \text{Alarm}\) in the old model, we introduce transitions \(((s_0, \ldots, s_4), m) \rightarrow ((s_1, \ldots, s_4, s), m + 1)\) for each \(m = 0, 1, \ldots, 6\), where we view the notation \((q, 7)\) as shorthand for the Alarm state. As usual, we introduce self-loops from Alarm to itself on each system call, and the accepting states are exactly the non-alarm states. Note that the size of the automaton has increased by only a small constant factor, making this transformation practical.