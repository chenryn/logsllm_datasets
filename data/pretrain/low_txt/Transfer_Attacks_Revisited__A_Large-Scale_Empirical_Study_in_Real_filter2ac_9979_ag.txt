### Optimized Text

**Evaluation of Adversarial Confidence and Transferability**

We find that SIK (Success Indicator for Kappa) is a suitable metric for assessing adversarial confidence in transfer attacks, whereas SSK (Success Successor for Kappa) is not sufficiently representative. In practical scenarios, it is important to determine whether increasing the SSK for the CW (Carlini-Wagner) attack effectively enhances transferability. To address this, we conducted CW attacks with varying κ (kappa) thresholds. The results are presented in Figure 9. Figures 9b and 9c illustrate that a higher SSK does not consistently improve transferability. For instance, in the blue line of Figure 9, the number of AEs (Adversarial Examples) decreases by approximately 25% when SSK is increased from 60 to 200, yet the transfer rate against the AWS platform does not improve. This indicates that increasing SSK is not a reliable method for enhancing real-world transfer attacks.

**Observation 9:**
The logit difference between the adversarial class and the second most likely class, known as the κ value, is neither an effective measure of transferability nor a useful tool for improving it.

**Intrinsic Classification Hardness and Transferability**

An image is defined as a natural AE with respect to a surrogate model if it can deceive the model without any additional perturbation. In real-world transfer attacks, natural AEs are inevitable because the surrogate model cannot achieve 100% accuracy. We sought to determine whether perturbing natural AEs could enhance transferability.

Figure 10 compares the transferability of AEs generated from natural AEs and those generated from other seed images. The results show that AEs derived from natural AEs consistently achieve higher success rates than those from other seed images. This suggests that intrinsic classification hardness is a crucial factor in determining transferability, and real-world attackers should prefer natural AEs for more effective transfer attacks. This finding is intuitive, as natural-AE seeds are believed to be closer to the theoretical classification boundary, making them more susceptible to adversarial perturbations.

**Observation 10:**
AEs generated from seed images that are difficult to classify on surrogates exhibit better transferability.

### Ethical Considerations

**VI. Discussion**

During our evaluations on the platforms, we used official APIs for normal queries and paid the required usage fees. Thus, our evaluations were legitimate and did not disrupt the operations of the cloud services. The relevant companies were informed about our experiments to ensure no potential harm, such as the misuse of uploaded images. We received confirmation from AWS and Baidu.

**B. Limitations and Future Work**

Our aim is to provide a systematic understanding of factors affecting transferability. Some conclusions remain partially understood. For example, we have not validated the hypothesis that the number of local optima increases exponentially with task complexity. These arguments are beyond the scope of this paper and require further study. Additionally, we did not identify the optimal hyperparameters in real settings, which would require substantial resources. Our conclusions about attack algorithms are specific to the chosen hyperparameters and may not represent their performance under different conditions. For instance, we set κ = 0 for the CW attack, but a larger κ might be preferable for some attackers. However, we used κ = 0 to ensure a fair comparison with other attacks that do not manipulate κ.

### Related Works

Early research on adversarial attacks [12], [19], [30], [32], [44], [46] primarily focused on white-box attacks, where the adversary has complete access to the target model. In black-box settings, where information is limited, efforts have been directed towards two approaches: 1) query-based attacks that estimate gradient information to generate AEs [13], [21], and 2) transfer attacks that use a surrogate model to create AEs expected to transfer to an unknown target model [36], [37]. Our work focuses on transfer attacks.

The transferability of AEs in deep learning models was first observed by Szegedy et al. [44]. Papernot et al. [36], [37] leveraged this property to perform black-box attacks on real MLaaS (Machine Learning as a Service) systems, demonstrating its practicality. Carlini et al. [12] proposed that high-confidence AEs increase transferability.

Several studies have aimed to understand the transferability of AEs. Liu et al. [29] found that targeted AEs rarely transfer, unlike untargeted AEs, and suggested using an ensemble of surrogates to enhance the transferability of targeted attacks. Wu et al. [49] identified that reduced variance in gradients leads to better transferability. Su et al. [41] conducted a comprehensive analysis using 18 different surrogates and concluded that relaxing norm constraints on adversarial perturbations improves transferability. They also found that FGSM > PGD > CW in terms of transferability. Demontis et al. [15] highlighted that simpler surrogates and better gradient alignment result in better transferability, based on adding different levels of regularization. They also noted that targets with large input gradients are more vulnerable to transfer attacks. Our work aims to extend these findings to real-world applications and provide deeper insights. Specifically, when evaluating the relationship between surrogate complexity and transferability, we believe that using surrogates with different depths but the same architecture family is a better approach than changing regularization. Our conclusion about the non-monotonic effect of surrogate depth on transferability complements the findings of Demontis et al.

### Conclusion

In this paper, we address the challenges of evaluating the transferability of AEs in real-world scenarios and propose customized metrics to overcome these challenges. Using these metrics, we conducted a systematic evaluation of real-world transfer attacks on four popular MLaaS systems: Aliyun, Baidu Cloud, Google Cloud Vision, and AWS Rekognition. Our evaluation led to the following new conclusions, building upon existing ones from lab settings:
1. The concept of model similarity is not well-suited for transfer attacks.
2. Surrogates of appropriate complexity can outperform both simpler and more complex models.
3. MLaaS systems vary in their robustness to transfer attacks and could benefit from further improvements.
4. Strong adversarial algorithms do not always transfer better, and single-step algorithms often outperform iterative ones.
5. The choice of adversarial algorithm and the target platform are the most critical factors for transfer attacks.
6. No dominant surrogate architecture exists for real-world transfer attacks.
7. Large L2 norms of adversarial perturbations can be a more direct source of transferability than L∞ norms.
8. A larger gap between the posterior logits improves transferability.
9. Intrinsic classification hardness is a key factor when selecting seed images for transfer attacks.

### Acknowledgements

We thank our shepherd and the anonymous reviewers for their valuable suggestions. This work was partly supported by the Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars (No. LR19F020003), NSFC (Nos. 62102360, U1836202), and the Fundamental Research Funds for the Central Universities (Zhejiang University NGICS Platform). Ting Wang was partially supported by the National Science Foundation (Grants No. 1951729, 1953813, and 1953893).

### References

[1] Alibaba Cloud. https://aliyun.com.
[2] AWS Rekognition. https://aws.amazon.com/rekognition.
[3] AWS Rekognition Documentation. https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html.
[4] Baidu Cloud. https://bce.baidu.com.
[5] Definition of Additively Separable Functions. https://calculus.subwiki.org/wiki/Additively_separable_function.
[6] Google Cloud Vision. https://cloud.google.com/vision.
[7] Google Open Images V6+. https://storage.googleapis.com/openimages/web/index.html.
[8] PyTorch. https://pytorch.org/.
[9] pytorch/vision. https://github.com/pytorch/vision, June 2020. Original-date: 2016-11-09T23:11:43Z.
[10] Statistical Mechanics of Deep Learning. Annual Review of Condensed Matter Physics 11, 1 (2020), 501–528.
[11] Box, G. E. P., and Cox, D. R. An Analysis of Transformations. Journal of the Royal Statistical Society. Series B (Methodological) 26, 2 (1964), 211–252.
[12] Carlini, N., and Wagner, D. Towards Evaluating the Robustness of Neural Networks. In 2017 IEEE Symposium on Security and Privacy (SP) (May 2017), pp. 39–57. ISSN: 2375-1207.
[13] Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J. ZOO: Zeroth Order Optimization Based Black-Box Attacks to Deep Neural Networks without Training Substitute Models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (2017), AISec '17, Association for Computing Machinery, pp. 15–26.
[14] Chen, S., He, Z., Sun, C., and Huang, X. Universal Adversarial Attack on Attention and the Resulting Dataset DamageNet, 2020.
[15] Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., Nita-Rotaru, C., and Roli, F. Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks. In 28th USENIX Security Symposium (USENIX Security 19) (2019), pp. 321–338.
[16] Deng, J., Dong, W., Socher, R., Li, L., Kai Li, and Li Fei-Fei. Imagenet: A Large-Scale Hierarchical Image Database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition (2009), pp. 248–255.
[17] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (June 2019), Association for Computational Linguistics, pp. 4171–4186.
[18] Eidinger, E., Enbar, R., and Hassner, T. Age and Gender Estimation of Unfiltered Faces. IEEE Transactions on Information Forensics and Security 9, 12 (2014), 2170–2179.
[19] Goodfellow, I., Shlens, J., and Szegedy, C. Explaining and Harnessing Adversarial Examples. In International Conference on Learning Representations (2015).
[20] He, K., Zhang, X., Ren, S., and Sun, J. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016), IEEE, pp. 770–778.
[21] Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-Box Adversarial Attacks with Limited Queries and Information. In ICML 2018 (July 2018). arXiv: 1804.08598.
[22] Krizhevsky, A. Learning Multiple Layers of Features from Tiny Images.
[23] Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial Machine Learning at Scale, 2016.
[24] Kurakin, A., Goodfellow, I. J., and Bengio, S. Adversarial Examples in the Physical World. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings (2017), OpenReview.net.
[25] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE 86, 11 (1998), 2278–2324.
[26] Li, C., Wang, L., Ji, S., Zhang, X., Xi, Z., Guo, S., and Wang, T. Seeing is Living? Rethinking the Security of Facial Liveness Verification in the Deepfake Era. CoRR abs/2202.10673 (2022).
[27] Li, X., Ji, S., Han, M., Ji, J., Ren, Z., Liu, Y., and Wu, C. Adversarial Examples Versus Cloud-Based Detectors: A Black-Box Empirical Study. IEEE Trans. Dependable Secur. Comput. 18, 4 (2021), 1933–1949.
[28] Ling, X., Ji, S., Zou, J., Wang, J., Wu, C., Li, B., and Wang, T. DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model. In 2019 IEEE Symposium on Security and Privacy (SP) (2019), pp. 673–690.
[29] Liu, Y., Chen, X., Liu, C., and Song, D. Delving into Transferable Adversarial Examples and Black-Box Attacks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings (2017), OpenReview.net.
[30] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards Deep Learning Models Resistant to Adversarial Attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings (2018), OpenReview.net.
[31] Moosavi-Dezfooli, S., Fawzi, A., Fawzi, O., and Frossard, P. Universal Adversarial Perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017), pp. 86–94.
[32] Moosavi-Dezfooli, S., Fawzi, A., and Frossard, P. DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), pp. 2574–2582.
[33] Pang, R., Xi, Z., Ji, S., Luo, X., and Wang, T. On the Security Risks of AutoML. CoRR abs/2110.06018 (2021).
[34] Pang, R., Zhang, X., Ji, S., Luo, X., and Wang, T. AdvMind: Inferring Adversary Intent of Black-Box Attacks. In KDD '20 (2020), pp. 1899–1907.
[35] Pang, R., Zhang, Z., Gao, X., Xi, Z., Ji, S., Cheng, P., and Wang, T. TrojanZoo: Everything You Ever Wanted to Know About Neural Backdoors (But Were Afraid to Ask). CoRR abs/2012.09302 (2020).
[36] Papernot, N., McDaniel, P., and Goodfellow, I. Transferability in Machine Learning: From Phenomena to Black-Box Attacks Using Adversarial Samples. arXiv preprint arXiv:1605.07277 (2016).
[37] Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., and Swami, A. Practical Black-Box Attacks Against Machine Learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security (2017), ASIA CCS '17, Association for Computing Machinery, pp. 506–519. Event-place: Abu Dhabi, United Arab Emirates.
[38] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., and Swami, A. The Limitations of Deep Learning in Adversarial Settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P) (2016), IEEE, pp. 372–387.
[39] Shen, L., Ji, S., Zhang, X., Li, J., Chen, J., Shi, J., Fang, C., Yin, J., and Wang, T. Backdoor Pre-Trained Models Can Transfer to All. In CCS '21: 2021 ACM SIGSAC Conference on Computer and Communications Security, November 15 - 19, 2021 (2021), Y. Kim, J. Kim, G. Vigna, and E. Shi, Eds., ACM, pp. 3141–3158.
[40] Simonyan, K., and Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv:1409.1556 [cs] (Apr. 2015). arXiv: 1409.1556.
[41] Su, D., Zhang, H., Chen, H., Yi, J., Chen, P., and Gao, Y. Is Robustness the Cost of Accuracy? - A Comprehensive Study on the Robustness of 18 Deep Image Classification Models. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XII (2018), V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, Eds., vol. 11216 of Lecture Notes in Computer Science, Springer, pp. 644–661.
[42] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the Inception Architecture for Computer Vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016), IEEE, pp. 2818–2826.
[43] Szegedy, C., Wei Liu, Yangqing Jia, Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going Deeper with Convolutions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 1–9.
[44] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing Properties of Neural Networks. In International Conference on Learning Representations (2014).
[45] Tramèr, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., and McDaniel, P. Ensemble Adversarial Training: Attacks and Defenses. 6th International Conference on Learning Representations, ICLR 2018; Conference date: 30-04-2018 Through 03-05-2018.
[46] Tramèr, F., Kurakin, A., Papernot, N., Goodfellow, I. J., Boneh, D., and McDaniel, P. D. Ensemble Adversarial Training: Attacks and Defenses. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings (2018), OpenReview.net.
[47] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is All You Need, 2017.
[48] Wilcoxon, F. Individual Comparisons by Ranking Methods. Biometrics Bulletin 1, 6 (1945), 80–83.
[49] Wu, L., Zhu, Z., Tai, C., and E, W. Understanding and Enhancing the Transferability of Adversarial Examples. arXiv:1802.09707 [cs, stat]