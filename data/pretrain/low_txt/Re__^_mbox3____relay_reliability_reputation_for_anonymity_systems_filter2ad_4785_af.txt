### Acknowledgments

This research was partially supported by the National Science Foundation (NSF) under grant CNS-0953655 and an International Fulbright Science & Technology Fellowship.

### References

1. Emulab. <https://www.emulab.net>.
2. Known bad relays in Tor. <https://trac.torproject.org/projects/tor/wiki/doc/badRelays>.
3. PlanetLab. <http://www.planet-lab.org/>.
4. Tor Compass. <https://compass.torproject.org/>.
5. Tor Consensus. <https://metrics.torproject.org/consensus-health.html>.
6. Tor Controller.
7. Tor Proposal 209. <https://svn.torproject.org/svn/blossom/trunk/TorCtl.py>. <https://gitweb.torproject.org/user/mikeperry/torspec.git/blob/path-bias-tuning:/proposals/209-path-bias-tuning.txt>.
8. TorFlow Project. <https://gitweb.torproject.org/torflow.git>.
9. TorStatus. <http://torstatus.blutmagie.de/index.php>.
10. Trotsky IP addresses. <https://trac.torproject.org/projects/tor/wiki/doc/badRelays/trotskyIps>.
11. Tor directory authorities compromised, 2010. <https://blog.torproject.org/blog/tor-project-infrastructure-updates>.
12. E. Androulaki, M. Raykova, S. Srivatsan, A. Stavrou, and S. Bellovin. PAR: Payment for Anonymous Routing. In *Proceedings of the 8th Symposium on Privacy Enhancing Technologies* (PETS'08), pages 219–236. Springer Berlin Heidelberg, 2008.
13. R. Aringhieri, E. Damiani, S. D. C. Di Vimercati, S. Paraboschi, and P. Samarati. Fuzzy Techniques for Trust and Reputation Management in Anonymous Peer-to-peer Systems. *J. Am. Soc. Inf. Sci. Technol.*, 57(4):528–537, 2006.
14. K. Bauer, J. Juen, N. Borisov, D. Grunwald, D. Sicker, and D. McCoy. On the Optimal Path Length for Tor, 2010. <http://petsymposium.org/2010/papers/hotpets10-Bauer.pdf>.
15. K. Bauer, D. McCoy, D. Grunwald, T. Kohno, and D. Sicker. Low-resource Routing Attacks Against Tor. In *Proceedings of the 6th ACM Workshop on Privacy in the Electronic Society* (WPES '07), pages 11–20. ACM, 2007.
16. N. Borisov, G. Danezis, P. Mittal, and P. Tabriz. Denial of Service or Denial of Security? In *Proceedings of the 14th ACM Conference on Computer and Communications Security* (CCS '07), pages 92–102. ACM, 2007.
17. S. Buchegger and J. Y. Le Boudec. A Robust Reputation System for P2P and Mobile Ad-hoc Networks. In *Proceedings of the 2nd Workshop on the Economics of Peer-to-Peer Systems* (P2PEcon), 2004.
18. D. L. Chaum. Untraceable online mail, return addresses, and digital pseudonyms. *Commun. ACM*, 24(2):84–90, 1981.
19. N. Danner, D. Krizanc, and M. Liberatore. Detecting Denial of Service Attacks in Tor. In *Proceedings of the 13th International Conference on Financial Cryptography and Data Security* (FC '09), pages 273–284. Springer Berlin Heidelberg, 2009.
20. R. Dingledine, M. Freedman, D. Hopwood, and D. Molnar. A Reputation System to Increase MIX-Net Reliability. In *Proceedings of the 4th International Workshop on Information Hiding*, pages 126–141. Springer Berlin Heidelberg, 2001.
21. R. Dingledine and N. Mathewson. Tor path specification. <https://gitweb.torproject.org/torspec.git?a=blob_plain;hb=HEAD;f=path-spec.txt>.
22. R. Dingledine, N. Mathewson, and P. Syverson. Tor: The Second-generation Onion Router. In *Proceedings of the 13th USENIX Security Symposium* (SSYM'04). USENIX Association, 2004.
23. R. Dingledine and P. Syverson. Reliable MIX Cascade Networks Through Reputation. In *Proceedings of the 6th International Conference on Financial Cryptography* (FC'03), pages 253–268. Springer-Verlag, 2003.
24. P. Eckersley, E. Galperin, and K. Rodriguez. Dutch government proposes cyberattacks against... everyone., 2012. <https://www.eff.org/deeplinks/2012/10/dutch-government-proposes-cyberattacks-against-everyone>.
25. M. Edman and P. Syverson. AS-awareness in Tor Path Selection. In *Proceedings of the 16th ACM Conference on Computer and Communications Security* (CCS '09), pages 380–389. ACM, 2009.
26. A. C. Estes. NSA attacks Tor. <http://gizmodo.com/the-nsas-been-trying-to-hack-into-tors-anonymous-inte-1441153819>.
27. N. Feamster and R. Dingledine. Location diversity in anonymity networks. In *Proceedings of the 2004 ACM Workshop on Privacy in the Electronic Society* (WPES '04), pages 66–76, New York, NY, USA, 2004. ACM.
28. S. D. Kamvar, M. T. Schlosser, and H. Garcia-Molina. The Eigentrust Algorithm for Reputation Management in P2P Networks. In *Proceedings of the 12th International Conference on World Wide Web* (WWW '03), pages 640–651. ACM, 2003.
29. H. Kwakernaak. *Linear Optimal Control Systems*. John Wiley & Sons, Inc., New York, NY, USA, 1972.
30. B. Levine, M. Reiter, C. Wang, and M. Wright. Timing Attacks in Low-Latency Mix Systems. In *Proceedings of the 8th International Conference on Financial Cryptography*, pages 251–265. Springer Berlin Heidelberg, 2004.
31. P. Michiardi and R. Molva. Core: A Collaborative Reputation Mechanism to Enforce Node Cooperation in Mobile Ad Hoc Networks. In *Proceedings of the 6th IFIP TC6/TC11 Joint Working Conference on Communications and Multimedia Security*, pages 107–121. Springer US, 2002.
32. L. Mui, M. Mohtashemi, and A. Halberstadt. A computational model of trust and reputation for e-businesses. In *Proceedings of the 35th Annual Hawaii International Conference on System Sciences* (HICSS '02), Washington, DC, USA, 2002. IEEE Computer Society.
33. S. J. Murdoch and P. Zieliński. Sampled Traffic Analysis by Internet-exchange-level Adversaries. In *Proceedings of the 7th Symposium on Privacy Enhancing Technologies* (PETS'07), pages 167–183. Springer-Verlag, 2007.
34. F. Oliviero and S. Romano. A Reputation-Based Metric for Secure Routing in Wireless Mesh Networks. In *Proceedings of the 27th IEEE Global Telecommunications Conference* (IEEE GLOBECOM '08), pages 1–5, 2008.
35. M. Reed, P. Syverson, and D. Goldschlag. Anonymous connections and onion routing. *IEEE Journal on Selected Areas in Communications*, 16(4):482–494, May 1998.
36. P. Resnick, K. Kuwabara, R. Zeckhauser, and E. Friedman. Reputation systems. *Commun. ACM*, 43(12):45–48, 2000.
37. V. Shmatikov and M.-H. Wang. Timing Analysis in Low-Latency Mix Networks: Attacks and Defenses. In *Proceedings of the 11th European Symposium On Research In Computer Security* (ESORICS'06), pages 18–33. Springer Berlin Heidelberg, 2006.
38. M. Srivatsa, L. Xiong, and L. Liu. TrustGuard: Countering Vulnerabilities in Reputation Management for Decentralized Overlay Networks. In *Proceedings of the 14th International Conference on World Wide Web* (WWW '05), pages 422–431. ACM, 2005.
39. P. Syverson, G. Tsudik, M. Reed, and C. Landwehr. Towards an Analysis of Onion Routing Security. In *Proceedings of International Workshop on Designing Privacy Enhancing Technologies: Design Issues in Anonymity and Unobservability*, pages 96–114. Springer Berlin Heidelberg, 2001.
40. Tsuen-Wan, R. Dingledine, and D. Wallach. Building Incentives into Tor. In *Proceedings of the 14th International Conference on Financial Cryptography and Data Security* (FC'10), pages 238–256. Springer Berlin Heidelberg, 2010.
41. M. Wright, M. Adler, B. Levine, and C. Shields. Defending anonymous communications against passive logging attacks. In *Proceedings of the 2003 IEEE Symposium on Security and Privacy* (SP '03), pages 28–41, 2003.
42. M. K. Wright, M. Adler, B. N. Levine, and C. Shields. An Analysis of the Degradation of Anonymous Protocols. In *Proceedings of the 9th Network and Distributed System Security Symposium* (NDSS '02), 2002.
43. L. Xiong and L. Liu. PeerTrust: supporting reputation-based trust for peer-to-peer online communities. *IEEE Transactions on Knowledge and Data Engineering*, 16(7):843–857, 2004.
44. R. Zhou and K. Hwang. PowerTrust: A Robust and Scalable Reputation System for Trusted Peer-to-Peer Computing. *IEEE Transactions on Parallel and Distributed Systems*, 18(4):460–473, April 2007.
45. Y. Zhu, X. Fu, B. Graham, R. Bettati, and W. Zhao. On Flow Correlation Attacks and Countermeasures in Mix Networks. In *Proceedings of the 4th International Workshop on Privacy Enhancing Technologies* (PET'04), pages 207–225. Springer Berlin Heidelberg, 2005.

### Appendix

#### A. Stability Analysis

From control theory, we know that for a discrete-time linear system to be stable, all poles of its transfer function must lie inside the unit circle [29]. To determine this, we first need to take the Z-transform of the transfer function and then determine its poles. We rewrite Equation (1) as the following first-order discrete-time linear system:

\[ y(n) = \alpha x(n) + (1 - \alpha) y(n-1) \]

where \( y(n) \) and \( x(n) \) denote the n-th output and input of the system, respectively (i.e., \( y(n) \) refers to the newly generated reputation value \( R_n \) of a relay, while \( x(n) \) refers to the reference value \( R_c \)).

Taking the Z-transform of Equation (9) yields:

\[ Y(z) = \alpha X(z) + (1 - \alpha) z^{-1} Y(z) \]

From this, we can compute the transfer function as:

\[ H(z) = \frac{Y(z)}{X(z)} = \frac{\alpha z}{z - (1 - \alpha)} \]

So, the transfer function \( H(z) \) has a pole at \( z = 1 - \alpha \). From Equation (3), we know that \( 0 < \alpha < 1 \), so the pole will always lie inside the unit circle. Thus, our closed-loop reputation system guarantees stability.

#### B. Tuning Model Parameters

In this section, we examine how the parameters related to Re3 affect the reputation score of compromised relays. As shown in Table 5, Re3 has a total of four parameters. We will study each of their impacts on the reputation score. In the following study, we mainly want to see how our model reacts to dynamic behavioral changes. For this purpose, we assume that a compromised relay, with all other relays being honest, participates in a total of 100 circuits, oscillating between honest and malicious behavior every 25 interactions.

##### B.1 Proportional Constant (\( K_p \))

The proportional constant, \( K_p \), determines the degree to which we react to the deviation between the reference value \( R_c \) and the current system output \( R_{n-1} \). This should not be set either too high (near 1) or too low (near 0). If it is set too high, it will oscillate too much, and if it is set too low, it will discount most of the deviation and result in slow convergence. Figure 11(a) highlights the impact of different values of \( K_p \). As we can see from the figure, for \( K_p = 1 \), it oscillates heavily, and for \( K_p = 0 \), it totally discards the difference between the reference value and system output. We, therefore, conservatively set \( K_p \) to 0.5, so that the model becomes neither too sensitive nor too insensitive to sudden deviations.

##### B.2 Reward (\( \mu \)) and Punishment (\( \nu \)) Factors

We now investigate how our reputation model responds to a series of failures and successes. We want the degree of punishment to be greater than that of reward. So whenever the model receives a negative feedback (in our case, a rating of -1), we want our Exponentially Weighted Moving Average (EWMA) function (Equation (3)) to give higher weight to the current feedback (i.e., consider a larger value of \( \alpha \)). As \( \alpha \) is dependent on \( \delta \) (see Equation (5)), we need to increase \( \delta \) more for failure than success. Therefore, under our setting, we require \( \mu > 1 \) and \( \nu \leq 1 \). Figure 11(b) highlights how the reputation score reacts to different combinations of \( (\mu, \nu) \). As long as \( \mu > 1 \) and \( \nu \leq 1 \), our model can effectively discourage selective DoS.

##### B.3 Confidence Factor (\( \beta \))

Now, we look at our confidence factor \( \beta \). The confidence factor determines how confident a user is about the reputation score of a particular relay. As the number of experiences with a particular relay increases, a user becomes more confident about the computed reputation score. \( \beta \) controls how quickly we become confident about a reputation score. Figure 11(c) highlights how the confidence factor increases as the number of interactions increases. It should be mentioned that any monotonically increasing function of the number of interactions can be used as a confidence metric.

#### C. Tuning Cutoffs for Outliers

In filtering outliers, we previously chose a deviation interval of 3 times the standard deviation from the average (see Section 6). Here, we investigate the impact of other deviation intervals on the performance of Re3. A tradeoff exists between the value of \( k \) and false errors — False Negatives (FN) and False Positives (FP). As we increase \( k \), more relays become acceptable, so FP goes down but FN rises. We tested for \( k = 1.3 \) and \( k = 2.0 \). Figure 12 illustrates the FN and FP errors for different values of \( k \). From the figure, we see that as we increase the allowed deviation from the average, more and more relays become acceptable, and as a result, FP decreases while FN increases.

![False Negative (FN) and False Positive (FP) errors for different values of \( k \). As \( k \) increases, FN tends to rise and FP tends to fall.](path_to_figure_12)

Figures 11(a), 11(b), and 11(c) highlight the sensitivity analysis of (a) controller gain \( K_p \), (b) reward \( \mu \) and punishment \( \nu \) factors, and (c) confidence factor \( \beta \).

- **Figure 11(a)**: Sensitivity analysis of controller gain \( K_p \).
- **Figure 11(b)**: Sensitivity analysis of reward \( \mu \) and punishment \( \nu \) factors.
- **Figure 11(c)**: Sensitivity analysis of confidence factor \( \beta \).

These figures provide visual insights into how the model parameters influence the reputation score and the overall performance of the system.