Certainly! Here is the optimized version of the provided text, aiming for clarity, coherence, and professionalism:

---

**Interpretation through Zero-Concentrated Differential Privacy (zCDP)**

Another way to interpret Theorem 2 is through the lens of zero-concentrated differential privacy (zCDP) [40]. A mechanism is said to satisfy \(\rho\)-zCDP if it meets the \((\alpha, \alpha\rho)\)-Rényi differential privacy (RDP) for every \(\alpha > 1\). This definition offers a natural and convenient framework to express the privacy guarantees of the widely used Gaussian mechanism [38].

Applying Theorem 2 to a \(\rho\)-zCDP mechanism and optimizing \(\alpha\) to minimize the upper bound, we obtain the following corollary.

**Corollary 4.** Fix \(\pi\), \(\ell\), and \(\eta > 0\), and let \(\kappa = \kappa_{\pi,\ell}(\eta)\). If a mechanism \(M\) satisfies \(\rho\)-zCDP with \(\rho > 0\), then \(M\) is \((\eta, \gamma)\)-ReRo with respect to \(N(w, \sigma^2 I_d)\) and \(\ell_2\) with \(\gamma = e^{-\Omega(d)}\) as long as \(\sigma \geq 2\eta\sqrt{d}\).

**Protection against Reconstruction in Large \(\epsilon\) Regimes**

The idea that large values of \(\epsilon\) can protect against reconstruction when the adversary's prior contains significant uncertainty (i.e., it is diffused) was previously observed in [41] in the context of local differential privacy (LDP) with priors close to uniform. Inspired by federated learning (FL) applications where adversaries have access to LDP gradients, the authors propose a notion of protection against reconstruction breaches that is more stringent than Definition 2: the adversary cannot effectively reconstruct a particular feature of interest about the target point, regardless of the mechanism's output. In contrast, ReRo uses an average-case requirement over the mechanism's outputs. Technically, [41, Lemma 2.2] shows that the bound in Corollary 3 also holds for this worst-case notion of protection against reconstruction. Such worst-case guarantees, however, are not attainable under relaxations of \(\epsilon\)-DP like RDP, which only ensures that the privacy loss will be small with high probability. Thus, Theorem 2 and Proposition 6 generalize the results from [41] to RDP, the default privacy notion in DP-SGD and other popular private machine learning algorithms [42, 43].

**Practical Implications of Reconstruction Robustness**

To deploy the bounds from Theorem 2, two elements are necessary: a criterion for reconstruction error \(\ell\) with an associated threshold \(\eta\), and an understanding of the success rate of \(\eta\)-approximate reconstruction by the adversary before the release. Equipped with \(\ell\) and \(\eta\), one can engage with stakeholders and domain experts to determine a reasonable success rate for potential adversaries. An interesting feature of Theorem 2 is that it reduces adversarial modeling to determining a single number \(\kappa_{\pi,\ell}(\eta)\). Furthermore, it may not be necessary to be overly conservative in estimating this number, as the theorem bounds the success probability of the worst-case adversary, which knows the entire fixed dataset. Realistic adversaries often have less knowledge, so it might be possible to trade off knowledge of the fixed dataset with the amount of diffusion required from the prior. This question is left for future work.

**Further Related Work**

**a) Threat Modeling and Privacy Semantics:**

The use of informed adversaries in formal privacy analyses can be traced back to the sub-linear queries (SuLQ) framework [44], which was later subsumed by differential privacy (DP) [6]. Although the widely used DP definition avoids explicit mentions of adversaries, [6, Appendix A] provides a "semantically flavored" definition involving the likelihood ratio between the prior and posterior beliefs of an informed adversary. The adversarial model in Section II uses the same notion of informed adversary.

In other frameworks where the adversary is not necessarily informed (e.g., Pufferfish privacy [46] and inferential privacy [47]), side knowledge about the whole dataset is encoded in a probabilistic prior capturing information about individual entries and their statistical dependencies. These frameworks extend the semantic approach to DP by replacing the prior-vs-posterior condition with an odds ratio condition. Alternatively, [48] provides posterior-vs-posterior semantics for DP in the presence of an uninformed adversary with an arbitrary prior. In the definition of reconstruction robustness, our use of an informed adversary with a prior over the target data point circumvents the complications arising from dependencies between points in the training data: the prior captures the adversary’s residual uncertainty about the target point after observing the fixed dataset.

**b) DP and Protection against Reconstruction:**

How standard DP offers concrete protection against reconstruction attacks has been studied in other contexts. One of the original motivations for DP was to defeat database reconstruction attacks in interactive query mechanisms [58, 59, 60, 61, 62]. In such attacks, the adversary receives noisy answers to a sequence of specially crafted queries and, if the noise is small enough, uses the answers to (partially) reconstruct every record in the database. The success of these attacks depends on the adversary’s ability to control the queries; in contrast, in ML applications, the computation is entirely under the model developer’s control.

The quantitative information flow literature seeks to provide information-theoretic bounds on data leakage in information processing systems [63, 64]. When applied to differentially private mechanisms, these ideas yield bounds on the protection against exact reconstruction when \(Z\) is finite. Specifically, [65, Theorem 3] shows that any \(\epsilon\)-DP mechanism is \((\eta, \gamma)\)-ReRo with \(\eta \in (0, 1)\) with respect to \(\ell_{0/1}\) and any prior \(\pi\) with \(\gamma \leq |Z|\kappa e^\epsilon / (|Z| + e^\epsilon - 1)\). Taking \(|Z| \to \infty\) recovers the bound from Corollary 3 in the case of \(\ell_{0/1}\). Our results can thus be interpreted as a generalization of this line of work where no assumptions about \(Z\) are necessary.

**Conclusions**

Our work provides compelling evidence that standard ML models can memorize enough information about their training data to enable high-fidelity reconstructions in a very stringent threat model. By instantiating an informed adversary that learns to map model parameters to training images, we successfully attacked standard MNIST and CIFAR-10 classifiers with up to 100K parameters, and showed that the attack is significantly robust to changes in training hyper-parameters. Future work will focus on improving the data and computational efficiency of the attack, and scaling it to larger, more performant released models. This would enable model developers to assess potential privacy leakage before deployment. Extending our attacks to reconstruct \(N > 1\) targets simultaneously is also of interest, though it is expected to be substantially harder. On the defensive side, we empirically showed that DP training with large values of \(\epsilon\) can effectively mitigate our reconstruction attacks. Our theoretical discussion, stemming from a new definition of reconstruction robustness and its connection to (R)DP, shows that this is a general phenomenon: informed reconstruction attacks can be prevented with large values of \(\epsilon\) under certain assumptions about the adversary. Validating these assumptions in specific applications would open the door to practical models that are both accurate and resilient against reconstruction attacks.

**Acknowledgments**

The authors would like to thank Leonard Berrada, Adrià Gascón, and Shakir Mohamed for feedback on an earlier version of this manuscript; Brendan McMahan for suggesting the idea that random initialization in SGD might make privacy attacks harder, which inspired some of our experiments; and Olivia Wiles for discussions on improving reconstructor network training on CIFAR-10. This work was done while G.C. was at the Alan Turing Institute.

**References**

[1] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding deep learning requires rethinking generalization,” in International Conference on Learning Representations (ICLR), 2017.
[2] V. Feldman, “Does learning require memorization? a short tale about a long tail,” in ACM Symposium on Theory of Computing (STOC), 2020.
[3] V. Feldman and C. Zhang, “What neural networks memorize and why: Discovering the long tail via influence estimation,” in Conference on Neural Information Processing Systems (NeurIPS), 2020.
[4] G. Brown, M. Bun, V. Feldman, A. D. Smith, and K. Talwar, “When is memorization of irrelevant training data necessary for high-accuracy learning?” in ACM Symposium on Theory of Computing (STOC), 2021.
[5] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference attacks against machine learning models,” in IEEE Symposium on Security and Privacy (SP), 2017.
[6] C. Dwork, F. McSherry, K. Nissim, and A. D. Smith, “Calibrating noise to sensitivity in private data analysis,” in Theory of Cryptography Conference (TCC), 2006.
[7] N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song, “The secret sharer: Evaluating and testing unintended memorization in neural networks,” in USENIX Security Symposium, 2019.
[8] N. Carlini, F. Tramér, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song, Ú. Erlingsson, A. Oprea, and C. Raffel, “Extracting training data from large language models,” in USENIX Security Symposium, 2021.
[9] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. Agüera y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.
[10] L. Zhu, Z. Liu, and S. Han, “Deep leakage from gradients,” in Conference on Neural Information Processing Systems (NeurIPS), 2019.
[11] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart, “Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing,” in USENIX Security Symposium, 2014.
[12] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, “Property inference attacks on fully connected neural networks using permutation invariant representations,” in ACM Conference on Computer and Communications Security (CCS), 2018.
[13] A. Suri and D. Evans, “Formalizing and estimating distribution inference risks,” arXiv:2109.06024, 2021.
[14] M. Nasr, S. Song, A. Thakurta, N. Papernot, and N. Carlini, “Adversary instantiation: Lower bounds for differentially private machine learning,” in IEEE Symposium on Security and Privacy (SP), 2021.
[15] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, “Deep learning with differential privacy,” in ACM Conference on Computer and Communications Security (CCS), 2016.
[16] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that exploit confidence information and basic countermeasures,” in ACM Conference on Computer and Communications Security (CCS), 2015.
[17] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in machine learning: Analyzing the connection to overfitting,” in IEEE Computer Security Foundations Symposium (CSF), 2018.
[18] Y. Zhang, R. Jia, H. Pei, W. Wang, B. Li, and D. Song, “The secret revealer: Generative model-inversion attacks,” in USENIX Security Symposium, 2018.

---

This version aims to enhance readability and maintain the technical accuracy of the original text.