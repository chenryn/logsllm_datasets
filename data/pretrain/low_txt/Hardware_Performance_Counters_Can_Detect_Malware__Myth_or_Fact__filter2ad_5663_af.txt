### Avoiding Overfitting in Machine Learning Models

Cross-validation is a technique used to ensure that the detection rates of machine learning models are consistent across multiple training and testing iterations [39]. If the detection rates fluctuate significantly during cross-validation, it indicates that the models may not be properly trained. Previous studies often lack proper cross-validation or do not report results from cross-validation, which motivates us to conduct a more thorough evaluation using this method. We use 3 times the standard deviation (3σ) to quantify the fluctuations in detection rates. In a Gaussian distribution, 3σ represents the range within which 99.7% of random instances fall. In the context of malware detection, a high value of 3σ in detection rates suggests that the model's performance is unstable across different datasets.

### 5.2.1 Cross-Validation for TTA1 Experiments

A common practice in cross-validation is the 10-fold cross-validation [39], where the dataset is divided into 10 equal subsets. The model is trained on 9 subsets and tested on the remaining one, with each subset serving as the test set once. The standard deviations of detection rates across the 10 experiments indicate the stability of the model's performance. We consider both a 60-20-20 split (training-testing-validation) and 10-fold cross-validation insufficient, as the standard deviations of detection rates tend to increase with larger datasets. To address this, we repeated the 10-fold cross-validation until the standard deviations stabilized. In this study, we performed cross-validation 1,000 times, which is three orders of magnitude more than previous works. Each iteration involved randomly shuffling the examples before splitting them into training and testing sets.

**Figure 6** shows the distributions of detection rates (precision, recall, and F1-scores) for various machine learning models using both TTA1 and TTA2. In the figure, red diamonds represent the means, and blue boxes correspond to the 25th to 75th percentiles of the detection rate distributions. The whiskers (short, horizontal lines outside the blue box) represent the range between 0.3% and 99.7%, equivalent to µ ± 3σ of a Gaussian distribution. Blue dots are outliers that lie outside the µ ± 3σ range. A wide spread of distributions indicates that the detection rates fluctuate across different training datasets, while a narrow spread suggests stability. For Decision Tree (DT), Random Forest (RF), K-Nearest Neighbors (KNN), Neural Network (NN), AdaBoost, and Naive Bayes models, the mean F1-scores are 82.17%, 83.75%, 82.28%, 74%, 72.27%, and 12.15%, respectively, with 3σ values of 1.416%, 1.326%, 1.388%, 13.2%, 2.365%, and 2.392%.

### 5.2.2 Cross-Validation for TTA2 Experiments

For the TTA2 experiments, the mean F1-scores for DT, RF, KNN, NN, AdaBoost, and Naive Bayes models are 82.13%, 83.61%, 82.2%, 73.69%, 73.43%, and 12.21%, respectively, compared to 82.17%, 83.75%, 82.28%, 74%, 72.27%, and 12.15% for TTA1. The 3σ values for TTA2 are 2.145%, 2.336%, 2.248%, 14.88%, 3.29%, and 2.611%, respectively, compared to 1.416%, 1.326%, 1.388%, 13.2%, 2.365%, and 2.392% for TTA1. The standard deviations for TTA2 increased by factors of 1.515×, 1.762×, 1.62×, 1.127×, 1.391×, and 1.092×, respectively. This indicates that the detection rates using TTA2 have much higher variations compared to TTA1.

Since previous works did not report standard deviations, direct comparisons are not possible. However, **Figure 6** clearly shows that reporting results from a single training-and-testing experiment does not provide sufficient information about the model's performance. A distribution of detection rates is necessary for a comprehensive evaluation.

The difference in standard deviations between **Figure 6(a)** and **Figure 6(b)** is due to the unrealistic assumption in TTA1 that programs in the training set appear in the testing set. **Figure 6(b)** presents results where malicious programs are not included in the training set. Overall, the mean of the distribution using TTA2 is lower, and the standard deviation is higher compared to TTA1. For a full evaluation of machine learning models, it is essential to use TTA2 and present a distribution of precision, recall, F1-score, and Area Under the ROC Curve (AUC).

### 5.3 Ransomware Detection

In previous sections, machine learning models were trained to distinguish malware from benign software using Hardware Performance Counters (HPCs). We created a malware embedded in benign software (Notepad++) and demonstrated that this malware can evade HPC-based detection. The malware was implemented by infusing Notepad++ with ransomware, which encrypts files and demands a ransom for decryption keys [40]. By 2016, ransomware had become one of the most prevalent types of malware, with one business being attacked every 40 seconds [41].

Our ransomware implementation encrypts files in the "Pictures" folder every 5 seconds using Microsoft Cryptography APIs [42]. We measured HPC values for the modified Notepad++ in our experimental setup. We randomly selected 90% of the benign and malware samples for training and tested the model on the original and modified Notepad++. The precision of the models (DT, Naive Bayes, NN, AdaBoost, RF, and KNN) was 0%, 0%, 0%, 50.85%, 0%, and 0%, respectively.

These results are expected because machine learning models tolerate noise and jitter during training on sampled HPCs to extract malicious behavior. This tolerance leads to errors even in the training data. In our example, the changes in HPC values caused by the ransomware were overshadowed by the sampled HPC values from running Notepad++, leading to the classification of the modified Notepad++ as benign.

### 6. Discussion

We conducted our experiments on a Windows 7 32-bit operating system running on an AMD 15h family Bulldozer micro-architecture machine. Weaver et al. [33] investigated the determinism of HPC values across different micro-architectures, showing that HPCs in various architectures have similar levels of variation during sampling. Thus, our conclusions from the Bulldozer micro-architecture are applicable to other micro-architectures. In our experiments, we allowed network access for benign software and prevented it for malware, but this did not affect the HPC measurements, as both benign and malicious programs functioned correctly. For dimensionality reduction, we used Principal Component Analysis (PCA), a popular method for this purpose.

### 7. Conclusion

HPCs are hardware units designed to count low-level, micro-architectural events. Many studies have explored malware detection using HPC profiles. However, we argue that there is no direct causation between low-level micro-architectural events and high-level software behavior. The positive results in previous works are likely due to optimistic assumptions and unrealistic experimental setups. Our rigorous evaluation, using realistic assumptions and extensive cross-validation, revealed low fidelity in HPC-based malware detection when the number of programs and cross-validation iterations were increased. Our best result showed an F1-score of 80.78% with a False Discovery Rate (F+/(F+ + T+)) of 15%, meaning that out of 1,323 executable files in the Windows operating system, 198 would be incorrectly flagged as malware. We also demonstrated the infeasibility of HPC-based detection with a Notepad++-infused ransomware, which evaded our detection system.

### References

[1] Intel Itanium Architecture Software Developer’s Manual. Intel Corporation, 2010.
[2] BIOS and Kernel Developer’s Guide (BKDG) for AMD Family 15h Models 10h-1Fh Processors. Advanced Micro Devices, Inc., 2015.
[3] John Demme, Matthew Maycock, Jared Schmitz, Adrian Tang, Adam Waksman, Simha Sethumadhavan, and Salvatore Stolfo. On the feasibility of online malware detection with performance counters. In Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA), page 559, 2013.
[4] Harish Patil, Robert Cohn, Mark Charney, Rajiv Kapoor, Andrew Sun, and Anand Karunanidhi. Pinpointing representative portions of large intel itanium programs with dynamic instrumentation. In Proceedings of 37th International Symposium on Microarchitecture (MICRO), pages 81–92, 2004.
[5] Mikhail Kazdagli, Vijay Janapa Reddi, and Mohit Tiwari. Quantifying and improving the efficiency of hardware-based mobile malware detectors. In Proceedings of the 49th International Symposium on Microarchitecture (MICRO), pages 1–13, 2016.
[6] Xueyang Wang, Sek Chai, Michael Isnardi, Sehoon Lim, and Ramesh Karri. Hardware performance counter-based malware identification and detection with adaptive compressive sensing. Transactions on Architecture and Code Optimization (TACO), 2016.
[7] Adrian Tang, Simha Sethumadhavan, and Salvatore J Stolfo. Unsupervised anomaly-based malware detection using hardware features. In International Workshop on Recent Advances in Intrusion Detection (RAID), pages 109–129, 2014.
[8] Baljit Singh, Dmitry Evtyushkin, Jesse Elwell, Ryan Riley, and Iliano Cervesato. On the detection of kernel-level rootkits using hardware performance counters. In Proceedings of the 17th Asia Conference on Computer and Communications Security (AsiaCCS), pages 483–493. ACM, 2017.
[9] Meltem Ozsoy, Caleb Donovick, Iakov Gorelik, Nael Abu-Ghazaleh, and Dmitry Ponomarev. Malware-aware processors: A framework for efficient online malware detection. In Proceedings of the 21st International Symposium on High Performance Computer Architecture (HPCA), pages 651–661, 2015.
[10] Khaled N Khasawneh, Meltem Ozsoy, Caleb Donovick, Nael Abu-Ghazaleh, and Dmitry Ponomarev. Ensemble learning for low-level hardware-supported malware detection. In International Workshop on Recent Advances in Intrusion Detection (RAID), pages 3–25, 2015.
[11] Khaled N Khasawneh, Nael Abu-Ghazaleh, Dmitry Ponomarev, and Lei Yu. RHMD: Evasion-resilient hardware malware detectors. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 315–327, 2017.
[12] Chi-Keung Luk, Robert Cohn, Robert Muth, Harish Patil, Artur Klauser, Geof Lowney, Steven Wallace, Vijay Janapa Reddi, and Kim Hazelwood. Pin: Building customized program analysis tools with dynamic instrumentation. In ACM SIGPLAN Notices, volume 40, pages 190–200. ACM, 2005.
[13] Fabrice Bellard. QEMU, a fast and portable dynamic translator. In USENIX Annual Technical Conference, FREENIX Track, pages 41–46, 2005.
[14] Nicholas Nethercote and Julian Seward. Valgrind: A framework for heavyweight dynamic binary instrumentation. In ACM SIGPLAN Notices, volume 42, pages 89–100. ACM, 2007.
[15] DynamoRIO Dynamic Instrumentation Tool Platform. http://www.dynamorio.org/, 2017. (Accessed on 12/02/2017).
[16] Benjamin Serebrin and Daniel Hecht. Virtualizing performance counters. In Proceedings of the European Conference on Parallel Processing, pages 223–233, Bordeaux, France, August 2011.
[17] John L Henning. SPEC CPU2006 benchmark descriptions. ACM SIGARCH Computer Architecture News, 34(4):1–17, 2006.
[18] Linux perf. http://www.brendangregg.com/perf.html, 2017. (Accessed on 11/19/2017).
[19] Dhilung Kirat, Giovanni Vigna, and Christopher Kruegel. BareCloud: Bare-metal analysis-based evasive malware detection. In USENIX Security Symposium (SP), pages 287–301, 2014.
[20] Vincent M Weaver and Sally A McKee. Can hardware performance counters be trusted? In Proceedings of International Symposium on Workload Characterization (IISWC), pages 141–150. IEEE, 2008.
[21] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
[22] Pivotal Software Inc. RabbitMQ. http://www.rabbitmq.com/, 2017. (Accessed on 11/12/2017).
[23] Samba - Opening Windows to a wider world. https://www.samba.org/, 2017. (Accessed on 12/05/2017).
[24] bindfs. https://bindfs.org/, 2017. (Accessed on 12/05/2017).
[25] Paul J. Drongowski. An introduction to analysis and optimization with AMD CodeAnalyst Performance Analyzer, 2008.
[26] VirusTotal. https://www.virustotal.com/, 2017. (Accessed on 07/12/2017).
[27] Marcos Sebastián, Richard Rivera, Platon Kotzias, and Juan Caballero. AVClass: A tool for massive malware labeling. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 230–253. Springer, 2016.
[28] Futuremark. https://www.futuremark.com/, 2017. (Accessed on 11/15/2017).
[29] Performance: Python Package Index. https://pypi.python.org/pypi/performance/0.5.1, 2017. (Accessed on 11/30/2017).
[30] Ninite. https://ninite.com/, 2017. (Accessed on 11/15/2017).
[31] Npackd. https://npackd.appspot.com/, 2017. (Accessed on 11/15/2017).
[32] Android Debug Bridge. https://developer.android.com/studio/command-line/adb.html, 2017. (Accessed on 11/12/2017).
[33] Vincent M Weaver, Dan Terpstra, and Shirley Moore. Non-determinism and overcount on modern hardware performance counter implementations. In Proceedings of International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 215–224. IEEE, 2013.
[34] S.S. Haykin. Communication System. Wiley Series in Management Series. John Wiley & Sons, 1983.
[35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
[36] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. In European conference on computational learning theory, pages 23–37. Springer, 1995.
[37] George H John. Robust decision trees: Removing outliers from databases. In KDD, pages 174–179, 1995.
[38] J Rennie, L Shih, J Teevan, and D Karger. Tackling the poor assumptions of naive Bayes classifiers (PDF). ICML, 2003.
[39] Ron Kohavi et al. A study of cross-validation and bootstrap for accuracy estimation and model selection. In IJCAI, volume 14, pages 1137–1145. Stanford, CA, 1995.
[40] Adam Young and Moti Yung. Cryptovirology: Extortion-based security threats and countermeasures. In Proceedings of Security and Privacy, pages 129–140. IEEE, 1996.
[41] Kaspersky Security Bulletin 2016. Review of the year. Overall statistics for 2016. https://securelist.com/kaspersky-security-bulletin-2016-executive-summary/76858/. (Accessed on 12/10/2017).
[42] Cryptography Reference (Windows). https://msdn.microsoft.com/en-us/library/aa380256.aspx, 2017. (Accessed on 11/18/2017).