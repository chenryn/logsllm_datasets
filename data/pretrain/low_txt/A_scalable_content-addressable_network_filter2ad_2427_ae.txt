# Data Store Routing Fault Tolerance and Data Store Availability

## 1. Topology Descriptions
- **R(α, β)**: A Transit-Stub topology with the delay of every link set to a random value between 10ms and 50ms.
- **αxH(β, γ, δ)**: This topology is similar to H(β, γ, δ) but with the backbone topology scaled by a factor of 10, resulting in a CAN node density that is approximately 10 times lower.

## 2. Latency Stretch Analysis
For each of the topologies described above, we measure the latency stretch, defined as the ratio of CAN latency to IP latency, for different system sizes. The results are presented in Figure 10. We observe that while the delay distribution affects the absolute value of the latency stretch, in all cases, the latency stretch grows very slowly with system size. For system sizes up to 130,000 nodes, the latency stretch never exceeds 3. The fastest growth in latency stretch is observed in the case of random delay distributions. This is because, as the CAN system size increases, the new links added at the edges of the network do not necessarily have low latency (unlike in hierarchical delay distributions). Finally, we note that the latency stretch in topology H(β, γ, δ) is slightly lower than in topology αxH(β, γ, δ), due to the higher density of CAN nodes in H(β, γ, δ), which allows the latency heuristics to yield higher gains.

## 3. Related Work
### 3.1 Related Algorithms
- **Distance Vector (DV) and Link State (LS) Algorithms**: These algorithms, used in IP routing, require each router to have some level of knowledge about the network topology. DV requires knowledge of the distance in hops, while LS requires knowledge of the exact link structure. Unlike our CAN routing algorithm, DV and LS require widespread dissemination of local topology information, making them less suitable for networks with frequent topology changes.
- **Plaxton Algorithm**: In this algorithm, each node is assigned a unique n-bit label, divided into l levels, with each level having w = n/l bits. The routing table contains entries that allow packets to be forwarded towards a destination label by incrementally resolving the destination label from left to right. For a system with n nodes, Plaxton’s algorithm routes in O(log n) hops and requires a routing table size of O(log n). Our CAN routing, by comparison, routes in O(dn/d) hops (where d is the number of dimensions) with a routing table size of O(dr), which is independent of n. Setting d = (log n)/k allows our CAN algorithm to match Plaxton’s scaling properties. However, Plaxton’s algorithm is better suited for web caching environments, which are typically more stable and smaller in scale, and does not provide a solution for decentralized neighbor discovery.
- **Geographic Routing Algorithms**: These algorithms, such as GPSR, are similar to our CAN routing in that they forward messages through a coordinate space. However, they are designed for true physical space and do not easily extend to multi-dimensional spaces or handle the neighbor discovery problem in a CAN context.

### 3.2 Related Systems
- **Domain Name System (DNS)**: While DNS provides key-value storage (domain name, IP address), it is less general than CANs. CANs offer a more flexible naming scheme and can provide distributed DNS-like services.
- **OceanStore**: This project at U.C. Berkeley uses the Plaxton algorithm for data location in a large-scale storage system. OceanStore aims to provide continuous access to persistent information across a global network.
- **Publius**: A web publishing system that provides high resistance to censorship and anonymity. It currently assumes a static list of servers, but incorporating self-organizing aspects from CANs could help it scale to larger numbers of servers.
- **Peer-to-peer File Sharing Systems**: Systems like Freenet use steepest-ascent hill-climbing search with backtracking for content location. Other systems, such as those using the Plaxton algorithm, aim to improve robustness through randomized path selection.

## 4. Discussion
Our work addresses scalable routing and indexing in Content-Addressable Networks (CANs). Simulation results show that for a CAN with over 260,000 nodes, the routing latency is less than twice the IP path latency. Future work includes designing a secure CAN resistant to denial-of-service attacks, handling mutable content, and integrating keyword searching.

## 5. Acknowledgments
We thank Steve McCanne, Jitendra Padhye, Brad Karp, Vern Paxson, Randy Katz, Petros Maniatis, and the anonymous reviewers for their valuable feedback.

## 6. References
[1] W. Bolosky, J. Douceur, D. Ely, and M. Theimer. Feasibility of a Serverless Distributed File System Deployed on an existing set of Desktop PCs. In Proceedings of SIGMETRICS 2000, Santa Clara, CA, June 2000.
[2] I. Clarke, O. Sandberg, B. Wiley, and T. Hong. Freenet: A Distributed Anonymous Information Storage and Retrieval System. ICSI Workshop on Design Issues in Anonymity and Unobservability, July 2000.
[3] S. Czerwinski, B. Zhao, T. Hodes, A. Joseph, and R. H. Katz. An Architecture for a Secure Service Discovery Service. In Proceedings of Fifth ACM Conf. on Mobile Computing and Networking (MOBICOM), Seattle, WA, 1999. ACM.
[4] P. Francis. Yoid: Extending the Internet Multicast Architecture. Unpublished paper, available at http://www.aciri.org/yoid/docs/index.html, Apr. 2000.
[5] FreeNet. http://freenet.sourceforge.net.
[6] Gnutella. http://gnutella.wego.com.
[7] J. Guterman. Gnutella to the Rescue? Not so Fast, Napster fiends. Link to article at http://gnutella.wego.com, Sept. 2000.
[8] Infrasearch. http://www.infrasearch.com.
[9] B. Karp and H. Kung. Greedy Perimeter Stateless Routing. In Proceedings of ACM Conf. on Mobile Computing and Networking (MOBICOM), Boston, MA, 2000. ACM.
[10] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski, P. Eaton, D. Geels, R. Gummadi, S. Rhea, H. Weatherspoon, W. Weimer, C. Wells, and B. Zhao. Oceanstore: An Architecture for Global-scale Persistent Storage. In Proceedings of ASPLOS 2000, Cambridge, Massachusetts, Nov. 2000.
[11] S. Kumar, C. Alaettinoglu, and D. Estrin. SCOUT: Scalable Object Tracking through Unattended Techniques. In Proceedings of the Eight IEEE International Conference on Network Protocols, Osaka, Japan, Nov. 2000.
[12] J. Li, J. Jannotti, D. D. Couto, D. Karger, and R. Morris. A Scalable Location Service for Geographic Ad-hoc Routing. In Proceedings of ACM Conf. on Mobile Computing and Networking (MOBICOM), Boston, MA, 2000. ACM.
[13] A. D. R. Marc Waldman and L. F. Cranor. Publius: A Robust, Tamper-evident, Censorship-resistant, Web Publishing System. In Proceedings of the 9th USENIX Security Symposium, pages 59–72, August 2000.
[14] Napster. http://www.napster.com.
[15] C. Plaxton, R. Rajaram, and A. W. Richa. Accessing nearby copies of replicated objects in a distributed environment. In Proceedings of the Ninth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), June 1997.
[16] J. B. Postel. Internet Protocol Specification. ARPANET Working Group Requests for Comment, DDN Network Information Center, SRI International, Menlo Park, CA, Sept. 1981. RFC-791.
[17] S. Ratnasamy, P. Francis, M. Handley, R. Karp, J. Padhye, and S. Shenker. Grass-roots Content Distribution: RAID meets the Web. Jan. 2001. unpublished document available at http://www.aciri.org/sylvia/.
[18] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. A Scalable Content-Addressable Network. In ICSI Technical Report, Jan. 2001.
[19] Y. Rekhter and T. Li. A Border Gateway Protocol 4 BGP-4. ARPANET Working Group Requests for Comment, DDN Network Information Center, Mar. 1995. RFC-1771.
[20] I. Stoica, R. Morris, D. Karger, F. Kaashoek, H. Balakrishnan. Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications. In Proceedings ACM Sigcomm 2001, San Diego, CA, Aug. 2001.
[21] M. Welsh, N. Borishov, J. Hill, R. von Behren, and A. Woo. Querying large collections of music for similarity. Technical report, University of California, Berkeley, CA, Nov. 1999.
[22] E. Zegura, K. Calvert, and S. Bhattacharjee. How to Model an Internetwork. In Proceedings IEEE Infocom ’96, San Francisco, CA, May 1996.
[23] Zeropaid.com. File sharing portal at http://www.zeropaid.com.