### Client Load and Tor Performance

Published studies on client-to-relay ratios [20] and protocol-level statistics [22] offer only a rough guide for creating clients and simulating the correct load. When generating a scaled topology, it is crucial to compare simulation performance measurements with live Tor statistics to ensure accuracy. Due to these challenges, we developed a script that generates and runs simulations based on a network consensus document. The script parses the consensus document and randomly selects relays according to configurable network sizes. Configurable parameters include the ratio of exit relays to normal relays, the number of clients, and the distribution of client types. This script facilitates the generation of accurate scaled topologies and significantly enhances simulator usability.

### Verifying Simulation Accuracy

Many aspects of Shadow's design (discussed in Section 3) were chosen to produce accurate simulations. Therefore, we conducted several experiments to verify Shadow’s accuracy.

#### 5.1 File Client and Server Plug-ins

HTTP client and server plug-ins were developed for Shadow to facilitate data transfer through the virtual network. These plug-ins also support a minimal SOCKS client and proxy. The client can download any number of specified files with configurable wait times between downloads, while the server supports buffering and multiple simultaneous connections. These plug-ins are used to test network performance during simulations. Stand-alone executables using the same code as the plug-ins are compiled to ensure that client and server functionality on a live system and network is identical to Shadow plug-in functionality.

#### 5.2 PlanetLab Private Tor Network

To verify Shadow’s accuracy, we performed experiments on PlanetLab. Our experiments involved file clients and servers running the software described in Section 5.1.

In our first PlanetLab experiment, 361 HTTP clients downloaded files directly from one of 20 HTTP servers, choosing a new server at random for each download. Eighteen of the 361 clients simulated bulk downloaders, requesting a 5 MiB file immediately after finishing a download, while the remaining 343 clients approximated web downloaders, pausing for a short time between 320 KiB file downloads. The pause length was drawn from the UNC think-time distribution [12], which represents the time between clicks for a user browsing the web (the median pause is 11 seconds). Clients tracked both the time to receive the first byte of the data payload and the time to complete the entire download. We selected the fastest PlanetLab nodes (based on bandwidth tests) as our HTTP servers to minimize potential server bottlenecks, although fine-grained control was complicated by PlanetLab’s dynamic resource adjustment algorithms.

Our second PlanetLab experiment was identical to the first, except all downloads were performed through a private PlanetLab Tor network consisting of 16 exit relays, 24 non-exit relays, and one directory authority. All HTTP clients also ran a Tor client and proxied their downloads through Tor using a local connection to the Tor SOCKS server.

#### Shadowing PlanetLab

To replicate the PlanetLab experiments in Shadow, we required measurements of PlanetLab node bandwidth, latency between nodes, and an estimate of node CPU speed. These measurements allowed us to configure virtual nodes and a virtual network that approximates PlanetLab and typical Internet conditions.

First, we estimated PlanetLab node bandwidth by performing Iperf [14] bandwidth tests from each node to every other node. A node’s bandwidth was estimated as the maximum upload rate to any other node. Figure 3a shows the results of our measurements compared with available bandwidth from Tor relays extracted from the Tor network status consensus. Notice the sharp increase in the number of nodes with 1.25 MiBps (10 Mbps) and 3.75 MiBps (30 Mbps) connections, likely due to PlanetLab rate-limiting. However, our PlanetLab distribution does not approximate the live Tor distribution well, indicating that our measurements may not provide a good indication of the live Tor network's performance. Our focus here is on accurately shadowing PlanetLab; re-creating a network consistent with live Tor is explored in Section 5.3.

To model network delays, we performed latency estimates between all pairs of nodes using the Unix command `ping`. The aggregate results of world latencies are shown in Figure 3b. We approximated a network model by creating nine geographical regions and placing each node in a region using a GeoIP lookup [21]. We then created 81 CDFs representing all possible inter- and intra-region latencies. We configured nine virtual networks in Shadow and connected them into a complete graph topology, where latencies for packets traveling over each link were drawn from the corresponding CDF. Latencies for a few selected regions are also shown in Figure 3b.

Finally, we measured CPU speed of each node to accurately configure delays for Shadow’s virtual CPU system described in Section 3.3.2. OpenSSL speed tests were run to get raw CPU throughput for PlanetLab nodes. Since PlanetLab nodes are often constrained, we also created a normalized distribution based on the CPU speed of `arcachon`—a standard desktop machine in our lab. CPU throughput is shown in Figure 3c. Tor application throughput, measured by benchmarks with a middle relay configured with a bandwidth bottleneck, was combined with raw CPU throughput measurements to configure each node’s virtual CPU delay.

#### Client Performance

Figure 4 shows the results of our PlanetLab and Shadow experiments. We focused on two metrics: the time to receive the first byte of the data payload (ttfb) and the time to complete a download (dt). The ttfb metric provides insight into the delays associated with sending a request through multiple hops and the responsiveness of a circuit, representing the minimum time a web user has to wait until anything is displayed in the browser. The dt metric captures overall performance.

Figures 4c and 4e show the ttfb metric for web and bulk clients with direct and Tor-proxied requests both in PlanetLab and Shadow. Downloads through Tor take longer than direct downloads, as expected, since data must be processed and forwarded by multiple relays. Shadow closely approximates the network conditions in PlanetLab, as shown by the close correspondence between the lower half of each CDF. However, PlanetLab exhibits slightly higher variability in ttfb than Shadow, especially when downloads are proxied through Tor. Higher variability in results is likely caused by increased PlanetLab node delay due to resource contention with other co-located experiments.

Figures 4d and 4f show similar conclusions for the dt metric. Shadow results appear off by a small factor, and we again see higher variability in download completion times for PlanetLab. However, inaccuracies in download times appear somewhat independent of file size. As shown in Figure 4a, statistics gathered from Tor relays support our conclusions about higher variability in delays. Shown is the number of processed cells for each relay over the one-hour experiment and the one-minute moving average. The moving average of processed cells is slightly higher for Shadow because of PlanetLab’s resource sharing complexity, while individual relay measurements also show higher variability for PlanetLab. Figure 4b shows that Shadow queue times are very close to those measured on PlanetLab, and again shows PlanetLab’s high variability. While we are optimistic about our conclusions, we emphasize that PlanetLab results should be analyzed carefully due to the issues discussed above.

### 5.3 Live Public Tor Network

Although the PlanetLab results show how Shadow performance compares to that achieved while running on PlanetLab and a private Tor network, they do not show how accurately Shadow can approximate the live public Tor network, which contains thousands of relays and hundreds of thousands of geographically distributed clients. Therefore, we performed a separate set of experiments to test Shadow’s ability to approximate live Tor network conditions as documented by The Tor Project [45]. Comparing results with statistics from Tor Metrics gives us strong evidence of Shadow’s ability to accurately simulate the live Tor network.

The experiments were similar to those performed on PlanetLab: web and bulk clients downloaded variable-sized files from servers through a private Tor network. However, file sizes were modified to 50 KiB, 1 MiB, and 5 MiB, as used by TorPerf, and the configuration of Shadow nodes was slightly modified to approximate resources available in the live Tor network. In these experiments, we used a directory authority, 50 relays, 950 web clients, 50 bulk clients, and 200 servers. We used a live Tor consensus to obtain bandwidth limits for Tor relays and ensured that we correctly scaled available bandwidth and network size. Client bandwidths were estimated with 1 MiB down-link and 3.5 MiB up-link speeds (not over-subscribed). Each relay was configured according to the live consensus: a `CircuitPriorityHalfLife` of 30, a 40 KiB `PerConnBWRate`, and a 100 MiB `PerConnBWBurst`. Geographical location and latencies were configured using our PlanetLab dataset [39].

Figure 5 shows Shadow’s accuracy while simulating a shadow of the live Tor network. CDFs of Shadow download completion times for each file size were compared with download times measured and collected by The Tor Project. The gray area represents the first-to-third quartile stretch, and the dotted line shows the median download time extracted from live Tor network statistical data available at The Tor Metrics Portal [45] (gathered during April 2011—the same month as our consensus). To maximize accuracy, the left edge of the gray area should intersect the CDF at 0.25, the right edge at 0.75, and the dotted line at 0.5. Our results show that the median download times are nearly identical for 50 KiB and 1 MiB downloads and within ten percent for 5 MiB downloads, while the first and third quartiles are within 15 percent in all cases. We believe these results provide strong evidence of Shadow’s ability to accurately simulate Tor. Further, we have shown that we can correctly scale down the Tor network in our simulations while maintaining the performance properties of the live Tor network.

### 6 Prioritizing Circuits

We now demonstrate Shadow’s powerful capabilities by exploring a Tor circuit scheduling algorithm recently proposed and integrated into the Tor software. In Tor, whenever there is room in an output buffer, the circuit scheduler must decide which circuit to flush. Tor’s original design used a round-robin algorithm for such decisions. Recently, an algorithm based on the Exponentially-Weighted Moving Average (EWMA) of cells sent in each circuit was proposed and incorporated into Tor, becoming the default scheduling algorithm used by Tor relays. This section attempts to validate the results originally obtained by Tang and Goldberg [42].

#### EWMA in Bottleneck Topology

The EWMA scheduler chooses the circuit with the lowest cell count, effectively prioritizing bursty web connections over bulk transfers. Tang and Goldberg evaluated the EWMA algorithm by creating a congested circuit on a synthetic PlanetLab network and measuring the performance of web downloads. Since the middle node was a circuit bottleneck, the benefits of EWMA for reducing web download times were clear. However, results for bulk downloads during this experiment were not given.

We performed a similar bottleneck experiment in Shadow. We configured a circuit consisting of a single entry, middle, and exit relay. Two bulk clients continuously downloaded 5 MiB files to congest the circuit. Ten minutes after booting the “congestion” clients, two “measurement” clients were started and downloaded for an hour: a third bulk client and a web client that waited 11 seconds (the median think-time for web browsers [12]) between 320 KiB file downloads. The middle relay was configured as a circuit bottleneck with a 1 MiBps connection, while all other nodes (relays, clients, and server) had 10 MiBps connections.

We ran the experiment, modifying only the scheduling algorithm. We tested both the round-robin scheduler and the EWMA scheduler with a `CircuitPriorityHalfLife` of 66, as in [42].

Relay buffer statistics [43] are shown in Figures 6a and 6b. Notice a significant increase in traffic at the ten-minute mark, at which point the “measurement” clients start downloading. Figure 6a shows that the number of processed cells is similar for all relays, except occasionally the exit relay processes fewer cells due to middle relay congestion. Figure 6b shows that the circuit queues increase for the exit and middle relay, while the entry relay’s circuit queues are empty due to sufficient bandwidth to immediately forward data to the client.

Figures 6c and 6d show the performance results obtained from the web client for both schedulers. As expected, the time to the first byte of the data payload and the time to complete a download are both reduced for the web client, since bursty traffic gets prioritized ahead of the bulk traffic. The time to first byte for the “measurement” bulk downloader in Figure 6e also improves for a large fraction of the downloads because each new download originating from a new circuit will be prioritized ahead of the “congestion” bulk downloads. However, after downloading enough data, the “measurement” bulk client loses its priority over the “congestion” bulk clients, and the time to first byte converges for each scheduler.

Tang and Goldberg claim that, according to Little’s Law [19], bulk transfers will not be negatively affected while using the new circuit scheduler. While this may be theoretically true, it is not clear that it will hold in practice. The authors find that Little’s Law holds when a single relay in the live Tor network uses the EWMA scheduler: their results show that bulk download times are not significantly affected.