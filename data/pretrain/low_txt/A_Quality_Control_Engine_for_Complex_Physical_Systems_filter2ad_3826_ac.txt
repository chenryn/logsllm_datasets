### Synthetic Time Series Generation

We generate synthetic time series from two types of distributions:

1. **Uniform Distribution:**
   \[
   x(t) \sim \text{U}(a, b)
   \]
   where \(a\) and \(b\) are randomly determined.

2. **Gaussian (Normal) Distribution:**
   \[
   x(t) \sim \mathcal{N}(\mu, \sigma^2)
   \]
   where the mean \(\mu\) and variance \(\sigma^2\) are randomly generated.

Figure 8 illustrates examples of these four types of synthetic time series. For each type, we generate 100 datasets. In each dataset, we simulate a Key Performance Indicator (KPI) curve and inject status changes in some time series with respect to the KPI, making those selected time series suspicious. The KPI curve \(y(t)\) is a binary sequence with a value change at a randomly generated position \(T_0\):

\[
y(t) = I(0 < t \leq T_0) - I(T_0 < t \leq T)
\]

where \(I(\cdot)\) is an indicator function. The KPI value is 1 before time \(T_0\) and changes to -1 after \(T_0\).

### Manipulation of Time Series

Given the KPI in each dataset, we randomly select two time series, \(x_i(t)\) and \(x_j(t)\), and manipulate their data to make them suspicious. Suppose the values of these time series are:

\[
x_i(t) = [x_i(1), \ldots, x_i(T_0), x_i(T_0 + 1), \ldots, x_i(T)]
\]
\[
x_j(t) = [x_j(1), \ldots, x_j(T_0), x_j(T_0 + 1), \ldots, x_j(T)]
\]

We exchange their samples after \(T_0\) and replace the old series with new ones:

\[
x_i(t) = [x_i(1), \ldots, x_i(T_0), x_j(T_0 + 1), \ldots, x_j(T)]
\]
\[
x_j(t) = [x_j(1), \ldots, x_j(T_0), x_i(T_0 + 1), \ldots, x_i(T)]
\]

After the switch, the two time series will encounter behavior changes at time \(T_0\). Our method is expected to pinpoint these two series to explain the quality changes.

### Experimental Results

In the experiments, we use the features shown in Table I and three rankers introduced in Section IV. Figure 9 shows examples of the synthetic abnormal time series with their KPI and the most important features found by the proposed method. The selected feature series change almost simultaneously with the KPI changes, capturing the characteristic dynamics of the sensors.

To show more reliable results, we calculate how many times the ground truths (exchanged time series) are ranked in the top five suspicious attributes over 100 trials for each synthetic type. Table II summarizes the results, indicating that the fused ranker and the regularization-based ranker achieve good ranking accuracy. Table III shows how many times a feature is ranked in the top five, demonstrating that different time series have different appropriate features to explain the KPI changes.

### Real Dataset Analysis

We also examined the efficiency of the proposed method using data from a real manufacturing system. Due to privacy issues, we only illustrate the results from seven sensors labeled as 'I', 'J', 'K', 'L', 'M', 'N', and 'O'. Each sensor records a system status every minute. The KPI time series of this dataset is shown in Figure 10. Each bump represents the execution process for each lot, and the KPI value indicates the quality of products or whether the process is working. We assign quality regions based on the KPI, where KPI = 0 is assigned to good quality regions and vice versa.

Our goal is to find sensors related to the KPI. Table IV shows the final result of the proposed method, with sensor 'J' being the most important relevant feature. This is confirmed by a domain expert. Table V shows the results of the top features from each ranker, with the kurtosis of sensor 'J' (kurt::J) being the most important feature, which has been confirmed by system operators.

### Implementation and Impact

We have built a cloud service platform based on the proposed quality control engine, which several customers are now using intensively. The engine has improved the production quality for these customers. For example, one customer's proprietary revenue indicator increased from 80% to 87% after changing system configurations based on our engine's output. We have also received valuable feedback, such as requests to provide solutions to automatically tune system parameters for optimal production quality, which we are currently working on.

### Conclusions

This paper proposes a general framework to pinpoint suspicious sensors that explain output quality changes in physical systems. The framework includes extracting informative features from time series, selecting and ranking feature series, and aggregating ranking scores. Our method successfully captures various aspects of system dynamics and the correlation between candidate time series and system output quality. Experimental results demonstrate that it can correctly discover responsible sensors contributing to quality degradation. The implemented tool serves as a promising engine for system debugging and quality control.

### References

[1] A. Y. Ng, “Feature selection, L1 vs. L2 regularization, and rotational invariance,” in Proceedings of the 21st International Conference on Machine Learning (ICML), 2004.

[2] L. Breiman, “Random forests,” Machine Learning, vol. 45, pp. 5–23, 2001.

[3] I. Kononenko, E. Šimšec, and M. R. Šikonja, “Overcoming the myopia of inductive learning algorithms with RELIEFF,” Applied Intelligence, vol. 7, pp. 39–55, 1997.

[4] S. Orfanidis, Introduction to Signal Processing. Prentice Hall, 1996.

[5] J. D. Hamilton, Time Series Analysis. Princeton University Press, 1994.

[6] H. Akaike, “A new look at the statistical model identification,” IEEE Transactions on Automatic Control, vol. 19, pp. 716–723, 1974.

[7] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical Society. Series B, vol. 58, pp. 267–288, 1996.

[8] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2009, pp. 547–556.

[9] R. E. Schapire, “The strength of weak learnability,” Machine Learning, vol. 5, pp. 197–227, 1990.

[10] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley-Interscience, 2006.

[11] V. N. Vapnik, The Nature of Statistical Learning Theory. Springer, 1995.