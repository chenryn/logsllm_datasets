### 图5：一对一流量模式下的Linux网络栈性能

(a) 每个核心的吞吐量（Gbps）  
每列展示了不同流数下的每个核心吞吐量。在8个流时，网络达到饱和，但随着流数的增加，每个核心的吞吐量有所下降。

(b, c) 当启用所有优化后，随着流数的增加，数据复制所占用的CPU周期比例减少。在接收端，网络饱和导致内存管理开销降低（由于更好的页面回收）和调度开销增加（由于频繁的空闲）。对于x=1、8、16和24的情况，接收端的整体CPU利用率分别为1、3.75、5.21和6.58核心。详见§3.2。

### 图6：广播流量模式下的Linux网络栈性能

(a) 每个核心的吞吐量（Gbps）  
每列展示了不同流数下的每个核心吞吐量（接收核心在所有情况下都是瓶颈）。总吞吐量随着流数的增加而减少。

(b) 启用所有优化后，每个组件使用的CPU周期比例随流数的变化并不显著。发送端的CPU分解见[7]。

(c) 接收端的L3缓存未命中率随着流数的增加而增加，导致每字节数据复制开销增加，进而降低了每个核心的吞吐量。详见§3.3。

### 网络饱和时的处理开销变化

如图5(a)所示，在8个流时，网络链路成为瓶颈，吞吐量在所有核心之间公平分配。图5(c)显示了在这种情况下瓶颈的变化：调度开销增加，而内存管理开销减少。直观地讲，当网络饱和时，接收核心在某些时候开始空闲—线程在等待数据时反复进入睡眠状态，并在新数据到达时唤醒，这导致了上下文切换和调度开销的增加。这种效应随着流数的增加变得更加明显（图5(b)，图5(c)），因为每个核心的CPU利用率下降。

为了理解内存分配/释放开销的减少，我们观察到内核页分配器维护了一个每个核心的页集，其中包括一定数量的空闲页。在分配请求时，如果页集中有可用页，则可以直接从中获取；否则需要访问全局空闲列表（这是一个更昂贵的操作）。当多个流共享接入链路带宽时，每个核心处理的流量相对较少。这使得使用过的页可以在页集变为空之前被回收，从而减少了内存分配开销（图5(c)）。

### 通过广播增加接收端的竞争

我们现在通过广播流量模式在接收核心上创建额外的竞争，流数从1增加到24（每个流在发送端使用一个唯一的核）。与之前的场景相比，这种情况增加了对以下资源的竞争：(1) CPU资源，如L3缓存；(2) 应用线程之间的CPU调度。我们将讨论这些变化如何影响网络处理开销。

每个核心的每字节数据复制开销随着流数的增加而增加。图6(a)显示，随着流数的增加，每个核心的吞吐量下降，与单一流情况相比，在8个流时最多下降约19%。图6(b)显示，随着流数的增加，CPU分解没有显著变化，这意味着没有明显的CPU开销转移。图6(c)提供了一些关于每个核心吞吐量下降的根本原因的直觉。随着接收端每个核心的流数增加，不同流的应用程序竞争相同的L3缓存空间，导致缓存未命中率增加（从1个流的48%增加到8个流的78%）。这导致了每字节数据复制开销增加和每个核心的吞吐量下降。如图6(c)所示，随着流数增加，L3缓存未命中率的增加与每个核心吞吐量的下降密切相关。

TCP协议的发送端驱动特性排除了接收端的调度。上述观察到的较高缓存竞争是由于同一核心上的多个活动流引起的。虽然发送端可以通过仔细的流调度来减少这种竞争，但在接收端的问题是根本性的：TCP协议的发送端驱动特性排除了接收端控制每个核心的活动流数量的能力，从而导致不可避免的CPU效率低下。我们认为接收端驱动的协议[18, 35]可以为接收端提供这种控制，从而实现高效的传输设计。