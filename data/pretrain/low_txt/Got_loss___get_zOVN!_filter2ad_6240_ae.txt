### CPU and Network Performance in Virtual Networks

The performance of a system is significantly influenced by the balance between CPU and network speeds. In transmission (TX), a fast network is advantageous, but it can negatively impact the reception (RX) side. Conversely, a fast CPU benefits the RX side, which is typically processor-limited, but it can degrade TX performance. Therefore, simply adjusting the ratio of CPU to network speed is not a viable substitute for implementing proper flow control in the virtual network.

### Related Work

In recent years, extensive research has been conducted on the TCP incast and flow completion time (FCT) performance of Partition-Aggregate applications. For instance, studies such as [15, 39] propose reducing retransmission timeouts by 10-1000 times. Other proposals have achieved significant FCT reductions for typical datacenter workloads using new single-path [8, 9, 41, 38] or multi-path [42, 24, 36, 7] transports, often coupled with deadline-aware or agnostic schedulers and per-flow queuing.

#### DeTail
DeTail [42] identifies packet loss in physical networks as one of the three main issues. The authors enable flow control, specifically Priority Flow Control (PFC), and introduce a new multi-path congestion management scheme to address flash hotspots typical of Partition-Aggregate workloads. They also employ Explicit Congestion Notification (ECN) to manage persistent congestion. DeTail uses a modified version of NewReno to reduce FCT by 50% at the 99.9th percentile, but it does not address virtual overlays.

#### pFabric
pFabric [10] re-evaluates the end-to-end argument by introducing a "deconstructed" light transport stack resident in the end node, designed specifically for latency-sensitive datacenter applications. It includes a greedy scheduler for deadline-aware global scheduling and a simplified retransmission scheme for loss recovery. By replacing both the TCP stack and the standard datacenter fabric, pFabric achieves near-ideal performance for short flows. However, open issues include scalability to datacenter-scale port counts, costs of replacing commodity fabrics, fairness, and compatibility with lossless converged datacenter applications.

#### DCTCP
DCTCP [8] uses a modified ECN feedback loop with a multibit feedback estimator to filter incoming ECN streams. This compensates for the stiff active queue management in the congestion point detector with a smooth congestion window reduction function, similar to QCN's rate decrease. DCTCP reduces FCT by 29%, but as a deadline-agnostic TCP, it misses about 7% of deadlines.

#### D3
D3 [41] is a deadline-aware, first-come, first-reserved non-TCP transport. While it performs well, it suffers from priority inversions for about 33% of requests [38] and requires a new protocol stack.

#### PDQ
PDQ [24] introduces a multi-path preemptive scheduling layer to meet flow deadlines using a FIFO taildrop mechanism similar to D3. By allocating resources to the most critical flows first, PDQ improves on D3, RCP, and TCP by approximately 30%. As it is not TCP, its fairness remains to be studied.

#### D2TCP
D2TCP [38] builds on D3 and DCTCP, sharing common features in the ECN filter. It penalizes the window size with a gamma factor, providing iterative feedback to near-deadline flows and preventing congestive collapse. This deadline-aware, TCP-friendly proposal yields 75% and 50% fewer deadline misses than DCTCP and D3, respectively.

#### Hedera and MP-TCP
Hedera and MP-TCP [7, 23, 31] propose multi-path TCP versions optimized for load balancing and persistent congestion. However, short flows with fewer than 10 packets or FCT-sensitive applications do not benefit, despite the complexity of introducing new sub-sequence numbers in the multi-path TCP loop.

### Concluding Remarks

Fabric-level per-lane flow control to prevent packet loss due to contention and transient congestion has long been a hallmark of high-end networks and HPC interconnects. The recent introduction of CEE priority flow control has made this feature more accessible. Despite these advances, current virtual overlays still lag behind. Congestion, whether inherent in the traffic pattern or as an artifact of transient CPU overloads, is still managed by dropping packets, leading to convergence issues, degraded performance, and wasted CPU and network resources.

We have provided initial evidence that packet loss is a costly issue for latency-sensitive virtualized datacenter applications. To address this, we identified the origins of packet drops across the entire virtualized communication stack and designed and implemented a fully lossless virtual network prototype. Our experimental results, using both prototype implementations and large-scale simulations, demonstrate average FCT improvements of one order of magnitude. Key takeaways include:
- Packet loss in virtualized datacenters is even more costly than previously studied in physical networking.
- FCT performance of Partition-Aggregate workloads is greatly improved by losslessness in the virtualized network.
- Commodity CEE fabrics and standard TCP stacks still have untapped performance benefits.
- zOVN can be orthogonally composed with other schemes for functional or performance enhancements on layers 2 to 5.

### Acknowledgements

We are deeply grateful to the anonymous reviewers and our shepherd, Amin Vahdat, for their valuable feedback. We also thank Rami Cohen and Katherine Barabash from IBM HRL for DOVE-related assistance, Andreea Anghel for the initial OVN loss experiments, and Anees Shaikh, Renato Recio, Vijoy Pandey, Vinit Jain, Keshav Kamble, Martin Schmatz, and Casimer DeCusatis for their support, comments, and feedback.

### References

[1] Iperf. URL: http://iperf.sourceforge.net.
[2] Linux Bridge. URL: http://www.linuxfoundation.org/collaborate/workgroups/networking/bridge.
[3] Open vSwitch. URL: http://openvswitch.org.
[4] QEMU-KVM. URL: http://www.linux-kvm.org.
[5] Fabric convergence with lossless Ethernet and Fibre Channel over Ethernet (FCoE), 2008. URL: http://www.bladenetwork.net/userfiles/file/PDFs/WP_Fabric_Convergence.pdf.
[6] P802.1Qbb/D2.3 - Virtual Bridged Local Area Networks - Amendment: Priority-based Flow Control, 2011. URL: http://www.ieee802.org/1/pages/802.1bb.html.
[7] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: Dynamic Flow Scheduling for Data Center Networks. In Proc. NSDI 2010, San Jose, CA, April 2010.
[8] M. Alizadeh, A. Greenberg, D. A. Maltz, et al. DCTCP: Efficient Packet Transport for the Commoditized Data Center. In Proc. ACM SIGCOMM 2010, New Delhi, India, August 2010.
[9] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat, and M. Yasuda. Less is More: Trading a little Bandwidth for Ultra-Low Latency in the Data Center. In Proc. NSDI 2012, San Jose, CA, April 2012.
[10] M. Alizadeh, S. Yang, S. Katti, N. McKeown, et al. Deconstructing Datacenter Packet Transport. In Proc. HotNets 2012, Redmond, WA.
[11] K. Barabash, R. Cohen, D. Hadas, V. Jain, et al. A Case for Overlays in DCN Virtualization. In Proc. DCCAVES’11, San Francisco, CA.
[12] P. Baran. On Distributed Communications Networks. IEEE Transactions on Communications, 12(1):1–9, March 1964.
[13] R. Birke, D. Crisan, K. Barabash, A. Levin, C. DeCusatis, C. Minkenberg, and M. Gusat. Partition/Aggregate in Commodity 10G Ethernet Software-Defined Networking. In Proc. HPSR 2012, Belgrade, Serbia, June 2012.
[14] M. S. Blumenthal and D. D. Clark. Rethinking the Design of the Internet: The End-to-End Arguments vs. the Brave New World. ACM Transactions on Internet Technology, 1(1):70–109, August 2001.
[15] Y. Chen, R. Griffith, J. Liu, R. H. Katz, and A. D. Joseph. Understanding TCP Incast Throughput Collapse in Datacenter Networks. In Proc. WREN 2009, Barcelona, Spain, August 2009.
[16] D. Cohen, T. Talpey, A. Kanevsky, et al. Remote Direct Memory Access over the Converged Enhanced Ethernet Fabric: Evaluating the Options. In Proc. HOTI 2009, New York, NY, August 2009.
[17] R. Cohen, K. Barabash, B. Rochwerger, L. Schour, D. Crisan, R. Birke, C. Minkenberg, M. Gusat, et al. An Intent-based Approach for Network Virtualization. In Proc. IFIP/IEEE IM 2013, Ghent, Belgium.
[18] D. Crisan, A. S. Anghel, R. Birke, C. Minkenberg, and M. Gusat. Short and Fat: TCP Performance in CEE Datacenter Networks. In Proc. HOTI 2011, Santa Clara, CA, August 2011.
[19] W. Dally and B. Towles. Principles and Practices of Interconnection Networks, Chapter 13. Morgan Kaufmann Publishers Inc., San Francisco, CA, 2003.
[20] N. Dukkipati and N. McKeown. Why Flow-Completion Time is the Right Metric for Congestion Control. ACM SIGCOMM CCR, 36(1):59–62, January 2006.
[21] H. Grover, D. Rao, D. Farinacci, and V. Moreno. Overlay Transport Virtualization. Internet draft, IETF, July 2011.
[22] M. Gusat, D. Crisan, C. Minkenberg, and C. DeCusatis. R3C2: Reactive Route and Rate Control for CEE. In Proc. HOTI 2010, Mountain View, CA, August 2010.
[23] H. Han, S. Shakkottai, C. V. Hollot, R. Srikant, and D. Towsley. Multi-Path TCP: A Joint Congestion Control and Routing Scheme to Exploit Path Diversity in the Internet. IEEE/ACM Transactions on Networking, 14(6):1260–1271, December 2006.
[24] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing Flows Quickly with Preemptive Scheduling. In Proc. ACM SIGCOMM 2012, Helsinki, Finland.
[25] S. Kandula, D. Katabi, S. Sinha, and A. Berger. Dynamic Load Balancing Without Packet Reordering. ACM SIGCOMM Computer Communication Review, 37(2):53–62, April 2007.
[26] M. Mahalingam, D. Dutt, K. Duda, et al. VXLAN: A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks. Internet draft, IETF, August 2011.
[27] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, et al. OpenFlow: Enabling Innovation in Campus Networks. ACM SIGCOMM Computer Communication Review, 38(2):69–74, April 2008.
[28] J. Mudigonda, P. Yalagandula, J. C. Mogul, et al. NetLord: A Scalable Multi-Tenant Network Architecture for Virtualized Datacenters. In Proc. ACM SIGCOMM 2011, Toronto, Canada.
[29] B. Pfaff, B. Lantz, B. Heller, C. Barker, et al. OpenFlow Switch Specification Version 1.1.0. Specification, Stanford University, February 2011. URL: http://www.openflow.org/documents/openflow-spec-v1.1.0.pdf.
[30] G. Pfister and V. Norton. Hot Spot Contention and Combining in Multistage Interconnection Networks. IEEE Transactions on Computers, C-34(10):943–948, October 1985.
[31] C. Raiciu, S. Barre, and C. Pluntke. Improving Datacenter Performance and Robustness with Multipath TCP. In Proc. ACM SIGCOMM 2011, Toronto, Canada, August 2011.
[32] L. Rizzo. netmap: A Novel Framework for Fast Packet I/O. In Proc. USENIX ATC 2012, Boston, MA.
[33] L. Rizzo and G. Lettieri. VALE, a Switched Ethernet for Virtual Machines. In Proc. CoNEXT 2012, Nice, France, December 2012.
[34] R. Russell. virtio: Towards a De-Facto Standard For Virtual I/O Devices. ACM SIGOPS Operating System Review, 42(5):95–103, July 2008.
[35] J. H. Saltzer, D. P. Reed, and D. D. Clark. End-to-End Arguments in System Design. ACM Transactions on Computer Systems, 2(4):277–288, November 1984.
[36] M. Scharf and T. Banniza. MCTCP: A Multipath Transport Shim Layer. In Proc. IEEE GLOBECOM 2011, Houston, TX, December 2011.
[37] M. Sridharan, K. Duda, I. Ganga, A. Greenberg, et al. NVGRE: Network Virtualization using Generic Routing Encapsulation. Internet draft, IETF, September 2011.
[38] B. Vamanan, J. Hasan, and T. N. Vijaykumar. Deadline-Aware Datacenter TCP (D2TCP). In Proc. ACM SIGCOMM 2012, Helsinki, Finland.
[39] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G. Andersen, G. R. Ganger, G. A. Gibson, and B. Mueller. Safe and Effective Fine-grained TCP Retransmissions for Datacenter Communication. In Proc. ACM SIGCOMM 2009, Barcelona, Spain.
[40] G. Wang and T. S. E. Ng. The Impact of Virtualization on Network Performance of Amazon EC2 Data Center. In Proc. INFOCOM 2010, San Diego, CA, March 2010.
[41] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron. Better Never than Late: Meeting Deadlines in Datacenter Networks. In Proc. ACM SIGCOMM 2011, Toronto, Canada, August 2011.
[42] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. Katz. DeTail: Reducing the Flow Completion Time Tail in Datacenter Networks. In Proc. ACM SIGCOMM 2012, Helsinki, Finland, August 2012.