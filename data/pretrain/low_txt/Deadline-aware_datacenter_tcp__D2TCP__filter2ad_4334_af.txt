### Exponent of the Gamma-Correction Function

The exponent of the gamma-correction function can cause undesirable behavior when set to extremely low or high values. Therefore, we cap the value of \( d \) within the range \((n, 1/n)\). In all our experiments, we set \( n = 2.0 \). This section evaluates the effects of varying \( n \).

In Figure 15, we show D2TCP’s percentage of missed deadlines for various fan-ins as we vary \( n \) between 1.25 and 3.0. As expected, when \( n \) is close to 1.0, D2TCP's behavior matches DCTCP, and the fraction of missed deadlines is high. As \( n \) increases to 2.0, the fraction drops dramatically but then levels off. At \( n = 3.0 \) and beyond, we see that the fraction starts to increase slowly as the larger \( n \) allows near-deadline flows to increasingly ignore congestion feedback.

### Coexisting with TCP

To demonstrate that D2TCP can coexist with TCP without hurting bandwidth or deadlines, we use the same production benchmark but with a mix of D2TCP and TCP for the various network traffic. We restrict this experiment to fan-in degrees where TCP performance is acceptable (i.e., missed deadline fraction of 5% or less, and fan-in degree of 15 and 20).

We run three experiments gradually adding D2TCP traffic to a TCP datacenter. Imagine that the set of five OLDI applications in our workload are divided into two sets: set A consists of three OLDIs, and set B consists of the other two OLDIs. We start with all five OLDIs and all long flows running on TCP. We first "upgrade" set B to D2TCP, while set A continues to run on TCP. Next, we upgrade the background long-flow traffic to D2TCP as well. The three setups are:

- **All-TCP**: 5x OLDIs + long flows; no D2TCP.
- **Mix#1 TCP**: 3x OLDIs + long flows; D2TCP: 2x OLDIs.
- **Mix#2 TCP**: 3x OLDIs; D2TCP: 2x OLDIs + long flows.

In Figure 16, we show the fraction of missed deadlines (Y axis) for sets A and B in the three runs as we vary the fan-in degree (X axis). For set A, comparing All-TCP and Mix#1 shows that TCP’s (i.e., set A’s) missed-deadline fraction is not worsened by sharing the network with D2TCP traffic (set B in Mix#1). Furthermore, set B sees a reduction in missed deadlines upon migrating from TCP (All-TCP) to D2TCP (Mix#1). Comparing Mix#1 and Mix#2, we see that TCP’s (i.e., set A’s) missed-deadline fraction is not hurt by background flows upgrading to D2TCP. Similarly, set B's missed-deadline fraction stays the same between Mix#1 and Mix#2, showing that background flows using D2TCP do not hurt the OLDIs using D2TCP.

In Table 2, we show the long-flow throughput achieved in the three runs. Going from All-TCP to Mix#1, the throughput does not degrade, showing that upgrading some OLDIs to D2TCP does not hurt long TCP flows. Comparing Mix#1 and Mix#2 shows that D2TCP’s long-flow throughput is similar to that of TCP.

### Tighter Deadlines

To show that D2TCP performs well over a range of deadlines, we evaluate D2TCP under tighter deadlines than our default (Section 4.2.1). Because we found that deadlines tightened by 10% or 20% lead to similar behavior, we show results only for the 20% case in Figure 17. As expected, the tighter deadlines here result in more deadlines being missed under all schemes than those missed in Figure 10. Nevertheless, D2TCP maintains its advantage over DCTCP and D3 under the tighter deadlines.

While the above results show the fractions of missed deadlines under tighter deadlines, it may be important to determine the inverse (i.e., how much tighter can the deadlines be for a target fraction of missed deadlines). For instance, datacenter operators who would like to maintain the fraction of missed deadlines within an acceptable threshold and wish to know how much the communication deadlines can be tightened to allow more time for computation and improve response quality. In Table 3, we show the tightness of deadlines supported by D2TCP as compared to D3 for a target fraction of missed deadlines. We limit the study to a reasonable fraction of missed deadlines (i.e., 5%). From the table, we see that D2TCP achieves deadlines that are tighter by 35-55%, which would make sizable room for computation.

### Related Work

There is an abundance of past work dealing with congestion control, network scheduling, and reducing latencies. Many schemes build on top of TCP, while others are novel protocols altogether. A comprehensive review of all such work is beyond the scope of this paper, but we summarize some of the most relevant work here.

- **Earliest Deadline First (EDF)** [17] is one of the earliest packet scheduling algorithms and is provably optimal when deadlines are associated with individual packets. When deadlines are associated with flows, applying EDF to individual packets as they arrive at the switch is not only suboptimal but can worsen the congestion in the network [26].
- **Rate Control Protocol (RCP)** [5] can achieve a 10-fold improvement in the completion times of small- to medium-sized flows in the Internet, particularly downloads representative of typical web browsing. RCP replaces TCP’s slow start phase with an allocation equal to the fair share available at the bottleneck. Like D3, RCP requires hardware modification to the routers.
- **Live multimedia traffic** also has a soft-real-time nature, and both proactive bandwidth reservation [6] and reactive [23] [16] schemes exist. TCP-RTM [16] observes that TCP always favors reliability over timeliness and proposes extensions that improve the performance of multimedia applications by allowing minimal packet reordering and loss in the TCP stack.
- **Active Queue Management schemes** like RED [8] and E-TCP [11] inject early warnings of congestion to TCP endhosts by randomly dropping packets when switch buffer occupancy is high. These schemes allow TCP to operate in the high throughput, fast-retransmit mode, instead of degrading to full back-off.
- **High-speed TCP** [7], CUBIC [22], and XCP [15] all successfully improve the performance of TCP in high bandwidth-delay-product networks. They exploit the large degree of statistical multiplexing present and mitigate TCP’s drastic reaction to packet losses. XCP shares some common design details with D3 in that senders request bandwidth via a congestion header, and the switches populate their responses in this header.
- **Re-feedback** [3] addresses the problem of fairness and stability in the Internet when untrusted senders may act selfishly in the face of congestion. Re-feedback incentivizes senders to populate packet headers with honest information about congestion situations so the network may schedule accordingly.
- **QCN** [20] proposes to improve Ethernet performance in datacenters via multibit feedback from the switches to endhosts. By utilizing smarter switches and hardware-based reaction logic in the endhost NICs, QCN dramatically reduces recovery time during congestions, thus improving flow completion times. However, QCN cannot span beyond L2 domains, limiting its scope of application. VCP [27] is another similar scheme that relies on ECN-like feedback via elaborate processing at the switches.

### Conclusion

Online, data-intensive applications (OLDI) in datacenters (e.g., Web search, online retail, and advertisement) achieve good user experience by controlling latency using soft-real-time constraints. OLDI applications typically employ tree-based algorithms, which, in the common case, result in fan-in bursts of children-to-parent traffic with tight deadlines. Previous work on datacenter network protocols is either deadline-agnostic or is deadline-aware but suffers under bursts due to race conditions.

We proposed Deadline-Aware DataCenter TCP (D2TCP) for which:
- Prioritizes near-deadline flows over far-deadline flows in the presence of fan-in-burst-induced congestion.
- Achieves high bandwidth for background flows even as short-lived OLDI flows come and go.
- Requires no changes to the switch hardware.
- Coexists with legacy TCP.

D2TCP uses a distributed and reactive approach for fundamentally enabling D2TCP’s bandwidth allocation properties. D2TCP’s key mechanism is a novel congestion avoidance algorithm, which uses ECN feedback and deadlines to modulate the congestion window via a gamma-correction function.

Using small-scale real implementation and at-scale simulations, we showed that D2TCP:
- Reduces the fraction of missed deadlines compared to both DCTCP and D3 by 75% and 50%, respectively.
- Achieves nearly as high bandwidth as TCP for background flows without degrading OLDI performance.
- Meets deadlines that are 35-55% tighter than those achieved by D3 for the same reasonable fraction of missed deadlines (i.e., 5%), giving OLDIs more time for actual computation.
- Coexists with TCP without degrading OLDI performance.

D2TCP has significant performance and practical advantages. On the performance side, by reducing the number of missed deadlines, D2TCP improves OLDI applications’ response quality and hence user experience. Further, by meeting tighter deadlines, D2TCP allows more time for computation in OLDI applications and thereby further enhances OLDI response quality and user experience. Given that OLDI applications are likely to scale up in size to accommodate ever-growing data on the Web, D2TCP’s tighter deadlines may fundamentally enable this scale-up without degrading OLDI response quality. On the practical side, by requiring no changes to the switch hardware, D2TCP can be deployed by merely upgrading the TCP and RPC stacks. Our prototype implementation of D2TCP amounted to only 100 lines of kernel code. Finally, by being able to coexist with TCP, D2TCP is amenable to incremental deployment, a key requirement for datacenter network protocols in the real world. The growing importance of OLDI applications implies that these significant advantages make D2TCP an important ingredient for datacenters.

### Acknowledgments

We thank the SIGCOMM reviewers and our shepherd David Maltz for their insightful comments, which helped us significantly improve the paper. We also thank Sridhar Raman and Abdul Kabbani for their help with real implementations of DCTCP and D2TCP, and Gwendolyn Voskuilen for reviewing the paper.

### References

[1] M. Alizadeh, A. G. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data center TCP (DCTCP). In Proc. SIGCOMM, 2010.
[2] Charles A. Poynton (2003). Digital Video and HDTV: Algorithms and Interfaces. Morgan Kaufmann. pp. 260, 630. ISBN 1558607927.
[3] B. Briscoe et al. Policing Congestion Response in an Internetwork using Re-feedback. In Proc. SIGCOMM 2005.
[4] Datacenter TCP, http://www.stanford.edu/~alizade/Site/DCTCP.htm
[5] Nandita Dukkipati. RCP: Congestion Control to Make Flows Complete Quickly. PhD Thesis, Department of Electrical Engineering, Stanford University, October 2006.
[6] D. Ferrari, A. Banerjea, and H. Zhang. Network support for multimedia: A discussion of the tenet approach. In Proc. Computer Networks and ISDN Systems, 1994.
[7] S. Floyd. RFC 3649: HighSpeed TCP for large congestion windows.
[8] S. Floyd and V. Jacobson. Random early detection gateways for congestion avoidance. IEEE/ACM Transactions on Networking, 1(4):397–413, 1993.
[9] S. Floyd and V. Jacobson. The synchronization of periodic routing messages. IEEE/ACM Transactions on Networking, 2(2):122-136, 1994.
[10] R. Griffith, Y. Chen, J. Liu, A. Joseph, and R. Katz. Understanding TCP incast throughput collapse in datacenter networks. In WREN Workshop, 2009.
[11] Y. Gu, D. Towsley, C. Hollot, and H. Zhang. Congestion control for small buffer high bandwidth networks. In Proc. INFOCOM, 2007.
[12] Urs Hoelzle, Jeffrey Dean, and Luiz André Barroso. Web Search for A Planet: The Architecture of the Google Cluster, In IEEE Micro Magazine, April 2003.
[13] T. Hoff. Latency is Everywhere and it Costs You Sales - How to Crush it, July 2009. http://highscalability.com/blog/2009/7/25/latency-iseverywhere-and-it-costs-you-sales-how-to-crush-it.html.
[14] S. Iyer et al. Analysis of a memory architecture for fast packet buffers. In IEEE HPSR Workshop, 2001.
[15] D. Katabi, M. Handley, and C. Rohrs. Congestion Control for High Bandwidth-Delay Product Networks. In Proc. SIGCOMM, 2002.
[16] Sam Liang and David Cheriton. TCP-RTM: Using TCP for Real Time Applications. In Proc. ICNP, 2002.
[17] C. L. Liu and J. W. Layland. Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment. Journal of the ACM, 20(1), 1973.
[18] D. Meisner, C. M. Sadler, L. A. Barroso, W. Weber, and T. F. Wenisch. Power Management of Online Data-Intensive Services. In Proc. ISCA, June 2011.
[19] The ns-3 discrete-event network simulator. http://www.nsnam.org/
[20] R. Pan, B. Prabhakar, and A. Laxmikantha. QCN: Quantized congestion notification an overview. http://www.ieee802.org/1/files/public/docs2007/au_prabhakar_qcn_overview_geneva.pdf
[21] K. Ramakrishnan, S. Floyd, and D. Black. RFC 3168: The addition of explicit congestion notification (ECN) to IP.
[22] I. R. Sangtae Ha and L. Xu. Cubic: A new TCP-friendly high-speed TCP variant. In Proc. SIGOPS-OSR, 2008.
[23] V. Tsaoussidis and C. Zhang. 2002. TCP-Real: receiver-oriented congestion control. The International Journal of Computer and Telecommunications Networking. 40(4), 2002.
[24] V. Vasudevan et al. Safe and effective fine-grained TCP retransmissions for datacenter communication. In Proc. SIGCOMM, 2009.
[25] C. Wilson, H. Ballani, T. Karagiannis, A. Rowstron. Better Never Than Late: Meeting Deadlines in Datacenter Networks. In Proc. SIGCOMM, 2011.
[26] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron. Better never than late: Meeting deadlines in datacenter networks. Technical Report MSR-TR-2011-66, Microsoft Research, May 2011.
[27] Y. Xia, L. Subramanian, I. Stoica, and S. Kalyanaraman. One more bit is enough. In Proc. SIGCOMM, 2005.