### References

1. **[67] F. Peng, Z. Deng, X. Zhang, D. Xu, Z. Lin, and Z. Su.** X-Force: Force-Executing Binary Programs for Security Applications. In *Proceedings of the 23rd USENIX Security Symposium (Security)*, pages 829–844, San Diego, CA, August 2014.

2. **[68] K. Rieck, T. Holz, C. Willems, P. Düssel, and P. Laskov.** Learning and Classification of Malware Behavior. In *Proceedings of the 5th Conference on Detection of Intrusions and Malware and Vulnerability Assessment (DIMVA)*, pages 108–125, 2008.

3. **[69] P. Royal, M. Halpin, D. Dagon, R. Edmonds, and W. Lee.** PolyUnpack: Automating the Hidden-Code Extraction of Unpack-Executing Malware. In *Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC)*, pages 289–300, 2006.

4. **[70] E. J. Schwartz, T. Avgerinos, and D. Brumley.** All You Ever Wanted to Know About Dynamic Taint Analysis and Forward Symbolic Execution (But Might Have Been Afraid to Ask). In *2010 IEEE Symposium on Security and Privacy*, pages 317–331, Oakland, CA, May 2010.

5. **[71] C. Spensky, H. Hu, and K. Leach.** Lo-Phi: Low-Observable Physical Host Instrumentation for Malware Analysis. In *Proceedings of the 23rd Annual Network and Distributed System Security Symposium (NDSS)*, San Diego, CA, February 2016.

6. **[72] P. Srivastava and N. Hopwood.** A Practical Iterative Framework for Qualitative Data Analysis. *International Journal of Qualitative Methods*, 8(1):76–84, 2009.

7. **[73] F. Tegeler, X. Fu, G. Vigna, and C. Kruegel.** BotFinder: Finding Bots in Network Traffic Without Deep Packet Inspection. In *Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies*, pages 349–360, 2012.

8. **[74] X. Ugarte-Pedrero, D. Balzarotti, I. Santos, and P. G. Bringas.** RAMBO: Run-Time Packer Analysis with Multiple Branch Observation. In *Proceedings of the 13th Conference on Detection of Intrusions and Malware and Vulnerability Assessment (DIMVA)*, pages 186–206, 2017.

9. **[75] D. Votipka, R. Stevens, E. Redmiles, J. Hu, and M. Mazurek.** Hackers vs. Testers: A Comparison of Software Vulnerability Discovery Processes. In *Proceedings of the 39th IEEE Symposium on Security and Privacy (Oakland)*, pages 374–391, San Jose, CA, May 2018.

10. **[76] D. Votipka, S. Rabin, K. Micinski, J. S. Foster, and M. L. Mazurek.** An Observational Investigation of Reverse Engineers’ Processes. In *29th USENIX Security Symposium (USENIX Security 20)*, pages 1875–1892, 2020.

11. **[77] S. Wang and D. Wu.** In-Memory Fuzzing for Binary Code Similarity Analysis. In *2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)*, pages 319–330. IEEE, 2017.

12. **[78] M. Xu and T. Kim.** PlatPal: Detecting Malicious Documents with Platform Diversity. In *Proceedings of the 25th USENIX Security Symposium (Security)*, pages 271–287, Vancouver, BC, Canada, August 2017.

13. **[79] Z. Xu, J. Zhang, G. Gu, and Z. Lin.** GoldenEye: Efficiently and Effectively Unveiling Malware’s Targeted Environment. In *International Workshop on Recent Advances in Intrusion Detection*, pages 22–45, 2014.

14. **[80] B. Yadegari and S. Debray.** Symbolic Execution of Obfuscated Code. In *Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security*, pages 732–744, Denver, Colorado, October 2015.

15. **[81] L.-K. Yan, M. Jayachandra, M. Zhang, and H. Yin.** V2E: Combining Hardware Virtualization and Software Emulation for Transparent and Extensible Malware Analysis. In *Proceedings of the 8th ACM SIGPLAN/SIGOPS Conference on Virtual Execution Environments*, pages 227–238, 2012.

16. **[82] T.-F. Yen and M. K. Reiter.** Traffic Aggregation for Malware Detection. In *Proceedings of the 5th Conference on Detection of Intrusions and Malware and Vulnerability Assessment (DIMVA)*, pages 207–227, 2008.

17. **[83] F. Zhang, K. Leach, K. Sun, and A. Stavrou.** Spectre: A Dependable Introspection Framework via System Management Mode. In *Proceedings of the International Conference on Dependable Systems and Networks (DSN)*, pages 1–12, 2013.

18. **[84] F. Zhang, K. Leach, A. Stavrou, H. Wang, and K. Sun.** Using Hardware Features for Increased Debugging Transparency. In *Proceedings of the 36th IEEE Symposium on Security and Privacy (Oakland)*, pages 55–69, San Jose, CA, May 2015.

### Acknowledgments

We thank Alex Bardas and the reviewers for their valuable feedback, Daniel Votipka for recruitment assistance, and our participants for providing insightful contributions.

The second author's work is supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-2039655. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.

### Appendix

#### A. Survey Questionnaire

**Background and Experience:**

- **Role Definition:** Which of the following best describes you? (Select all that apply)
  - **Malware Engineer:** I work on configuring malware sandbox analyzers and/or develop programs to process the inputs and outputs of these analyzers (e.g., prioritizing, clustering, parsing, preventing evasion techniques, capturing system calls).
  - **Malware Analyst:** I analyze malware samples to understand their functionality, potential impact, and origin.
- **Years of Experience:**
  - [If Malware Engineer] How many years have you worked as a malware engineer?
  - [If Malware Analyst] How many years have you worked as a malware analyst?
- **Skills and Specialties:**
  - Please select your most highly qualified skills or specialty areas (check all that apply):
    - Operating Systems
    - Networks
    - Compilers
    - Computer Architecture
    - Programming
    - Cryptography
    - Virtualization
    - Scalability
    - Data Analysis
    - Sandboxing
    - Reverse Engineering
    - Assembly Code
    - Signature Creation

**Job Description:**

- **Current Job Title:**
- **End Goals of Threat/Malware Analysis:**
  - (Select all that apply) Attribution, Forensics, Recovery Remediation, Detection, Classification, Signature Creation, Indication of Compromise, Research, N/A, Other
- **Primary Activities:**
  - Please describe the primary activities you perform day-to-day.
- **Malware Analysis Techniques:**
  - How often do you use the following malware analysis techniques? (Dynamic Analysis, Static Analysis, Other)
- **Sandboxing:**
  - Is your job associated with malware sandboxing? If so, how? (Select all that apply)
    - Collecting malware specimens for the sandbox
    - Prioritizing the malware specimens that get sent to the sandbox
    - Configuring the malware sandbox
    - Categorizing and clustering the malware specimens from sandbox output (usually with ML feature selection)
    - Manually analyzing the output of the malware sandbox
    - Providing feedback to update the sandbox

**Demographics:**

- **Gender:**
  - Woman, Man, Non-binary, Prefer not to answer
- **Age Range:**
  - 18-29, 30-39, 40-49, 50-59, 60-69, >70, Prefer not to answer
- **Ethnicity:**
  - American Indian or Alaska Native, Asian, Black or African American, Hispanic or Latino, White, Prefer not to answer
- **Education Level:**
  - High school credit, no diploma; High school graduate, diploma or equivalent; College credit, no diploma; Trade/Technical/Vocational Training; Associate degree; Bachelor’s degree; Master’s degree; Doctorate degree; Prefer not to answer
- **Major (if applicable):**
- **Business Sector:**
  - Technology, Government, Healthcare, Retail, Construction, Education, Finance, Arts, Other

#### B. Interview Questions

**Background and Experience:**

- **Professional Background:**
  - Could you tell me a little bit about what you do in your profession?
  - How long have you been doing this?
  - What got you into this profession? Have you always been doing this?

**Malware Sources:**

- **Malware Acquisition and Analysis:**
  - Can you walk me through the steps you take to get malware samples and how you decide which ones to analyze? Imagine that you have just started your day at work and are trying to figure out where to begin.
  - How do you receive malware specimens that need to be analyzed?
  - Roughly how many malware specimens do you get per day?
  - What types of malware specimens do you usually encounter? (e.g., spyware, virus, trojan, keylogger, rootkit, botnets, ransomware, worms, malvertising, etc.)
  - Do you analyze all of these malware specimens or only a subset? How do you decide which malware to analyze first?
  - What data about the malware do you generally have available before starting your analysis? How do you prioritize the various data types you look at? Why do you prioritize them this way? What data do you consider to be the most important or helpful in doing your analysis?
  - Do you determine whether a malware is a variation of a previously seen malware? If yes, how do you do it?
  - Is there information you wished you had available that isn’t in the report generated by the malware sandbox analyzer?
  - Do you perform the same analysis steps when looking at an unknown malware versus a variant of a previously seen malware? If not, please describe the differences between the two.

**Evolution:**

- **Changes in Analysis Process:**
  - Considering your past experiences in malware analysis, has the analysis process changed over time? If so, what parts have changed? What do you think has been the biggest factor contributing to these changes? (e.g., business model, personal goals, malware type, etc.)
  - Is there any other topic that I haven’t touched on which you would like to elaborate on?

#### C. Follow-up Survey

- **Dynamic Analysis Environment:**
  - Do you or your team install the following settings in your dynamic analysis environment? (Microsoft Office, Web browser, Adobe Acrobat, software libraries)
  - Do you or your team configure or modify the following settings in your dynamic analysis environment? (Location, Time, Languages, Username, File names, Browsing history, Populate files)
  - Which of the following settings do you configure per individual sample? (Location, Time, Languages, Username, File names, Browsing history, Populate files)

- **Malware Variants:**
  - What percentage of the malware would you say are variants of malware you have previously seen, and what percentage are previously unseen?

#### D. Codebook

The codes used to analyze the participant interviews are presented in Table 5.

**High-Level Codes:**

- **Runtime:** Refers to the amount of time the participants run the malware sample in the dynamic analysis system.
- **Number of Runs:** Refers to the number of times the participant runs a malware sample in the dynamic analysis system.
- **Number of Malware Samples:** Refers to the number of malware samples that a participant analyzes.
- **Data Source:** Refers to the source of malware samples that participants analyze.
- **Fresh Malware Samples:** Refers to the amount of time since a malware sample was released.
- **Prioritization of Malware Samples:** Refers to the process that participants use to determine the order in which they analyze their malware samples.
- **Malware Variants:** Refers to the process that participants use to determine whether an unknown sample is a variant of a known malware.
- **Simulated Network:** Refers to whether the participant simulates the network when performing dynamic analysis.
- **Use of Open Source or Commercial Sandboxes:** Refers to whether participants use open-source and/or commercial dynamic analysis tools.
- **Preference of Open vs. Commercial Sandbox:** Refers to the participants' preference between open-source and commercial dynamic analysis tools.
- **Use of Bare Metal:** Refers to the participants' use of bare metal for dynamic analysis.
- **Operating System:** Refers to the operating system that participants use for dynamic malware analysis.
- **Applications Installed in Dynamic Analysis Systems:** Refers to the applications that participants install in the dynamic analysis system.
- **Environment Settings:** Refers to the configuration of the environment within the dynamic analysis system.
- **Mimic Real User:** Refers to whether participants configure their dynamic analysis environments to appear as if the system was used by a real user.
- **Monitoring of the Dynamic Analysis Execution:** Refers to the process used to monitor the execution of the malware sample in a dynamic analysis system.
- **Evasion:** Refers to whether participants discussed strategies they use to analyze evasive malware samples.
- **Static Analysis Process and Tools:** Refers to the participants' static analysis process and tools used.
- **Generate Signatures:** Refers to how the participants generate signatures.

**Subcodes:**

- **Runtime:**
  - Minutes, Hours, Weeks, Short
- **Number of Runs:**
  - Numbers
- **Frequency:**
  - Per day, Per week, Per month
- **Data Source:**
  - Clients, Repository, VirusTotal, Pastebin, Twitter, Blog posts, Open source, Crawl
- **Fresh Malware Samples:**
  - New, Today, Novel, First
- **Prioritization:**
  - Priority, Customer, Harmful, FIFO, Risk, Novel, Complexity, New, Damage
- **Malware Variants:**
  - Variant, Family, Cluster, Campaign
- **Simulated Network:**
  - Network, Simulation
- **Sandbox Type:**
  - Open Source, Commercial, Joe Sandbox, Any.run, Cape Sandbox, VirtualBox, FireEye
- **Sandbox Preference:**
  - Open Source, Commercial, Joe Sandbox, Any.run, Cape Sandbox, VirtualBox, FireEye
- **Hardware:**
  - Laptop, VM, Sandbox, Cloud, Server
- **Operating System:**
  - OS, Windows, Linux, Unix
- **Applications:**
  - Web browser, Microsoft, Java, Adobe, Libraries
- **Environment Settings:**
  - Libraries, Timezone, Language, Usernames, User privilege
- **User Activity:**
  - Browser history, Files, Documents, Directories, Usage, Names
- **Monitoring:**
  - Procmon, Hooking, Syscalls, Registry, Files created, Network activity, Pcap, Logs
- **Evasion:**
  - Evasion, Encryption, Packing, Obfuscation, Detection, Sleep, Debug, Breakpoint, Skip, Patch
- **Static Analysis:**
  - Static, Analysis, Tool, Ghidra, IDA, Reverse Engineer, Unpack, Decrypt, Disassembler, Decode, Decrypt, Script, Injection, Code, Binary, Function, Registry Keys, Logs
- **Signature Generation:**
  - Signature, IDS, Network behavior, System Behavior, Tool, IP, Domain, Hash, Tactics, Techniques, Procedure, Capabilities

---

This version of the text is more structured, clear, and professional. Each section is clearly labeled, and the content is organized for better readability and coherence.