### MTBF and Application Performance

- **MTBF: 5 hours**
- **MTBF: 20 hours**

### Data Points
- 1.0, 60, 40, 0.8, 20
- 0.6, -20, 75, 0.4, 50, 25, 0.2, 0
- 0.0, 0, 1, 0.0, 2, 3, 0.2, 4, 6, 5, 7, 0.6, 0.4
- 8, 9, 0.8, 10, 1.0, 0.0, -25, 0.0, 2, 0.2, 0.4, 3, 0.6, OCI Stretch Factor, 4, 0.8, 1.0

### Figure 14: Shiraz Improvements in Multi-Application Mix
**Figure 14 (Left):** 
- Shiraz provides significant improvement in real-world multi-application mixes selected from Table 1, simulated over a year-long period.
- The horizontal lines represent the average improvement in useful work per application.

**Figure 14 (Right):**
- Shiraz+ significantly reduces checkpointing overhead for the same mix of applications.
- With a 3× OCI stretch factor, the checkpointing overhead is reduced by up to 52% without any loss in overall system throughput.
- Increasing the OCI stretch factor to 4× results in a slight degradation (less than 1%) in total useful work but reduces the checkpointing overhead by up to 60%.

### Evaluation Strategy
- **Pairing Applications:** We paired applications with different checkpointing overheads and ran one such pair between two failures using Shiraz, switching to a different pair after each failure.
- **Optimal Strategy:** Combine the application with the highest checkpointing overhead with the one with the lowest, maximizing the average of the ratios of checkpointing overheads.
- **Random Pairs:** While not delivering the maximum possible improvement, random pairing is easier to implement.

### Experimental Setup
- **Multi-Application Environment:** We evaluated Shiraz using 10 applications, simulating a year-long run (8,700 hours).
- **Statistical Stability:** The simulation was repeated 15,000 times, and the average results were reported.

### Observations
- **System Throughput Improvement:** No application suffered performance degradation, and the average throughput improvement was 15 hours.
- **Useful Work:** Shiraz improved the total useful work by approximately 91 hours and 157 hours for petascale and exascale systems, respectively.

### Energy and Cost Savings
- **Exascale System (5 hours MTBF, 20MW power consumption):**
  - Energy savings: 1.78 MW-Hour
  - Monetary savings: $178,000 per year
- **Petascale System (20 hours MTBF, 10MW power consumption):**
  - Energy savings: 0.57 MW-Hour
  - Monetary savings: $57,000 per year

### Future Investments
- **Energy Savings Investment:** These savings could be invested in faster storage systems and more computing power, further increasing profits due to faster completion times.
- **Cost Savings:** For the petascale system, the energy expenditure cuts enabled by Shiraz translate to $285,000 over 5 years, which can cover 5.7% of the cost of SSD-based burst buffers.
- **Exascale System:** The cost savings enabled by Shiraz would amount to $890,000 over 5 years.

### Prototype Implementation and Evaluation
- **Shiraz and Shiraz+ Prototype:** We developed a scheduler plugin that implements the core scheduling algorithm, maintaining records of checkpointing overhead and temporal characteristics of system failures.
- **Real-World Applications:** Evaluated using Co-Design Molecular Dynamics Proxy (CoMD) and Finite Element Solver (miniFE).
- **Checkpointing Library:** DMTCP was used for checkpoints, and the optimal switch point was decided based on experimentally measured checkpointing overhead.
- **Evaluation Results:** Shiraz resulted in 10.2% more useful work system-wide compared to the baseline case, where applications are switched at every failure.

### Related Work
- **Failure Analysis and Checkpointing Methods:** Previous HPC works have focused on failure analysis and developing checkpointing methods for fault tolerance.
- **Application-Specific OCI:** Studies have provided different derivations for application-specific OCI, and our work complements these by scheduling jobs to improve system throughput.
- **Multi-Level Checkpointing:** Strategies that checkpoint at different levels (memory, SSD, PFS) to tolerate different types of failures.
- **Incremental Checkpointing:** Stores only the state of data modified since the last checkpoint, reducing I/O overhead.
- **Faster Storage Options:** Using SSD-based burst buffers to reduce the overhead of writing checkpoint files.
- **Resilience Schemes:** Combining predictive, proactive, and preventive checkpointing, and reliability-aware task scheduling strategies.

### Conclusion
- **Shiraz and Shiraz+:** These schemes improve overall system throughput and reduce checkpointing overhead, leading to significant energy and monetary savings.
- **Future Work:** Improving predictability in a dynamic application-mix will be a worthy goal for future research.

### Acknowledgment
- **Support:** This work was partially supported by Northeastern University, NSF Grant ACI-1440788, and resources from the Mass Open Cloud (MOC).

### References
- [1] CFDR Data. https://tinyurl.com/yd6ornwa. [Online; accessed 28-Nov-2017].
- [2] EIA - Electricity Data. https://tinyurl.com/ya6o3eas. [Online; accessed 04-Dec-2017].
- [3] Intel DC P3608 SSDPECME040T401. https://tinyurl.com/ybng8ll3. [Online; accessed 04-Dec-2017].
- [4] Samsung PM1725a Series 1.6TB TLC. https://tinyurl.com/yd8rcy55. [Online; accessed 04-Dec-2017].
- [5] Large Scale Computing and Storage Requirements for Biological and Environmental Science: Target 2017. Technical Report LBNL-6256E, LBNL, 2012.
- [6] Large Scale Production Computing and Storage Requirements for High Energy Physics: Target 2017. Technical report, LBNL, 2012.
- [7] Jason Ansel, Kapil Arya, and Gene Cooperman. DMTCP: Transparent Checkpointing for Cluster Computations and the Desktop. In IPDPS 2009, pages 1–12. IEEE, 2009.
- [8] L. Bautista-Gomez, N. Maruyama, F. Cappello, and S. Matsuoka. Distributed Diskless Checkpoint for Large-scale Systems. In CCGrid 2010, pages 63–72. IEEE Computer Society, 2010.
- [9] Leonardo Bautista-Gomez et al. Reducing Waste in Extreme Scale Systems Through Introspective Analysis. In IPDPS 2016, pages 212–221. IEEE, 2016.
- [10] Anne Benoit, Aurélien Cavelan, Valentin Le Fèvre, Yves Robert, and Hongyang Sun. Towards Optimal Multi-level Checkpointing. IEEE Trans. Comput, 66(7):1212–1226, 2017.
- [11] R. Birke, I. Giurgiu, L. Y Chen, D. Wiesmann, and T. Engbersen. Failure analysis of virtual and physical machines: Patterns, causes and characteristics. In DSN 2014, pages 1–12. IEEE, 2014.
- [12] Mohamed Slim Bouguerra et al. Improving the Computing Efficiency of HPC Systems Using a Combination of Proactive and Preventive Checkpointing. In IPDPS 2013, pages 501–512. IEEE, 2013.
- [13] Franck Cappello. Fault Tolerance in Petascale/Exascale Systems: Current Knowledge, Challenges and Research Opportunities. IJHPCA, 23(3):212–226, 2009.
- [14] John T Daly. A Higher Order Estimate of the Optimum Checkpoint Interval for Restart Dumps. Future Generation Computer Systems, 22(3):303–312, 2006.
- [15] S. Di, M. S Bouguerra, L. Bautista-Gomez, and F. Cappello. Optimization of Multi-level Checkpoint Model for Large Scale HPC Applications. In IPDPS 2014, pages 1181–1190. IEEE, 2014.
- [16] Sheng Di, Yves Robert, Frédéric Vivien, and Franck Cappello. Towards an Optimal Online Checkpoint Solution Under a Two-level HPC Checkpoint Model. TPDS 2017, 28(1):244–259, 2017.
- [17] N. El-Sayed and B. Schroeder. Reading Between the Lines of Failure Logs: Understanding How HPC Systems Fail. In DSN 2013, pages 1–12. IEEE, 2013.
- [18] Elmootazbellah N Elnozahy and James S Plank. Checkpointing for Peta-scale Systems: A Look into the Future of Practical Rollback-Recovery. TDSC 2004, 1(2):97–108, 2004.
- [19] Kurt Ferreira et al. Evaluating the Viability of Process Replication Reliability for Exascale Systems. In SC 2011, page 44. ACM, 2011.
- [20] Kurt B Ferreira, Rolf Riesen, Patrick Bridges, Dorian Arnold, and Ron Brightwell. Accelerating incremental checkpointing for extreme-scale computing. Future Generation Computer Systems, 30:66–77, 2014.
- [21] Ana Gainaru, Franck Cappello, and William Kramer. Taming of the Shrew: Modeling the Normal and Faulty Behaviour of Large-scale HPC Systems. In IPDPS 2012, pages 1168–1179. IEEE, 2012.
- [22] M Heroux. MiniFE: Finite Element Solver. https://tinyurl.com/y7hslf65. [Online; accessed 15-Apr-2018].
- [23] Ning Liu et al. On the Role of Burst Buffers in Leadership-class Storage Systems. In MSST 2012, pages 1–11. IEEE, 2012.
- [24] Yudan Liu et al. An Optimal Checkpoint/Restart Model for a Large-scale High Performance Computing System. In IPDPS 2008, pages 1–9. IEEE, 2008.
- [25] Robert Lucas. Top Ten Exascale Research Challenges. In DOE ASCAC Subcommittee Report, 2014.
- [26] Jamaludin Mohd-Yusof, S Swaminarayan, and TC Germann. Co-Design for Molecular Dynamics: An Exascale Proxy Application, 2013.
- [27] Adam Moody, Greg Bronevetsky, Kathryn Mohror, and Bronis R De Supinski. Design, Modeling, and Evaluation of a Scalable Multi-level Checkpointing System. In SC 2010, pages 1–11. IEEE, 2010.
- [28] A. Namazi, M. Abdollahi, S. Safari, S. Mohammadi, and M. Daneshtalab. Reliability-Aware Task Scheduling using Clustered Replication for Multi-core Real-Time Systems. In NoCArc 2016, pages 45–50. ACM, 2016.
- [29] Bogdan Nicolae and Franck Cappello. AI-Ckpt: Leveraging Memory-access Patterns for Adaptive Asynchronous Incremental Checkpointing. In HPDC 2013, pages 155–166. ACM, 2013.
- [30] Ron A Oldfield et al. Modeling the Impact of Checkpoints on Next-generation Systems. In MSST 2007, pages 30–46. IEEE, 2007.
- [31] Narasimha Raju, Y Liu Gottumukkala, Chokchai B Leangsuksun, Raja Nassar, and Stephen Scott. Reliability Analysis in HPC Clusters. In HAPCW, pages 673–684, 2006.
- [32] Marvin Rausand and Arnljot Hoyland. System Reliability Theory: Models, Statistical Methods and Applications. Wiley-IEEE, 3 edition, November 2003.
- [33] Andrea Rosà, Lydia Y Chen, and Walter Binder. Predicting and Mitigating Jobs Failures in Big Data Clusters. In CCGrid 2015, pages 221–230. IEEE, 2015.
- [34] Andrea Rosà, Lydia Y Chen, and Walter Binder. Failure Analysis and Prediction For Big-data Systems. TSC 2016, 2016.
- [35] Ramendra K Sahoo, Mark S Squillante, A Sivasubramaniam, and Yanyong Zhang. Failure Data Analysis of a Large-scale Heterogeneous Server Environment. In DSN 2004, pages 772–781. IEEE, 2004.
- [36] B Schroeder and Garth Gibson. A Large-scale Study of Failures in High-performance Computing Systems. TDSC 2010, 7(4):337–350, 2010.
- [37] Jim Stevens, Paul Tschirhart, and Bruce Jacob. Fast Full System Memory Checkpointing with SSD-aware Memory Controller. In MemSys 2016, pages 96–98. ACM, 2016.
- [38] Omer Subasi, Gokcen Kestor, and Sriram Krishnamoorthy. Toward a General Theory of Optimal Checkpoint Placement. In CLUSTER 2017, pages 464–474. IEEE, 2017.
- [39] Xiaoyong Tang, Kenli Li, Renfa Li, and Bharadwaj Veeravalli. Reliability-aware Scheduling Strategy for Heterogeneous Distributed Computing Systems. JPDC, 70(9):941–952, 2010.
- [40] D. Tiwari, S. Gupta, and S S Vazhkudai. Lazy Checkpointing: Exploiting Temporal Locality in Failures to Mitigate Checkpointing Overheads on Extreme-scale Systems. In DSN 2014, pages 25–36. IEEE, 2014.
- [41] S. Wang, K. Li, J. Mei, G. Xiao, and K. Li. A Reliability-aware Task Scheduling Algorithm Based on Replication on Heterogeneous Computing Systems. JGC, 15(1):23–39, 2017.
- [42] John W Young. A First-order Approximation to the Optimum Checkpoint Interval. CACM, 17(9):530–531, 1974.
- [43] L. Zhang, K. Li, Y. Xu, J. Mei, F. Zhang, and K. Li. Maximizing Reliability with Energy Conservation for Parallel Task Scheduling in a Heterogeneous Cluster. Information Sciences, 319:113–131, 2015.

---

This version of the text is more structured, clear, and professional, making it easier to read and understand.