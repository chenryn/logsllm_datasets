### Biometric Authentication and Random Input Attacks

#### Introduction
The biometric data presented must be a characteristic of a real person, not a recreation. For example, face masks are relatively static and unmoving compared to a real face [55]. Our attack surface becomes relevant once the front-end has been bypassed. The mitigation measures we propose can be used in conjunction with existing detection mechanisms to thwart random input attacks. These measures are generic and can also be applied to systems without liveness detection or similar defense mechanisms.

#### Attack Surface and Mitigation
- **Feature Vector API**: Once an accepting sample is found via the feature vector API, it may be possible to generate an input that results in this sample after feature extraction. Garcia et al. [23] demonstrated this by training an auto-encoder for both feature extraction and the regeneration of the input image.
- **Authentication as a Binary Classification Problem**: We have focused on authentication as a binary classification problem due to its widespread use in biometric authentication [8-16, 26]. However, authentication can also be framed as a one-class classification problem [56, 26] or as multi-class classification [26], such as in a discrimination model. In one-class classification, only samples from the target user are used to create the template, and the goal is to detect outliers. If this is achieved using distance-based classifiers, the acceptance rate (AR) is expected to be small, as seen in Section V-C and [17]. In multi-class settings, each of the n users is treated as a different class, which is expected to lower the AR. However, whether this behavior is observed in real-world data requires additional experimentation. As noted in Section V-B, AR is highly dependent on the relative variance of the positive and negative user features, which may lead to larger AR for some users, increasing the risk of attack. A thorough investigation of one-class and multi-class settings is left for future work.

#### Related Work
- **Random Input Attacks**: Pagnin et al. [17] defined a blind brute-force attack on biometric systems where the attacker submits random inputs to find an accepting sample. They concluded that the probability of success is exponentially small, assuming authentication is done via a distance function. However, this conclusion does not apply to machine learning-based algorithms, as we have shown that the acceptance region for these classifiers is not exponentially small. It has also been argued that the success rate of random input attacks can be determined by the false positive rate (FPR) in fingerprint and face authentication [18, 19]. We have shown that for sophisticated machine learning classifiers, the success rate of random input attacks can be higher than the FPR. A more involved method is hill-climbing [57, 18], which seeks an accepting sample by exploiting the confidence scores returned by the matching algorithm. The authentication systems considered in our paper do not return confidence scores.
- **Physical Attacks**: Serwadda and Phoha [58] used a robotic finger and population statistics of touch behavior on smartphones to launch a physical attack on touch-based biometric authentication systems. Their attack reduces system accuracy by increasing the equal error rate (EER). In contrast, our work does not assume any knowledge of population biometric statistics.
- **Explainable-AI Techniques**: Garcia et al. [23] used explainable-AI techniques to construct queries (feature vectors) to find an accepting sample in machine learning-based biometric authentication systems. On a system with 0 FPR, their attack was successful with up to a 93% success rate. However, their attack requires constructing a seed dataset and training a substitute neural network. They also reported a random feature vector attack, but it was only successful on one out of 16 victims. The random feature vector was constructed by sampling each feature value via a normal distribution, unlike our uniform distribution. They proposed including images with randomly perturbed pixels as a counter-measure, which is different from our beta-distributed noise mitigation technique.
- **Frog-Boiling Attack**: The frog-boiling attack [59, 60] studies the impact of gradual variations in training data samples to manipulate the classifier decision boundary. We do not consider an adversary with access to the training process or models with an iterative update process. If this threat model is considered, an adversary might seek to maximize the acceptance region by gradually poisoning the training dataset. We have shown that the relative variance between the user’s data and population dataset directly impacts AR. Beta-distributed noise is effective in minimizing AR, but an adversary might poison the training data by labeling beta noise as positive samples, resulting in a maximization of the acceptance region.
- **Evasion Attacks**: Evasion attacks, such as those described in [61], force the classifier to misclassify an input by slightly perturbing it. These attacks rely on confidence values, which are not available in our authentication setting. Similarly, the work in [62] shows how to steal a machine learning model, which is not applicable to our context.
- **Defenses Against Evasion Attacks**: Defenses against evasion attacks aim to make classifiers robust against adversarial inputs. Madry et al. [64] proposed a theoretical framework that includes adversarially perturbed samples in the loss function of deep neural networks (DNNs). Cao and Gong [66] proposed a defense where random points within a hypercube surrounding the input are sampled, and the majority label is assigned to the test input. Randomized smoothing [65] creates a separate classifier whose prediction within a Gaussian noise region around any input is constant. In contrast, our random input need not be close to the target user’s samples or follow the same distribution.
- **Membership Inference Attacks**: Membership inference attacks [24, 25] attempt to determine if a record obtained by an adversary was part of the original training data. These attacks create a shadow model to mimic the behavior of the target model. Salem et al. [25] constructed a shadow model using only positive class samples and negative noise generated via uniformly random feature vectors. We have shown that a large portion of these random inputs may also belong to the positive class.
- **Other Works**: Sugrim et al. [45] surveyed and evaluated performance metrics used in biometric authentication schemes, proposing the Frequency Count Score (FCS) metric. The FCS metric shows a distribution of scores of legitimate and unauthorized users, identifying the overlap between the two distributions to select the appropriate threshold for the classification decision. The work in [67] investigates the accuracy of authentication systems reported on a small number of participants when evaluated over an increasing number of users, suggesting that performance limits should be evaluated iteratively by increasing the participant count until the performance degrades below a tolerable limit.

#### Conclusion
It is crucial to assess the security of biometric authentication systems against random input attacks, similar to the security of passwords against random guess attacks. We have demonstrated that without intentionally including random inputs as part of the training process of the underlying machine learning algorithm, the authentication system is likely to be susceptible to random input attacks at a rate higher than indicated by EER. Absent any other detection mechanism, such as liveness detection, this renders the system vulnerable. The mitigation measures proposed in this paper can be adopted to defend against such attacks.

#### Acknowledgments
This research was funded by the Optus Macquarie University Cybersecurity Hub, Data61 CSIRO, and an Australian Government Research Training Program (RTP) Scholarship. We would like to thank the anonymous reviewers and our shepherd Kevin Butler for their feedback to improve the paper.

#### References
[1] D. Maltoni, D. Maio, A. K. Jain, and S. Prabhakar, Handbook of fingerprint recognition. Springer Science & Business Media, 2009.
[2] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face representation from scratch,” arXiv preprint arXiv:1411.7923, 2014.
[3] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding for face recognition and clustering,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015.
[4] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale speaker identification dataset,” arXiv preprint arXiv:1706.08612, 2017.
[5] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker recognition,” arXiv preprint arXiv:1806.05622, 2018.
[6] U. Mahbub, S. Sarkar, V. M. Patel, and R. Chellappa, “Active user authentication for smartphones: A challenge data set and benchmark results,” in Biometrics Theory, Applications and Systems (BTAS), 2016 IEEE 8th International Conference on. IEEE, 2016, pp. 1–8.
[7] W. Xu, G. Lan, Q. Lin, S. Khalifa, N. Bergmann, M. Hassan, and W. Hu, “Keh-gait: Towards a mobile healthcare user authentication system by kinetic energy harvesting.” in NDSS, 2017.
[8] J. Chauhan, B. Z. H. Zhao, H. J. Asghar, J. Chan, and M. A. Kaafar, “Behaviocog: An observation resistant authentication scheme,” in International Conference on Financial Cryptography and Data Security. Springer, 2017, pp. 39–58.
[9] M. T. Curran, N. Merrill, J. Chuang, and S. Gandhi, “One-step, three-factor authentication in a single earpiece,” in Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium.