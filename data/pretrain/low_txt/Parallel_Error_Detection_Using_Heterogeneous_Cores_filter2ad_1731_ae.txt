### C. Power Overhead

Estimating power consumption is more challenging. Models such as McPAT [48] are unable to accurately account for the low power consumption of small cores, which is approximately 34 μW per MHz at 40 nm [45], compared to 800 μW per MHz for a Cortex A57 at 20 nm [46]. Using twelve small cores without scaling for feature size, we estimate a power overhead of approximately 16% for our system. This indicates that the impact on power consumption is minimal. Since the power consumption for the Rocket core will be lower at 20 nm, this represents an upper bound, and the actual value is expected to be significantly lower.

### D. Bigger Cores

The out-of-order cores we simulate are relatively small, designed to mimic the behavior of conventional ARM systems. However, our technique can be extended to larger main cores, which exhibit only sublinear increases in single-threaded performance. Although more checker cores would be required, the performance scales linearly with the power and area budget allocated to them, thanks to the exploitation of thread-level parallelism. This means that relative overheads diminish significantly. Such cores may also support simultaneous multithreading (SMT), and in these cases, each concurrent thread would be assigned to a different checker core. The overall scheme remains similar, but with enhanced parallelism.

### E. Summary

We have demonstrated that twelve checker cores running at 1 GHz are sufficient to achieve a maximum performance impact of 3.4% across a wide range of benchmarks, with mean error-detection times of 770 ns. We have estimated the area and power overheads of our technique compared to an unchecked core to be around 24% and 16%, respectively. These values are significantly lower than those of existing techniques [3], [10], [12], [13], [49].

### VII. RELATED WORK

#### A. Lock-Stepping

Hardware duplication for error detection, where copies of a program are run through identical logic and the results are compared, is a well-established technique. This method is currently used in the ARM Cortex R series of processors [9], which are designed for high-reliability environments such as automotive and aerospace applications. More recently, triple-lockstep designs, which use majority voting to correct errors, have been developed [3]. Historically, similar techniques were employed by the IBM G5 [10] and the Compaq Himalaya [49].

Mukherjee et al. [12] present a chip-level redundantly multithreaded scheme where the second core trails the first to reduce cache misses. Gupta et al. [50] advocate for finer-grained duplication, using multiple copies of individual pipeline stages rather than duplicating entire cores. This approach provides better tolerance to hard faults when errors are common. Hernandez and Abella [2] propose a scheme to improve detection delay in light-lockstep systems, where hardware can be repurposed if the second core is needed for error detection.

#### B. Redundant Multi-Threading Hardware

Instead of static hardware duplication, some schemes suggest dynamic scheduling on processors with simultaneous multithreading (SMT). AR-SMT [11] presents a redundant multi-threading scheme for fault detection, where two threads are run on the same processor. However, this does not cover hard errors, as the same hardware is used for both computations. Additionally, it consumes a processor context that could be used for more computation and incurs a significant performance overhead. Mukherjee et al. [12] report a performance overhead of 32% for redundant multi-threading techniques. Schuchman and Vijaykumar [13] enhance the ability of redundant multi-threading to detect hard faults by rearranging instructions within the trailing thread, altering the hardware resources used, but at the cost of a further 15% performance degradation.

Reinhardt and Mukherjee [1] introduce the concept of a "sphere of replication" in redundant multi-threading, which refers to the parts of the system that are replicated. They also propose using a load-value queue to forward results from the computation thread to the replication thread, instead of duplicating the page file as in AR-SMT [11]. Smolens et al. [51] suggest removing this queue by noting that, in most cases, two threads will observe the same values from cache loads without explicit duplication, and instead use detection and recovery to correct any differences, albeit with a performance penalty.

Rashid et al. [14] utilize a similar form of parallelism to run error detection on a homogeneous multicore system. Their scheme has a large area cost but reduces energy usage through dynamic frequency-voltage scaling. We build on their insights by using a heterogeneous system to further reduce area and energy, and design an alternative forwarding system to increase parallelism and eliminate the need for a large L1 cache per core.

#### C. Software Schemes

Error detection can also be achieved entirely in software without additional hardware. Khudia and Mahlke [52] detect errors in software for soft applications, where only specific parts of the application are error-intolerant, such as video decoding. The significant overheads are reduced by repeating computation only for error-intolerant portions. Thomas and Pattabiraman [53] identify heuristics to select which parts of applications to check for high error coverage. Wang and Patel [54] provide a scheme for partial fault detection by responding to errors that trigger exceptions. Reis et al. [25] present SWIFT, a solution that duplicates instructions in the same thread to provide limited coverage of soft faults. Jeffery and Figueiredo [55] introduce a virtual lockstepping scheme, where a hypervisor is used to duplicate inputs and perform comparisons of multiple virtualized copies of an operating system. Veeraraghavan et al. [56] use a form of program slicing, similar to our system, to solve a related problem: deterministic recording of execution for multicore workloads in software.

#### D. Hybrid Schemes

Hardware schemes suffer from high silicon area costs, while software schemes lack coverage for hard errors and incur high performance costs. To mitigate these issues, hybrid schemes have been proposed. For example, Reis et al. [24] present CRAFT, which combines SWIFT [25], a software-only scheme, with redundant multi-threading [1], [11], [12]. This uses compiler assistance to duplicate instructions and change redundant stores to perform checks using a special hardware detection structure.

#### E. Heterogeneity

The use of heterogeneous cores for error resilience has precedent. Ansari et al. [57] couple a lightweight core with a newer, faster core. When the fast core begins to fail, it provides hints, such as branches and loads, to the slower, functionally correct core to reduce the performance gap. LaFrieda et al. [58] dynamically couple cores that can differ due to manufacturing defects, so that faster cores are matched together for error detection, as are slower or broken cores. DIVA [21], [22] adds in-order execution units towards the end of an unverified out-of-order pipeline to repeat computation and check data forwarding. These units run at the same clock speed as the rest of the core, achieving parallelism by checking each instruction individually. This requires ECC on all architectural state within the original processor to avoid communication errors, which is impractical in high-performance designs.

#### F. Other Hardware Schemes

Other hardware fault tolerance schemes have been proposed, such as Clover [59], which uses hardware wave detection to detect cosmic rays hitting a system. Various schemes have been proposed to efficiently retire components once hard errors are detected. Aggarwal et al. [60] partition multicore hardware into fault zones, redistributing power dynamically based on how much of the core is still functional. Romanescu and Sorin [61] allow a fraction of the cores in a system to be used for spare parts at the pipeline granularity to fix hard faults. Gupta et al. [62] use a tiled web architecture that allows slow or broken pipeline stages to be weaved out. Powell et al. [63] enable the use of partially broken hardware by detecting and migrating just the operations known to be faulty.

### VIII. CONCLUSION

Current fault detection techniques are limited by high overheads in terms of energy, silicon area, and performance. We have developed a technique to perform error detection for high-performance, out-of-order processors with low area, performance, and energy costs by exploiting new parallelism in the redundant repetition of the program. Our scheme checks multiple parts of the execution simultaneously on a set of small cores embedded beside the main out-of-order CPU.

Evaluating over a wide variety of benchmarks, twelve small checker cores running at 1 GHz provide enough performance to limit the average slowdown to 1.75% (maximum 3.4%). The mean error detection delay for each evaluated benchmark averages 770 ns, with 99.9% of all loads and stores checked within 5000 ns, and all checked within 45 μs. While this delay is larger than with a lock-step system, it is more than offset by the reduction in chip area and power usage, making it justifiable in relevant domain spaces.

Future work will focus on extending the scheme to perform error correction within a microprocessor, rather than just detection, to enable low-overhead complete fault tolerance.

### ACKNOWLEDGEMENTS

This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) through grant references EP/K026399/1 and EP/M506485/1, and Arm Ltd. Additional data related to this publication is available in the data repository at https://doi.org/10.17863/CAM.21857.

### REFERENCES

[1] S. K. Reinhardt and S. S. Mukherjee, “Transient fault detection via simultaneous multithreading,” in ISCA, 2000.
[2] C. Hernandez and J. Abella, “Timely error detection for effective recovery in light-lockstep automotive systems,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 34, no. 11, 2015.
[3] X. Iturbe, B. Venu, E. Ozer, and S. Das, “A triple core lock-step (TCLS) ARM R5 processor for safety-critical and ultra-reliable applications,” in DSN-W, 2016.
[4] M. Rausand, Reliability of Safety-Critical Systems: Theory and Applications. Wiley, 2014.
[5] M. Snir, R. W. Wisniewski, J. A. Abraham et al., “Addressing failures in exascale computing,” Int. J. High Perform. Comput. Appl., vol. 28, no. 2, May 2014.
[6] A. Geist and S. Dosanjh, “IESP exascale challenge: Co-design of architectures and algorithms,” Int. J. High Perform. Comput. Appl., vol. 23, no. 4, Nov. 2009.
[7] D. Zhao, D. Zhang, K. Wang, and I. Raicu, “Exploring reliability of exascale systems through simulations,” in HPC, 2013.
[8] C. Turner, “Safety and security for automotive SoC design - ARM,” http://www.arm.com/files/pdf/20160628 B02 ATF Korea Chris Turner.pdf, 2016.
[9] N. Werdmuller, “Addressing functional safety applications with ARM Cortex-R5,” http://community.arm.com/groups/embedded/blog/2015/01/22/addressing-functional-safety-applications-with-arm-cortex-r5, 2015.
[10] T. J. Slegel, R. M. Averill III, M. A. Check et al., “IBM’s S/390 G5 microprocessor design,” IEEE Micro, vol. 19, no. 2, 1999.
[11] E. Rotenberg, “AR-SMT: A microarchitectural approach to fault tolerance in microprocessors,” in FTCS, 1999.
[12] S. S. Mukherjee, M. Kontz, and S. K. Reinhardt, “Detailed design and evaluation of redundant multithreading alternatives,” in ISCA, 2002.
[13] E. Schuchman and T. N. Vijaykumar, “Blackjack: Hard error detection with redundant threads on SMT,” in DSN, 2007.
[14] M. W. Rashid, E. J. Tan, M. C. Huang, and D. H. Albonesi, “Exploiting coarse-grain verification parallelism for power-efficient fault tolerance,” in PACT, 2005.
[15] A. R. Pargeter, “An example of strong induction,” The Mathematical Gazette, vol. 80, no. 488, 1996.
[16] https://www.sifive.com/products/coreplex-risc-v-ip/e51/.
[17] J. Srinivasan, S. V. Adve, P. Bose, and J. A. Rivers, “The impact of technology scaling on lifetime reliability,” in DSN, 2004.
[18] S. E. Michalak, K. W. Harris, N. W. Hengartner, B. E. Takala, and S. A. Wender, “Predicting the number of fatal soft errors in Los Alamos National Laboratory’s ASC Q supercomputer,” IEEE Transactions on Device and Materials Reliability, 2005.
[19] S. Borkar and A. A. Chien, “The future of microprocessors,” Communications of the ACM, vol. 54, no. 5, 2011.
[20] B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM errors in the wild: A large-scale field study,” in SIGMETRICS, 2009.
[21] T. M. Austin, “DIVA: A reliable substrate for deep submicron microarchitecture design,” in MICRO, 1999.
[22] C. Weaver and T. M. Austin, “A fault-tolerant approach to microprocessor design,” in DSN, 2001.
[23] B. Stolt, Y. Mittlefehldt, S. Dubey, G. Mittal, M. Lee, J. Friedrich, and E. Fluhr, “Design and implementation of the POWER6 microprocessor,” IEEE Journal of Solid-State Circuits, vol. 43, no. 1, 2008.
[24] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, D. I. August, and S. S. Mukherjee, “Design and evaluation of hybrid fault-detection systems,” in ISCA, 2005.
[25] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. I. August, “SWIFT: Software-implemented fault tolerance,” in CGO, 2005.
[26] R. Hameed, W. Qadeer, M. Wachs, O. Azizi, A. Solomatnikov, B. C. Lee, S. Richardson, C. Kozyrakis, and M. Horowitz, “Understanding sources of inefficiency in general-purpose chips,” in ISCA, 2010.
[27] M. Shafique and J. Henkel, “Agent-based distributed power management for kilo-core processors,” in ICCAD, 2013.
[28] “Green 500,” http://www.top500.org/green500/lists/2017/11/, Nov. 2017.
[29] J. Fang, H. Sips, L. Zhang, C. Xu, Y. Che, and A. L. Varbanescu, “Test-driving Intel Xeon Phi,” in ICPE, 2014.
[30] G. Blake, R. G. Dreslinski, T. Mudge, and K. Flautner, “Evolution of thread-level parallelism in desktop applications,” in ISCA, 2010.
[31] K. Mitropoulou, V. Porpodas, and T. M. Jones, “COMET: Communication-optimized multi-threaded error-detection technique,” in CASES, 2016.
[32] C. Wang, H. S. Kim, Y. Wu, and V. Ying, “Compiler-managed software-based redundant multi-threading for transient fault detection,” in CGO, 2007.
[33] International Organization for Standardization, “ISO 26262: Road vehicles – functional safety,” 2011.
[34] N. Werdmuller, “Addressing functional safety applications with ARM Cortex-R5,” https://community.arm.com/iot/embedded/b/embedded-blog/posts/addressing-functional-safety-applications-with-arm-cortex-r5, Jan. 2015.
[35] D. J. Sorin, M. M. K. Martin, M. D. Hill, and D. A. Wood, “SafetyNet: Improving the availability of shared memory multiprocessors with global checkpoint/recovery,” in ISCA, 2002.
[36] A. Jhingran and P. Khedkar, “Analysis of recovery in a database system using a write-ahead log protocol,” in SIGMOD, 1992.
[37] M. Herlihy and J. E. B. Moss, “Transactional memory: Architectural support for lock-free data structures,” in ISCA, 1993.
[38] https://community.arm.com/processors/f/discussions/4503/lock-step-mode-execution-on-cortex-r5/11365#11365.
[39] P. R. Luszczek, D. H. Bailey, J. J. Dongarra, J. Kepner, R. F. Lucas, R. Rabenseifner, and D. Takahashi, “The HPC Challenge (HPCC) benchmark suite,” in SC, 2006.
[40] M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, T. Mudge, and R. B. Brown, “MiBench: A free, commercially representative embedded benchmark suite,” in WWC, 2001.
[41] C. Bienia, “Benchmarking modern multiprocessors,” Ph.D. dissertation, Princeton University, January 2011.
[42] N. Binkert, B. Beckmann, G. Black et al., “The gem5 simulator,” SIGARCH Comput. Archit. News, vol. 39, no. 2, 2011.
[43] A. Gutierrez, J. Pusdesris, R. G. Dreslinski et al., “Sources of error in full-system simulation,” in ISPASS, 2014.
[44] L. Bautista-Gomez, S. Tsuboi, D. Komatitsch, F. Cappello, N. Maruyama, and S. Matsuoka, “FTI: High-performance fault tolerance interface for hybrid systems,” in SC, 2011.
[45] https://riscv.org/wp-content/uploads/2015/02/riscv-rocket-chip-generator-tutorial-hpca2015.pdf.
[46] http://www.anandtech.com/show/8718/the-samsung-galaxy-note-4-exynos-review/6.
[47] M. Yabuuchi, Y. Tsukamoto, M. Morimoto, M. Tanaka, and K. Nii, “20 nm high-density single-port and dual-port SRAMs with wordline-voltage-adjustment system for read/write assists,” in ISSCC, 2014.
[48] S. Li, J. H. Ahn, R. D. Strong, J. B. Brockman, D. M. Tullsen, and N. P. Jouppi, “McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures,” in MICRO, 2009.
[49] A. Wood, “Data integrity concepts, features, and technology,” Tandem Division, Compaq Computer Corporation, White Paper, 1999.
[50] S. Gupta, S. Feng, A. Ansari, J. Blome, and S. Mahlke, “The StageNet fabric for constructing resilient multicore systems,” in MICRO, 2008.
[51] J. C. Smolens, B. T. Gold, B. Falsafi, and J. C. Hoe, “Reunion: Complexity-effective multicore redundancy,” in MICRO, 2006.
[52] D. S. Khudia and S. Mahlke, “Harnessing soft computations for low-budget fault tolerance,” in MICRO, 2014.
[53] A. Thomas and K. Pattabiraman, “Error detector placement for soft computation,” in DSN, June 2013.
[54] N. J. Wang and S. J. Patel, “RESTORE: Symptom-based soft error detection in microprocessors,” in DSN, 2005.
[55] C. M. Jeffery and R. J. O. Figueiredo, “A flexible approach to improving system reliability with virtual lockstep,” IEEE Transactions on Dependable and Secure Computing, vol. 9, no. 1, 2012.
[56] K. Veeraraghavan, D. Lee, B. Wester et al., “DoublePlay: Parallelizing sequential logging and replay,” in ASPLOS, 2011.
[57] A. Ansari, S. Feng, S. Gupta, and S. Mahlke, “Necromancer: Enhancing system throughput by animating dead cores,” in ISCA, 2010.
[58] C. LaFrieda, E. Ipek, J. F. Martinez, and R. Manohar, “Utilizing dynamically coupled cores to form a resilient chip multiprocessor,” in DSN, 2007.
[59] Q. Liu, C. Jung, D. Lee, and D. Tiwari, “Clover: Compiler-directed lightweight soft error resilience,” in LCTES, 2015.
[60] N. Aggarwal, P. Ranganathan, N. P. Jouppi, and J. E. Smith, “Configurable isolation: Building high-availability systems with commodity multi-core processors,” in ISCA, 2007.
[61] B. F. Romanescu and D. J. Sorin, “Core cannibalization architecture: Improving lifetime chip performance for multicore processors in the presence of hard faults,” in PACT, 2008.
[62] S. Gupta, A. Ansari, S. Feng, and S. Mahlke, “StageWeb: Interweaving pipeline stages into a wearout and variation-tolerant CMP fabric,” in DSN, 2010.
[63] M. D. Powell, A. Biswas, S. Gupta, and S. S. Mukherjee, “Architectural core salvaging in a multi-core processor for hard-error tolerance,” in ISCA, 2009.