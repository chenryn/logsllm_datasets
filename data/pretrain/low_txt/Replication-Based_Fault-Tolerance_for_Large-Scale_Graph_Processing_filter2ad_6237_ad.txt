### 3.55X) for Rebirth and Migration Approaches

For the Rebirth and Migration approaches, Imitator can recover 0.95 million and 1.43 million vertices (including replicas) from a single failed node in just 2.32 and 3.4 seconds, respectively, for the LJournal and Wiki datasets.

### Performance Comparison of Rebirth and Migration

For large graphs such as LJournal and Wiki, the Migration approach generally outperforms Rebirth. This is because Migration avoids the need to move data (e.g., vertex and edge values) during the reloading phase and distributes replay operations across all surviving nodes rather than concentrating them on a single new node. Conversely, for smaller graphs like SYN-GL and DBLP, Rebirth performs better. The multiple rounds of message exchanges required by Migration can slow down recovery by 28% to 63% compared to Rebirth.

### 6.6 Scalability of Recovery

We evaluated the recovery scalability of Imitator for PageRank using the Wiki dataset with varying numbers of nodes participating in the recovery process. As shown in Figure 9, both recovery modes scale well with an increase in the number of recovery machines, as all machines can share the workload during the reloading phase. Since the local graph is constructed during the reloading phase, there is no explicit reconstruction phase for the Rebirth mode. Additionally, replay operations are executed only on the new node in Rebirth, while they are distributed across all surviving nodes in Migration.

### 6.7 Impact of Graph Partitioning

To analyze the impact of different graph partitioning algorithms, we implemented Fennel [29] on Imitator, which is a heuristic graph partitioning algorithm. As shown in Figure 10(a), Fennel significantly reduces the replication factor for all datasets compared to the default Hash-based partitioning. The replication factors for GWeb, LJournal, and Wiki are 1.61, 3.84, and 5.09, respectively.

Figure 10(b) illustrates the overhead of Imitator under Fennel partitioning. Despite the lower replication factor, Imitator requires more additional replicas for fault tolerance, leading to an increase in message overhead. However, the runtime overhead remains small, ranging from 1.8% to 4.7%.

### 6.8 Handling Multiple Failures

When Imitator is configured to handle multiple node failures, the overhead increases due to the need for more extra replicas. Figure 11 shows the overall overhead when Imitator is configured to tolerate 1, 2, and 3 node failures. As shown in Figure 11(a), even when configured to tolerate 3 node failures simultaneously, the overhead of Imitator is less than 10%.

Figure 11(b) presents the recovery time for the largest dataset, Wiki, with different numbers of crashed nodes. In Rebirth mode, the time to send and receive recovery messages increases as the number of crashed nodes grows, but the time to rebuild graph states and replay pending operations remains similar to that of a single node failure. In contrast, the Migration strategy leverages the cluster's resources for recovery, resulting in a relatively small time for each operation.

### 6.9 Memory Consumption

Imitator adds extra replicas to tolerate faults, so we also measured the memory overhead using jstat, a memory tool in JDK. Table 3 shows the memory behavior of the baseline system and Imitator for the largest dataset, Wiki. When Imitator is configured to tolerate one node failure during computation, the memory overhead is modest, and the memory usage of the baseline system and Imitator is comparable.

### 6.10 Case Study

Figure 12 presents a case study of running PageRank using the LJournal dataset with either no or one machine failure during 20 iterations. Different recovery strategies were applied to illustrate their performance. The symbols BASE, REP, and CKPT/4 denote the execution of the baseline, replication, and checkpoint-based fault tolerance systems without failure, respectively, while others show cases with a failure between the 6th and 7th iterations. The interval for checkpointing is 4 iterations.

The failure detection scheme is the same for all strategies, with a time span of about 7 seconds. For recovery speed, the Migration strategy, with a recovery time of about 2.6 seconds, is the fastest due to its efficient use of all resources and minimal data movement. The Rebirth strategy takes about 8.8 seconds, which is still faster than the 45-second recovery time of CKPT/4, which uses incremental checkpointing every four iterations.

After recovery, REP with Rebirth can still execute at full speed because the execution environment before and after the failure is the same. In contrast, REP with Migration is slightly slower due to the reduced available computing resources. CKPT/4 must replay 2 lost iterations after a long recovery time.

### 7 Related Work

Checkpoint-based fault tolerance is widely used in graph-parallel computation systems. Pregel [5] and its open-source clones [8], [9] use synchronous checkpointing to save the graph state, including vertex and edge values, and incoming messages. GraphLab [11] employs an asynchronous alternative based on the Chandy-Lamport [13] snapshot. Trinity [10] and PowerGraph [12] provide both synchronous and asynchronous checkpointing.

Piccolo [30] is a data-centric distributed computation system that offers a user-assisted checkpoint mechanism to reduce runtime overhead. However, users must save additional information for recovery. MapReduce [22] and other data-parallel models [31] use simple re-execution to recover tasks on crashed machines, assuming all tasks are deterministic and independent. Spark [23] and Discretized Streams [32] propose Resilient Distributed Datasets (RDD) for coarse-grained operations on datasets, logging transformations rather than actual data. RDD is not easily applicable to graph-parallel models due to fine-grained updates.

Replication is commonly used in large-scale distributed file systems [24], [20] and streaming systems [33], [34] to provide high availability and fault tolerance. All replicas are full-time for fault tolerance, which can introduce high performance costs. RAMCloud [15] achieves fast recovery by scattering backup data across the entire cluster and leveraging all cluster resources. Distributed storage provides simple abstractions (e.g., key-value) and does not consider data dependencies and computations.

SPAR [14] is a graph-structured middleware for social data in key-value stores, mentioning the storage of more ghost vertices for fault tolerance. However, it does not consider vertex interactions and only provides background synchronization and eventual consistency, which is not suitable for graph-parallel systems.

### 8 Conclusion

This paper introduces Imitator, a replication-based approach for low-overhead fault tolerance and fast crash recovery. Imitator leverages and extends existing replication mechanisms with additional mirrors and complete states, allowing vertices in a failed machine to be reconstructed using states from their mirrors. Evaluation shows that Imitator incurs very small normal execution overhead and provides fast crash recovery from failures.

### 9 Acknowledgment

We thank the anonymous reviewers for their insightful comments. This work is supported in part by the Doctoral Fund of the Ministry of Education of China (Grant No. 20130073120040), the Program for New Century Excellent Talents in University of the Ministry of Education of China, Shanghai Science and Technology Development Funds (No. 12QA1401700), a foundation for the Author of National Excellent Doctoral Dissertation of PR China, China National Natural Science Foundation (No. 61303011), and Singapore NRF (CREATE E2S2).

### References

[1] S. Brin and L. Page, “The anatomy of a large-scale hypertextual web search engine,” in WWW, 1998, pp. 107–117.
[2] J. Ye, J. Chow, J. Chen, and Z. Zheng, “Stochastic gradient boosted distributed decision trees,” in ACM CIKM, 2009, pp. 2061–2064.
[3] J. E. Gonzalez, Y. Low, C. Guestrin, and D. O’Hallaron, “Distributed parallel inference on large factor graphs,” in Proc. Conference on Uncertainty in Artificial Intelligence, 2009, pp. 203–212.
[4] A. Smola and S. Narayanamurthy, “An architecture for parallel topic models,” Proceedings of the VLDB Endowment, vol. 3, no. 1-2, pp. 703–710, 2010.
[5] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski, “Pregel: a system for large-scale graph processing,” in SIGMOD, 2010.
[6] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng, “Building high-level features using large scale unsupervised learning,” in Proc. ICML, 2012.
[7] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng, “Large scale distributed deep networks,” in Proc. NIPS, 2012, pp. 1232–1240.
[8] “Apache Giraph,” http://giraph.apache.org/.
[9] “Apache Hama,” http://hama.apache.org/.
[10] B. Shao, H. Wang, and Y. Li, “Trinity: A distributed graph engine on a memory cloud,” in Proc.SIGMOD, 2013.
[11] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and J. M. Hellerstein, “Distributed GraphLab: a framework for machine learning and data mining in the cloud,” VLDB Endow., vol. 5, no. 8, pp. 716–727, 2012.
[12] J. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin, “PowerGraph: Distributed graph-parallel computation on natural graphs,” in OSDI, 2012.
[13] K. Chandy and L. Lamport, “Distributed snapshots: determining global states of distributed systems,” ACM TOCS, vol. 3, no. 1, pp. 63–75, 1985.
[14] J. Pujol, V. Erramilli, G. Siganos, X. Yang, N. Laoutaris, P. Chhabra, and P. Rodriguez, “The little engine (s) that could: scaling online social networks,” in ACM SIGCOMM, 2010, pp. 375–386.
[15] D. Ongaro, S. M. Rumble, R. Stutsman, J. Ousterhout, and M. Rosenblum, “Fast crash recovery in RAMCloud,” in Proc. SOSP, 2011, pp. 29–41.
[16] G. Wang, W. Xie, A. J. Demers, and J. Gehrke, “Asynchronous large-scale graph processing made easy.” in CIDR, 2013.
[17] Y. Tian, A. Balmin, S. A. Corsten, S. Tatikonda, and J. McPherson, “From ‘think like a vertex’ to ‘think like a graph’,” in Proc. VLDB, 2013.
[18] J. W. Young, “A first order approximation to the optimum checkpoint interval,” Comm. of the ACM, vol. 17, no. 9, pp. 530–531, 1974.
[19] H. Haselgrove, “Wikipedia page-to-page link database,” http://haselgrove.id.au/wikipedia.htm, 2010.
[20] “HDFS System,” http://hadoop.apache.org/common/docs/current/hdfs design.html.
[21] S. N. A. Project, “Stanford large network dataset collection,” http://snap.stanford.edu/data/.
[22] J. Dean and S. Ghemawat, “MapReduce: simplified data processing on large clusters,” Commun. of the ACM, vol. 51, no. 1, pp. 107–113, 2008.
[23] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. Franklin, S. Shenker, and I. Stoica, “Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing,” in Proc. NSDI, 2012.
[24] S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google File System,” in Proc. SOSP, 2003, pp. 29–43.
[25] M. Castro and B. Liskov, “Practical Byzantine fault tolerance,” in Proc. OSDI, 1999.
[26] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong, “Zyzzyva: speculative Byzantine fault tolerance,” in Proc. SOSP, 2007.
[27] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed, “Zookeeper: wait-free coordination for internet-scale systems,” in Proc. Usenix ATC, 2010.
[28] C. Wilson, B. Boe, A. Sala, K. P. Puttaswamy, and B. Y. Zhao, “User interactions in social networks and their implications,” in EuroSys, 2009, pp. 205–218.
[29] C. E. Tsourakakis, C. Gkantsidis, B. Radunovic, and M. Vojnovic, “Fennel: Streaming graph partitioning for massive scale graphs,” Microsoft, Tech. Rep. 175918, 2012.
[30] R. Power and J. Li, “Piccolo: building fast, distributed programs with partitioned tables,” in OSDI, 2010, pp. 1–14.
[31] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, “Dryad: distributed data-parallel programs from sequential building blocks,” in EuroSys, 2007, pp. 59–72.
[32] M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker, and I. Stoica, “Discretized streams: Fault-tolerant streaming computation at scale,” in Proc. SOSP, 2013.
[33] M. Balazinska, H. Balakrishnan, S. R. Madden, and M. Stonebraker, “Fault-tolerance in the Borealis distributed stream processing system,” ACM Transactions on Database Systems (TODS), vol. 33, no. 1, p. 3, 2008.
[34] M. A. Shah, J. M. Hellerstein, and E. Brewer, “Highly available, fault-tolerant, parallel dataflows,” in Proceedings of the 2004 ACM SIGMOD international conference on Management of data. ACM, 2004, pp. 827–838.