### Model Parallel Training and Hardware Innovations

Model parallel training can be enhanced through intelligent task device placement [121, 122] and more efficient pipelining strategies [4, 123]. Additionally, there is a significant body of work on new electrical hardware designs to accelerate machine learning computations [118, 124–129]. The research presented here is complementary to these techniques, as they can still be applied to further improve both data and model parallel training. Our work is distinct in that it investigates the system requirements for using Silicon Photonics (SiP) as a new underlying technology to interconnect hundreds of GPUs in an all-optical architecture.

### Datacenter Interconnects

The overarching vision of this paper is to leverage all-optical interconnects for future distributed machine learning (ML) systems. Optical interconnects have a long and rich history in the datacenter research community [24–26, 55, 66, 70, 71, 130–135]. Previous studies have shown the benefits of reconfigurable topologies in datacenter networks by adding optical links to the electrical topology [24, 66, 71, 133, 136] or by creating all-optical datacenter interconnects [26, 55, 70, 131, 132]. The unpredictability of legacy datacenter workloads and the complexity of managing hybrid topologies are two main reasons for the limited adoption of all-optical datacenters so far. In contrast, this paper proposes an all-optical interconnect with a simple and practical task placement algorithm primarily designed to accelerate ML workloads. Our ring topology (SiP-Ring) is inspired by Quartz [70], Mordia [71], and Megaswitch [26], which all use a fiber ring to interconnect the datacenter topology but do not leverage microring resonators (MRRs). Furthermore, Mordia achieves a microsecond switching circuit switch but does not reuse wavelengths, significantly reducing its bandwidth efficiency compared to SiP-Ring. Consequently, Mordia's number of ports is limited by the number of wavelengths.

Jellyfish [137], Rotornet [66], and Opera [69] exploit the unpredictability of datacenter workloads and use expander-based topologies to improve the completion time of short and long flows. However, random permutations are not ideal for ML workloads, as training involves periodic repetition of thousands of iterations. Shoal [135], Larry [138], XFabric [139], and Sirius [55] have proposed reconfigurable datacenter interconnects with nanosecond switching fabric. While these proposals have the potential to revolutionize datacenter environments, they are not yet commercially available and do not support terabits-per-second (Tbps) bandwidth between communicating nodes. Our results show that microsecond reconfiguration latency is close to optimal for ML; a control plane with nanosecond response time might be necessary for general-purpose datacenter traffic but is excessive for distributed ML training. Finally, there is a substantial body of research on silicon photonics [17, 140–142], embedding silicon photonics switches in High Performance Computing clusters [143], and energy-efficient datacenters [144]. By focusing on ML, our work takes an application-level perspective to build an interconnect with SiP components.

### Conclusion

In this paper, we propose optical network interconnects for distributed ML training clusters capable of providing multiple terabits-per-second of bandwidth per GPU. Our results show that the predictability of ML workloads makes them well-suited for optical interconnects. We develop a new task partitioning and placement algorithm that exploits the degree requirement of optical networks to find a parallelization strategy suitable for a given network topology. This approach can mitigate and largely overcome concerns such as limited communication degree and reconfigurability of optical circuit-switched networks. Simulations using three real deep neural network (DNN) models demonstrate that, compared to today’s electrical network fabrics with limited server-to-server bandwidth, SiP-ML improves training time by 1.3–9.1× at scale.

### Acknowledgments

We would like to thank our shepherd Hitesh Ballani and anonymous reviewers for their feedback. This work was partly supported by AEPA-E ENLITENED PINE, DARPA FastNICs, DARPA PIPES, a Cisco Research Center Award, NSF ASCENT-2023468, NSF CNS-2008624, NSF CNS-1751009, NSF CNS-2006827, NSF CNS-1563826, as well as by a SystemsThatLearn@CSAIL Ignite Grant and a MachineLearningApplications@CSAIL Award.

### References

[1] AI and Compute. https://openai.com/blog/ai-and-compute/.

[2] Minsik Cho, Ulrich Finkler, David Kung, and Hillery Hunter. BlueConnect: Decomposing All-Reduce for Deep Learning on Heterogeneous Network Hierarchy. In SysML Conference, 2019.

[3] Siddharth Das. CNN Architectures, 2017.

[4] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. PipeDream: Generalized Pipeline Parallelism for DNN Training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP '19, page 1-15, New York, NY, USA, 2019. Association for Computing Machinery.

[5] NVIDIA DGX A100. https://www.nvidia.com/en-us/data-center/dgx-a100/.

[6] NVIDIA Selene Cluster. https://blogs.nvidia.com/blog/2020/12/18/nvidia-selene-busy/.

[7] S. S. Vazhkudai, B. R. de Supinski, A. S. Bland, A. Geist, J. Sexton, J. Kahle, C. J. Zimmer, S. Atchley, S. H. Oral, D. E. Maxwell, V. G. Vergara Larrea, A. Bertsch, R. Goldstone, W. Joubert, C. Chambreau, D. Appelhans, R. Blackmore, B. Casses, G. Chochia, G. Davison, M. A. Ezell, E. Gonsiorowski, L. Grinberg, B. Hanson, B. Hartner, I. Karlin, M. L. Leininger, D. Leverman, C. Marroquin, A. Moody, M. Ohmacht, R. Panjajakshan, F. Pizzano, J. H. Rogers, B. Rosenburg, D. Schmidt, M. Shankar, F. Wang, P. Watson, B. Walkup, L. D. Weems, and J. Yin. The Design, Deployment, and Evaluation of the CORAL Pre-Exascale Systems. July 2018.

[8] Valerie Coffey. DARPA PIPES Program Demonstrates 2 Tbit/s Optical Interconnects at the Chip Level, July 2020. https://www.laserfocusworld.com/fiber-optics/article/14176186/darpa-pipes-program-demonstrates-2-tbits-optical-interconnects-at-the-chip-level.

[9] Mark Wade. Optical I/O Chiplets Eliminate Bottlenecks to Unleash Innovation, 2020. https://ayarlabs.com/ayar-labs-solving-critical-computing-challenges-through-optical-i-o/.

[10] Yutaka Urino, Takahiro Nakamura, and Yasuhiko Arakawa. Silicon Optical Interposers for High-Density Optical Interconnects, pages 1-39. Springer Berlin Heidelberg, Berlin, Heidelberg, 2016.

[11] D. Kim, K. Y. Au, H. Y. L. X. Luo, Y. L. Ye, S. Bhattacharya, and G. Q. Lo. 2.5D Silicon Optical Interposer for 400 Gbps Electronic-Photonic Integrated Circuit Platform Packaging. In 2017 IEEE 19th Electronics Packaging Technology Conference (EPTC), pages 1-4, Dec 2017.

[12] E. R. H. Fuchs, R. E. Kirchain, and S. Liu. The Future of Silicon Photonics: Not So Fast? Insights from 100G Ethernet LAN Transceivers. Journal of Lightwave Technology, 29(15):2319-2326, Aug 2011.

[13] David Thomson, Aaron Zilkie, John E. Bowers, Tin Komljenovic, Graham T. Reed, Laurent Vivien, Delphine Marris-Morini, Eric Cassan, Leopold Virot, Jean-Marc Fedeli, Jean-Michel Hartmann, Jens H. Schmid, Dan-Xia Xu, Frederic Boeuf, Peter O'Brien, Goran Z. Mashanovich, and M. Nedeljkovic. Roadmap on Silicon Photonics. Journal of Optics, 18(7):073003, 2016.

[14] M. Wade, M. Davenport, M. De Cea Falco, P. Bhargava, J. Fini, D. Van Orden, R. Meade, E. Yeung, R. Ram, M. Popovic, V. Stojanovic, and C. Sun. A Bandwidth-Dense, Low Power Electronic-Photonic Platform and Architecture for Multi-Tbps Optical I/O. Pages 1-3, Sep. 2018.

[15] N. Ophir, C. Mineo, D. Mountain, and K. Bergman. Silicon Photonic Microring Links for High-Bandwidth-Density, Low-Power Chip I/O. IEEE Micro, 33(1):54-67, Jan 2013.

[16] G. T. Reed and A. P. Knights. Silicon Photonics: An Introduction. Wiley, 2004.

[17] Qixiang Cheng, Meisam Bahadori, Madeleine Glick, Sebastien Rumley, and Keren Bergman. Recent Advances in Optical Technologies for Data Centers: A Review. Optica, 5(11):1354-1370, Nov 2018.

[18] Madeleine Glick, Lionel C. Kimmerling, and Robert C. Pfahl. A Roadmap for Integrated Photonics. Opt. Photon. News, 29(3):36-41, Mar 2018.

[19] Amir H. Atabaki, Sajjad Moazeni, Fabio Pavanello, Hayk Gevorgyan, Jelena Notaros, Luca Alloatti, Mark T. Wade, Chen Sun, Seth A. Kruger, Huaiyu Meng, Kenaish Al Qubaisi, Imbert Wang, Bohan Zhang, Anatol Khilo, Christopher V. Baiocco, Milovs A. Popovic, Vladimir M. Stojanovic, and Rajeev J. Ram. Integrating Photonics with Silicon Nanoelectronics for the Next Generation of Systems on a Chip. Nature, 556(7701):349-354, 2018.

[20] Mark Wade, Erik Anderson, Shahab Ardalan, Pavan Bhargava, Sidney Buchbinder, Michael Davenport, John Fini, Anatoly Khilo, Chandru Ramamurthy Roy Meade, Michael Rust, Vladimir Stojanovic, Forrest Sedgwick, Derek Van Orden, Chong Zhang, Edward Wang, Chen Sun, Sergey Shumarayev, Conor O'Keeffe, Tim T. Hoang, David Kehlet, Ravi V. Mahajan, Allen Chan, and Tina Tran. TeraPHY: A Chiplet Technology for Low-Power, High-Bandwidth Optical I/O. HotChips, pages i-xlviii, August 2019. https://www.hotchips.org/hc31/HC312.9AyarLabs20190820HCFINAL.pdf.

[21] Valentina Donzella, Ahmed Sherwali, Jonas Flueckiger, Samantha M. Grist, Sahba Talebi Fard, and Lukas Chrostowski. Design and Fabrication of SOI Microring Resonators Based on Sub-Wavelength Grating Waveguides. Opt. Express, 23(4):4791-4803, Feb 2015.

[22] W. Bogaerts, P. De Heyn, T. Van Vaerenbergh, K. De Vos, S. Kumar Selvaraja, T. Claes, P. Dumon, P. Bienstman, D. Van Thourhout, and R. Baets. Silicon Microring Resonators. Laser & Photonics Reviews, 6(1):47-73, 2012. https://onlinelibrary.wiley.com/doi/abs/10.1002/lpor.201100017.

[23] Q. Cheng, M. Bahadori, Y. Hung, Y. Huang, N. Abrams, and K. Bergman. Scalable Microring-Based Silicon Clos Switch Fabric with Switch-and-Select Stages. IEEE Journal of Selected Topics in Quantum Electronics, 25(5):1-11, Sep. 2019.

[24] Nathan Farrington, George Porter, Sivasankar Radhakrishnan, Hamid Hajabdolali Bazzaz, Vikram Subramanya, Yeshaiahu Fainman, George Papen, and Amin Vahdat. Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers. SIGCOMM'10, pages 339-350.

[25] Guohui Wang, David G. Andersen, Michael Kaminsky, Konstantina Papagiannaki, T.S. Eugene Ng, Michael Kozuch, and Michael Ryan. c-Through: Part-Time Optics in Data Centers. SIGCOMM'10, pages 327-338.

[26] Li Chen, Kai Chen, Zhonghua Zhu, Minlan Yu, George Porter, Chunming Qiao, and Shan Zhong. Enabling Wide-Spread Communications on Optical Fabric with MegaSwitch. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), pages 577-593, Boston, MA, 2017. USENIX Association.

[27] Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang Yu, and Eric Xing. Lighter-Communication Distributed Machine Learning via Sufficient Factor Broadcasting. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pages 795-804, Arlington, Virginia, USA, 2016. AUAI Press.

[28] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling Distributed Machine Learning with the Parameter Server. OSDI'14, pages 583-598. USENIX Association, 2014.

[29] Rajeev Thakur, Rolf Rabenseifner, and William Gropp. Optimization of Collective Communication Operations in MPICH. Int. J. High Perform. Comput. Appl., 19(1):49-66, February 2005.

[30] Baidu, 2017. https://github.com/baidu-research/baidu-allreduce.

[31] Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, Tiegang Chen, Guangxiao Hu, Shaohuai Shi, and Xiaowen Chu. Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes. CoRR, abs/1807.11205, 2018.

[32] J. R. Quinlan. Induction of Decision Trees. Mach. Learn., 1(1):81-106, March 1986.

[33] Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A. Gibson, and Eric P. Xing. On Model Parallelization and Scheduling Strategies for Distributed Machine Learning. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2834-2842. Curran Associates, Inc., 2014.

[34] Zhihao Jia, Sina Lin, Charles R. Qi, and Alex Aiken. Exploring Hidden Dimensions in Accelerating Convolutional Neural Networks. Volume 80 of Proceedings of Machine Learning Research, pages 2274-2283, Stockholmsmässan, Stockholm Sweden, 10-15 Jul 2018. PMLR.

[35] Tal Ben-Nun and Torsten Hoefler. Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis. CoRR, abs/1802.09941, 2018.

[36] L. Song, F. Chen, Y. Zhuo, X. Qian, H. Li, and Y. Chen. AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 342-355, 2020.

[37] Nikoli Dryden, Naoya Maruyama, Tim Moon, Tom Benson, Marc Snir, and Brian Van Essen. Channel and Filter Parallelism for Large-Scale CNN Training. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC'19, New York, NY, USA, 2019. Association for Computing Machinery.

[38] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond Data and Model Parallelism for Deep Neural Networks. SysML, 2019.

[39] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and Andrew Y. Ng. Large Scale Distributed Deep Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1223-1231. Curran Associates, Inc., 2012.

[40] Amir Gholami, Ariful Azad, Kurt Keutzer, and Aydin Buluç. Integrated Model and Data Parallelism in Training Neural Networks. CoRR, abs/1712.04432, 2017.

[41] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta, Hongzi Mao, and Mohammad Alizadeh. Learning Generalizable Device Placement Algorithms for Distributed Machine Learning. In Advances in Neural Information Processing Systems 32, pages 3983-3993. Curran Associates, Inc., 2019.

[42] Shar Narasimhan. NVIDIA Clocks World’s Fastest BERT Training Time and Largest Transformer-Based Model, Paving Path For Advanced Conversational AI, Aug. 2019. https://devblogs.nvidia.com/training-bert-with-gpus/.

[43] Nikoli Dryden, Naoya Maruyama, Tom Benson, Tim Moon, Marc Snir, and Brian Van Essen. Improving Strong-Scaling of CNN Training by Exploiting Finer-Grained Parallelism, 2019.

[44] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, pages 1729-1739, Red Hook, NY, USA, 2017. Curran Associates Inc.

[45] Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. CoRR, abs/1706.02677, 2017.

[46] Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. Measuring the Effects of Data Parallelism on Neural Network Training. Journal of Machine Learning Research, 20(112):1-49, 2019.

[47] Yosuke Oyama, Naoya Maruyama, Nikoli Dryden, Erin McCarthy, Peter Harrington, Jan Balewski, Satoshi Matsuoka, Peter Nugent, and Brian Van Essen. The Case for Strong Scaling in Deep Learning: Training Large 3D CNNs with Hybrid Parallelism. IEEE Transactions on Parallel and Distributed Systems, 2020.

[48] MLPerf v0.6: NVIDIA Implementation of Attention Mechanisms for Translation, Aug. 2019. https://github.com/mlperf/trainingresultsv0.6/tree/master/NVIDIA/benchmarks/transformer/implementations/pytorch.

[49] ResNet v1.5 for TensorFlow, 2020.

[50] NVIDIA Data Center Deep Learning Product Performance. https://developer.nvidia.com/deep-learning-performance-training-inference.

[51] Nvidia DGX-2. https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-2/dgx-2-print-datasheet-738070-nvidia-a4-web-uk.pdf.

[52] MegatronLM: Training Billion+ Parameter Language Models Using GPU Model Parallelism, Jul. 2019. https://nv-adlr.github.io/MegatronLM.

[53] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory Optimizations Toward Training Trillion Parameter Models, 2019. https://www.deepspeed.ai/.

[54] Saeed Rashidi, Srinivas Sridharan, Sudarshan Srinivasan, Matthew Denton, and Tushar Krishna. Efficient Communication Acceleration for Next-Gen Scale-Up Deep Learning Training Platforms, 2020.

[55] Hitesh Ballani, Paolo Costa, Raphael Behrendt, Daniel Cletheroe, Istvan Haller, Krzysztof Jozwik, Fotini Karinou, Sophie Lange, Kai Shi, Benn Thomsen, and Hugh Williams. Sirius: A Flat Datacenter Network with Nanosecond Optical Switching. SIGCOMM'20, Aug. 2020.

[56] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam, and D. Burger. Dark Silicon and the End of Multicore Scaling. In 2011 38th Annual International Symposium on Computer Architecture (ISCA), pages 365-376, June 2011.

[57] R. Colwell. The Chip Design Game at the End of Moore's Law. In 2013 IEEE Hot Chips 25 Symposium (HCS), pages 1-16, Aug 2013.

[58] H. J. S. Dorren, E. H. M. Wittebol, R. de Kluijver, G. Guelbenzu de Villota, P. Duan, and O. Raz. Challenges for Optically Enabled High-Radix Switches for Data Center Networks. Journal of Lightwave Technology, 33(5):1117-1125, March 2015.

[59] Alexis Björlin and Manish Mehta. Broadcom Discusses Its Co-Packaged Optics Plans. http://www.gazettabyte.com/home/2021/4/27/broadcom-discusses-its-co-packaged-optics-plans.html, 2021. [Online; last accessed 25-June-2021].

[60] Steven Leibson. Ayar Labs and Intel Demo FPGA with Optical Transceivers in DARPA PIPES Project: 2 Tbps Now, >100 Tbps is the Goal, Mar. 2020. https://blogs.intel.com/psg/ayar-labs-and-intel-demo-fpga-with-optical-transceivers-in-darpa-pipes-project-2-tbps-now-100-tbps-is-the-goal/.