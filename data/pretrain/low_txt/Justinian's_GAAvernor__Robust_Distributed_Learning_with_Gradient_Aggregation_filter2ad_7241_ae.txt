### Assumption 2 if Assumption 1 is Valid
If Assumption 1 holds, then Assumption 2 can be validated. For example, the parameter server can allocate specific computational resources to simulate a worker node on its own devices. Given the validity of Assumption 1, we can assert that the simulated worker is always benign, thus satisfying the second assumption.

### On Assumptions 3 & 4
These two assumptions define the range of learning tasks that GAA can assist with. Assumption 3 is a common assumption in many known defenses [4, 11, 15, 16, 20, 31, 62]. If workers share the same training set, as in many conventional distributed learning systems (e.g., MNIST and CIFAR-10) [3, 34, 41, 43, 50, 64], both Assumptions 3 and 4 are naturally satisfied due to the availability of a validation set from the same data source. For newer distributed learning systems, such as federated learning [34], where workers have their local datasets (e.g., Yelp and Healthcare), we demonstrate through experimental results in Figure 9 that the requirement for the quasi-validation (QV) set is relatively easy to meet with a small number of samples from similar data domains. In Figure 9(b), we observe that the final accuracy on Yelp under randomized attacks is close to the bottleneck accuracy, regardless of whether the QV set size is 1 or 1,000, though there is a slightly larger variance and lower convergence rate when the QV set is smaller.

Moreover, Section 6.3 shows that a small QV set is unlikely to be exploited as a weak point, even if it has missing classes or a similar distribution to the manipulated workers' local datasets. However, we acknowledge that the QV set could be a vulnerability if fully known by the adversary, which is rare in practice due to the randomness in preparing the QV set and the security of the server.

To validate the requirement on the QV set in Assumption 4, we numerically estimate the average KL divergence among the local datasets and the full QV set on the Healthcare dataset. The empirical value is approximately 0.1. By substituting this and other empirical values into the equations in Section 4.4, we find that the predicted convergence rate aligns well with the empirical learning curves. More details are provided in Appendices A.2 and A.4. However, GAA may have limitations in guaranteeing Assumption 4 when the server has no knowledge about the data domain of the distributed learning process or when privacy requirements are stringent [61], which is an interesting area for future work.

### On Threat Model
Does the real-world distributed learning environment exhibit such malice that the Byzantine ratio has no explicit upper bound or fluctuates? This may not be the case for current distributed learning systems in stable local network environments [52]. Real-world examples include distributed systems in unstable network environments with low-specification machines, where a majority of nodes might send faulty gradients unpredictably due to network or computation errors. In such scenarios, GAA can help the underlying learning process converge to a near-optimal solution. Other use cases include federated learning systems [34, 61], where end users collaborate to build a global learning model. From our perspective, the threat model should assume the worst-case scenario, as the reliability of end users can be difficult to ensure, similar to DDoS attacks [45].

### Limitations and Future Directions
In one test of GAA, we observed fluctuating results on MNIST, which, based on our detailed analysis in Appendix A.5, could occur when the reward distribution of malicious workers is almost indistinguishable from that of benign workers. This may weaken GAA's defense against targeted attacks aimed at misclassifying specific data samples rather than overall accuracy. Such targeted attacks can be highly stealthy [8] and remain a challenge in building robust distributed learning systems [24].

Due to limited access to industrial distributed learning systems, we have focused on typical use cases in image classification, sentiment analysis, and intelligent healthcare, using minimally preprocessed real-world datasets. Further research is needed to evaluate GAA's security and performance in more application domains within industrial environments. Although the distributed learning paradigm we study is mainstream, other paradigms, such as second-order optimization [50] or model-parallel approaches [33], exist. Generalizing GAA to these paradigms would be an interesting direction for future work.

### More Related Work
**Byzantine Robustness of Gradient-Based Distributed Learning Systems:**
Recent years have seen growing interest in Byzantine robustness in distributed learning systems. Most works focus on statistical approaches to Byzantine robustness [4, 11, 16, 31, 62]. Recent studies [6, 25] have developed sophisticated attacks against Krum and GeoMed, but these are highly dependent on the target defense and are hard to generalize to GAA. We investigate GAA's robustness under adaptive attacks on its mechanism in Sections 6.2 and 6.3. During our paper preparation, we noticed a recent work [60] that also attempts to break the Î² = 0.5 bound. This work uses loss decrease on the training set to rank worker credibility, which can be seen as a special case of our algorithm when workers share the same training set and T = 1 in Algorithm 1. However, it only considers a 4-layer convolutional network on CIFAR-10, while we provide more comprehensive evaluations across four typical scenarios.

**Byzantine Problem in Other Contexts:**
Beyond gradient-based distributed learning, there are studies on other distributed learning protocols. For example, Chen et al. [15] proposed a robust protocol by requiring workers to submit redundant information; Damaskinos et al. [20] studied asynchronous distributed learning; and other works [5, 7, 28] explored vulnerabilities in protocols where workers directly submit local models to the master. We focus on the gradient-based system, making these works less directly related.

**Byzantine Robustness in Other Domains:**
Byzantine robustness has been studied in contexts like multi-agent systems [46] and file systems [21], with seminal work by Lamport [37]. Challenges in adversarial machine learning, such as adversarial examples [30], data poisoning [9], and privacy issues [26, 44, 51], remain open problems requiring further research to build more robust and reliable machine learning systems.

### Conclusion
In this paper, we propose a novel RL-based defense, GAA, against Byzantine attacks, which learns to be Byzantine robust through interactions with distributed learning systems. Due to the interpretability of its policy space, we apply our method to Byzantine worker detection and behavioral pattern analysis. Through theoretical and experimental efforts, we show that GAA is effective, efficient, and interpretable, complementing existing defenses and ensuring the robustness of distributed learning systems in more general and challenging use cases.

### Acknowledgements
We sincerely appreciate the shepherding from Yuan Tian and thank the anonymous reviewers for their constructive comments. This work was supported by the National Natural Science Foundation of China, the National Key Research and Development Program of China, the Natural Science Foundation of Shanghai, the Zhejiang Provincial Natural Science Foundation, and the Ant Financial Research Funding. Min Yang is the corresponding author and a faculty member at the Shanghai Institute of Intelligent Electronics & Systems, Shanghai Institute for Advanced Communication and Data Science, and the Engineering Research Center of CyberSecurity Auditing and Monitoring, Ministry of Education, China.

### References
[References listed as in the original text]

---

This version of the text is more structured and professional, with clearer transitions and a more coherent flow. It also ensures that all key points are covered and presented in a logical order.