### Parallel Call Execution and Attack Simulation

To simulate the behavior of multiple users from a single node, calls were executed in parallel. Attacks were initiated from two Wireless User Agents (W-UAs): W-UA-1 as the caller and W-UA-2 as the callee. SIPp, a tool typically used for testing SIP (Session Initiation Protocol) functionality, was repurposed to stage these attacks. Scenario files were provided to SIPp, specifying malicious behaviors such as flooding requests or excessive duplication of responses.

For example, in a BYE flood attack, W-UA-1 initiates numerous spurious BYE transactions, each with a unique call ID to represent a new transaction. Since SIP does not associate a BYE request with a prior INVITE, the server accepts the BYE and reserves transaction memory, even though the transaction is illegitimate. W-UA-2 collaborates with W-UA-1 by deliberately not responding to the BYE requests, further extending the time the server holds the transaction memory. As with the benign workload, the intensity of the attack can be controlled by adjusting the number of simultaneous attack calls in SIPp.

### Cogo Model for OpenSIPS

The Cogo model for OpenSIPS was constructed using data from five observation runs, totaling 8 hours of benign measurements. During these runs, OpenSIPS was subjected to a standard load between SIPp clients (UA-1, UA-2) and the SIP server. The clients initiated calls to the server, with call setup and disconnect specified using XML files. These files followed standard SIP conventions for INVITE, RINGING, BYE, and appropriate response and status messages. The call hold duration was determined using a log-normal distribution. The SIPp settings were configured to a maximum of 10 calls per second, with a call limit of 200, resulting in a steady call rate of approximately 7 calls per second.

Additional observation runs were conducted during which OpenSIPS was started and terminated to capture the variability in startup and shutdown processes. The total size of the observation data was 515 MB, which was processed to create a model of 55 MB. Test runs using this model showed that Cogo exhibited virtually zero false positives under a load similar to the observation runs.

### Detection Results

Table 3 summarizes the detection results. Cogo significantly reduced training time from about 12 hours to just 4 minutes, representing a reduction of over 169 times. The model size increased slightly from 41 MB to 55 MB, a 0.75x increase. In terms of accuracy, Cogo had only 4 false positives (FPs) throughout the experiment, all occurring at the startup time of OpenSIPS. Radmin, in comparison, triggered 9 FPs, also at startup.

The impact of BYE and INVITE floods on OpenSIPS and the detection behavior of Cogo are illustrated in Figure 7. Radmin failed to detect these attacks because OpenSIPS uses a fixed-size memory pool, preventing memory exhaustion. However, Cogo detected the attacks almost immediately, within less than six seconds after the onset. It's important to note that no remediation policy was implemented for OpenSIPS; proper remediation would require a protocol-specific solution to time out or hang up the attack calls.

### Performance Overhead

Cogo incurred negligible overhead. We measured the throughput of Apache and OpenSIPS under both benign workloads and with Cogo enabled. Apache maintained a steady rate of 130 requests per second. Using HTTPerf, we observed a marginal increase in response time of 0.2 ± 0.3 ms per request, from 10.3 ms to 10.5 ms. For OpenSIPS, it maintained a steady call rate of 200 calls per second. Experiments with call rates ranging from 300 to 1000 calls per second showed no degradation in throughput.

### Related Work

Modern operating systems have threshold-based facilities like ulimit and AppArmor to limit resource consumption. However, these limits are static and do not account for varying resource usage across different program segments and inputs. This allows attackers to maximize DoS time by crafting inputs that trigger prolonged resource consumption, such as slow-rate attacks. Tools like Valgrind and Intel Pin exist for profiling, but their high instrumentation overhead makes them unsuitable for continuous use, especially for detecting resource exhaustion.

Apostolico presented a theoretical study on linear prediction using Generalized Suffix Trees (GST), but there is no known implementation or quantitative study. Our approach uses a simpler Probabilistic Finite Automata (PFA) construction, providing tight detection time and space guarantees.

Other related works include surveys on anomalous traffic detection, systems for testing server vulnerabilities, and formal models for DoS resilience. Cogo differs from these systems by not requiring access to source code or side information and by covering network resources, not just CPU and memory. Elsabagh et al. proposed Radmin, a system for early detection of application-layer DoS attacks, which served as the starting point for Cogo. Radmin, however, did not monitor network I/O, had a quadratic training time, and could not monitor containerized processes or attach to running processes.

### Conclusions

This paper introduces Cogo, a practical and accurate system for early detection of DoS attacks at the application layer. Cogo builds a PFA model from temporal and spatial resource usage information in linear time. It monitors network state, supports containerized processes, and can attach to running processes. Cogo successfully detected real-world attacks on Apache and OpenSIPS, achieving high accuracy, early detection, and negligible overhead. Training time was less than 12 minutes, with a false positive rate of less than 0.0194%, and it detected a wide range of attacks within seven seconds. Cogo is scalable and accurate, suitable for large-scale deployment.

### Acknowledgments

We thank the anonymous reviewers for their insightful comments and suggestions. This work was supported in part by the National Science Foundation (NSF) SaTC award 1421747, the National Institute of Standards and Technology (NIST) award 60NANB16D285, and the Defense Advanced Research Projects Agency (DARPA) contract no. HR0011-16-C-0061 in conjunction with Vencore Labs. Opinions, findings, conclusions, and recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF, NIST, DARPA, or the US Government.

### References

1. Myths of DDoS Attacks. <http://blog.radware.com/security/2012/02/4-massive-myths-of-ddos/>
2. The Apache HTTP Server Project. <https://httpd.apache.org/>
3. AppArmor. <http://wiki.apparmor.net/index.php/Main_Page>
4. Application Layer DoS Attack Simulator. <http://github.com/shekyan/slowhttptest>
5. Are You Ready for Slow Reading? <https://blog.qualys.com/securitylabs/2012/01/05/slow-read>
6. Availability Overrides Security. <http://hrfuture.net/performance-and-productivity/availability-over-rides-cloud-security-concerns.php?Itemid=169>
7. Httperf - HTTP Performance Measurement Tool. <http://linux.die.net/man/1/httperf>
8. Mobile Users Favor Productivity Over Security. <http://www.infoworld.com/article/2686762/security/mobile-users-favor-productivity-over-security-as-they-should.html>
9. OpenSIPS: The New Breed of Communication Engine. <https://www.opensips.org/>
10. Sipp: Traffic Generator Proxy for the SIP Protocol. <http://sipp.sourceforge.net/>
11. Slow-Rate Attack. <https://security.radware.com/ddos-knowledge-center/ddospedia/slow-rate-attack/>
12. Slowloris - Apache Server Vulnerabilities. <https://security.radware.com/ddos-knowledge-center/ddospedia/slowloris/>
13. When the Lights Went Out: Ukraine Cybersecurity Threat Briefing. <https://www.boozallen.com/insights/2016/09/ukraine-cybersecurity-threat-briefing/>
14. Denial of Service Attacks: A Comprehensive Guide to Trends, Techniques, and Technologies. ADC Monthly Web Attacks Analysis 12 (2012)
15. Ahrenholz, J.: Comparison of Core Network Emulation Platforms. In: Military Communications Conference (2010)
16. Aiello, W., Bellovin, S.M., Blaze, M., Ioannidis, J., Reingold, O., Canetti, R., Keromytis, A.D.: Efficient, DoS-Resistant, Secure Key Exchange for Internet Protocols. In: 9th ACM Conference on Computer and Communications Security (2002)
17. Antunes, J., Neves, N.F., Veríssimo, P.J.: Detection and Prediction of Resource-Exhaustion Vulnerabilities. In: International Symposium on Software Reliability Engineering (2008)
18. Apostolico, A., Bejerano, G.: Optimal Amnesic Probabilistic Automata. J. Comput. Biol. 7(3–4), 381–393 (2000)
19. Burnim, J., Juvekar, S., Sen, K.: WISE: Automated Test Generation for Worst-Case Complexity. In: 31st International Conference on Software Engineering (2009)
20. Chang, R.M., et al.: Inputs of COMA: Static Detection of Denial-of-Service Vulnerabilities. In: 22nd Computer Security Foundations Symposium (2009)
21. Chee, W.O., Brennan, T.: Layer-7 DDoS. (2010). <https://www.owasp.org/images/4/43/Layer_7_DDOS.pdf>
22. Choi, H.K., Limb, J.O.: A Behavioral Model of Web Traffic. In: 7th International Conference on Network Protocols (1999)
23. Crosby, S., Wallach, D.: Algorithmic DoS. In: van Tilborg, H.C.A., Jajodia, S. (eds.) Encyclopedia of Cryptography and Security, pp. 32–33. Springer, USA (2011)
24. Desnoyers, M.: Using the Linux Kernel Tracepoints. <https://www.kernel.org/doc/Documentation/trace/tracepoints.txt>
25. Elsabagh, M., Barbará, D., Fleck, D., Stavrou, A.: RADMIN: Early Detection of Application-Level Resource Exhaustion and Starvation Attacks. In: 18th International Conference on Research in Attacks, Intrusions and Defenses (2015)
26. Gray, R.M., Neuho, D.L.: Quantization. IEEE Trans. Inform. Theory 44(6), 2325–2383 (1998)
27. Groza, B., Minea, M.: Formal Modelling and Automatic Detection of Resource Exhaustion Attacks. In: Symposium on Information, Computer and Communications Security (2011)
28. Gulavani, B.S., Gulwani, S.: A Numerical Abstract Domain Based on Expression Abstraction and Max Operator with Application in Timing Analysis. In: Computer Aided Verification, pp. 370–384 (2008)
29. Hilt, V., Eric, N., Charles, S., Ahmed, A.: Design Considerations for Session Initiation Protocol (SIP) Overload Control (2011). <https://tools.ietf.org/html/rfc6357>
30. Kostadinov, D.: Layer-7 DDoS Attacks: Detection and Mitigation. InfoSec Institute (2013)
31. Luk, C.K., Cohn, R., Muth, R., Patil, H., Klauser, A., Lowney, G., Wallace, S., Reddi, V.J., Hazelwood, K.: PIN: Building Customized Program Analysis Tools with Dynamic Instrumentation. In: Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2005, pp. 190–200. ACM, New York (2005). <http://doi.acm.org/10.1145/1065010.1065034>
32. Nethercote, N., Seward, J.: Valgrind: A Framework for Heavyweight Dynamic Binary Instrumentation. In: ACM Sigplan Notices, vol. 42, pp. 89–100. ACM (2007)
33. Rosenberg, J., et al.: SIP: Session Initiation Protocol (2002). <https://www.ietf.org/rfc/rfc3261.txt>
34. Ruiz-Alvarez, A., Hazelwood, K.: Evaluating the Impact of Dynamic Binary Translation Systems on Hardware Cache Performance. In: International Symposium on Workload Characterization (2008)
35. Uh, G.R., Cohn, R., Yadavalli, B., Peri, R., Ayyagari, R.: Analyzing Dynamic Binary Instrumentation Overhead. In: Workshop on Binary Instrumentation and Application (2007)
36. Ukkonen, E.: Online Construction of Suffix Trees. Algorithmica 14(3), 249–260 (1995)
37. Zargar, S.T., Joshi, J., Tipper, D.: A Survey of Defense Mechanisms Against Distributed Denial of Service (DDoS) Flooding Attacks. IEEE Commun. Surv. Tutorials 15(4), 2046–2069 (2013)
38. Zheng, L., Myers, A.C.: End-to-End Availability Policies and Noninterference. In: 18th IEEE Workshop Computer Security Foundations (2005)