### Request Processing and Client Behavior

In this system, a dedicated Emulab host generates client requests. The request process is as follows: the client initially sends a request to the server. If the server is busy, the thinning layer (thinner) responds by instructing the client to issue two HTTP requests: the original request and a payment request. Each client's request generation is driven by a Poisson process with a rate of \(\lambda\) requests per second. However, each client enforces a configurable limit \(w\) (the window) on the number of outstanding requests. If the Poisson process triggers when more than \(w\) requests are already in progress, the new request is placed in a backlog queue. This queue drains as responses to earlier requests are received. If a request remains in the queue for over 10 seconds, it times out, and the client logs a service denial. All requests are identical, and the server processes each request on average every \(1/c\) seconds (see §6).

### Modeling Good and Bad Clients

We use this behavior to model both good and bad clients. By definition, a bad client attempts to capture more than its fair share of the server's resources. In our experiments, bad clients send requests at a higher rate (\(\lambda = 40\)) and with a larger window (\(w = 20\)), while good clients have \(\lambda = 2\) and \(w = 1\). The chosen values for bad clients are intentionally pessimistic (see §7.4).

The total number of clients and their access bandwidth are determined by the testbed's capabilities and a rough model of current client access links. Typically, there are 50 clients, each with 2 Mbits/s of access bandwidth, making the total bandwidth \(B + G = 100\) Mbits/s. Although this scale is smaller than most attacks, we believe the results are generalizable because they focus on how the prototype's behavior differs from the theoretical predictions in §3.

### Thinner Capacity

To ensure that the experimental scale does not overload the thinner, we separately measured its capacity. At 90% CPU utilization with multiple gigabit Ethernet interfaces, the thinner can handle 1451 Mbits/s (with a standard deviation of 38 Mbits/s) for 1500-byte packets and 379 Mbits/s (with a standard deviation of 24 Mbits/s) for 120-byte packets. This capacity is comparable to recent attacks (see §2.1 and §4.3). The thinner's capacity also depends on the number of concurrent clients it supports, limited only by the RAM for each connection (see §6).

### Server Allocation Validation

When the incoming request rate exceeds the server's capacity, the goal of speak-up is to allocate the server's resources proportionally to the clients' aggregate bandwidth. We evaluate the implementation's performance in achieving this goal.

In the first experiment, 50 clients connect to the thinner over a 100 Mbits/s LAN, each with 2 Mbits/s of bandwidth. We vary the fraction \(f\) of "good" clients. In this homogeneous setting, \(G/(G+B)\) (the fraction of "good client bandwidth") equals \(f\), and the server's capacity is \(c = 100\) requests/s.

**Figure 2** shows the fraction of the server allocated to good clients as a function of \(f\). Without speak-up, bad clients capture a larger fraction of the server due to their higher request rate and the server's random request dropping. With speak-up, good clients can "pay" more for each request, allowing them to capture a proportionate share of the server. The small discrepancy between measured and ideal values is due to good clients not using as much of their bandwidth as bad clients (see §3.4, §7.3, and §7.4).

**Figure 3** shows the server allocation to good and bad clients and the fraction of good requests served, with and without speak-up. For \(c = 50, 100\), the allocation under speak-up is roughly proportional to the aggregate bandwidths, and for \(c = 200\), all good requests are served. When \(c = 100\), the good demand is not fully satisfied even with speak-up enabled.

### Latency and Byte Cost

We also explore the byte and latency costs of speak-up. For the latency cost, we measure the time clients spend uploading dummy bytes, which captures the extra latency introduced by speak-up. **Figure 4** shows the average and 90th percentile measurements for served good requests.

**Figure 5** shows the average number of bytes sent on the payment channel (the "price") for served requests. When the server is overloaded (\(c = 50, 100\)), the price is close to the upper bound \((G + B)/c\). When the server is not overloaded (\(c = 200\)), good clients pay almost nothing.

### Conclusion

These experiments demonstrate that speak-up effectively allocates server resources in proportion to client bandwidth, with some minor discrepancies. The system introduces minimal latency when the server is not overloaded, and the byte cost is manageable.