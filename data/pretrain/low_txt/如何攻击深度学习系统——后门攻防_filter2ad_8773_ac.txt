### 模型窃取简介

模型窃取是指攻击者通过查询和观察受害模型的响应来窃取供应商提供的机器学习即服务（MLaaS）模型。这种攻击类似于密码学中的明文选择攻击，其中攻击者通过向模型发送精心设计的查询并分析其输出来推断模型的内部结构。

传统的水印技术可以用于保护模型，但存在一个关键问题：主要任务和水印（后门）任务的参数是分开的。因此，当攻击者旨在窃取主要任务功能时，作为不同子任务的水印可能不会传播到被窃副本中。相比之下，Jia等人[19]提出了一种防御方案，其中插入的后门将不可避免地传播到被盗模型中。

### 后门技术的应用

后门技术既可以用于消极目的，也可以用于积极目的，具体取决于应用场景。本文仅列举了部分典型工作，感兴趣的读者可以进一步深入研究。

## 研究方向

后门攻防领域的研究一直在不断探索中。根据笔者的经验，以下是一些值得进一步研究的方向：

1. **运行机制**
   - 当前，后门的生成机制和触发机制并不透明，这涉及到深度学习系统的不可解释性问题。如果这些机制能够被深入研究清楚，未来后门领域的攻防将会更加有效和精彩。

2. **防御措施**
   - 目前的防御措施都是针对特定攻击手段设计的，并不存在一种通用解决方案。是否存在这样的通用方案以及如何实现它，仍然是未知数。此外，一些防御方案要求大量良性数据集或强大的计算资源，这些需求是否必要以及能否进一步改进也是值得研究的问题。

3. **攻击方案**
   - 尽管深度学习应用广泛，但大部分后门攻击研究集中在图像识别、自动驾驶等领域。在语音识别、推荐系统等方面的研究相对较少。此外，对抗攻击具有可迁移性，后门攻击是否也具备这一特性尚不清楚，这也是一个值得探索的方向。

4. **触发器设计**
   - 尽管现有研究中的一些触发器设计得非常自然，但它们并未完全消除触发器的存在。是否有可能在触发器的设计上进行优化，例如使其自动适应图像并将触发器叠加在肉眼不可见的地方，目前的研究还不完善。现有的触发器设计多为启发式的，是否可以将其形式化为一个可优化的数学问题进行研究也是一个开放的问题。

## 参考文献

[1] https://zh.wikipedia.org/wiki/%E8%BB%9F%E9%AB%94%E5%BE%8C%E9%96%80  
[2] Gu T, Dolan-Gavitt B, Garg S. Badnets: Identifying vulnerabilities in the machine learning model supply chain[J]. arXiv preprint arXiv:1708.06733, 2017.  
[3] Liu Y, Ma S, Aafer Y, et al. Trojaning attack on neural networks[J]. 2017.  
[4] https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db  
[5] Yuan X, He P, Zhu Q, et al. Adversarial examples: Attacks and defenses for deep learning[J]. IEEE transactions on neural networks and learning systems, 2019, 30(9): 2805-2824.  
[6] Li J, Ji R, Liu H, et al. Universal perturbation attack against image retrieval[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 4899-4908.  
[7] Tang R, Du M, Liu N, et al. An embarrassingly simple approach for trojan attack in deep neural networks[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020: 218-228.  
[8] Liu Y, Ma X, Bailey J, et al. Reflection backdoor: A natural backdoor attack on deep neural networks[C]//European Conference on Computer Vision. Springer, Cham, 2020: 182-199.  
[9] Chen, X., Liu, C., Li, B., Lu, K., Song, D.: Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526 (2017)  
[10] Barni, M., Kallas, K., Tondi, B.: A new backdoor attack in cnns by training set corruption without label poisoning. In: IEEE International Conference on Image Processing (ICIP). pp. 101–105. IEEE (2019)  
[11] Tran, B., Li, J., Madry, A.: Spectral signatures in backdoor attacks. In: NIPS (2018)  
[12] Turner A, Tsipras D, Madry A. Clean-label backdoor attacks[J]. 2018.  
[13] Te Lester Juin Tan and Reza Shokri. Bypassing Backdoor Detection Algorithms in Deep Learning. In Proceedings of IEEE European Symposium on Security and Privacy (Euro S&P), 2020.  
[14] W. Xu, D. Evans, and Y. Qi. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. In Proceedings of Network and Distributed System Security Symposium (NDSS), 2018.  
[15] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith Ranas- inghe, and Surya Nepal. STRIP: A Defence Against Trojan Attacks on Deep Neural Networks. In Proceedings of Annual Computer Security Applications Conference (ACSAC), 2019.  
[16] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks. In Proceedings of Symposium on Research in Attacks, Intrusions and Defenses (RAID), 2018.  
[17] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. DeepIn- spect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks. In Proceedings of International Joint Confer- ence on Artificial Intelligence, 2019.  
[18] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning your weakness into a strength: Watermarking deep neural networks by backdooring,” in USENIX Security Symposium, 2018.  
[19] H. Jia, C. A. Choquette-Choo, and N. Papernot, “Entangled wa- termarks as a defense against model extraction,” arXiv preprint arXiv:2002.12200, 2020.  
[20] Li Y, Wu B, Jiang Y, et al. Backdoor learning: A survey[J]. arXiv preprint arXiv:2007.08745, 2020.  
[21] Gao Y, Doan B G, Zhang Z, et al. Backdoor attacks and countermeasures on deep learning: a comprehensive review[J]. arXiv preprint arXiv:2007.10760, 2020.  
[22] Pang R, Zhang Z, Gao X, et al. TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask)[J]. arXiv preprint arXiv:2012.09302, 2020.

希望上述内容对您有所帮助。如有任何疑问或需要进一步的信息，请随时告知。