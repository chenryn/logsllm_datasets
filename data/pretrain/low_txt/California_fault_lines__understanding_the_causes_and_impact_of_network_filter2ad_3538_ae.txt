### Announcement

**Table 5: Major Causes of Failure According to Administrator Announcements, Ordered by Median Time to Repair**

| Cause       | Events | Avg (m) | Med (m) | Time to Repair (m) |
|-------------|--------|---------|---------|--------------------|
| Hardware    | 20%    | 95      | 95      | 95                 |
| Power       | 6%     | 93      | 93      | 93                 |
| External    | -      | 5       | 5       | 5                  |
| Software    | 15%    | 61      | 4.6     | 4.6                |
| Configuration | -    | 18      | 18      | 18                 |
| Other       | 32%    | 10      | 5       | 5                  |
| Unknown     | 12%    | 46      | 6       | 6                  |
| -           | 5%     | 52      | 9       | 9                  |

**Table 6: Breakdown of Administrator Notices by Failure Cause**

| Cause       | Notices (%) | Scheduled (%) | Impacting (%) |
|-------------|-------------|---------------|---------------|
| Hardware    | 25%         | 29%           | 84%           |
| Power       | 20%         | 69%           | 91%           |
| External    | 15%         | 0%            | 71%           |
| Software    | 12%         | 99%           | 95%           |
| Configuration | 8%        | 99%           | 82%           |
| Other       | 7%          | 45%           | 99%           |
| Unknown     | 65%         | -             | -             |

**Table 7: Summary of the CENIC Network Partitions**

| Sites  | Pw  | Hw  | Sw  | N/A | AS2152 | Other AS |
|--------|-----|-----|-----|-----|--------|----------|
| AS2152 | 41  | 19  | 361 | 147 | 28     | 6        |
| Other  | 12  | 5   | 39  | 26  | 287    | 105      |

**Figure 14: Annualized Link Downtime in the DC Network, by Year**

- **Year**: 2005, 2006, 2007, 2008, 2009
- **Metrics**: 75th percentile, Median, 25th percentile
- **Downtime (s)**:
  - 2005: 3h, 2h, 1h
  - 2006: 30m, 1m, 2005
  - 2007: 2006, 2007, 2008
  - 2008: 2009, 2008, 2007
  - 2009: 2008, 2009, 2006

### Analysis and Findings

**Figure 14** illustrates the annualized link downtime in the DC network for each year in the measurement period. The data shows that the performance indicators varied from year to year without a discernible trend. Notably, 2006 and, to a lesser extent, 2008, had lower link downtimes compared to other years. The median number of failures per link was 0.0 in 2006 and 6.0 in 2005.

Further investigation revealed that the distribution of failure causes also varied. Network-wide events, such as software-related link failures and configuration changes, significantly impacted the number of link failures. The three vertical bands in Figure 6, due to network-wide upgrades and configuration changes, affected the median number of failures and median link downtime in 2005, 2007, and 2009. Longitudinal trends, if present, were overshadowed by major but infrequent events.

**Table 8: Duration of Network Partition for All Isolating Events**

### Conclusion

As discussed in Section 5, the only type of impact we can infer are isolating network partitions. Table 7 presents the 508 isolating failures identified in the failure log, separated into networks with and without their own AS, and provides the cause annotation if available. Interestingly, software failures dominate the breakdown of failure causes for partition events. Table 8 summarizes the distribution of times to repair for different causes of isolating failures, independent of the AS involved. As with non-isolating events, power and hardware events have significantly longer durations than those caused by software failures.

### Time Dynamics

The CENIC network is continually evolving. A significant change, starting in 2008, was the designation of some DC routers as "core" routers and the rest as "access" routers. This resulted in the decommissioning of 235 links and the introduction of 432 new links. We examined whether the network qualities changed as well. 

In this paper, we present a methodology for inferring and analyzing the link failure history of a network absent dedicated monitoring infrastructure. We show that existing "low-quality" data sources, such as syslog, router configurations, and operational mailing lists, can be combined to reconstruct topology, dynamic state, and failure causes. Using this approach, we analyzed five years of link failure history from the CENIC network, a large California Internet service provider. We validated existing understandings about failure (e.g., the prevalence of link flapping) and documented less appreciated issues (e.g., the large amounts of downtime attributable to third-party leased line problems). Our approach is general and should be straightforward to adapt to a wide variety of IP networks.

### Acknowledgments

The authors would like to thank Brian Court, Darrel Newcomb, Jim Madden, and our shepherd, Aman Shaikh, for their advice and suggestions. This work is supported in part by a grant from the UCSD Center for Networked Systems.

### References

[1] Internet2. http://www.internet2.edu.
[2] A. Akella, J. Pang, B. Maggs, S. Seshan, and A. Shaikh. A comparison of overlay routing and multihoming route control. In Proceedings of SIGCOMM, pages 93–106, 2004.
[3] M. Balakrishnan and A. Reibman. Characterizing a lumping heuristic for a Markov network reliability model. In Proceedings of FTCS, pages 56–65, 1993.
[4] P. Baran. On distributed communications networks. IEEE Transactions on Communications Systems, 12(1):1–9, March 1964.
[5] K. Claffy, T. Monk, and D. McRobb. Internet tomography. Nature, Web Matters, January 1999.
[6] M. Coates, R. Castro, and R. Nowak. Maximum likelihood network topology identification from edge-based unicast measurements. In Proceedings of SIGMETRICS, pages 11–20, 2002.
[7] Corporation for Education Network Initiatives in California. The CalREN network. http://www.cenic.org/calren/index.html.
[8] C. Cranor, T. Johnson, O. Spataschek, and V. Shkapenyuk. Gigascope: a stream database for network applications. In Proceedings of SIGMOD, pages 647–651, 2003.
[9] M. Dahlin, B. B. V. Chandra, L. Gao, and A. Nayate. End-to-end WAN service availability. IEEE/ACM Transactions on Networking, 11(2):300–313, April 2003.
[10] A. Dhamdhere, R. Teixeira, C. Dovrolis, and C. Diot. NetDiagnoser: Troubleshooting network unreachabilities using end-to-end probes and routing data. In Proceedings of CoNEXT, 2007.
[11] N. Duffield. Network tomography of binary network performance characteristics. IEEE Transactions on Information Theory, 52(12):5373–5388, 2006.
[12] N. Feamster and H. Balakrishnan. Detecting BGP configuration faults with static analysis. In Proceedings of NSDI, pages 43–56, 2005.
[13] A. Feldmann. Netdb: IP network configuration debugger/database. Technical report, AT&T, 1999.
[14] A. Feldmann, A. Greenberg, C. Lund, N. Reingold, and J. Rexford. Netscope: Traffic engineering for IP networks. IEEE Network, 14(2):11–19, 2000.
[15] K. P. Gummadi, H. V. Madhyastha, S. D. Gribble, H. M. Levy, and D. Wetherall. Improving the reliability of Internet paths with one-hop source routing. In Proceedings of OSDI, pages 13–13, 2004.
[16] Y. Huang, N. Feamster, and R. Teixeira. Practical issues with using network tomography for fault diagnosis. Computer Communication Review, 38(5):53–57, October 2008.
[17] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren. IP fault localization via risk modeling. In Proceedings of NSDI, pages 57–70, 2005.
[18] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren. Detection and localization of network black holes. In Proceedings of INFOCOM, pages 2180–2188, 2007.
[19] C. Labovitz, A. Ahuja, and F. Jahanian. Experimental study of Internet stability and backbone failures. In Proceedings of FTCS, pages 278–285, 1999.
[20] K. Levchenko, G. M. Voelker, R. Paturi, and S. Savage. XL: An efficient network routing algorithm. In Proceedings of SIGCOMM, pages 15–26, 2008.
[21] C. Lonvick. RFC 3164: The BSD syslog protocol, August 2001.
[22] Y. Mao, H. Jamjoom, S. Tao, and J. M. Smith. NetworkMD: Topology inference and failure diagnosis in the last mile. In Proceedings of IMC, pages 189–202, 2007.
[23] A. Markopoulou, G. Iannaccone, S. Bhattacharyya, C.-N. Chuah, Y. Ganjali, and C. Diot. Characterization of failures in an operational IP backbone network. Transactions on Networking, 16(4), 2008.
[24] V. N. Padmanabhan, S. Ramabhadran, S. Agarwal, and J. Padhye. A study of end-to-end web access failures. In Proceedings of CoNEXT, pages 1–13, 2006.
[25] V. D. Park and M. S. Corson. A performance comparison of the temporally-ordered routing algorithm and ideal link-state routing. In Proceedings of ISCC, pages 592–598, 1998.
[26] V. Paxson. End-to-end routing behavior in the Internet. In Proceedings of SIGCOMM, pages 25–38, 1996.
[27] A. Shaikh, C. Isett, A. Greenberg, M. Roughan, and J. Gottlieb. A case study of OSPF behavior in a large enterprise network. In Proceedings of IMC, pages 217–230, 2002.
[28] A. U. Shankar, C. Alaettinoğlu, I. Matta, and K. Dussa-Zieger. Performance comparison of routing protocols using MaRS: Distance vector versus link-state. ACM SIGMETRICS Performance Evaluation Review, 20(1):181–192, June 1992.
[29] Shrubbery Networks, Inc. RANCID. http://www.shrubbery.net/rancid/.
[30] L. Tang, J. Li, Y. Li, and S. Shenker. An investigation of the Internet’s IP-layer connectivity. Computer Communications, 32(5):913–926, 2009.
[31] University of Oregon. University of Oregon Route Views project. http://www.routeviews.org.
[32] F. Wang, Z. M. Mao, J. Wang, L. Gao, and R. Bush. A measurement study on the impact of routing events on end-to-end Internet path performance. In Proceedings of SIGCOMM, pages 375–386, 2006.
[33] D. Watson, F. Jahanian, and C. Labovitz. Experiences with monitoring OSPF on a regional service provider network. In Proceedings ICDCS, pages 204–212, 2003.
[34] W. Xu, L. Huang, A. Fox, D. Paterson, and M. Jordan. Detecting large-scale system problems by mining console logs. In Proceedings of SOSP, 2009.
[35] M. Zhang, C. Zhang, V. Pai, L. Peterson, and R. Wang. PlanetSeer: Internet path failure monitoring and characterization in wide-area services. In Proceedings of OSDI, pages 167–182, 2004.

### Appendix

While the primary focus of this paper is the CENIC network, we also applied our methodology to the Internet2 Network to establish a point of comparison [1]. This allowed us to assess the effort required to apply our methodology to a different network. Here, we briefly describe the Internet2 Network and its measurement data, how we adapted the methodology for this dataset, and highlights of the results.

**The Internet2 Network**

- **AS Number**: 11537
- **Routers**: Nine Juniper T640 routers in nine major US cities
- **Links**: 1-Gb/s or 10-Gb/s Ethernet
- **Protocol**: IS-IS
- **Operation**: Since 1996, serving 330 member institutions
- **Data**: Router configuration snapshots and syslog message logs (01/01/2009 to 12/31/2009)

**Adapting the Methodology**

- **Challenge**: Different data format (Juniper vs. Cisco)
- **Solution**: Wrote approximately 300 lines of new parsing code

**Brief Summary**

- **Performance**: Between DC and HPR networks in terms of failures and annual link downtime, with a longer time to repair
- **Distribution**: No bias towards short failures (unlike CENIC networks)

This appendix provides a comparative analysis and demonstrates the adaptability of our methodology to different network environments.