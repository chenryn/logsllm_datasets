根据提供的日志信息，可以判断这条日志是由 Apache Spark 应用程序生成的。具体来说，它显示了调度器（scheduler）正在提交来自 shuffle map 阶段 198 的 13 个缺失任务，并且指出了这些操作与 `pairwiseRDD` 及其在 `aggregateByKey` 方法中的处理有关，该方法调用位于文件 `ip_lom.py` 的第 518 行。

日志内容解析如下：
- `<DATETIME>`：记录事件的时间戳。
- `info`：表明这是一个信息级别的日志条目。
- `scheduler.dagScheduler`：标识了日志来源于 DAG 调度器组件。
- `submitting 13 missing tasks from shuffle map stage 198`：说明正在为 shuffle map 阶段 198 提交 13 个未完成的任务。
- `(pairwiseRDD[393] at aggregateByKey at ip_lom.py:518)`：提供了关于 RDD (Resilient Distributed Dataset) 操作的具体上下文，这里涉及到的是 `pairwiseRDD` 在进行 `aggregateByKey` 操作时的情况，相关代码位于 `ip_lom.py` 文件的第 518 行。

综上所述，这条日志明确指向了使用 Spark 进行大数据处理的应用场景。