### Optimized Text

#### Model Extension for Decision Making
The model must be extended to facilitate reasonable interpretation of its results as a basis for decision making. For instance, this can be achieved by defining the thresholds at which the assessed confidence is deemed sufficient to accept or reject specific claims.

#### Internal Measures of Quality
The aforementioned items suggest a promising approach to developing an integrated internal measure of quality. For example, extending the measure of hazard coverage to incorporate argument validity, expressed as a confidence level, could be beneficial. As an initial step, we defined a set of measures (including coverage) to objectively evaluate our approach. Table I presents the results of applying these measures to the Swift UAS safety case. The measures indicate reasonably high, but not complete, coverage of the hazards and corresponding high-level requirements, at approximately 73% and 87%, respectively. This reflects that not all claims related to the hazards and high-level requirements in the argument fragment have been fully developed, i.e., they do not all end with available evidence.

On the other hand, we observed perfect coverage of low-level requirements by claims, as all corresponding claims are both auto-generated and terminate in evidence. However, the actual coverage of low-level requirements by verification is not perfect. This is reflected in the last row of Table I, where about 92% of the code is covered by auto-generated claims. This indicates that some parts of the code were not reasoned about, not covered by a requirement, and thus remained unproven. To use these initial measures for quality assessment, a quality model similar to the one used for interpreting uncertainty assessment outcomes is required.

#### Implications on Processes
The existing framework used by the Swift UAS engineering team for system development is mature, based on extensive engineering knowledge and experience from developing previous UAVs. Therefore, there is a need to tailor the safety methodology to ensure well-defined interfaces with the system development process, allowing the safety process to keep pace with and actively influence development. One approach is to establish predefined intervals in the development process to synchronize with the safety process. Another alternative is to synthesize the development process from safety activities, as described in [18].

The process and resulting arguments, as applied, are "heavyweight" relative to the development costs and effort involved. However, their application is justified due to the intolerable risk associated with the loss of the Swift UAV. This is partly due to the unique avionics system, mission-specific modifications to the airframe, and significant research investments. For smaller UAVs, the loss of aircraft may not always be an intolerable risk, necessitating a lightweight version of our approach, where the effort is proportionate to the costs.

The development team found software hazard analysis useful in understanding the subtleties of software contributions to system hazards. The engineers also integrated procedures for system/software hazard analysis into their development process, ensuring that safety is proactively considered in the development of subsequent functionality, such as the electric-battery management system.

#### Implications of Existing Guidelines/Standards
Many pieces of evidence used in the ASSC (Section III) can be mapped to lifecycle data required for compliance with DO-178B. For example, evidence E1 of requirements review in the argument (Figure 2(a)) corresponds to Objective 2 in Table A4 of DO-178B (i.e., "Low-level requirements are accurate and consistent"). The added value of our software safety argument is that it explains how potentially DO-178B-compliant evidence can provide sufficient assurance, particularly regarding claims about how software behavior relates to specific system hazards (e.g., the link between incorrect autopilot behavior at the software level and a loss event at the system level).

Areas of a hazard-directed safety argument least supported by DO-178B evidence are those related to the analysis of hazardous software failure modes. DO-178B guidance implies that safety analysis is a system-level process, and the role of software development is to demonstrate correctness against requirements, including safety requirements allocated to software. The process of refining and implementing these requirements does not involve explicit software hazard analysis. In DO-178B, the link between software behavior and required safety properties is implicit.

However, others [9] argue that hazard analysis should be applied at the software level for each development stage (e.g., software architecture, design, and source code stages). The reasoning is that errors introduced at each software development stage could lead to hazardous failure conditions at the system level. Therefore, it is important to provide assurance through explicit software safety arguments that these errors have been identified and managed.

Finally, the explicit representation of a software safety argument is useful for developers using alternative means for compliance, such as generating evidence from formal mathematical analysis rather than testing. A documented safety argument makes it easier to convince reviewers of the relevance and suitability of alternative evidence. This is particularly important as DO-178B has been updated to DO-178C, which includes guidance on formal methods and allows assurance arguments as an alternative method for establishing compliance.

In the software assurance for the Swift UAS, we performed hazard analysis down to the software architecture level, identifying how software components and interactions contribute to system hazards. We then defined safety requirements and allocated them to the components and interactions. Subsequently, we demonstrated safety assurance by verifying the correctness of the implementation of these interactions and components against the allocated and formalized safety requirements (e.g., correctness of the autopilot software against allocated safety requirements). The criterion for deciding that no further hazard analysis was needed at subsequent stages was that a software component was simple enough (as gauged by the developers) to formulate requirements demonstrating desired behavior and the absence of unintended behavior.

#### Conclusions and Outlook
Our perspectives in this paper are based on an interim safety case, which represents only a small part of the overall Swift UAS safety case. The interim safety case alone is insufficient to make categorical claims about system safety improvement. Instead, the safety argument is intended to communicate that specific claims can be defended with reasonable confidence based on the evidence, assumptions, and context provided. Although we created the safety case using a formal verification tool, the methodology is more broadly applicable to include other safety and dependability techniques.

Based on feedback, we are pursuing research directions to improve the practical application of safety cases, including the Swift UAS safety case. This includes enhancing the modularity of the argument via abstraction to aid comprehension, assessing the soundness of manually created argument fragments, better characterizing available evidence to facilitate automatic assembly of the safety argument, and tailoring the methodology to make it more lightweight.

The need to manage and reconcile diverse information in both the system and software safety cases is apparent from the perspective of safety and compliance with airworthiness requirements for operating a UAS [6], where the overarching goal is to show that a level of safety equivalent to that of manned operations exists [21]. We believe that the approach and experience documented in this paper is a step towards that end.

#### Acknowledgments
This work was funded by NASA contract NNA10DE83C. We also thank Mark Sumich and Corey Ippolito for their feedback.

#### References
[1] RTCA SC-167 and EUROCAE WG-12, “Software Considerations in Airborne Systems and Equipment Certification,” DO-178B/ED-12B, 1992.
[2] F. Redmill, “Safety integrity levels – theory and problems, lessons in system safety,” in Proc. 18th Safety-Critical Sys. Symp., Feb. 2010.
[3] Goal Structuring Notation Working Group, “GSN Community Standard Version 1,” Nov. 2011. Available: http://www.goalstructuringnotation.info/ [Online].
[4] R. Weaver, “The safety of software – constructing and assuring arguments,” Ph.D. dissertation, Dept. of Comp. Sci., Univ. of York, 2003.
[5] R. Bloomfield and P. Bishop, “Safety and assurance cases: Past, present and possible future – an Adelard perspective,” in Proc. 18th Safety-Critical Sys. Symp., Feb. 2010.
[6] K. D. Davis, “Unmanned Aircraft Systems Operations in the U.S. National Airspace System,” Interim Operational Approval Guidance 08-01, FAA Unmanned Aircraft Systems Program Office, Mar. 2008.
[7] European Organisation for the Safety of Air Navigation, Safety Case Development Manual, 2nd ed., EUROCONTROL, Oct. 2006.
[8] J. Fenn, R. Hawkins, P. Williams, and T. Kelly, “Safety case composition using contracts - refinements based on feedback from an industrial case study,” in Proc. 15th Safety-Critical Sys. Symp., Feb. 2007.
[9] R. Hawkins, K. Clegg, R. Alexander, and T. Kelly, “Using a software safety argument pattern catalogue: Two case studies,” in Proc. Intl. Conf. on Comp. Safety, Reliability and Security (SafeComp), Sep. 2011.
[10] E. Denney and S. Trac, “A software safety certification tool for automatically generated guidance, navigation and control code,” in IEEE Aerospace Conf. Electronic Proc, 2008.
[11] FAA, System Safety Handbook, Federal Aviation Administration, Dec. 2000.
[12] C. Menon, R. Hawkins, and J. McDermid, “Interim standard of best practice on software in the context of DS 00-56 Issue 4,” Soft. Sys. Eng. Initiative, Univ. of York, Standard of Best Practice Issue 1, 2009.
[13] E. Denney, I. Habli, and G. Pai, “Towards measurement of confidence in safety cases,” in Proc. 5th Intl. Symp. Empirical Soft. Eng. and Measurement (ESEM), Sep. 2011.
[14] UK Ministry of Defence (MOD), “Safety management requirements for defence systems,” Defence Standard 00-56 Issue 4, Jun. 2007.
[15] T. Kelly and J. McDermid, “Safety case patterns – reusing successful arguments,” in Proc. IEE Colloq. on Understanding Patterns and Their Application to Sys. Eng., 1998.
[16] P. Bishop, R. Bloomfield, B. Littlewood, A. Povyakalo, and D. Wright, “Towards a formalism for conservative claims about the dependability of software-based systems,” IEEE Trans. Soft. Eng., vol. 37, no. 5, pp. 708 – 717, 2011.
[17] V. Basili, G. Caldiera, and D. Rombach, “Goal question metric approach,” in Encyclopedia of Soft. Eng.. John Wiley, 1994, pp. 528–532.
[18] P. Graydon and J. Knight, “Software process synthesis in assurance based development of dependable systems,” in Proc. European Dependable Comp. Conf. (EDCC), Apr. 2010, pp. 75–84.
[19] I. Habli and T. Kelly, “A generic goal-based certification argument for the justification of formal analysis,” in Proc. Certification of Safety-critical Software Controlled Sys. (SafeCert), Mar. 2008.
[20] W. Greenwell, J. Knight, C. M. Holloway, and J. Pease, “A taxonomy of fallacies in system safety arguments,” in Proc. Intl. Sys. Safety Conf., 2006.
[21] J. Elston, M. Stachura, B. Argrow, E. Frew, and C. Dixon, “Guidelines and Best Practices for FAA Certificate of Authorization Applications for Small Unmanned Aircraft,” in Proc. AIAA Infotech@Aerospace Conf., Mar. 2011.