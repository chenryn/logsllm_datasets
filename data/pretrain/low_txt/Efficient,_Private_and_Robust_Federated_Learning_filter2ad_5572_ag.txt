### Impact of the Fraction of Malicious Parties

Figures 6(c) and 6(d) illustrate the test errors for different proportions of malicious parties under label flipping and local model poisoning attacks. Our SecureFL framework can tolerate up to 80% of Byzantine parties while maintaining test errors comparable to those of FedAvg in a benign environment. Specifically, when the test error is less than 0.1, SecureFL can resist 95% of malicious parties in the case of label flipping attacks and 80% of malicious parties in the case of local model poisoning attacks. In contrast, existing Byzantine-robust federated learning (FL) frameworks, such as Krum, are more vulnerable. For example, under label flipping attacks, Krum can only withstand 40% of malicious parties, with a 5% sacrifice in test accuracy. Under local model poisoning attacks, even 10% of malicious parties can completely compromise the training process. Therefore, SecureFL demonstrates superior Byzantine robustness against a large number of malicious parties.

### Conclusion

In this paper, we introduce SecureFL, a new federated learning framework that simultaneously achieves full privacy protection, high scalability, and robustness against strong Byzantine attacks. SecureFL leverages crypto-friendly FL algorithms and customized cryptographic protocols to ensure efficient mathematical operations. We evaluated SecureFL on three real-world datasets and various neural network architectures against two types of recent Byzantine attacks, validating our claims.

The primary limitation of SecureFL is its adaptation of the FLTrust scheme to a privacy-preserving context, which inherits some constraints of the original scheme. Specifically, it requires a clean seed dataset collected by the service provider. Additionally, our techniques require two non-colluding servers and do not scale efficiently to a single-server setting, where malicious clients must be tolerated. Overcoming these limitations would necessitate the development of entirely different secure multi-party computation (MPC) protocols, which we leave for future work.

### Acknowledgments

We thank the anonymous reviewers and our shepherd, Shagufta Mehnaz, for their constructive comments. This work is supported by the National Natural Science Foundation of China under Grants 62020106013, 61972454, 61802051, 61772121, and 61728102, the Sichuan Science and Technology Program under Grants 2020JDTD0007 and 2020YFG0298, and the Fundamental Research Funds for Chinese Central Universities under Grant ZYGX2020ZB027.

### References

[1] Nitin Agrawal, Ali Shahin Shamsabadi, Matt J Kusner, and Adrià Gascón. 2019. QUOTIENT: Two-party secure neural network training and prediction. In Proceedings of ACM CCS. 1231–1247.
[2] Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al. 2017. Privacy-preserving deep learning via additively homomorphic encryption. IEEE Transactions on Information Forensics and Security 13, 5 (2017), 1333–1345.
[3] Gilad Asharov, Yehuda Lindell, Thomas Schneider, and Michael Zohner. 2013. More efficient oblivious transfer and extensions for faster secure computation. In Proceedings of ACM CCS. 535–548.
[4] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2020. How to backdoor federated learning. In Proceedings of AISTATS. 2938–2948.
[5] Moran Baruch, Gilad Baruch, and Yoav Goldberg. 2019. A little is enough: Circumventing defenses for distributed learning. In Proceedings of NeuIPS.
[6] Donald Beaver. 1991. Efficient multiparty protocols using circuit randomization. In Proceedings of CRYPTO. 420–432.
[7] James Henry Bell, Kallista A Bonawitz, Adrià Gascón, Tancrède Lepoint, and Mariana Raykova. 2020. Secure single-server aggregation with (poly) logarithmic overhead. In Proceedings of ACM CCS. 1253–1269.
[8] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. 2019. Analyzing federated learning through an adversarial lens. In Proceedings of ICML. 634–643.
[9] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. 2017. Machine learning with adversaries: Byzantine tolerant gradient descent. In Proceedings of NeurIPS. 118–128.
[10] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Practical secure aggregation for privacy-preserving machine learning. In Proceedings of ACM CCS. 1175–1191.
[11] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. 2021. FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping. In Proceedings of NDSS.
[12] Hao Chen, Ilaria Chillotti, Yihe Dong, Oxana Poburinnaya, Ilya Razenshteyn, and M Sadegh Riazi. 2020. {SANNS}: Scaling up secure approximate k-nearest neighbors search. In Proceedings of USENIX Security. 2111–2128.
[13] Henry Corrigan-Gibbs and Dan Boneh. 2017. Prio: Private, robust, and scalable computation of aggregate statistics. In Proceedings of USENIX NSDI. 259–282.
[14] Daniel Demmler, Thomas Schneider, and Michael Zohner. 2015. ABY-A framework for efficient mixed-protocol secure two-party computation. In Proceedings of NDSS.
[15] Whitfield Diffie and Martin Hellman. 1976. New directions in cryptography. IEEE Transactions on Information Theory 22, 6 (1976), 644–654.
[16] El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Louis Alexandre Rouault. 2018. The Hidden Vulnerability of Distributed Learning in Byzantium. In Proceedings of ICML.
[17] Junfeng Fan and Frederik Vercauteren. 2012. Somewhat practical fully homomorphic encryption. IACR Cryptology ePrint Archive (2012).
[18] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local model poisoning attacks to Byzantine-robust federated learning. In Proceedings of USENIX Security. 1605–1622.
[19] FeatureCloud. [n.d.]. Transforming health care and medical research with federated learning. https://featurecloud.eu/about/our-vision/.
[20] Wei Gao, Shangwei Guo, Tianwei Zhang, Han Qiu, Yonggang Wen, and Yang Liu. 2021. Privacy-preserving collaborative learning with automatic transformation search. In Proceedings of CVPR. 114–123.
[21] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. 2020. Inverting Gradients–How easy is it to break privacy in federated learning? In Proceedings of NeurIPS.
[22] Hanieh Hashemi, Yongqin Wang, Chuan Guo, and Murali Annavaram. 2021. Byzantine-Robust and Privacy-Preserving Framework for FedML. In ICLR Workshop on Security and Safety in Machine Learning Systems.
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of IEEE CVPR. 770–778.
[24] Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. 2020. Secure Byzantine-robust machine learning. arXiv:2006.04747 (2020).
[25] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. 2017. Deep models under the GAN: Information leakage from collaborative deep learning. In Proceedings of ACM CCS. 603–618.
[26] Yuval Ishai, Joe Kilian, Kobbi Nissim, and Erez Petrank. 2003. Extending oblivious transfers efficiently. In Proceedings of CRYPTO. 145–161.
[27] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. 2018. GAZELLE: A low latency framework for secure neural network inference. In Proceedings of USENIX Security. 1651–1669.
[28] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. 2019. Advances and open problems in federated learning. arXiv:1912.04977 (2019).
[29] Youssef Khazbak, Tianxiang Tan, and Guohong Cao. 2020. MLGuard: Mitigating Poisoning Attacks in Privacy Preserving Distributed Collaborative Learning. In Proceedings of IEEE ICCCN. 1–9.
[30] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. Backpropagation applied to handwritten zip code recognition. Neural computation 1, 4 (1989), 541–551.
[31] Beibei Li, Yuhao Wu, Jiarui Song, Rongxing Lu, Tao Li, and Liang Zhao. 2021. DeepFed: Federated Deep Learning for Intrusion Detection in Industrial Cyber-Physical Systems. IEEE Transactions on Industrial Informatics 17, 8 (2021), 5615–5624.
[32] Jian Liu, Mika Juuti, Yao Lu, and Nadarajah Asokan. 2017. Oblivious neural network predictions via minionn transformations. In Proceedings of ACM CCS. 619–631.
[33] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks from decentralized data. In Proceedings of AISTATS. 1273–1282.
[34] Pratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, and Raluca Ada Popa. 2020. Delphi: A cryptographic inference service for neural networks. In Proceedings of USENIX Security. 2505–2522.
[35] Payman Mohassel, Mike Rosulek, and Ni Trieu. 2020. Practical privacy-preserving k-means clustering. Proceedings on Privacy Enhancing Technologies 2020, 4 (2020), 414–433.
[36] Payman Mohassel and Yupeng Zhang. 2017. SecureML: A system for scalable privacy-preserving machine learning. In Proceedings of IEEE S&P. 19–38.
[37] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In Proceedings of S&P. 739–753.
[38] Thien Duc Nguyen, Phillip Rieger, Hossein Yalame, Helen Möllering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Ahmad-Reza Sadeghi, Thomas Schneider, et al. 2021. FLGUARD: Secure and Private Federated Learning. arXiv preprint arXiv:2101.02281 (2021).
[39] Sundar Pichai. 2019. Privacy should not be a luxury good. The New York Times.
[40] Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem Rastogi, and Rahul Sharma. 2020. CrypTFlow2: Practical 2-party secure inference. In Proceedings of ACM CCS. 325–342.
[41] Virat Shejwalkar and Amir Houmansadr. 2021. Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning. In Proceedings of NDSS.
[42] Nigel P Smart and Frederik Vercauteren. 2014. Fully homomorphic SIMD operations. Designs, codes and cryptography 71, 1 (2014), 57–81.
[43] Jinhyun So, Başak Güler, and A Salman Avestimehr. 2020. Byzantine-resilient secure federated learning. IEEE Journal on Selected Areas in Communications (2020).
[44] Jo Van Bulck, Marina Minkin, Ofir Weisse, Daniel Genkin, Baris Kasikci, Frank Piessens, Mark Silberstein, Thomas F Wenisch, Yuval Yarom, and Raoul Strackx. 2018. Foreshadow: Extracting the keys to the Intel SGX kingdom with transient out-of-order execution. In Proceedings of USENIX Security. 991–1008.
[45] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. 2020. Fall of empires: Breaking Byzantine-tolerant SGD by inner product manipulation. In Uncertainty in Artificial Intelligence. 261–270.
[46] Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, and Xiaodong Lin. 2019. VerifyNet: Secure and verifiable federated learning. IEEE Transactions on Information Forensics and Security 15 (2019), 911–926.
[47] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Françoise Beaufays. 2018. Applied federated learning: Improving Google keyboard query suggestions. arXiv:1812.02903 (2018).
[48] Andrew C Yao. 1982. Theory and application of trapdoor functions. In Proceedings of FOCS. 80–91.