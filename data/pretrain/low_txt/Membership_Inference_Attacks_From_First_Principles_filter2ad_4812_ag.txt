以下是经过优化后的文本，使其更加清晰、连贯和专业：

---

**参考文献**

1. Michael Backes, Emiliano De Cristofaro, Mario Fritz, and Yang Zhang. *ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models*. arXiv preprint arXiv:2102.02551, 2021.

2. Yunhui Long, Vincent Bindschaedler, and Carl A Gunter. *Towards Measuring Membership Privacy*. arXiv preprint arXiv:1712.09136, 2017.

3. Yunhui Long, Lei Wang, Diyue Bu, Vincent Bindschaedler, Xiaofeng Wang, Haixu Tang, Carl A Gunter, and Kai Chen. *A Pragmatic Approach to Membership Inferences on Machine Learning Models*. In 2020 IEEE European Symposium on Security and Privacy (EuroS&P), pages 521–534. IEEE, 2020.

4. Ilya Loshchilov and Frank Hutter. *SGDR: Stochastic Gradient Descent with Warm Restarts*. arXiv preprint arXiv:1608.03983, 2016.

5. Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. *Exploiting Unintended Feature Leakage in Collaborative Learning*. In 2019 IEEE Symposium on Security and Privacy (SP), pages 691–706. IEEE, 2019.

6. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. *Pointer Sentinel Mixture Models*. arXiv preprint arXiv:1609.07843, 2016.

7. Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras. *Spam Filtering with Naive Bayes – Which Naive Bayes?* In CEAS, volume 17, pages 28–69, 2006.

8. Sasi Kumar Murakonda and Reza Shokri. *ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning*. arXiv preprint arXiv:2007.09339, 2020.

9. Sasi Kumar Murakonda, Reza Shokri, and George Theodorakopoulos. *Ultimate Power of Inference Attacks: Privacy Risks of Learning High-Dimensional Graphical Models*. arXiv e-prints, pages arXiv–1905, 2019.

10. Sasi Kumar Murakonda, Reza Shokri, and George Theodorakopoulos. *Quantifying the Privacy Risks of Learning High-Dimensional Graphical Models*. In International Conference on Artificial Intelligence and Statistics, pages 2287–2295. PMLR, 2021.

11. Milad Nasr, Reza Shokri, and Amir Houmansadr. *Machine Learning with Membership Privacy Using Adversarial Regularization*. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, pages 634–646, 2018.

12. Milad Nasr, Reza Shokri, and Amir Houmansadr. *Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-Box Inference Attacks Against Centralized and Federated Learning*. In 2019 IEEE Symposium on Security and Privacy (SP), pages 739–753. IEEE, 2019.

13. Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini. *Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning*. arXiv preprint arXiv:2101.04535, 2021.

14. Jerzy Neyman and Egon Sharpe Pearson. *On the Problem of the Most Efficient Tests of Statistical Hypotheses*. Philosophical Transactions of the Royal Society of London., 231(694-706): 289–337, 1933.

15. Patrick Pantel and Dekang Lin. *SpamCop: A Spam Classification & Organization Program*. In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, pages 95–98, 1998.

16. Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Úlfar Erlingsson. *Scalable Private Learning with PATE*. arXiv preprint arXiv:1802.08908, 2018.

17. Huy Phan. *huyvnphan/pytorch cifar10*, January 2021. URL: https://doi.org/10.5281/zenodo.4431043.

18. Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. *Knock Knock, Who’s There? Membership Inference on Aggregate Location Data*. arXiv preprint arXiv:1708.06145, 2017.

19. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. *Language Models are Unsupervised Multitask Learners*. OpenAI Blog, 2019.

20. Shadi Rahimian, Tribhuvanesh Orekondy, and Mario Fritz. *Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries*. arXiv preprint arXiv:2009.00395, 2020.

21. Md Atiqur Rahman, Tanzila Rahman, Robert Laganière, Noman Mohammed, and Yang Wang. *Membership Inference Attack Against Differentially Private Deep Learning Model*. Trans. Data Priv., 11(1):61–79, 2018.

22. Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Hervé Jégou. *White-Box vs Black-Box: Bayes Optimal Strategies for Membership Inference*. In International Conference on Machine Learning, pages 5558–5567. PMLR, 2019.

23. Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. *ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models*, 2018.

24. Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario Fritz, and Yang Zhang. *Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning*. In 29th USENIX Security Symposium (USENIX Security 20), pages 1291–1308, 2020.

25. Sriram Sankararaman, Guillaume Obozinski, Michael I Jordan, and Eran Halperin. *Genomic Privacy and Limits of Individual Detection in a Pool*. Nature Genetics, 41(9):965–967, 2009.

26. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. *Membership Inference Attacks Against Machine Learning Models*. arXiv preprint arXiv:1610.05820, 2016.

27. Liwei Song and Prateek Mittal. *Systematic Evaluation of Privacy Risks of Machine Learning Models*. In 30th USENIX Security Symposium (USENIX Security 21), 2021.

28. Liwei Song, Reza Shokri, and Prateek Mittal. *Privacy Risks of Securing Machine Learning Models Against Adversarial Examples*. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 241–257, 2019.

29. Shuang Song. *Introducing TensorFlow Privacy Testing Library*. https://blog.tensorflow.org/2020/06/introducing-new-privacy-testing-library.html, 2020.

30. Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. *Stochastic Gradient Descent with Differentially Private Updates*. In 2013 IEEE Global Conference on Signal and Information Processing, pages 245–248. IEEE, 2013.

31. Thomas Steinke and Jonathan Ullman. *The Pitfalls of Average-Case Differential Privacy*. DifferentialPrivacy.org, July 2020. URL: https://differentialprivacy.org/average-case-dp/.

32. Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. *Towards Demystifying Membership Inference Attacks*. arXiv preprint arXiv:1807.09173, 2018.

33. David A Van Dyk and Xiao-Li Meng. *The Art of Data Augmentation*. Journal of Computational and Graphical Statistics, 10(1):1–50, 2001.

34. Lauren Watson, Chuan Guo, Graham Cormode, and Alex Sablayrolles. *On the Importance of Difficulty Calibration in Membership Inference Attacks*. arXiv preprint arXiv:2111.08440, 2021.

35. Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, and Reza Shokri. *Enhanced Membership Inference Attacks Against Machine Learning Models*. arXiv preprint arXiv:2111.09679, 2021.

36. Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. *Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting*. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pages 268–282. IEEE, 2018.

37. Sergey Zagoruyko and Nikos Komodakis. *Wide Residual Networks*. arXiv preprint arXiv:1605.07146, 2016.

38. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. *Understanding Deep Learning (Still) Requires Rethinking Generalization*. Communications of the ACM, 64(3):107–115, 2021.

39. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. *Random Erasing Data Augmentation*. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13001–13008, 2020.

---

### 附录 A：附加实验

#### A. 攻击 DP-SGD
差分隐私（Differential Privacy, DP）是防御隐私攻击的主要机制之一，包括针对机器学习模型的成员推理攻击。差分隐私提供了一个上界，限制了任何成员推理攻击的成功率。近期的研究 [24, 47] 使用成员推理攻击来实证审计差分隐私的边界，特别是那些通过 DP-SGD [1] 获得的边界。在本工作中，我们感兴趣的是 DP-SGD 对我们的成员推理攻击性能的影响。

我们在评估中考虑了不同组合的 DP-SGD 的噪声乘数和剪裁范数参数。表 IV 总结了使用 DP-SGD 在 CIFAR-10 上训练的标准 CNN 模型在不同参数设置下的平均准确率。我们在图 14 中评估了这些设置下我们成员推理攻击的有效性。即使只是剪裁梯度范数而不添加任何噪声也会显著降低我们攻击的性能。然而，较小的剪裁范数会降低模型的准确率，如表 IV 所示。

**表 IV：使用 DP-SGD 在 CIFAR-10 上训练的模型在不同噪声参数下的准确率**

| 噪声乘数 (σ) | C = 10 | C = 5  | C = 1  |
|--------------|--------|--------|--------|
| 0.0          | 61.3%  | 62.8%  | 61.3%  |
| 0.2          | 84.0%  | 73.9%  | 36.9%  |
| 0.8          | 78.5%  | 77.1%  | 43.3%  |

对于较高的剪裁范数，添加非常少量的噪声（图 14-b）可以将成员推理攻击的有效性降低到随机水平，同时得到更高准确率的模型。

使用非常少量的噪声进行模型训练是一种有效的防御措施，尽管会导致非常大的可证明的 DP 边界 ε。

**图 14：使用 DP-SGD 防御我们的攻击在不同隐私预算下的效果**

#### B. 白盒攻击
先前的工作 [46, 62] 表明，如果对手具有白盒访问权限，则可以实现更好的成员推理。具体来说，先前的工作表明，使用目标点处模型梯度的范数可以提高成员推理攻击的平衡准确率。图 15 突出了白盒和黑盒对手之间的比较。结果表明，使用梯度范数可以提高在线攻击的整体 AUC，以及使用全局阈值时的情况。然而，在较低的假阳性率下，我们没有观察到使用梯度范数比仅使用模型置信度有任何改进。

**图 15：我们的方法与黑盒设置中的白盒攻击对比**

#### C. 高斯分布拟合的完整 ROC 曲线
在图 17 中，我们展示了第 VI-B 节实验中完整（对数尺度）ROC 曲线，该实验探讨了影子模型数量对在线攻击成功率的影响。我们将影子模型的数量从 4 变化到 256，并考虑两种攻击变体：（1）通过估计每个样本的均值 µin, µout 和方差 σ²in, σ²out 来拟合高斯分布；（2）估计每个样本的均值 µin, µout，但估计全局方差 σ²in, σ²out。正如我们在第 VI-B 节中观察到的，当影子模型数量较少（<64）时，估计每个样本的方差效果较差。使用全局方差估计，攻击表现几乎与最佳攻击相当，只需 16 个影子模型即可。

**图 17：改变影子模型数量对攻击成功率的影响。始终有用的是估计每个样本的难度；然而，当只有少量模型可用时，为所有样本分配相同的方差要有效得多。**

### 附录 B：附加图表和表格

#### A. 攻击性能与模型准确率
在第 V-D 节和图 7 中，我们绘制了模型的训练-测试差距与其对成员推理攻击的脆弱性之间的关系。在图 16 中，我们查看了相同模型的测试准确率与攻击成功率之间的关系。有一个明显趋势，即更好的模型更容易受到攻击。先前的工作报告了类似的现象，用于数据提取攻击 [3, 4]。

**图 16：攻击真阳性率与模型测试准确率的关系**

#### B. 与其他工作的比较
类似于图 1 中的 CIFAR-10，我们在其他数据集上将我们的攻击与先前的成员推理攻击进行了比较：CIFAR-100 在图 18 中，WikiText-103 在图 19 中，Texas 在图 20 中，Purchase 在图 21 中。

**图 18：CIFAR-100 上的 ROC 曲线，比较了我们的攻击与先前的成员推理攻击。**

**图 19：WikiText-103 上的 ROC 曲线，比较了我们的攻击与先前的成员推理攻击。我们省略了依赖于模型特征 z(x) 的先前攻击，因为这些攻击不是为序列模型设计的。**

**图 20：Texas 数据集上的 ROC 曲线，比较了我们的攻击与先前的成员推理攻击。**

**图 21：Purchase 数据集上的 ROC 曲线，比较了我们的攻击与先前的成员推理攻击。**

---

希望这些修改能使你的文档更加清晰和专业。如果有任何进一步的需求，请告诉我！