### Persistent Memory Implementation and Disk Interface

A file system-level implementation of Persistent Memory (PM) offers the same application transparency benefits as a file buffer cache. In this setup, files that require persistence are stored on the PM disk, while non-critical files can use traditional disks with write-back behavior. This approach also simplifies transaction granularity through natural blocking, although the size of disk blocks may not be suitable for all applications.

The need to modify commercial operating systems is minimized in this realization. Most of the additional functionality required can be implemented as a set of disk drivers, which is a well-understood problem that avoids the complexities associated with non-standard buffer caches or file systems. Additionally, the PM disk can support any vendor or industry file systems layered on top of it.

For these reasons of transparency and minimal impact on the operating system, we chose the Disk Interface as the point to implement Persistent Memory for the DNCP platform.

### Persistent Memory Architecture

The architecture of the Persistent Memory (PM) system is depicted in Figure 1. The components identified in bold-italic type are part of the PM project.

#### Memory Layout
- **PM Region**: A continuous area of physical and virtual memory.
- **Size**: Fixed during normal operation; can only be expanded by rebooting the system.
- **PM Segments**: Allocated from the PM region. The first segment, known as the base segment, holds PM metadata. Additional segments can be defined as long as sufficient memory is available in the PM region.

#### Basic Components
- **pminit**: Senses the PM configuration from the bootloader and allocates the appropriate range of physical memory for the PM region. It is called twice during the boot sequence: once to allocate physical memory and again to set up equivalent virtual mappings, preserving the contents and virtual address location across reboots.
- **Base Kernel Hooks**: Small modifications to the HP-UX startup code that call pminit early in the boot sequence if persistent memory is installed.

#### pmcore
- **Implementation**: Dynamically loadable kernel module (DLKM).
- **Functionality**: Manages the persistent memory region and allocates segments to clients. It supports segment creation, destruction, resizing, and read/write operations. It also implements the base segment for PM metadata, using checksums and replicated data for reliability.

#### Block Copy Interface (pmbc)
- **Implementation**: DLKM.
- **Functionality**: Provides a block access abstraction from pmcore. It can be called from the PM EUM Disk Driver or directly from user programs via the PM management library. It supports creation and destruction of block-type segments, transactional interfaces for accessing persistent memory, and a reader-writer locking scheme for synchronization. Per-block checksums are computed after each write to detect corruption.

#### RAM Disk Interface (pmramd)
- **Implementation**: DLKM.
- **Functionality**: Provides disk I/O semantics to user programs. It supports standard disk driver entry points, allowing any type of filesystem to be mounted on pmramd.

#### PM Management Library (libpm)
- **Functionality**: Provides user-friendly interfaces to both pmcore and pmbc. It is used by the PM administration utility (pmadmin) for managing PM, including changing the region size.

### Performance Evaluation

#### Failure-Free Performance
- **Benchmark**: TimesTen main memory database with 20,000 rows.
- **Tests**: Non-durable mode, durable mode with disk, and durable mode with persistent memory.
- **Results**: Enabling durable mode significantly reduces performance. Using the PM ramdisk function improves performance to within 16% of non-durable mode, representing a 38% execution time savings over the durable mode with disk.

#### Failure Recovery Performance
- **Metric**: Time to recover state from disk vs. persistent memory.
- **Experiments**:
  - **mmap(1)**: Mapped application data into memory and touched each page to ensure availability.
  - **File System Reads**: Read data into application memory.
- **Data Size**: 100MB datafile.
- **Results**: Substantial relative savings, but the raw wall clock saving is not significant compared to the total cost of a system reboot (about ten minutes with HP-UX).

#### Discussion
- **Availability Improvement**: Not significant for recovery from system software failures but more impactful for application failures.
- **Performance Gains**: More substantial in the failure-free scenario, especially in environments where software failures are infrequent.

### Conclusions and Ongoing Work

We aimed to solve an availability problem related to long application restart times due to database reloads, but ended up solving a performance problem for other customers. The gains in durable update performance are significant and enable improved throughput in key applications.

As processor performance continues to outstrip disk performance, moving stable storage from disk to memory will be crucial for designing future systems. Ongoing work includes exploring a memory-based realization with the integration of an MMDB and adding protection to the memory region using separate PA-RISC space identifiers.

### Acknowledgements

We thank the entire development organization at Lucent San Jose Labs, particularly Deepak Gupta, Derek Godfrey, and Dave Clough. We are also grateful to Robert Cooper for initiating this project and Peter Chen for his early feedback.

### References

[1] M. Baker and M. Sullivan. "The recovery box: Using fast recovery to provide high availability in the UNIX environment." Proceedings of the Summer USENIX Conference, June 1992.

[2] K. Birman and R. van Renesse, editors. "Reliable Distributed Computing with the Isis Toolkit." IEEE Computer Society Press, 1994.

[3] P. Bohannon, D. Lieuwen, R. Rastogi, S. Seshadri, A. Silberschatz, and S. Sudarshan. "The architecture of the Dali main-memory storage manager." Journal of Multimedia Tools and Applications, 4(2), 1997.

[4] P. M. Chen, W. T. Ng, S. Chandra, C. Ayock, G. Rajamani, and D. Lowell. "The Rio file cache: Surviving operating system crashes." 1996 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), October 1996.

[5] D. J. DeWitt, R. H. Katz, E. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. "Implementation techniques for main memory database systems." Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data, pages 1-8, June 1984.