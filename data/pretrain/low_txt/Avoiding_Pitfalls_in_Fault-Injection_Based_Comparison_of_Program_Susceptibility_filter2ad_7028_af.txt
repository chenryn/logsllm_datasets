### Without Challenging the Possibility of Inaccuracy in High-Level Fault Injection

While it is possible that high-level fault injection (FI) may indeed be inaccurate, the fault-coverage metric used, with its varying fault-space size quotients, could significantly contribute to this error. This suggests a reevaluation of the obtained result data using our comparison metric.

### VII. Related Work

Over time, several metrics have been developed for assessing fault-tolerance mechanisms. The classic fault-coverage factor metric, introduced by Bouricius et al. [29], provides a mathematical model that has been widely adopted and instantiated by subsequent approaches. This metric is detailed in Section III-B. Arlat et al. [12] initially defined fault injection as a practical method for measuring fault coverage, leading most FI tools to provide a way to map their results to this metric [13], [14], [15].

Reis et al. [41] recognized the need for a metric that adequately captures the tradeoff between performance and reliability in software-based fault tolerance techniques. They developed the Mean Work To Failure (MWTF) metric, based on an application-specific definition of "work units" and FI measurements. Unlike our metric, MWTF is based on measuring the Architectural Vulnerability Factor (AVF) [42], implying a constant Δm. The authors do not derive the connection between MWTF and P(Failure) or relate it to common practices in the field. More recently, Santini et al. [43] introduced a similar Mean Workload Between Failures (MWBF) metric, parameterized with results from radiation measurements.

Several other metrics do not explicitly account for the performance/reliability tradeoff. Many are based on dynamic analysis techniques like FI but abstract from low-level details to provide more information for guiding software development. For example, Hiller et al. [44] use their EPIC framework to analyze error propagation in modular software, detecting the most exposed modules and signals using permeability and exposure metrics. Johansson and Suri [45] extend this approach to the dynamic behavior of operating systems. Similarly, Gawkowski and Sosnowski [46] use FI to trace fault propagation across multiple levels, from logic to the application level.

Mukherjee et al. [42] introduced the Architectural Vulnerability Factor (AVF), a classic static fault-tolerance assessment metric. Using low-level simulations, they measured the reliability of microarchitectural structures. On the software level, Sridharan et al. [47] developed the Program Vulnerability Factor, which is independent of expert knowledge on the microarchitecture. Both AVF and PVF weight their results by observed data lifetimes, avoiding Pitfall 1 (Section III-D). Benso et al. [48] created a high-level data criticality metric to determine the probability that each variable propagates an error to the program's output. More recently, Rehman et al. [49] proposed the Application Vulnerability Index (AVI), composed of values from their Function Vulnerability Index (FVI) and recursively their Instruction Vulnerability Index (IVI), derived similarly to Mukherjee’s AVF [42]. Based on their metrics, they control reliability optimization passes in a compiler. At an even more abstract level, Oz et al. [50] analyze multithreaded applications with the Thread Vulnerability Factor (TVF).

### VIII. Conclusions

After a step-by-step analysis of current practices in software-implemented FI, we identified three common pitfalls in interpreting FI result data for comparing program susceptibility to soft errors in memory. Using a real-world dataset, we demonstrated that each pitfall can independently skew or invalidate the analysis, leading to incorrect conclusions about the effectiveness of software-based fault-tolerance mechanisms.

Specifically, we highlighted that:
1. Special care must be taken when processing FI results after def/use fault-space pruning.
2. Sampling combined with def/use pruning must account for different equivalence-class sizes.
3. The widely used fault-coverage metric is inadequate for comparing different benchmark variants.

As a remedy, we derived an objective comparison metric that can be calculated both with full fault-space scans and from sampling results: absolute failure counts, extrapolated to the fault-space size in the case of sampling.

For each pitfall, we found FI studies that are likely affected. The use of the fault-coverage metric for benchmark comparison is widespread, and the examples cited in Section V-B are not unique. Although many described software-based hardware fault-tolerance mechanisms would likely still be effective, we suggest reevaluating them with our comparison metric to identify those that actually decrease fault tolerance. A recent study by Shrivastava et al. [51] using the AVF metric [42] surprisingly showed that five control-flow checking schemes, claimed effective by their original authors, actually increase system vulnerability.

In future work, we plan to investigate different fault models, compare simulation-obtained results of our metric to radiation measurements, and evaluate and improve existing software-based hardware fault-tolerance mechanisms.

### Acknowledgments

We thank our anonymous reviewers for their helpful and encouraging comments. We also thank Michael Engel for his detailed comments and his suggestion for naming the "dilution delusion" (Section IV-B). This work was partly supported by the German Research Foundation (DFG) priority program SPP 1500 under grant no. SP 968/5-3.

### References

[1] D. Binder, E. Smith, and A. Holman, “Satellite anomalies from galactic cosmic rays,” IEEE TNS, vol. 22, no. 6, pp. 2675–2680, Dec. 1975.
[2] T. C. May and M. H. Woods, “Alpha-particle-induced soft errors in dynamic memories,” IEEE Transactions on Electron Devices, vol. 26, no. 1, pp. 2–9, Jan. 1979.
[3] S. Mukherjee, Architecture Design for Soft Errors. San Francisco, CA, USA: Morgan Kaufmann, 2008.
[4] E. Fuchs, “An evaluation of the error detection mechanisms in MARS using software-implemented fault injection,” in 2nd Europ. Depend. Comp. Conf. (EDCC ’96), A. Hlawiczka, J. G. Silva, and L. Simoncini, Eds. Springer, 1996, pp. 73–90.
[5] M. Rebaudengo, M. S. Reorda, M. Violante, and M. Torchiano, “A source-to-source compiler for generating dependable software,” in 1st IEEE Int. W’shop on Source Code Analysis and Manipulation, 2001, pp. 33–42.
[6] B. Nicolescu, Y. Savaria, and R. Velazco, “Software detection mechanisms providing full coverage against single bit-flip faults,” IEEE TNS, vol. 51, no. 6, pp. 3510–3518, Dec. 2004.
[7] G. Chen, M. Kandemir, N. Vijaykrishnan, and M. J. Irwin, “Object duplication for improving reliability,” in Proceedings of the 2006 Asia and South Pacific Design Automation Conference, ser. ASP-DAC ’06. Piscataway, NJ, USA: IEEE, 2006, pp. 140–145.
[8] C. Borchert, H. Schirmeier, and O. Spinczyk, “Generative software-based memory error detection and correction for operating system data structures,” in 43rd IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN ’13). IEEE, Jun. 2013.
[9] X. Li, M. C. Huang, K. Shen, and L. Chu, “A realistic evaluation of memory hardware errors and software system susceptibility,” in 2010 USENIX TC. Berkeley, CA, USA: USENIX, 2010.
[10] V. Sridharan and D. Liberty, “A study of DRAM failures in the field,” in Int. Conf. for High Perf. Computing, Networking, Storage and Analysis (SC ’12). Los Alamitos, CA, USA: IEEE, 2012, pp. 76:1–76:11.
[12] J. Arlat, M. Aguera, L. Amat, Y. Crouzet, J.-C. Fabre, J.-C. Laprie, E. Martins, and D. Powell, “Fault injection for dependability validation: A methodology and some applications,” IEEE TOSE, vol. 16, no. 2, pp. 166–182, Feb. 1990.
[11] J. A. Clark and D. K. Pradhan, “Fault injection: A method for validating computer-system dependability,” IEEE Comp., vol. 28, no. 6, pp. 47–56, Jun. 1995.
[13] M.-C. Hsueh, T. K. Tsai, and R. K. Iyer, “Fault injection techniques and tools,” IEEE Comp., vol. 30, no. 4, pp. 75–82, Apr. 1997.
[14] A. Benso and P. E. Prinetto, Fault injection techniques and tools for embedded systems reliability evaluation, ser. Frontiers in electronic testing. Boston, Dordrecht, London: Kluwer, 2003.
[15] H. Ziade, R. A. Ayoubi, and R. Velazco, “A survey on fault injection techniques,” The International Arab Journal of Information Technology, vol. 1, no. 2, pp. 171–186, 2004.
[16] J. Karlsson, P. Liden, P. Dahlgren, R. Johansson, and U. Gunneflo, “Using heavy-ion radiation to validate fault-handling mechanisms,” IEEE Micro, vol. 14, no. 1, pp. 8–23, Feb. 1994.
[17] J. Karlsson, P. Folkesson, J. Arlat, Y. Crouzet, G. Leber, and J. Reisinger, “Application of three physical fault injection techniques to the experimental assessment of the MARS architecture,” in Conf. on Dep. Comp. for Crit. App. (DCCA ’95). Washington, DC, USA: IEEE, 1995.
[18] S. K. Sastry Hari, S. V. Adve, H. Naeimi, and P. Ramachandran, “Relyzer: Exploiting application-level fault equivalence to analyze application resiliency to transient faults,” in 17th Int. Conf. on Arch. Support for Programming Languages and Operating Systems (ASPLOS ’12). New York, NY, USA: ACM, 2012, pp. 123–134.
[19] K. Parasyris, G. Tziantzoulis, C. D. Antonopoulos, and N. Bellas, “GemFI: A fault injection tool for studying the behavior of applications on unreliable substrates,” in 44th IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN ’14). IEEE, Jun. 2014, pp. 622–629.
[20] D. Skarin, R. Barbosa, and J. Karlsson, “GOOFI-2: A tool for experimental dependability assessment,” in 40th IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN ’10). Los Alamitos, CA, USA: IEEE, Jun./Jul. 2010, pp. 557–562.
[21] A. Avižienis, J.-C. Laprie, B. Randell, and C. Landwehr, “Basic concepts and taxonomy of dependable and secure computing,” IEEE TDSC, vol. 1, no. 1, pp. 11–33, Jan. 2004.
[22] A. Dixit and A. Wood, “The impact of new technology on soft error rates,” in IEEE Int’l Reliab. Physics Symp. (IRPS ’11), Apr. 2011, pp. 5B.4.1–5B.4.7.
[23] A. Massa, Embedded Software Development with eCos. Prentice Hall Professional Technical Reference, 2002.
[24] H. Schirmeier, M. Hoffmann, R. Kapitza, D. Lohmann, and O. Spinczyk, “FAIL*: Towards a versatile fault-injection experiment framework,” in 25th Int. Conf. on Arch. of Comp. Sys. (ARCS ’12), Workshop Proceedings, ser. Lecture Notes in Informatics, G. Mühl, J. Richling, and A. Herkersdorf, Eds., vol. 200. German Society of Informatics, Mar. 2012, pp. 201–210.
[25] K. S. Trivedi, Probability and Statistics with Reliability, Queuing, and Computer Science Applications, 2nd ed. Wiley, 2002.
[26] D. Powell, E. Martins, J. Arlat, and Y. Crouzet, “Estimators for fault tolerance coverage evaluation,” IEEE TC, vol. 44, no. 2, pp. 261–274, Feb. 1995.
[27] R. Leveugle, A. Calvez, P. Maistri, and P. Vanhauwaert, “Statistical fault injection: quantified error and confidence,” in 2009 Conf. on Design, Autom. & Test in Europe (DATE ’09). IEEE, 2009, pp. 502–506.
[28] W. G. Bouricius, W. C. Carter, and P. R. Schneider, “Reliability modeling techniques for self-repairing computer systems,” in 24th National Conference, ser. ACM ’69. New York, NY, USA: ACM, 1969, pp. 295–309.
[29] D. T. Smith, B. W. Johnson, J. A. Profeta, III, and D. G. Bozzolo, “A method to determine equivalent fault classes for permanent and transient faults,” in Annual Reliability and Maintainability Symposium. IEEE, Jan. 1995, pp. 418–424.
[30] J. Güthoff and V. Sieh, “Combining software-implemented and simulation-based fault injection into a single fault injection method,” in 25th Annual Int. Symp. on Fault-Tol. Comp. (FTCS ’95). IEEE, Jun. 1995, pp. 196–206.
[31] A. Benso, M. Rebaudengo, L. Impagliazzo, and P. Marmo, “Fault-list collapsing for fault-injection experiments,” in Annual Reliability and Maintainability Symposium. IEEE, Jan. 1998, pp. 383–388.
[32] L. Berrojo, I. Gonzalez, F. Corno, M. Reorda, G. Squillero, L. Entrena, and C. Lopez, “New techniques for speeding-up fault-injection campaigns,” in 2002 Conf. on Design, Autom. & Test in Europe (DATE ’02). IEEE, 2002, pp. 847–852.
[33] R. Barbosa, J. Vinter, P. Folkesson, and J. Karlsson, “Assembly-level pre-injection analysis for improving fault injection efficiency,” in 5th Europ. Depend. Comp. Conf. (EDCC ’05), vol. 3463. Springer, Apr. 2005, p. 246.
[34] J. Grinschgl, A. Krieg, C. Steger, R. Weiss, H. Bock, and J. Haid, “Efficient fault emulation using automatic pre-injection memory access analysis,” in 25th IEEE SoC Conf. (SOCC ’12). IEEE, 2012, pp. 277–282.
[35] M. Hoffmann, C. Dietrich, and D. Lohmann, “Failure by design: Influence of the RTOS interface on memory fault resilience,” in 2nd GI W’shop on SW-Based Methods for Robust Embedded Sys. (SOBRES ’13), ser. Lecture Notes in Informatics. German Society of Informatics, Sep. 2013.
[36] R. Alexandersson and J. Karlsson, “Fault injection-based assessment of aspect-oriented implementation of fault tolerance,” in 41st IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN ’11). IEEE, Jun. 2011, pp. 303–314.
[37] C. Borchert, H. Schirmeier, and O. Spinczyk, “Protecting the dynamic dispatch in C++ by dependability aspects,” in 1st GI W’shop on SW-Based Methods for Robust Embedded Sys. (SOBRES ’12), ser. Lecture Notes in Informatics. German Society of Informatics, Sep. 2012, pp. 521–535.
[38] H. Cho, S. Mirkhani, C.-Y. Cher, J. A. Abraham, and S. Mitra, “Quantitative evaluation of soft error injection techniques for robust system design,” in 50th Design Autom. Conf. (DAC ’13). IEEE, May 2013.
[39] J. Wei, A. Thomas, G. Li, and K. Pattabiraman, “Quantifying the accuracy of high-level fault injection techniques for hardware faults,” in 44th IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN ’14). IEEE, Jun. 2014, pp. 375–382.
[40] G. A. Reis, J. Chang, N. Vachharajani, S. S. Mukherjee, R. Rangan, and D. I. August, “Design and evaluation of hybrid fault-detection systems,” in 32nd Int. Symp. on Comp. Arch. (ISCA ’05). IEEE, Jun. 2005, pp. 148–159.
[41] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt, and T. Austin, “A systematic methodology to compute the architectural vulnerability factors for a high-performance microprocessor,” in IEEE/ACM MICRO 36. Los Alamitos, CA, USA: IEEE, 2003.
[42] T. Santini, P. Rech, G. Nazar, L. Carro, and F. Rech Wagner, “Reducing embedded software radiation-induced failures through cache memories,” in 19th IEEE Europ. Test Symp. (ETS ’14), May 2014.
[43] M. Hiller, A. Jhumka, and N. Suri, “EPIC: Profiling the propagation and effect of data errors in software,” IEEE TC, vol. 53, no. 5, pp. 512–530, May 2004.
[44] A. Johansson and N. Suri, “Error propagation profiling of operating systems,” in 35th IEEE/IFIP Int. Conf. on Dep. Sys. & Netw. (DSN ’05), Jun./Jul. 2005, pp. 86–95.
[45] P. Gawkowski and J. Sosnowski, “Evaluation of transient fault susceptibility in microprocessor systems,” in Euromicro Symp. on Digital System Design (DSD ’04), Aug. 2004, pp. 432–439.
[46] V. Sridharan and D. R. Kaeli, “Eliminating microarchitectural dependency from architectural vulnerability,” in 15th IEEE Int. Symp. on High Performance Computer Architecture (HPCA ’09). IEEE, Feb. 2009, pp. 117–128.
[47] A. Benso, S. Di Carlo, G. Di Natale, P. E. Prinetto, and L. Tagliaferri, “Data criticality estimation in software applications,” in 2003 Int’l Test Conf. (ITC ’03). Los Alamitos, CA, USA: IEEE, 2003.
[48] S. Rehman, M. Shaﬁque, F. Kriebel, and J. Henkel, “Reliable software for unreliable hardware: Embedded code generation aiming at reliability,” in 9th IEEE/ACM Int. Conf. on HW/SW Codesign and Sys. Synth. (CODES+ISSS ’11), Taipei, Taiwan, Oct. 2011, pp. 237–246.
[49] I. Oz, H. R. Topcuoglu, M. Kandemir, and O. Tosun, “Examining thread vulnerability analysis using fault-injection,” in 21st Int. Conf. on Very Large Scale Integration (VLSI-SoC ’13). IEEE, Oct. 2013, pp. 240–245.
[50] A. Shrivastava, A. Rhisheekesan, R. Jeyapaul, and C.-J. Wu, “Quantitative analysis of control flow checking mechanisms for soft errors,” in 51st Design Autom. Conf. (DAC ’14). ACM, 2014.