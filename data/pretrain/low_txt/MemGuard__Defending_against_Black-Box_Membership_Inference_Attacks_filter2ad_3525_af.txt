### Inference Attacks

Inference attacks, such as white-box membership inference attacks, website fingerprinting attacks, and side-channel attacks, pose significant threats to the security and privacy of machine learning models.

### Acknowledgments

We would like to express our gratitude to the anonymous reviewers for their insightful feedback. We also thank Hao Chen from the University of California, Davis, for his valuable discussions. This work was partially supported by NSF grant No. 1937786.

### References

1. **Abadi, M., Chu, A., Goodfellow, I., McMahan, B., Mironov, I., Talwar, K., & Zhang, L. (2016).** Deep Learning with Differential Privacy. *Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 308–318.
2. **Ateniese, G., Felici, G., Mancini, L. V., Spognardi, A., Villani, A., & Vitali, D. (2013).** Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers. *CoRR abs/1306.4447.*
3. **Athalye, A., Carlini, N., & Wagner, D. A. (2018).** Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. *Proceedings of the 2018 International Conference on Machine Learning (ICML).* JMLR, 274–283.
4. **Backes, M., Berrang, P., Humbert, M., & Manoharan, P. (2016).** Membership Privacy in MicroRNA-based Studies. *Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 319–330.
5. **Backes, M., Humbert, M., Pang, J., & Zhang, Y. (2017).** walk2friends: Inferring Social Links from Mobility Profiles. *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 1943–1957.
6. **Bassily, R., Smith, A., & Thakurta, A. (2014).** Differentially Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds. *Proceedings of the 2014 Annual Symposium on Foundations of Computer Science (FOCS).* IEEE, 464–473.
7. **Cai, X., Zhang, X. C., Joshi, B., & Johnson, R. (2012).** Touching from a Distance: Website Fingerprinting Attacks and Defenses. *Proceedings of the 2012 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 605–616.
8. **Caliskan, A., Yamaguchi, F., Dauber, E., Harang, R., Rieck, K., Greenstadt, R., & Narayanan, A. (2018).** When Coding Style Survives Compilation: De-anonymizing Programmers from Executable Binaries. *Proceedings of the 2018 Network and Distributed System Security Symposium (NDSS).* Internet Society.
9. **Cao, X. & Gong, N. Z. (2017).** Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification. *Proceedings of the 2017 Annual Computer Security Applications Conference (ACSAC).* ACM, 278–287.
10. **Carlini, N. & Wagner, D. (2017).** Towards Evaluating the Robustness of Neural Networks. *Proceedings of the 2017 IEEE Symposium on Security and Privacy (S&P).* IEEE, 39–57.
11. **Chaabane, A., Acs, G., & Kaafar, M. A. (2012).** You Are What You Like! Information Leakage Through Users’ Interests. *Proceedings of the 2012 Network and Distributed System Security Symposium (NDSS).* Internet Society.
12. **Chaudhuri, K., Monteleoni, C., & Sarwate, A. D. (2011).** Differentially Private Empirical Risk Minimization. *Journal of Machine Learning Research.*
13. **Dwork, C., McSherry, F., Nissim, K., & Smith, A. (2006).** Calibrating Noise to Sensitivity in Private Data Analysis. *Proceedings of the 2006 Theory of Cryptography Conference (TCC).* Springer, 265–284.
14. **Fredrikson, M., Jha, S., & Ristenpart, T. (2015).** Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures. *Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 1322–1333.
15. **Fredrikson, M., Lantz, E., Jha, S., Lin, S., Page, D., & Ristenpart, T. (2014).** Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing. *Proceedings of the 2014 USENIX Security Symposium (USENIX Security).* USENIX, 17–32.
16. **Ganju, K., Wang, Q., Yang, W., Gunter, C. A., & Borisov, N. (2018).** Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations. *Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 619–633.
17. **Gong, N. Z. & Liu, B. (2016).** You are Who You Know and How You Behave: Attribute Inference Attacks via Users’ Social Friends and Behaviors. *Proceedings of the 2016 USENIX Security Symposium (USENIX Security).* USENIX, 979–995.
18. **Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014).** Generative Adversarial Nets. *Proceedings of the 2014 Annual Conference on Neural Information Processing Systems (NIPS).* NIPS.
19. **Goodfellow, I., Shlens, J., & Szegedy, C. (2015).** Explaining and Harnessing Adversarial Examples. *Proceedings of the 2015 International Conference on Learning Representations (ICLR).*
20. **Hagestedt, I., Zhang, Y., Humbert, M., Berrang, P., Tang, H., Wang, X. F., & Backes, M. (2019).** MBeacon: Privacy-Preserving Beacons for DNA Methylation Data. *Proceedings of the 2019 Network and Distributed System Security Symposium (NDSS).* Internet Society.
21. **Hayes, J., Melis, L., Danezis, G., & De Cristofaro, E. (2019).** LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks. *Symposium on Privacy Enhancing Technologies Symposium (2019).*
22. **Herrmann, D., Wendolsky, R., & Federrath, H. (2009).** Website Fingerprinting: Attacking Popular Privacy Enhancing Technologies with the Multinomial Naive-Bayes Classifier. *Proceedings of the 2009 ACM Cloud Computing Security Workshop (CCSW).* ACM, 31–41.
23. **Homer, N., Szelinger, S., Redman, M., Duggan, D., Tembe, W., Muehling, J., Pearson, J. V., Stephan, D. A., Nelson, S. F., & Craig, D. W. (2008).** Resolving Individuals Contributing Trace Amounts of DNA to Highly Complex Mixtures Using High-Density SNP Genotyping Microarrays. *PLOS Genetics (2008).*
24. **Iyengar, R., Near, J. P., Song, D. X., Thakkar, O. D., Thakurta, A., & Wang, L. (2019).** Towards Practical Differentially Private Convex Optimization. *Proceedings of the 2019 IEEE Symposium on Security and Privacy (S&P).* IEEE.
25. **Jayaraman, B. & Evans, D. (2014).** Evaluating Differentially Private Machine Learning in Practice. *Proceedings of the 2014 USENIX Security Symposium (USENIX Security).* USENIX, 1895–1912.
26. **Jia, J. & Gong, N. Z. (2018).** AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning. *Proceedings of the 2018 USENIX Security Symposium (USENIX Security).* USENIX.
27. **Jia, J. & Gong, N. Z. (2019).** Defending against Machine Learning based Inference Attacks via Adversarial Examples: Opportunities and Challenges. *CoRR abs/1909.08526 (2019).*
28. **Jia, J., Wang, B., Zhang, L., & Gong, N. Z. (2017).** AttriInfer: Inferring User Attributes in Online Social Networks Using Markov Random Fields. *Proceedings of the 2017 International Conference on World Wide Web (WWW).* ACM, 1561–1569.
29. **Juarez, M., Afroz, S., Acar, G., Diaz, C., & Greenstadt, R. (2014).** A Critical Evaluation of Website Fingerprinting Attacks. *Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 263–274.
30. **Kifer, D., Smith, A., & Thakurta, A. (2012).** Private Convex Optimization for Empirical Risk Minimization with Applications to High-dimensional Regression. *Proceedings of the 2012 Annual Conference on Learning Theory (COLT).* JMLR, 1–25.
31. **Kurakin, A., Goodfellow, I., & Bengio, S. (2016).** Adversarial Examples in the Physical World. *CoRR abs/1607.02533 (2016).*
32. **Liu, Y., Chen, X., Liu, C., & Song, D. (2016).** Delving into Transferable Adversarial Examples and Black-box Attacks. *CoRR abs/1611.02770 (2016).*
33. **Long, Y., Bindschaedler, V., & Gunter, C. A. (2017).** Towards Measuring Membership Privacy. *CoRR abs/1712.09136 (2017).*
34. **Long, Y., Bindschaedler, V., Wang, L., Bu, D., Wang, X., Tang, H., Gunter, C. A., & Chen, K. (2018).** Understanding Membership Inferences on Well-Generalized Learning Models. *CoRR abs/1802.04889 (2018).*
35. **Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2018).** Towards Deep Learning Models Resistant to Adversarial Attacks. *Proceedings of the 2018 International Conference on Learning Representations (ICLR).*
36. **Melis, L., Song, C., De Cristofaro, E., & Shmatikov, V. (2019).** Exploiting Unintended Feature Leakage in Collaborative Learning. *Proceedings of the 2019 IEEE Symposium on Security and Privacy (S&P).* IEEE.
37. **Meng, D. & Chen, H. (2017).** MagNet: A Two-Pronged Defense against Adversarial Examples. *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 135–147.
38. **Meng, X., Miller, B. P., & Jha, S. (2018).** Adversarial Binaries for Authorship Identification. *CoRR abs/1809.08316 (2018).*
39. **Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., & Frossard, P. (2017).** Universal Adversarial Perturbations. *Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).* IEEE, 1765–1773.
40. **Moosavi-Dezfooli, S.-M., Fawzi, A., & Frossard, P. (2016).** DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. *Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).* IEEE, 2574–2582.
41. **Narayanan, A., Paskov, H. S., Gong, N. Z., Bethencourt, J., Stefanov, E., Shin, E. C. R., & Song, D. (2012).** On the Feasibility of Internet-Scale Author Identification. *Proceedings of the 2012 IEEE Symposium on Security and Privacy (S&P).* IEEE, 300–314.
42. **Nasr, M., Shokri, R., & Houmansadr, A. (2018).** Machine Learning with Membership Privacy using Adversarial Regularization. *Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM.
43. **Nasr, M., Shokri, R., & Houmansadr, A. (2019).** Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning. *Proceedings of the 2019 IEEE Symposium on Security and Privacy (S&P).* IEEE.
44. **Oh, S. J., Augustin, M., Schiele, B., & Fritz, M. (2018).** Towards Reverse-Engineering Black-Box Neural Networks. *Proceedings of the 2018 International Conference on Learning Representations (ICLR).*
45. **Oya, S., Troncoso, C., & Pérez-González, F. (2017).** Back to the Drawing Board: Revisiting the Design of Optimal Location Privacy-preserving Mechanisms. *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 1943–1957.
46. **Panchenko, A., Niessen, L., Zinnen, A., & Engel, T. (2011).** Website Fingerprinting in Onion Routing Based Anonymization Networks. *Proceedings of the 2011 Workshop on Privacy in the Electronic Society (WPES).* ACM, 103–114.
47. **Papernot, N., McDaniel, P., & Goodfellow, I. (2016).** Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. *CoRR abs/1605.07277 (2016).*
48. **Papernot, N., McDaniel, P., Sinha, A., & Wellman, M. (2018).** SoK: Towards the Science of Security and Privacy in Machine Learning. *Proceedings of the 2018 IEEE European Symposium on Security and Privacy (Euro S&P).* IEEE.
49. **Papernot, N., McDaniel, P. D., Goodfellow, I., Jha, S., Celik, Z. B., & Swami, A. (2017).** Practical Black-Box Attacks Against Machine Learning. *Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security (ASIACCS).* ACM, 506–519.
50. **Papernot, N., McDaniel, P. D., Jha, S., Fredrikson, M., Celik, Z. B., & Swami, A. (2016).** The Limitations of Deep Learning in Adversarial Settings. *Proceedings of the 2016 IEEE European Symposium on Security and Privacy (Euro S&P).* IEEE, 372–387.
51. **Papernot, N., McDaniel, P. D., Wu, X., Jha, S., & Swami, A. (2016).** Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks. *Proceedings of the 2016 IEEE Symposium on Security and Privacy (S&P).* IEEE, 582–597.
52. **Pyrgelis, A., Troncoso, C., & De Cristofaro, E. (2018).** Knock Knock, Who’s There? Membership Inference on Aggregate Location Data. *Proceedings of the 2018 Network and Distributed System Security Symposium (NDSS).* Internet Society.
53. **Pyrgelis, A., Troncoso, C., & De Cristofaro, E. (2019).** Under the Hood of Membership Inference Attacks on Aggregate Location Time-Series. *CoRR abs/1902.07456 (2019).*
54. **Quiring, E., Maier, A., & Rieck, K. (2019).** Misleading Authorship Attribution of Source Code using Adversarial Learning. *Proceedings of the 2019 USENIX Security Symposium (USENIX Security).* USENIX, 479–496.
55. **Salem, A., Bhattacharya, A., Backes, M., Fritz, M., & Zhang, Y. (2019).** Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning. *CoRR abs/1904.01067 (2019).*
56. **Salem, A., Zhang, Y., Humbert, M., Berrang, P., Fritz, M., & Backes, M. (2019).** ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models. *Proceedings of the 2019 Network and Distributed System Security Symposium (NDSS).* Internet Society.
57. **Shokri, R. & Shmatikov, V. (2015).** Privacy-Preserving Deep Learning. *Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 1310–1321.
58. **Shokri, R., Stronati, M., Song, C., & Shmatikov, V. (2017).** Membership Inference Attacks Against Machine Learning Models. *Proceedings of the 2017 IEEE Symposium on Security and Privacy (S&P).* IEEE, 3–18.
59. **Song, L., Shokri, R., & Mittal, P. (2019).** Privacy Risks of Securing Machine Learning Models against Adversarial Examples. *Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM.
60. **Song, S., Chaudhuri, K., & Sarwate, A. D. (2013).** Stochastic Gradient Descent with Differentially Private Updates. *Proceedings of the 2013 IEEE Global Conference on Signal and Information Processing (GlobalSIP).* IEEE, 245–248.
61. **Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).** Dropout: A Simple Way to Prevent Neural Networks from Overfitting. *Journal of Machine Learning Research (2014).*
62. **Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2013).** Intriguing Properties of Neural Networks. *CoRR abs/1312.6199 (2013).*
63. **Tramèr, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., & McDaniel, P. (2017).** Ensemble Adversarial Training: Attacks and Defenses. *Proceedings of the 2017 International Conference on Learning Representations (ICLR).*
64. **Tramér, F., Zhang, F., Juels, A., Reiter, M. K., & Ristenpart, T. (2016).** Stealing Machine Learning Models via Prediction APIs. *Proceedings of the 2016 USENIX Security Symposium (USENIX Security).* USENIX, 601–618.
65. **Wang, B. & Gong, N. Z. (2018).** Stealing Hyperparameters in Machine Learning. *Proceedings of the 2018 IEEE Symposium on Security and Privacy (S&P).* IEEE.
66. **Wang, D., Ye, M., & Xu, J. (2017).** Differentially Private Empirical Risk Minimization Revisited: Faster and More General. *Proceedings of the 2017 Annual Conference on Neural Information Processing Systems (NIPS).* NIPS, 2722–2731.
67. **Wang, T., Cai, X., Nithyanand, R., Johnson, R., & Goldberg, I. (2014).** Effective Attacks and Provable Defenses for Website Fingerprinting. *Proceedings of the 2014 USENIX Security Symposium (USENIX Security).* USENIX, 143–157.
68. **Xu, W., Evans, D., & Qi, Y. (2018).** Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. *Proceedings of the 2018 Network and Distributed System Security Symposium (NDSS).* Internet Society.
69. **Yeom, S., Giacomelli, I., Fredrikson, M., & Jha, S. (2018).** Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting. *Proceedings of the 2018 IEEE Computer Security Foundations Symposium (CSF).* IEEE.
70. **Yu, L., Liu, L., Pu, C., Gursoy, M. E., & Truex, S. (2019).** Differentially Private Model Publishing for Deep Learning. *Proceedings of the 2019 IEEE Symposium on Security and Privacy (S&P).* IEEE.
71. **Zhang, X., Hamm, J., Reiter, M. K., & Zhang, Y. (2019).** Statistical Privacy for Streaming Traffic. *Proceedings of the 2019 Network and Distributed System Security Symposium (NDSS).* Internet Society.
72. **Zhang, Y., Humbert, M., Rahman, T., Li, C.-T., Pang, J., & Backes, M. (2018).** Tagvisor: A Privacy Advisor for Sharing Hashtags. *Proceedings of the 2018 Web Conference (WWW).* ACM, 287–296.
73. **Zhang, Y., Juels, A., Reiter, M. K., & Ristenpart, T. (2012).** Cross-VM Side Channels and Their Use to Extract Private Keys. *Proceedings of the 2012 ACM SIGSAC Conference on Computer and Communications Security (CCS).* ACM, 305–316.

### Synthesizing Non-Members

**Figure 7: Inference Accuracy of the NN Attack as the Confidence Score Distortion Budget Increases on the Location Dataset when Synthesizing Non-Members for Training the Defense Classifier (MemGuard-S)**

When training the defense classifier, we can use \( D_1 \) as members and synthesize non-members based on \( D_1 \). For each data sample in \( D_1 \) and each of its features, we keep the feature value with a probability of 0.9 and randomly sample a value from the corresponding data domain for the feature with a probability of 0.1, which synthesizes a non-member data sample. We then train the defense classifier using \( D_1 \) as members and the synthesized data samples as non-members. Figure 7 shows the comparison results on the Location dataset (binary features), where MemGuard-S is the scenario where we synthesize the non-members for training the defense classifier. We observe that MemGuard and MemGuard-S achieve similar performance. Our results show that MemGuard does not necessarily need to split the training dataset to train the defense classifier.