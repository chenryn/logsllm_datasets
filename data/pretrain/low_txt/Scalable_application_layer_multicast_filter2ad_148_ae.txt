# Distribution and Loss Analysis in Random Membership Change Phase

## Figures and Data
- **Figure 17: Fraction of Members Receiving Data Packets**
  - **Y-Axis:** Fraction of members that received data packets.
  - **X-Axis:** Time (in seconds).
  - **Data Points:** 0, 0.1, 0.2, ..., 1.0.
  - **Description:** This figure shows the fraction of members that successfully received data packets over time as group membership continuously changed.

- **Figure 15: Stretch Distribution (Testbed)**
  - **Y-Axis:** Cumulative distribution of stretch.
  - **X-Axis:** Stretch values.
  - **Data Points:** 0, 0.1, 0.2, ..., 1.0.
  - **Description:** This figure illustrates the distribution of stretch across different sites (A, B, C, D, E, F, G, H) in the testbed.

- **Figure 16: Latency Distribution (Testbed)**
  - **Y-Axis:** Cumulative distribution of latency.
  - **X-Axis:** Latency values.
  - **Data Points:** 0, 0.1, 0.2, ..., 1.0.
  - **Description:** This figure shows the distribution of latency across different sites (A, B, C, D, E, F, G, H) in the testbed.

- **Figure 18: Cumulative Distribution of Packet Losses**
  - **Y-Axis:** Fraction of packets lost.
  - **X-Axis:** Number of packet losses.
  - **Data Points:** 0, 0.01, 0.02, ..., 0.05.
  - **Description:** This figure presents the cumulative distribution of packet losses experienced by different members during the 15-minute rapid membership change phase.

## Failure Recovery

In this section, we describe the effects of group membership changes on the data delivery tree. To evaluate this, we observe how effectively the overlay network delivers data during topology changes. We measured the number of correctly received packets by different (remaining) members during the rapid membership change phase, which begins after the initial member set has stabilized into the appropriate overlay topology. This phase lasts for 15 minutes. Members join and leave the group at random, with an average member lifetime of 30 seconds.

- **Figure 17** plots the fraction of members that successfully received different data packets over time. A total of 30 group membership changes occurred during the experiment.
- **Figure 18** shows the cumulative distribution of packet losses experienced by different members over the entire 15-minute duration. The maximum number of packet losses seen by a member was 50 out of 900 (approximately 5.6%), and 30% of the members did not encounter any packet losses.

Even under these rapid changes, the longest continuous period of packet loss for any single host was 34 seconds, while typical members experienced a maximum continuous data loss for only two seconds. These failure recovery statistics are sufficient for most data stream applications deployed over the Internet. Notably, only three individual packets (out of 900) suffered heavy losses: data packets at times 76 s, 620 s, and 819 s were not received by 51, 36, and 31 members, respectively.

## Control Overheads

Finally, we present the control traffic overheads (in Kbps) in Table 2 for different group sizes. The overheads include both sent and received control packets. We show the average and maximum control overhead at any member. We observed that the control traffic at most members lies between 0.2 Kbps to 2.0 Kbps for different group sizes. In fact, about 80% of the members require less than 0.9 Kbps of control traffic for topology management. More interestingly, the average control overheads and distributions do not change significantly as the group size varies. The worst-case control overhead is also fairly low (less than 3 Kbps).

| Group Size | Mean Stretch | Max. Stretch | Mean Latency | Max. Latency | Mean Control Overhead (Kbps) | Max. Control Overhead (Kbps) |
|------------|--------------|--------------|--------------|--------------|------------------------------|------------------------------|
| 32         | 0.84         | 2.34         | 0.77         | 2.70         | 0.73                         | 2.65                         |
| 64         | 0.77         | 2.70         | 0.73         | 2.65         | 0.73                         | 2.65                         |
| 96         | 1.85         | 2.65         | 1.73         | 2.65         | 1.86                         | 2.65                         |

## Related Work

Several other projects have explored implementing multicast at the application layer. These can be classified into two broad categories: mesh-first (Narada [10], Gossamer [7]) and tree-first protocols (Yoid [12], ALMI [15], Host-Multicast [22]). Yoid and Host-Multicast define a distributed tree-building protocol between end-hosts, while ALMI uses a centralized algorithm to create a minimum spanning tree rooted at a designated single source of multicast data distribution. The Overcast protocol [14] organizes a set of proxies (called Overcast nodes) into a distribution tree rooted at a central source for single-source multicast. A distributed tree-building protocol is used to create this source-specific tree, similar to Yoid. RMX [8] provides support for reliable multicast data delivery to end-hosts using a set of similar proxies, called Reliable Multicast proXies. Application end-hosts are configured to affiliate themselves with the nearest RMX. The architecture assumes the existence of an overlay construction protocol, using which these proxies organize themselves into an appropriate data delivery path. TCP is used to provide reliable communication between each pair of peer proxies on the overlay.

Some recent projects (Chord [21], Content Addressable Networks (CAN) [17], Tapestry [23], and Pastry [19]) have also addressed the scalability issue in creating application-layer overlays and are closely related to our work. CAN defines a virtual d-dimensional Cartesian coordinate space, and each overlay host "owns" a part of this space. In [18], the authors leveraged the scalable structure of CAN to define an application-layer multicast scheme, where hosts maintain O(d) state and the path lengths are O(dN^(1/d)) application-level hops, where N is the number of hosts in the network. Pastry [19] is a self-organizing overlay network of nodes, where logical peer relationships on the overlay are based on matching prefixes of the node identifiers. Scribe [6] is a large-scale event notification infrastructure that leverages the Pastry system to create groups and build efficient application-layer multicast paths to the group members for dissemination of events. Being based on Pastry, it has similar overlay properties, namely O(log_b N) state at members and O(log_b N) application-level hops between members. Bayeux [24] is another architecture for application-layer multicast, where the end-hosts are organized into a hierarchy as defined by the Tapestry overlay location and routing system [23]. A level of the hierarchy is defined by a set of hosts that share a common suffix in their host IDs. Such a technique was proposed by Plaxton et al. [16] for locating and routing to named objects in a network. Therefore, hosts in Bayeux maintain O(b log_b N) state and end-to-end overlay paths have O(log_b N) application-level hops.

As discussed in Section 2.3, our proposed NICE protocol incurs an amortized O(k) state at members and the end-to-end paths between members have O(log_k N) application-level hops. Like Pastry and Tapestry, NICE also chooses overlay peers based on network locality, leading to low-stretch end-to-end paths.

We summarize the above as follows: For both NICE and CAN-multicast, members maintain constant state for other members and consequently exchange a constant amount of periodic refresh messages. This overhead is logarithmic for Scribe and Bayeux. The overlay paths for NICE, Scribe, and Bayeux have a logarithmic number of application-level hops, and path lengths in CAN-multicast asymptotically have a larger number of application-level hops. Both NICE and CAN-multicast use a single well-known host (the RP, in our nomenclature) to bootstrap the join procedure of members. The join procedure, therefore, incurs a higher overhead at the RP and the higher layers of the hierarchy than the lower layers. Scribe and Bayeux assume members can find different "nearby" members on the overlay through out-of-band mechanisms, from which to bootstrap the join procedure. Using this assumption, the join overheads for a large number of joining members can be amortized over the different such "nearby" bootstrap members in these schemes.

## Conclusions

In this paper, we presented a new protocol for application-layer multicast. Our main contribution is an extremely low-overhead hierarchical control structure over which different data distribution paths can be built. Our results show that it is possible to build and maintain application-layer multicast trees with very little overhead. While the focus of this paper has been on low-bandwidth data stream applications, our scheme is generalizable to different applications by appropriately choosing data paths and metrics used to construct the overlays. We believe that the results of this paper are a significant first step towards constructing large wide-area applications over application-layer multicast.

## Acknowledgments

We thank Srinivas Parthasarathy for implementing a part of the Narada protocol used in our simulation experiments. We also thank Kevin Almeroth, Lixin Gao, Jorg Liebeherr, Steven Low, Martin Reisslein, and Malathi Veeraraghavan for providing us with user accounts at the different sites for our wide-area experiments. We thank Peter Druschel for shepherding the submission of the final version of this paper.

## References

[1] D. Andersen, H. Balakrishnan, M. Frans Kaashoek, and R. Morris. Resilient overlay networks. In Proceedings of 18th ACM Symposium on Operating Systems Principles, Oct. 2001.

[2] T. Ballardie, P. Francis, and J. Crowcroft. Core Based Trees (CBT): An Architecture for Scalable Multicast Routing. In Proceedings of ACM Sigcomm, 1995.

[3] S. Banerjee and B. Bhattacharjee. Scalable Secure Group Communication over IP Multicast. In Proceedings of International Conference on Network Protocols, Nov. 2001.

[4] S. Banerjee, B. Bhattacharjee, and C. Kommareddy. Scalable application layer multicast. Technical report, UMIACS TR-2002-53 and CS-TR 4373, Department of Computer Science, University of Maryland, College Park, MD 20742, USA, May 2002.

[5] K. Calvert, E. Zegura, and S. Bhattacharjee. How to Model an Internetwork. In Proceedings of IEEE Infocom, 1996.

[6] M. Castro, P. Druschel, A.-M. Kermarrec, and A. Rowstron. SCRIBE: A large-scale and decentralized application-level multicast infrastructure. IEEE Journal on Selected Areas in Communications (JSAC), 2002. To appear.

[7] Y. Chawathe. Scattercast: An Architecture for Internet Broadcast Distribution as an Infrastructure Service. Ph.D. Thesis, University of California, Berkeley, Dec. 2000.

[8] Y. Chawathe, S. McCanne, and E. A. Brewer. RMX: Reliable Multicast for Heterogeneous Networks. In Proceedings of IEEE Infocom, 2000.

[9] Y.-H. Chu, S. G. Rao, S. Seshan, and H. Zhang. Enabling Conferencing Applications on the Internet using an Overlay Multicast Architecture. In Proceedings of ACM SIGCOMM, Aug. 2001.

[10] Y.-H. Chu, S. G. Rao, and H. Zhang. A Case for End System Multicast. In Proceedings of ACM SIGMETRICS, June 2000.

[11] S. Deering and D. Cheriton. Multicast Routing in Datagram Internetworks and Extended LANs. In ACM Transactions on Computer Systems, May 1990.

[12] P. Francis. Yoid: Extending the Multicast Internet Architecture, 1999. White paper http://www.aciri.org/yoid/.

[13] A. Gupta. Steiner points in tree metrics don't (really) help. In Symposium of Discrete Algorithms, Jan. 2001.

[14] J. Jannotti, D. Gifford, K. Johnson, M. Kaashoek, and J. Oâ€™Toole. Overcast: Reliable Multicasting with an Overlay Network. In Proceedings of the 4th Symposium on Operating Systems Design and Implementation, Oct. 2000.

[15] D. Pendarakis, S. Shi, D. Verma, and M. Waldvogel. ALMI: An Application Level Multicast Infrastructure. In Proceedings of 3rd Usenix Symposium on Internet Technologies & Systems, March 2001.

[16] C. G. Plaxton, R. Rajaraman, and A. W. Richa. Accessing nearby copies of replicated objects in a distributed environment. In ACM Symposium on Parallel Algorithms and Architectures, June 1997.

[17] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. A scalable content-addressable network. In Proceedings of ACM Sigcomm, Aug. 2001.

[18] S. Ratnasamy, M. Handley, R. Karp, and S. Shenker. Application-level multicast using content-addressable networks. In Proceedings of 3rd International Workshop on Networked Group Communication, Nov. 2001.

[19] A. Rowstron and P. Druschel. Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems. In IFIP/ACM International Conference on Distributed Systems Platforms (Middleware), Nov. 2001.

[20] S. Savage, T. Anderson, A. Aggarwal, D. Becker, N. Cardwell, A. Collins, E. Hoffman, J. Snell, A. Vahdat, G. Voelker, and J. Zahorjan. Detour: A Case for Informed Internet Routing and Transport. IEEE Micro, 19(1), Jan. 1999.

[21] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and H. Balakrishnan. Chord: A scalable peer-to-peer lookup service for Internet applications. In Proceedings of ACM Sigcomm, Aug. 2001.

[22] B. Zhang, S. Jamin, and L. Zhang. Host multicast: A framework for delivering multicast to end users. In Proceedings of IEEE Infocom, June 2002.

[23] B. Y. Zhao, J. Kubiatowicz, and A. Joseph. Tapestry: An Infrastructure for Fault-tolerant Wide-area Location and Routing. Technical report, UCB/CSD-01-1141, University of California, Berkeley, CA, USA, Apr. 2001.

[24] S. Q. Zhuang, B. Y. Zhao, A. D. Joseph, R. Katz, and J. Kubiatowicz. Bayeux: An architecture for scalable and fault-tolerant wide-area data dissemination. In Eleventh International Workshop on Network and Operating Systems Support for Digital Audio and Video (NOSSDAV 2001), 2001.