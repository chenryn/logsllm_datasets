### Energy Computation in Fuzzing Iterations

In each fuzzing iteration, the energy of state \(i\) is calculated with respect to \(s(i)\). The LINEAR mode increases the energy of state \(i\) linearly, while the QUAD mode, which is based on the LINEAR mode, computes the energy using a quadratic function.

The energy for fuzzing is computed as follows:
\[
p_l(i) = \min\left(\frac{\alpha(i)}{\beta} \cdot s(i), M\right)
\]
\[
p_d(i) = \min\left(\frac{\alpha(i)}{\beta} \cdot s(i)^2, M\right)
\]
where:
- \(p_l(i)\) and \(p_d(i)\) represent the linear and quadratic power schedules, respectively.
- \(M\) is a maximum threshold.
- \(\alpha(i)\) and \(\beta\) are parameters specific to the state \(i\).

The final energy \(q(i)\) is determined by:
\[
q(i) = \max(p(i), L)
\]
where \(p(i)\) can be either \(p_f(i)\), \(p_l(i)\), or \(p_d(i)\), and \(L\) is a lower bound set for AFLFast. This lower bound helps to mitigate the issue of cycle explosion, where the energy becomes very low as \(f(i)\) grows large.

### Evaluation on 90 Binaries

We evaluated our improved version, AFLFast+, on 90 binaries from a previous experiment. The results, shown in Figure 6, indicate that AFLFast+ and AFLFast have nearly the same efficiency. However, AFLFast+ discovered 88 bugs, compared to 77 bugs found by AFLFast. The two bugs not detected by AFLFast+ were due to time constraints, and no cycle explosion was observed. Therefore, AFLFast+ maintains the same efficiency but detects more bugs than AFLFast.

### Related Work

The performance of a fuzzer is largely determined by its ability to handle search-hampering features in the context of bugs. Many fuzzers [7, 9, 12, 15, 19–21, 25, 26] are designed to address one or several of these features to increase code coverage.

Researchers have made efforts to address the evaluation challenges by creating corpora based on real-world programs, such as student code [24], existing bug report databases [16], or public bug registries [3, 11]. Despite providing contextual details, these corpora remain static and relatively small for fuzzing evaluation. Other attempts include independently defined public benchmark suites like DaCapo [6] and SPEC [5], which, despite their large volume, require significant manual effort to triage crashes and filter bugs.

Synthetic corpora, such as those for buffer overflow detection [27, 28], LAVA and LAVA-M [10], DARPA CGC corpus [1], and NIST SAMATE project [4], ensure the existence of bugs but sacrifice realism. None of these synthetic corpora fully implement contexts of bugs related to search-hampering features, making them insufficient for comprehensive fuzzing evaluation.

### Discussion: Threats to Validity

Although FEData works well with AFL and AFLFast, it can be further improved. The following issues need to be addressed:

1. **Evaluation with Different Fuzzers**: We only ran AFL and AFLFast on FEData due to the unavailability or incompatibility of other fuzzers. To better assess FEData, we should run different fuzzers, such as Driller and T-Fuzz, which focus on resolving magic values and checksums, respectively.

2. **Realism of Generated Programs**: To make FEData more realistic, we can add functionality through functional programming or research the contexts behind bugs and incorporate them into FEData when inserting bugs.

### Conclusion

To evaluate fuzzing more effectively, we propose generating corpora based on search-hampering features. We designed a prototype corpus, FEData, to demonstrate the effectiveness of this approach. Evaluating AFL and AFLFast on FEData, we found that AFLFast finds execution paths faster than AFL. However, some programs in FEData highlight AFLFast's vulnerability to cycle explosion. By setting a lower bound on its energy strategies, AFLFast was improved to AFLFast+, which can find more bugs while maintaining the same efficiency.

### Acknowledgments

The authors would like to thank Dr. Toby Murray and Dr. Jianhai Chen for their valuable discussions on important issues.

### References

[1] Cyber Grand Challenge Corpus. 2017. http://www.lungetech.com/cgc-corpus/.

[2] American Fuzzy Lop. 2018. http://lcamtuf.coredump.cx/afl/.

[3] Fuzzer Test Suite. 2018. https://github.com/google/fuzzer-test-suite.

[4] National Institute of Standards and Technology. 2018. https://www.nist.gov.

[5] Standard Performance Evaluation Corporation. 2018. https://www.spec.org/benchmarks.html.

[6] Stephen M Blackburn, et al. 2006. The DaCapo benchmarks: Java benchmarking development and analysis. In ACM Sigplan Notices, Vol. 41(10). ACM, New York, NY, USA, 169–190.

[7] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-based greybox fuzzing as Markov chain. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, Vienna, Austria, 1032–1043.

[8] Peng Chen and Hao Chen. 2018. Angora: Efficient Fuzzing by Principled Search. In 2018 IEEE Symposium on Security and Privacy. IEEE, San Francisco, CA, USA, 711–725.

[9] Jake Corina, et al. 2017. Difuze: Interface-aware fuzzing for kernel drivers. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, Dallas, Texas, USA, 2123–2138.

[10] Brendan Dolan-Gavitt, et al. 2016. LAVA: Large-scale automated vulnerability addition. In Security and Privacy, 2016 IEEE Symposium on. IEEE, San Jose, CA, USA, 110–121.

[11] Jeffrey Foster. 2005. A call for a public bug and tool registry. In Workshop on the Evaluation of Software Defect Detection Tools.

[12] Shuitao Gan, et al. 2018. CollAFL: Path sensitive fuzzing. In 2018 IEEE Symposium on Security and Privacy. IEEE, San Francisco, CA, USA, 679–696.

[13] Istvan Haller, et al. 2013. Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations. In Presented as part of the 22nd USENIX Security Symposium (USENIX Security 13). USENIX, Washington, D.C., 49–64.

[14] George Klees, et al. 2018. Evaluating Fuzz Testing. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. ACM, Toronto, Canada, 2123–2138.

[15] Yuekang Li, et al. 2017. Steelix: Program-state based binary fuzzing. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. ACM, Paderborn, Germany, 627–637.

[16] Shan Lu, et al. 2005. Bugbench: Benchmarks for evaluating bug detection tools. In Workshop on the Evaluation of Software Defect Detection Tools, Vol. 5.

[17] Barton P Miller, Louis Fredriksen, and Bryan So. 1990. An empirical study of the reliability of UNIX utilities. Commun. ACM 33, 12 (1990), 32–44.

[18] Marius Muench, et al. 2018. What you corrupt is not what you crash: Challenges in fuzzing embedded devices. In 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018. The Internet Society, San Diego, CA, USA.

[19] Hui Peng, Yan Shoshitaishvili, and Mathias Payer. 2018. T-Fuzz: Fuzzing by program transformation. In 2018 IEEE Symposium on Security and Privacy. IEEE, San Francisco, CA, USA, 697–710.

[20] Van-Thuan Pham, Marcel Böhme, and Abhik Roychoudhury. 2016. Model-based whitebox fuzzing for program binaries. In Automated Software Engineering (ASE), 2016 31st IEEE/ACM International Conference on. IEEE, Singapore, Singapore, 543–553. https://doi.org/10.1145/2970276.2970316

[21] Sanjay Rawat, et al. 2017. Vuzzer: Application-aware evolutionary fuzzing. In 24th Annual Network and Distributed System Security Symposium, February 26 - March 1, 2017. The Internet Society, San Diego, California, USA.

[22] Sergej Schumilo, et al. 2017. kAFL: Hardware-assisted feedback fuzzing for OS kernels. In 26th USENIX Security Symposium (USENIX Security 17). USENIX Association, Vancouver, BC, 167–182.

[23] Koushik Sen. 2007. Concolic testing. In Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering. ACM, 571–572.

[24] Jaime Spacco, David Hovemeyer, and William Pugh. 2005. Bug specimens are important. In Workshop on the Evaluation of Software Defect Detection Tools.

[25] Nick Stephens, et al. 2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In 3rd Annual Network and Distributed System Security Symposium, NDSS 2016, San Diego, California, USA, February 21-24, 2016, Vol. 16. NDSS, The Internet Society, San Diego, California, USA, 1–16.

[26] Junjie Wang, et al. 2017. Skyfire: Data-driven seed generation for fuzzing. In 2017 IEEE Symposium on Security and Privacy. IEEE, San Jose, CA, USA, 579–594.

[27] John Wilander and Mariam Kamkar. 2003. A Comparison of Publicly Available Tools for Dynamic Buffer Overflow Prevention. In Proceedings of the Network and Distributed System Security Symposium, NDSS 2003, Vol. 3. The Internet Society, San Diego, California, USA, 149–162.

[28] Misha Zitser, Richard Lippmann, and Tim Leek. 2004. Testing static analysis tools using exploitable buffer overflows from open source code. In Proceedings of the 12th ACM SIGSOFT International Symposium on Foundations of Software Engineering, Vol. 29(6). ACM, Newport Beach, CA, USA, 97–106.

Session 9: Fuzzing
AsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand