### Interesting Findings

As shown in Table 3, factor covering designs, on average, outperformed random testing across all cases. However, for 2-factor covering designs, the difference was not significant. This finding is inconsistent with previous research, which has reported that 2-way coverage was sufficient to achieve higher detectability than random testing (e.g., [9]). In contrast, the mutation scores of 3- and 4-factor covering designs were significantly higher than those of random testing for all specifications. As illustrated in Table 2, the mutation scores of 3- and 4-factor covering designs were comparable to those of test suites generated by BMIS for some specifications. For example, the mutation scores for Specification 7 were 89.5%, 98.9%, and 97.6% for 3-factor, 4-factor covering designs, and BMIS, respectively.

### 4.3 Mutation Analysis by Ranges

In this section, we provide a more detailed analysis of cost and effectiveness based on the number of variables in the specifications and fault types.

Let \( w \) denote the number of variables. We categorized the specifications into three groups based on the number of variables: \( 5 \leq w \leq 8 \), \( 9 \leq w \leq 11 \), and \( 12 \leq w \leq 14 \). Table 4 presents the average percentage of the size of generated test suites relative to the exhaustive test suite size for each method and each group.

### Detailed Mutation Scores

**Table 11. Mutation scores for expression negation faults (ENF).**
| avg | 67.5 | 57.1 | 89.6 | 78.5 | 95.2 | 87.9 | 96.5 | 87.9 | 22.4 |

**Table 12. Mutation scores for variable reference faults (VRF).**
| avg | 26.2 | 23.5 | 60.8 | 46.1 | 73.2 | 61.8 | 87.6 | 55.7 | 93.6 |

**Table 13. Mutation scores for operator reference faults (ORF).**
| avg | 47.1 | 47.3 | 77.7 | 16.4 |

**Table 14. Mutation scores for associative shift faults (ASF).**
| avg | 38.0 | 35.7 | 75.3 | 62.5 | 82.4 | 71.7 | 86.6 | 66.3 | 9.0 |

### Comparative Analysis

Table 5 shows the average mutation score for each method and each group. In all cases, test suites generated by BMIS were more effective than factor covering designs. As indicated in Table 4, the sizes of 3-factor and 4-factor covering designs were approximately equal to those of BMIS when \( 5 \leq w \leq 8 \) and \( 9 \leq w \leq 11 \), respectively. When \( 12 \leq w \leq 14 \), the sizes of factor covering designs were smaller than those of BMIS. Therefore, if available, the specification-based approach is preferable to achieve high detectability at the same cost. Otherwise, using 3-factor covering designs is preferable for \( 5 \leq w \leq 8 \), as their average mutation score exceeded 80%. For \( w \geq 9 \), 4-factor covering designs or larger test suites are necessary to achieve the same effectiveness.

Tables 6, 7, 8, and 9 show the average mutation scores for each mutation type and each group by methods. The left and right side values in these tables represent the average mutation scores for each method and for randomly selected test suites of exactly the same size, respectively. These tables summarize the raw data presented in Tables 10, 11, 12, 13, and 14.

From these four tables, it is evident that combinatorial testing did not perform well in detecting VRFs. For instance, the average mutation scores of 3-factor covering designs where \( 5 \leq w \leq 8 \) were 92.1%, 93.6%, 70.4%, 86.9%, and 86.9% for VNF, ENF, VRF, ORE, and ASF, respectively. This suggests that further research should focus on enhancing the detection of VRFs in combinatorial testing.

On the other hand, BMIS achieved more than 85% for every mutation type. This is counterintuitive because BMIS constructs a test suite for a given boolean formula by hypothesizing only VNFs in the disjunctive normal form of the formula. We believe this can be explained by the following:

First, we discuss the relationship among VNF, ENF, and VRF. Recent studies have shown that VNF, ENF, and VRF are highly related [18]. Specifically, [18] demonstrated that when a specification is in a disjunctive form, tests that detect VRF will also detect VNF, and tests that detect VNF will also detect ENF. These results explain the high mutation score for ENF and justify the suggestion to improve combinatorial testing. The hierarchy of fault models is exhibited in Tables 6, 7, 8, and 9. This can also be seen more clearly in Tables 10, 11, and 12, which show the mutation scores for each specification for VNF, ENF, and VRF, respectively. As can be seen in these tables, for almost all specifications, the score for ENF is higher than VNF, and the score for VNF is higher than VRF.

The high mutation scores for ORF and ASF of BMIS cannot be explained by the above reasons. We think this is because many of these faults were easy to detect. As shown in Tables 13 and 14, the mutation scores of 3-factor and 4-factor covering designs and BMIS for ORF and ASF were close to 100% for many specifications.

However, it should be noted that there are some specifications for which the mutation scores were very low for ASF. For example, Specifications #9 and #18 had mutation scores of only 66.7% and 40.0%, respectively. At present, we do not know the cause of this low performance. Further research is needed to investigate the properties of various fault types, which will be important for developing efficient test generation schemes.

### 5 Conclusions

In this paper, we examined the applicability of non-specification-based approaches to logic testing for software using a set of 20 specifications from a real aircraft collision avoidance system (TCAS II). As non-specification-based approaches, we selected combinatorial testing, which uses factor covering designs, and random testing. We used mutation analysis to assess effectiveness. To our knowledge, this study is the first to examine the applicability of combinatorial testing to logic testing. The results showed that combinatorial testing was often comparable to specification-based testing and always superior to random testing. It was also found that it is difficult to detect VRFs using combinatorial testing. Therefore, we believe that improving combinatorial testing by focusing on detecting VRFs is possible.

We plan to further develop research on factor covering designs, including an analysis using a much larger number of formulas.

### Acknowledgements

The authors wish to thank Dr. Jean Arlat for his helpful suggestions on how to improve the paper.

### References

[1] B. Beizer, *Black-Box Testing*, John Wiley & Sons, 1995.
[2] K. Burr and W. Young, “Combinatorial test techniques: Table-based automation, test generation and code coverage,” Proc. Software Testing Analysis & Review (STAR’98 West), Oct. 1998.
[3] K. Burroughs, A. Jain, and R. L. Erickson, “Improved quality of protocol testing through techniques of experimental design,” Proc. International Conference on Communications (ICC’94) New York, pp. 745-752, 1994.
[4] T. Y. Chen and M. F. Lau, “Two test data selection strategies towards testing of boolean specifications,” Proc. International Computer Software and Applications Conference (COMPSAC’97), pp. 608-611, 1997.
[5] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton, “The AETG system: An approach to testing based on combinatorial design,” IEEE Trans. on Software Engineering, Vol. 23, No. 7, pp. 437-443, July 1997.
[6] D. M. Cohen and M. L. Fredman, “New techniques for designing qualitatively independent systems,” J. of Combinatorial Designs, Vol. 6, No. 6, pp. 411-416, 1998.
[7] S. R. Dalal and C. L. Mallows, “Factor-covering designs for testing software,” Technometrics, Vol. 40, No. 3, pp. 234-243, Aug. 1998.
[8] R. A. DeMillo, R. J. Lipton, and F. G. Sayward, “Hints on test data selection: Help for the practicing programmer,” IEEE Computer Magazine, Vol. 11, 1978.
[9] I. S. Dunietz, C. L. Mallows, and A. Iannino, “Applying design of experiments to software testing,” Proc. the 19th International Conference of Software Engineering (ICSE ’97), pp. 205-215, 1997.
[10] J. W. Duran and S. C. Ntafos, “An evaluation of random testing,” IEEE Trans. Software Engineering, Vol. 10, No. 4, pp. 438-444, 1984.
[11] K. A. Foster, “Sensitive test data for logic expressions,” ACM SIGSOFT Software Engineering Notes, Vol. 9, No. 2, pp. 120-125, 1984.
[12] D. Harel, “Statecharts: A visual formalism for complex systems,” Science of Computer Programming, Vol. 8, pp. 231-274, 1987.
[13] C. Heitmeyer, J. Kirby, and B. Labaw, “The SCR method for formally specifying, verifying and validating software requirements: tool support,” Proc. the 19th International Conference on Software Engineering (ICSE’97), pp. 610-611, May 1997.
[14] K. L. Heninger, “Specifying software requirements for complex systems: new techniques and their application,” IEEE Trans. Software Engineering, Vol. 6, No. 1, pp. 2-13, 1980.
[15] J. Huller, “Reducing time to market with combinatorial design method testing,” Proc. International Council on Systems Engineering (INCOSE2000), 2000.
[16] R. B. Hurley, *Decision Tables in Software Engineering*, Van Nostrand Reinhold, 1983.
[17] D. C. Ince and S. Hekmatpour, “Empirical evaluation of random testing,” The Computer Journal, Vol. 29, No. 4, p. 380, 1986.
[18] D. R. Kuhn, “Fault classes and error detection capability of specification-based testing,” ACM Trans. Software Engineering and Methodology, Vol. 8, No. 4, pp. 411-424, 1999.
[19] N. G. Leveson, M. P. E. Heimdahl, H. Hildreth, and J. D. Reese, “Requirements specification for process-control systems,” IEEE Trans. Software Engineering, Vol. 20, No. 9, pp. 684-707, 1994.
[20] G. J. Myers, *The Art of Software Testing*, John Wiley & Sons, 1979.
[21] D. J. Richardson and M. C. Thompson, “An analysis of test data selection criteria using the RELAY model of fault detection,” IEEE Trans. Software Engineering, Vol. 19, No. 6, pp. 533-553, 1993.
[22] P. Thévenod-Fosse, H. Waeselynck, and Y. Crouzet, “An experimental study of software structural testing: Deterministic versus random input generation,” Proc. the 21st International Symposium on Fault-Tolerant Computing (FTCS-21), Montréal, Canada, pp. 410-417, 1991.
[23] P. Thévenod-Fosse, H. Waeselynck, and Y. Crouzet, “Software statistical testing,” in Predictably Dependable Computing Systems (B. Randell, J-C. Laprie, H. Kopetz and B. Littlewood, Eds), ESPRIT Basic Research Series, pp. 253-72, Springer Verlag, 1995.
[24] M. Vouk, K. Tai, and A. Paradkar, “Empirical studies of predicate-based software testing,” Proc. International Symposium on Software Reliability Engineering (ISSRE’94), pp. 55-64, 1994.
[25] C. H. West, “Protocol validation - principles and applications,” Computer Networks and ISDN Systems, Vol. 24, No. 3, pp. 219-242, May 1992.
[26] E. Weyuker, T. Goradia, and A. Singh, “Automatically generating test data from a boolean specification,” IEEE Trans. Software Engineering, Vol. 20, No. 5, pp. 353-363, 1994.
[27] L. White, “Regression testing of CUI event interactions,” Proc. of the International Conference on Software Maintenance (ICSM’96), Washington, DC, pp. 350-358, 1996.
[28] A. W. Williams and R. L. Probert, “A practical strategy for testing pair-wise coverage of network interfaces,” Proc. of International Symposium on Software Reliability Engineering (ISSRE ’97), pp. 246-254, 1997.