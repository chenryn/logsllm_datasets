### 8. CONCLUSION

In this paper, we introduced PoWerStore, an efficient and robust storage protocol that achieves optimal latency, measured in the maximum (worst-case) number of communication rounds between a client and storage servers. We also presented a multi-writer variant of our protocol called M-PoWerStore. The efficiency of our proposals is achieved by combining lightweight cryptography, erasure coding, and metadata writebacks, where readers only write back metadata to ensure linearizability. While robust Byzantine fault-tolerant (BFT) systems have often been criticized for their inefficiency, our findings suggest that efficient and robust BFTs can be realized in practice using lightweight cryptographic primitives without compromising worst-case performance.

At the core of both PoWerStore and M-PoWerStore are Proofs of Writing (PoW), a novel storage technique inspired by commitment schemes, as seen in [21]. This technique enables single-writer PoWerStore to perform writes and reads in just two rounds, which we prove to be optimal. Similarly, M-PoWerStore, which relies on PoW, features three-round writes and reads, with the third read round only invoked under active attacks. Finally, we demonstrated M-PoWerStore's superior performance compared to existing crash and Byzantine-tolerant atomic storage implementations.

### 9. ACKNOWLEDGEMENTS

This work was supported in part by the EU CLOUDSPACES (FP7-317555) and SECCRIT (FP7-312758) projects, and by LOEWE TUD CASED.

### 10. REFERENCES

[1] Jerasure. https://github.com/tsuraan/Jerasure, 2008.
[2] The Neko Project. http://ddsg.jaist.ac.jp/neko/, 2009.
[3] Ittai Abraham, Gregory Chockler, Idit Keidar, and Dahlia Malkhi. Byzantine Disk Paxos: Optimal Resilience with Byzantine Shared Memory. Distributed Computing, 18(5):387–408, 2006.
[4] Amitanand S. Aiyer, Lorenzo Alvisi, and Rida A. Bazzi. Bounded Wait-free Implementation of Optimally Resilient Byzantine Storage Without (Unproven) Cryptographic Assumptions. In Proceedings of DISC, 2007.
[5] Hagit Attiya, Amotz Bar-Noy, and Danny Dolev. Sharing Memory Robustly in Message-Passing Systems. J. ACM, 42:124–142, January 1995.
[6] Rida A. Bazzi and Yin Ding. Non-skipping Timestamps for Byzantine Data Storage Systems. In Proceedings of DISC, pages 405–419, 2004.
[7] Alysson Neves Bessani, Miguel P. Correia, Bruno Quaresma, Fernando André, and Paulo Sousa. DepSky: dependable and secure storage in a cloud-of-clouds. In Proceedings of EuroSys, pages 31–46, 2011.
[8] Kevin D. Bowers, Ari Juels, and Alina Oprea. HAIL: a high-availability and integrity layer for cloud storage. In CCS, pages 187–198, 2009.
[9] Kevin D. Bowers, Ari Juels, and Alina Oprea. Proofs of retrievability: theory and implementation. In CCSW, pages 43–54, 2009.
[10] Christian Cachin and Stefano Tessaro. Optimal Resilience for Erasure-Coded Byzantine Distributed Storage. In Proceedings of DSN, pages 115–124, 2006.
[11] Brian Cho and Marcos K. Aguilera. Surviving congestion in geo-distributed storage systems. In Proceedings of USENIX ATC, pages 40–40, 2012.
[12] Gregory Chockler, Dahlia Malkhi, and Danny Dolev. Future directions in distributed computing. Chapter A data-centric approach for scalable state machine replication, pages 159–163, 2003.
[13] Allen Clement, Edmund L. Wong, Lorenzo Alvisi, Michael Dahlin, and Mirco Marchetti. Making Byzantine fault tolerant systems tolerate Byzantine faults. In Proceedings of NSDI, pages 153–168, 2009.
[14] Wei Dai. Crypto++ 5.6.0 benchmarks. Website, 2009. Available online at http://www.cryptopp.com/benchmarks.html.
[15] Dan Dobre, Rachid Guerraoui, Matthias Majuntke, Neeraj Suri, and Marko Vukolić. The Complexity of Robust Atomic Storage. In Proceedings of PODC, pages 59–68, 2011.
[16] Partha Dutta, Rachid Guerraoui, Ron R. Levy, and Marko Vukolić. Fast Access to Distributed Atomic Memory. SIAM J. Comput., 39:3752–3783, December 2010.
[17] Rui Fan and Nancy Lynch. Efficient Replication of Large Data Objects. In Proceedings of DISC, pages 75–91, 2003.
[34] Dahlia Malkhi and Michael K. Reiter. Secure and Scalable Replication in Phalanx. In Proceedings of SRDS, pages 51–58, 1998.
[35] Jean-Philippe Martin, Lorenzo Alvisi, and Michael Dahlin. Minimal Byzantine Storage. In Proceedings of DISC, pages 311–325, 2002.
[36] David Mazières and Dennis Shasha. Building secure file systems out of Byzantine storage. In PODC, pages 108–117, 2002.
[18] Chryssis Georgiou, Nicolas C. Nicolaou, and Alexander A. Shvartsman. Fault-tolerant Semifast Implementations of Atomic Read/Write Registers. J. Parallel Distrib. Comput., 69(1):62–79, January 2009.
[19] Garth R. Goodson, Jay J. Wylie, Gregory R. Ganger, and Michael K. Reiter. Efficient Byzantine-Tolerant Erasure-Coded Storage. In Proceedings of DSN, 2004.
[20] Rachid Guerraoui and Marko Vukolić. Refined quorum systems. Distributed Computing, 23(1):1–42, 2010.
[21] Shai Halevi and Silvio Micali. Practical and provably-secure commitment schemes from collision-free hashing. In Proceedings of CRYPTO, pages 201–215, 1996.
[22] James Hendricks, Gregory R. Ganger, and Michael K. Reiter. Low-overhead Byzantine fault-tolerant storage. In Proceedings of SOSP, pages 73–86, 2007.
[23] Maurice Herlihy. Wait-Free Synchronization. ACM Trans. Program. Lang. Syst., 13(1), 1991.
[24] Maurice P. Herlihy and Jeannette M. Wing. Linearizability: A Correctness Condition for Concurrent Objects. ACM Trans. Program. Lang. Syst., 12(3), 1990.
[25] Prasad Jayanti, Tushar Deepak Chandra, and Sam Toueg. Fault-tolerant Wait-free Shared Objects. J. ACM, 45(3), 1998.
[26] Aniket Kate, Gregory M. Zaverucha, and Ian Goldberg. Constant-size commitments to polynomials and their applications. In Proceedings of ASIACRYPT, volume 6477, pages 177–194, 2010.
[27] Petr Kuznetsov and Rodrigo Rodrigues. BFTW3: Why? When? Where? Workshop on the Theory and Practice of Byzantine Fault Tolerance. SIGACT News, 40(4):82–86, 2009.
[28] Leslie Lamport. On Interprocess Communication. Distributed Computing, 1(2):77–101, 1986.
[29] Leslie Lamport, Robert E. Shostak, and Marshall C. Pease. The Byzantine generals problem. ACM Trans. Program. Lang. Syst., 4(3):382–401, 1982.
[30] Harry C. Li, Allen Clement, Amitanand S. Aiyer, and Lorenzo Alvisi. The Paxos Register. In Proceedings of SRDS, pages 114–126, 2007.
[31] Barbara Liskov and Rodrigo Rodrigues. Tolerating Byzantine Faulty Clients in a Quorum System. In Proceedings of ICDCS, 2006.
[32] Nancy A. Lynch and Mark R. Tuttle. An introduction to input/output automata. CWI Quarterly, 2:219–246, 1989.
[33] Dahlia Malkhi and Michael K. Reiter. A High-Throughput Secure Reliable Multicast Protocol. J. Comput. Secur., 5(2):113–127, March 1997. Available online at http://www.linuxfoundation.org/collaborate/workgroups/networking/netem.
[38] Michael K. Reiter. Secure Agreement Protocols: Reliable and Atomic Group Multicast in Rampart. In Proceedings of CCS, pages 68–80, 1994.
[39] Alexander Shraer, Christian Cachin, Asaf Cidon, Idit Keidar, Yan Michalevsky, and Dani Shaket. Venus: verification for untrusted cloud storage. In CCSW, pages 19–30, 2010.
[40] Alexander Shraer, Jean-Philippe Martin, Dahlia Malkhi, and Idit Keidar. Data-centric reconfiguration with network-attached disks. In Proceedings of LADIS, pages 22–26, 2010.
[41] Atul Singh, Tathagata Das, Petros Maniatis, Peter Druschel, and Timothy Roscoe. BFT protocols under fire. In Proceedings of NSDI, pages 189–204, 2008.
[42] Emil Stefanov, Marten van Dijk, Ari Juels, and Alina Oprea. IRIS: a scalable cloud file system with efficient integrity checks. In ACSAC, pages 229–238, 2012.
[43] Sue-Hwey Wu, Scott A. Smolka, and Eugene W. Stark. Composition and behaviors of probabilistic I/O automata. In Proceedings of CONCUR, pages 513–528, 1994.

### APPENDIX
#### A. OPTIMALITY OF PoWerStore

In this section, we prove that PoWerStore features optimal latency by showing that writing in two rounds is necessary. We refer the reader to [16] for the necessity of reading in two rounds. We start by providing some informal definitions.

A distributed algorithm \( A \) is a set of automata [32], where automaton \( A_p \) is assigned to process \( p \). Computation proceeds in steps of \( A \), and a run is an infinite sequence of steps of \( A \). A partial run is a finite prefix of some run. We say that a (partial) run \( r \) extends some partial run \( pr \) if \( pr \) is a prefix of \( r \). We say that an implementation is selfish if clients write back metadata to achieve linearizability (instead of the full value) [17]. Furthermore, we say that an operation is fast if it completes in a single round.

**Theorem A.1.** There is no fast WRITE implementation \( I \) of a multi-reader selfish robust storage that makes use of less than \( 4t + 1 \) servers.

**Preliminaries.** We prove Theorem A.1 by contradiction, assuming at most \( 4t \) servers. An illustration of the proof is given in Figure 3. We partition the set of servers into four distinct subsets (blocks), denoted by \( T_1, T_2, T_3 \) each of size exactly \( t \), and \( T_4 \) of size at least 1 and at most \( t \). Without loss of generality, we assume that each block contains at least one server. We say that an operation \( op \) skips a block \( T_i \) (where \( 1 \leq i \leq 4 \)) when all messages by \( op \) to \( T_i \) are delayed indefinitely (due to asynchrony) and all other blocks \( T_j \) receive all messages by \( op \) and reply.

**Proof:**

We construct a series of runs of a linearizable implementation \( I \) towards a partial run that violates linearizability, i.e., that features two consecutive read operations by distinct readers that return different values.

- **Run 1:** Let \( run_1 \) be the partial run in which all servers are correct except \( T_1 \), which crashed at the beginning of \( run_1 \). Let \( wr \) be the operation invoked by the writer \( w \) to write a value \( v \neq \bot \) in the storage. The WRITE \( wr \) is the only operation invoked in \( run_1 \), and \( w \) crashes after writing \( v \) to \( T_3 \). Hence, \( wr \) skips blocks \( T_1, T_2 \), and \( T_4 \).

- **Run 1':** Let \( run_1' \) be the partial run in which all servers are correct except \( T_4 \), which crashed at the beginning of \( run_1' \). Writer \( w \) is correct, and \( wr \) completes by writing \( v \) to all blocks except \( T_4 \), which it skips.

- **Run 2:** Let \( run_2 \) be the partial run similar to \( run_1' \), in which all servers except \( T_2 \) are correct, but due to asynchrony, all messages from \( w \) to \( T_4 \) are delayed. Like in \( run_1' \), \( wr \) completes by writing \( v \) to all servers except \( T_4 \), which it skips. To see why, note that \( wr \) cannot distinguish \( run_2 \) from \( run_1' \). After \( wr \) completes, \( T_2 \) fails Byzantine by reverting its memory to the initial state.

- **Run 3:** Let \( run_3 \) extend \( run_1 \) by appending a complete READ \( rd_1 \) invoked by \( r_1 \). By our assumption, \( I \) is wait-free. As such, \( rd_1 \) completes by skipping \( T_1 \) (because \( T_1 \) crashed) and returns (after a finite number of rounds) a value \( v_R \).

- **Run 4:** Let \( run_4 \) extend \( run_2 \) by appending \( rd_1 \). In \( run_4 \), all servers except \( T_2 \) are correct, but due to asynchrony, all messages from \( r_1 \) to \( T_1 \) are delayed indefinitely. Moreover, since \( T_2 \) reverted its memory to the initial state, \( v \) is held only by \( T_3 \). Note that \( r_1 \) cannot distinguish \( run_4 \) from \( run_3 \) in which \( T_1 \) has crashed. As such, \( rd_1 \) completes by skipping \( T_1 \) and returns \( v_R \). By linearizability, \( v_R = v \).

- **Run 5:** Let \( run_5 \) be similar to \( run_3 \) in which all servers except \( T_3 \) are correct, but due to asynchrony, all messages from \( r_1 \) to \( T_1 \) are delayed. Note that \( r_1 \) cannot distinguish \( run_5 \) from \( run_3 \). As such, \( rd_1 \) returns \( v_R \) in \( run_5 \), and by \( run_4 \), \( v_R = v \). After \( rd_1 \) completes, \( T_3 \) fails by crashing.

- **Run 6:** Let \( run_6 \) extend \( run_5 \) by appending a READ \( rd_2 \) invoked by \( r_2 \) that completes by returning \( v' \). Note that in \( run_5 \), (i) \( T_3 \) is the only server to which \( v \) was written, (ii) \( rd_1 \) did not write back \( v \) (to any other server) before returning \( v \), and (iii) \( T_3 \) crashed before \( rd_2 \) is invoked. As such, \( rd_2 \) does not find \( v \) in any server and hence \( v' \neq v \), violating linearizability.

It is important to note that Theorem A.1 allows for self-verifying data and assumes clients that may fail only by crashing. Furthermore, the impossibility extends to crash-tolerant storage using less than \( 3t + 1 \) servers when deleting the Byzantine block \( T_2 \) in the above proof.

**Figure 3: Sketch of the runs used in the proof of Theorem A.1.**

- **(a) Run 1**
- **(b) Run 2**
- **(c) Run 3**
- **(d) Run 4**
- **(e) Run 5**
- **(f) Run 6**

This proof demonstrates that a fast WRITE implementation of a multi-reader selfish robust storage requires at least \( 4t + 1 \) servers to ensure linearizability.