### The fsync Call and Journaling Behavior

The `fsync` call completed without flushing any blocks to the journal, contrary to the expectation that the file system should write the metadata blocks associated with the file to the disk.

### Conference Information

**Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN'05)**  
**ISBN: 0-7695-2282-3/05 $20.00 © 2005 IEEE**  
**Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021, at 12:10:16 UTC from IEEE Xplore. Restrictions apply.**

### Design Flaws in File Systems

| Feature | Ext3 | Reiserfs | IBM JFS |
|---------|------|----------|---------|
| Committing Failed Transactions | × | × | × |
| Checkpointing Failed Transactions | × | × | × |
| Not Replaying Failed Checkpoint Writes | × | × | × |
| Not Replaying Transactions | × | × | × |
| Replaying Failed Transactions | × | × | × |
| Crashing File System | × | × | × |

**Table 1: Design Flaws.** This table summarizes the design flaws identified in ext3, Reiserfs, and IBM JFS.

### Analysis of Block Write Failures

| Block Type | Ext3 | Reiserfs | IBM JFS |
|------------|------|----------|---------|
| Journal Descriptor Block | × | × | × |
| Journal Revoke Block | × | × | × |
| Journal Commit Block | × | × | × |
| Journal Super Block | × | × | × |
| Journal Data Block | × | × | × |
| Checkpoint Block | × | - | - |
| Data Block | × | × | × |

**Table 2: Analysis Summary.** This table presents a summary of the types of failures that can occur in ext3, Reiserfs, and IBM JFS when block writes fail. "Data Block" represents both ordered and unordered writes in ext3 and Reiserfs, and only ordered writes in JFS. Key: DC = Data Corruption, DL = Data Loss, FDL = Files and Directory Loss, UFS = Unmountable File System, CR = Crash. A "–" indicates that the corresponding block type is not available in that file system. Although JFS does not have separate commit or revoke blocks, it has records of that type.

### Summary of Analysis

Tables 1 and 2 summarize our analysis. Table 1 lists the design flaws we identified in Linux journaling file systems, while Table 2 details the different types of file system failures that can occur when block writes fail. Our findings suggest that Linux journaling file systems need more careful design of their failure handling policies.

### Related Work

#### Fault Injection

Fault injection has been used for a long time to measure system robustness. Koopman argues that faults injected directly into the modules under test do not provide representative results for dependability evaluation [11]. He suggests that faults should be injected into the external environments of the module under test and activated by inputs during real execution. Our approach aligns with this, as we inject faults external to the file system module and activate them by running workloads on top of the file system.

We use software to simulate the effects of hardware faults and inject faults by dynamically determining the block types of the file system. FTAPE is a tool that performs dynamic workload measurements and injects faults by automatically determining the time and location that will maximize fault propagation [20]. FIAT is one of the early systems to use fault injection techniques to simulate hardware errors by changing the contents of memory or registers [9]. FINE, developed by Kao et al., injects hardware-induced software faults into the UNIX kernel and traces the execution flow of the kernel [12]. More recent work uses fault injection techniques to test the Linux kernel behavior under errors [7].

#### File and Storage System Testing

Most file system testing tools focus on testing the file system API with various types of invalid arguments. Siewiorek et al. developed a benchmark to measure system robustness and used it to test the dependability of file system libraries [17]. Similarly, Koopman et al. used the Ballista testing suite to find robustness problems in the Safe/Fast IO (SFIO) library [5]. Another approach is to use model checking techniques to test file system code. Yang et al. used model checking comprehensively to find bugs in ext3, Reiserfs, and JFS [22]. They used formal verification techniques to systematically enumerate a set of file system states and verify them against valid file system states. Their work can identify issues like deadlocks and NULL pointers, while our work focuses on how file systems handle latent sector errors.

Previous work has also studied the reliability of storage systems. Brown et al. developed a method to measure system robustness and applied it to measure the availability of software RAID systems in Linux, Solaris, and Windows [3]. They used a PC to emulate a disk and used the disk emulator to inject faults. They tested software RAID systems, whereas our work targets file systems. We use file system knowledge to carefully select and fail specific block types, unlike their approach, which does not require semantic information for fault injection. Other studies have evaluated RAID storage systems for reliability and availability [8, 10], developing detailed simulation models of RAID storage arrays and network clusters.

### Conclusion

In this paper, we propose a new method to evaluate the robustness of journaling file systems under disk write failures. We build semantic models of different journaling modes and use them to inject faults into the file system disk requests. We evaluate three widely used Linux journaling file systems and find that ext3 and IBM JFS violate journaling semantics on block write failures, potentially leading to corrupt file systems. In contrast, Reiserfs maintains file system integrity by crashing the entire system on most write failures, but this results in repeated crashes and restarts on permanent write failures. Based on our analysis, we identify various design flaws and correctness bugs in these file systems that can catastrophically affect on-disk data. Overall, we conclude that modern file systems need more design consideration placed into their failure handling policies.

### Acknowledgments

We thank the members of the ADSL research group for their insightful comments and the anonymous reviewers for their thoughtful suggestions. This work is sponsored by NSF CCR-0092840, CCR-0133456, CCR-0098274, NGS-0103670, ITR-0325267, IBM, EMC, and ITR-0086044.

### References

[1] R. H. Arpaci-Dusseau and A. C. Arpaci-Dusseau. Fail-Stutter Fault Tolerance. In HotOS VIII, pages 33–38, Schloss Elmau, Germany, May 2001.

[2] S. Best. JFS Overview. www.ibm.com/developerworks/library/l-jfs.html, 2004.

[4] P. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, and S. Sankar. Row-Diagonal Parity for Double Disk Failure Correction. In Proceedings of the 3rd USENIX Symposium on File and Storage Technologies (FAST ’04), pages 1–14, San Francisco, California, April 2004.

[5] J. DeVale and P. Koopman. Performance Evaluation of Exception Handling in I/O Libraries. In Dependable Systems and Networks, June 2001.

[6] J. Gray and A. Reuter. Transaction Processing: Concepts and Techniques. Morgan Kaufmann, 1993.

[7] W. Gu, Z. Kalbarczyk, I. K. Ravishankar, and Z. Yang. Characterization of Linux Kernel Behavior Under Error. In Dependable Systems and Networks, pages 459–468, June 2003.

[8] Y. Huang, Z. T. Kalbarczyk, and R. K. Iyer. Dependability Analysis of a Cache-Based RAID System via Fast Distributed Simulation. In The 17th IEEE Symposium on Reliable Distributed Systems, 1998.

[9] Z. S. D. S. J.H. Barton, E.W. Czeck. Fault Injection Experiments Using FIAT. In IEEE Transactions on Computers, volume 39, pages 1105–1118, April 1990.

[10] M. Kaniche, L. Romano, Z. Kalbarczyk, R. K. Iyer, and R. Karcich. A Hierarchical Approach for Dependability Analysis of a Commercial Cache-Based RAID Storage Architecture. In The Twenty-Eighth Annual International Symposium on Fault-Tolerant Computing, June 1998.

[11] P. Koopman. What’s Wrong with Fault Injection as a Dependability Benchmark? In Workshop on Dependability Benchmarking (in conjunction with DSN 2002), Washington DC, July 2002.

[12] W. lun Kao, R. K. Iyer, and D. Tang. FINE: A Fault Injection and Monitoring Environment for Tracing the UNIX System Behavior Under Faults. In IEEE Transactions on Software Engineering, pages 1105–1118, 1993.

[13] M. K. McKusick, W. N. Joy, S. J. Lefﬂer, and R. S. Fabry. Fsck - The UNIX File System Check Program. Unix System Manager’s Manual - 4.3 BSD Virtual VAX-11 Version, April 1986.

[14] V. Prabhakaran, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau. Analysis and Evolution of Journaling File Systems. In Proceedings of the USENIX Annual Technical Conference (USENIX ’05), Anaheim, California, April 2005.

[15] H. Reiser. ReiserFS. www.namesys.com, 2004.

[16] F. B. Schneider. Implementing Fault-Tolerant Services Using The State Machine Approach: A Tutorial. ACM Computing Surveys, 22(4):299–319, December 1990.

[17] D. Siewiorek, J. Hudak, B.-H. Suh, and Z. Segal. Development of a Benchmark to Measure System Robustness. In The Twenty-Third International Symposium on Fault-Tolerant Computing, 1993.

[18] A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nishimoto, and G. Peck. Scalability in the XFS File System. In Proceedings of the USENIX Annual Technical Conference (USENIX ’96), San Diego, California, January 1996.

[19] N. Talagala and D. Patterson. An Analysis of Error Behaviour in a Large Storage System. In The IEEE Workshop on Fault Tolerance in Parallel and Distributed Systems, San Juan, Puerto Rico, April 1999.

[20] T. K. Tsai and R. K. Iyer. Measuring Fault Tolerance with the FTAPE Fault Injection Tool. In 8th Intl. Conf. On Modeling Techniques and Tools for Comp. Perf. Evaluation, pages 26–40, Sept 1995.

[21] S. C. Tweedie. Journaling the Linux ext2fs File System. In The Fourth Annual Linux Expo, Durham, North Carolina, May 1998.

[22] J. Yang, P. Twohey, D. Engler, and M. Musuvathi. Using Model Checking to Find Serious File System Errors. In Proceedings of the 6th Symposium on Operating Systems Design and Implementation (OSDI ’04), San Francisco, California, December 2004.

[3] A. Brown and D. A. Patterson. Towards Maintainability, Availability, and Growth Benchmarks: A Case Study of Software RAID Systems. In Proceedings of the USENIX Annual Technical Conference (USENIX ’00), pages 263–276, San Diego, California, June 2000.