### Cluster and Warp Deformation Requirements

For the fault map SP 0 3 SP 1 3 relative to the baseline fault-free run, benchmarks with high thread activity experience a performance overhead ranging from 50% to 130%. This is expected because all warps, where at least one cluster has three or four active threads, need to be split into three or four sub-warps issued in consecutive cycles. Conversely, benchmarks with lower thread activity experience a performance overhead of around 10%, as they benefit more from intra-cluster thread shuffling. The weighted average performance overhead is 35.5%, which is five times the average performance overhead for the common case fault map.

### Asymmetric Fault Maps

For the common and worst-case scenarios, SP0 and SP1 have symmetric fault maps. However, due to process variations and utilization differences, fault maps can be asymmetric. We consider four scenarios divided into two groups:

1. **First Group:**
   - SP1 has no faulty lanes, while all clusters in SP0 suffer from two or three faulty lanes (i.e., SP 0 2 SP 1 0 and SP 0 3 SP 1 0).
   
2. **Second Group:**
   - All clusters in SP0 suffer from three faulty lanes, while SP1 clusters suffer from one or two faulty lanes (i.e., SP 0 3 SP 1 1 and SP 0 3 SP 1 2).

#### Performance Overhead Analysis

- **SP 0 2 SP 1 0 Fault Map:**
  - The weighted average performance overhead drops to 3.5%, representing a two-fold improvement over the common case fault map and a ten-fold improvement over the worst-case fault map.
  - The maximum performance overhead across all benchmarks is less than 18%.

- **SP 0 3 SP 1 0 Fault Map:**
  - The performance overhead is twice that of the SP 0 2 SP 1 0 fault map, as more SIMT lanes become faulty.
  - The maximum performance overhead reaches 30%.

- **SP 0 3 SP 1 1 Fault Map:**
  - The weighted average performance overhead is 15%, which is double the performance overhead of the common case fault map.
  - Warps deformed on SP1 will always require two sub-warps, but those deformed on SP0 might require two, three, or four sub-warps, leading to increased performance overhead.

- **SP 0 3 SP 1 2 Fault Map:**
  - The performance overhead is almost identical to that of the SP 0 3 SP 1 1 fault map.
  - The only difference is in the number of faulty SIMT lanes per cluster for SP1.
  - In rare cases, SP 0 3 SP 1 1 performs better when the warp issued to SP1 has a maximum of three active threads per cluster, requiring no deformation, while SP 0 3 SP 1 2 splits the warp into two sub-warps.

### Intra-Cluster Shuffling vs. Dynamic Warp Deformation

This section discusses the frequency of activation for the proposed techniques. Figure 13 shows the percentage of time intra-cluster thread shuffling is sufficient versus the percentage of time dynamic warp deformation is required for the six fault maps evaluated. Additionally, the figure reports the percentage of time inter-SP warp shuffling helps to avoid potential deformation by issuing the warp to the appropriate SP. For symmetric fault maps, this percentage is zero.

- **Symmetric Fault Maps:**
  - Warp deformation is activated more than 80% of the time when fault tolerance is needed.
  
- **Asymmetric Fault Maps:**
  - The combination of intra-cluster thread shuffling and inter-SP warp shuffling reduces the percentage of time during which deformation is activated.
  - For example, when SP1 is completely healthy (i.e., SP 0 2 SP 1 0 and SP 0 3 SP 1 0), shuffling becomes sufficient for more than 50% of the time.

### Area and Power Overheads

To evaluate the area and power overheads of the proposed techniques, we implemented the reliability-aware-scheduler (RASc) circuitry, the reliability-aware-split (RASp) unit, and the shuffling and reshuffling MUXes in RTL using Synopsis design compiler and NCSU PDK 45nm library.

- **Power Overhead:**
  - The total dynamic power of all additional components is 0.0334uW, representing less than 0.9% of the power consumed by the SIMT lanes in the GPGPU.

- **Area Overhead:**
  - The area consumed by the additional components is estimated to be 0.031mm², resulting in an area overhead of 0.01% of the total area of the SIMT lanes, which is 32mm².

### Related Work

**Reliable GPGPUs:**
- Hardware and software techniques have been used to handle the reliability of GPGPUs. 
- Software-based DMR solutions by Dimitrov et al. [9] and Nathan et al. [17] check execution correctness.
- On the hardware side, Jeon and Annavaram [13] proposed Warped-DMR, a technique to detect faults but not correct them.

**Warp Formation:**
- Techniques like large warp formation [11, 16] aim to improve performance by grouping active threads from different warps.
- Our proposed dynamic warp deformation divides warps into multiple sub-warps to avoid faulty SIMT lanes.

**Tolerating Hard Faults:**
- Bower et al. [6, 7] and Dweik et al. [10] proposed hardware-based techniques to tolerate hard faults in microprocessor array structures.
- Our work leverages the available micro-architectural redundancy to de-configure faulty blocks and continue execution with reduced resources, specifically tailored for GPGPUs.

### Conclusion

In this paper, we propose two techniques to tolerate hard faults in the SIMT lanes of GPGPUs:
1. **Intra-cluster thread shuffling:** Rearranges threads within a cluster to avoid mapping any active thread to a faulty SIMT lane.
2. **Dynamic warp deformation:** Splits the original warp into multiple sub-warps to distribute the active threads among the sub-warps.

To minimize performance overhead, we introduce inter-SP warp shuffling, which issues warps to the SP that incurs less performance overhead whenever possible. We evaluated these techniques using various fault maps, including common and worst-case scenarios. In the worst case, 75% of the GPGPU SIMT lanes are faulty, but the proposed techniques guarantee forward progress with an average performance overhead of 35.5%. In the common case (50% of the GPGPU SIMT lanes are faulty), the average performance overhead drops to 7%. The proposed techniques incur minimal area and power overheads of 0.01% and 0.9%, respectively.

### Acknowledgements

This work was supported by the following grants: DARPA-PERFECT-HR0011-12-2-0020 and NSF-CAREER-0954211, NSF-0834798.

### References

[1] "The freepdk process design kit," http://www.eda.ncsu.edu/wiki/FreePDK.

[2] "Parboil benchmark suite," http://impact.crhc.illinois.edu/parboil.php.

[3] "Nvidia's next generation cuda compute architecture: Fermi," Nvidia, Tech. Rep., 2009.

[4] "Nvidia's next generation cuda compute architecture: Kepler tm gk110," Nvidia, Tech. Rep., 2012.

[5] A. Bakhoda, G. Yuan, W. Fung, H. Wong, and T. Aamodt, "Analyzing cuda workloads using a detailed gpu simulator," in Performance Analysis of Systems and Software, 2009. ISPASS 2009. IEEE International Symposium on, April 2009, pp. 163–174.

[6] F. Bower, P. Shealy, S. Ozev, and D. Sorin, "Tolerating hard faults in microprocessor array structures," in Dependable Systems and Networks, 2004 International Conference on, June 2004, pp. 51–60.

[7] F. Bower, D. Sorin, and S. Ozev, "A mechanism for online diagnosis of hard faults in microprocessors," in Microarchitecture, 2005. MICRO-38. Proceedings. 38th Annual IEEE/ACM International Symposium on, Nov 2005, pp. 12 pp.–.

[8] S. Che, M. Boyer, J. Meng, D. Tarjan, J. Sheaffer, S.-H. Lee, and K. Skadron, "Rodinia: A benchmark suite for heterogeneous computing," in Workload Characterization, 2009. IISWC 2009. IEEE International Symposium on, Oct 2009, pp. 44–54.

[9] M. Dimitrov, M. Mantor, and H. Zhou, "Understanding software approaches for gpgpu reliability," in Proceedings of 2Nd Workshop on General Purpose Processing on Graphics Processing Units, ser. GPGPU-2. New York, NY, USA: ACM, 2009, pp. 94–104.

[10] W. Dweik, M. Annavaram, and M. Dubois, "Reliability-aware exceptions: Tolerating intermittent faults in microprocessor array structures," in Design, Automation and Test in Europe Conference and Exhibition (DATE), 2014, March 2014, pp. 1–6.

[11] W. Fung and T. Aamodt, "Thread block compaction for efficient simt control flow," in High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on, Feb 2011, pp. 25–36.

[12] M. Gebhart, D. Johnson, D. Tarjan, S. Keckler, W. Dally, E. Lindholm, and K. Skadron, "Energy-efficient mechanisms for managing thread context in throughput processors," in Computer Architecture (ISCA), 2011 38th Annual International Symposium on, June 2011, pp. 235–246.

[13] H. Jeon and M. Annavaram, "Warped-dmr: Light-weight error detection for gpgpu," in Microarchitecture (MICRO), 2012 45th Annual IEEE/ACM International Symposium on, Dec 2012, pp. 37–47.

[14] J. Leng, T. Hetherington, A. ElTantawy, S. Gilani, N. S. Kim, T. M. Aamodt, and V. J. Reddi, "Gpuwattch: Enabling energy optimizations in gpgpus," in Proceedings of the 40th Annual International Symposium on Computer Architecture, ser. ISCA '13. New York, NY, USA: ACM, 2013, pp. 487–498.

[15] S. Li, J.-H. Ahn, R. Strong, J. Brockman, D. Tullsen, and N. Jouppi, "Mcpat: An integrated power, area, and timing modeling framework for multicore and manycore architectures," in Microarchitecture, 2009. MICRO-42. 42nd Annual IEEE/ACM International Symposium on, Dec 2009, pp. 469–480.

[16] V. Narasiman, M. Shebanow, C. J. Lee, R. Miftakhutdinov, O. Mutlu, and Y. N. Patt, "Improving gpu performance via large warps and two-level warp scheduling," in Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-44. New York, NY, USA: ACM, 2011, pp. 308–317.

[17] R. Nathan and D. J. Sorin, "Argus-g: A low-cost error detection scheme for gpgpus," Workshop on Resilient Architectures.