### 7.4. Future Work

In the near term, we plan to extend our research methodology to a different context. Instead of analyzing an existing product through refactoring, we will gather both product and process data from scratch in the setting of a programming contest. Our objective is to evaluate how the chosen development process, in terms of its complexity and efficiency, affects the overall effectiveness, as measured by an automatic judge.

In a previous study [62], we observed that even for well-defined software development tasks, there can be significant variability in the processes due to human factors. Less focused teams produced more complex process models, often including non-essential actions, which resulted in lower efficiency. Building on this, we aim to use clustering techniques to derive a catalog of process "smells" or fingerprints that characterize different development behaviors. This taxonomy will then be integrated into a personal software process dashboard, which, through self-awareness, is expected to enhance process efficiency (e.g., by reducing wasted effort) and effectiveness (e.g., by improving deliverables).

We also identify several areas that warrant further research:

- **Software Repository Diversity**: Traditional software repositories have limitations and imprecisions. To broaden the scope of analytics in mining software development processes, we should explore less conventional repositories, such as Integrated Development Environments (IDEs). This is particularly interesting for studies aiming to combine perspectives on product quality and the underlying development process.
  
- **Software Development Process Mining Pipeline**: Many current process mining tools are not fully automated and require some level of human intervention. As a result, many metrics in this article had to be extracted semi-automatically, using tools but still requiring user interaction. This is a significant limitation for advancing research based on event data and current process mining methods. A microservices-based architecture appears to be a promising alternative for building a coherent pipeline for software development process mining.
  
- **Data Sharing**: Research that combines software product and process data is limited, and experiments in this area are challenging to design and execute. To address this, we anticipate an increase in shared datasets containing hybrid data, provided that privacy and/or anonymity of sensitive information is maintained.

### Acknowledgement

This work was partially supported by the Portuguese Foundation for Science and Technology under ISTAR-Iscte projects UIDB/04466/2020 and UIDP/04466/2020.

### References

[1] W.E. Deming, *Out of the Crisis: Quality, Productivity, and Competitive Position*, Massachusetts Institute of Technology, Center for Advanced Engineering Study, 1986.

[2] K. Ishikawa, *What is Total Quality Control? The Japanese Way*, Prentice-Hall, 1985.

[3] G. Taguchi, *Introduction to Quality Engineering: Designing Quality into Products and Processes*, Asian Productivity Organization, 1986.

[4] A. Fuggetta, E. Di Nitto, *Software Process*, in: Proceedings of the Future of Software Engineering - FOSE 2014, ACM Press, New York, NY, USA, 2014, pp. 1–12. doi:10.1145/2593882.2593883.

[5] W. Van Der Aalst, *Process Mining: Data Science in Action*, 2nd Edition, Springer-Verlag Berlin Heidelberg, 2016. doi:10.1007/978-3-662-49851-4.

[6] N. Moha, Y. G. Guéhéneuc, P. Leduc, *Automatic Generation of Detection Algorithms for Design Defects*, in: 21st IEEE/ACM International Conference on Automated Software Engineering, ASE 2006, 2006, pp. 297–300. doi:10.1109/ASE.2006.22.

[7] R. Malhotra, A. Sharma, *Analyzing Machine Learning Techniques for Fault Prediction Using Web Applications*, Journal of Information Processing Systems 14 (3) (2018) 751–770. doi:10.3745/JIPS.04.0077.

[8] H. Karna, L. Vicković, S. Gotovac, *Application of Data Mining Methods for Effort Estimation of Software Projects*, in: Software - Practice and Experience, Vol. 49, John Wiley and Sons Ltd, 2019, pp. 171–191. doi:10.1002/spe.2651.

[9] F. A. Batarseh, A. J. Gonzalez, *Predicting Failures in Agile Software Development Through Data Analytics*, Software Quality Journal 26 (1) (2018) 49–66. doi:10.1007/s11219-015-9285-3.

[10] M. Kersten, G. C. Murphy, *Using Task Context to Improve Programmer Productivity*, in: Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering, ACM, Portland, OR, USA, 2006, pp. 1–11.

[11] G. C. Murphy, P. Viriyakattiyaporn, D. Shepherd, *Using Activity Traces to Characterize Programming Behavior Beyond the Lab*, IEEE International Conference on Program Comprehension (2009) 90–94. doi:10.1109/ICPC.2009.5090031.

[12] S. Negara, N. Chen, M. Vakilian, R. E. Johnson, D. Dig, *A Comparative Study of Manual and Automated Refactorings*, in: Proceedings of the 27th European Conference on Object-Oriented Programming, Springer, Berlin, Heidelberg, 2013, pp. 552–576. doi:10.1007/978-3-642-39038-8_23.

[13] M. Vakilian, N. Chen, S. Negara, B. Ambresh, R. Roshanak, Z. Moghadam, R. E. Johnson, *The Need for Richer Refactoring Usage Data*, in: Proceedings of the 3rd ACM SIGPLAN Workshop on Evaluation and Usability of Programming Languages and Tools, Association for Computing Machinery, 2011, pp. 31–38.

[14] M. Vakilian, N. Chen, S. Negara, B. A. Rajkumar, B. P. Bailey, R. E. Johnson, *Use, Disuse, and Misuse of Automated Refactorings*, in: Proceedings - International Conference on Software Engineering, 2012, pp. 233–243. doi:10.1109/ICSE.2012.6227190.

[15] J. Ratzinger, T. Sigmund, P. Vorburger, H. Gall, *Mining Software Evolution to Predict Refactoring*, in: Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007, IEEE, 2007, pp. 354–363. doi:10.1109/ESEM.2007.9.

[16] M. Aniche, E. Maziero, R. Durelli, V. H. S. Durelli, *The Effectiveness of Supervised Machine Learning Algorithms in Predicting Software Refactoring*, Tech. rep., Delft University of Technology (2020).

[17] S. Negara, M. Vakilian, N. Chen, R. E. Johnson, D. Dig, *Is It Dangerous to Use Version Control Histories to Study Source Code Evolution?*, in: European Conference on Object-Oriented Programming, ECOOP 2012: ECOOP 2012 – Object-Oriented Programming, Springer, Berlin, Heidelberg, 2012, pp. 79–103. doi:10.1007/978-3-642-31057-7_5.

[18] M. Kim, T. Zimmermann, N. Nagappan, *An Empirical Study of Refactoring Challenges and Benefits at Microsoft*, Transactions on Software Engineering (2014) 633–649.

[19] T. Menzies, T. Zimmermann, *Software Analytics: So What?*, IEEE Software 30 (4) (2013) 31–37. doi:10.1109/MS.2013.86.

[20] M. Kim, D. Cai, S. Kim, *An Empirical Investigation into the Role of API-Level Refactorings During Software Evolution*, in: 2011 33rd International Conference on Software Engineering (ICSE), Honolulu, HI, USA, 2011, pp. 151–160.

[21] E. Murphy-Hill, C. Parnin, A. P. Black, *How We Refactor, and How We Know It*, in: Proceedings of the 31st International Conference on Software Engineering, IEEE Computer Society, 2009, p. 287–297.

[22] J. Finlay, R. Pears, A. M. A. Connor, *Data Stream Mining for Predicting Software Build Outcomes Using Source Code Metrics*, Information and Software Technology 56 (2) (2014) 183–198. doi:10.1016/j.infsof.2013.09.001.

[23] P. Lerthathairat, N. Prompoon, *An Approach for Source Code Classification Using Software Metrics and Fuzzy Logic to Improve Code Quality with Refactoring Techniques*, in: International Conference on Software Engineering and Computer Systems (ICSECS 2011), 2011, p. 830.

[24] T. H. Chen, S. W. Thomas, A. E. Hassan, *A Survey on the Use of Topic Models When Mining Software Repositories*, Empirical Software Engineering 21 (5) (2016) 1843–1919. doi:10.1007/s10664-015-9402-8.

[25] F. Peters, *On Privacy and Utility While Improving Software Quality*, in: International Conference on Current Trends in Theory and Practice of Computer Science, Vol. 75, SOFSEM SRF, 2017, pp. –.

[26] S. A. Fahrenkrog-Petersen, W. M. van der Aa Hanand, *PRIPEL: Privacy-Preserving Event Log Publishing Including Contextual Information*, in: Business Process Management, Springer International Publishing, Cham, 2020, pp. 111–128.

[27] G. Casale, C. Chesta, P. Deussen, E. Di Nitto, P. Gouvas, S. Koussouris, V. Stankovski, A. Symeonidis, V. Vlassiou, A. Zafeiropoulos, Z. Zhao, *Current and Future Challenges of Software Engineering for Services and Applications*, in: Procedia Computer Science, Vol. 97, Elsevier B.V., 2016, pp. 34–42. doi:10.1016/j.procs.2016.08.278.

[28] F. Rahman, P. Devanbu, *How, and Why, Process Metrics Are Better*, in: Proceedings of the 2013 International Conference on Software Engineering, IEEE Press, San Francisco, CA, USA, 2013, p. 432–441.

[29] J. Caldeira, F. Brito e Abreu, *Software Development Process Mining: Discovery, Conformance Checking, and Enhancement*, in: Proceedings - 2016 10th International Conference on the Quality of Information and Communications Technology, QUATIC 2016, IEEE, 2016, pp. 254–259. doi:10.1109/QUATIC.2016.061.

[30] F. Brito e Abreu, R. Esteves, M. Goulão, *The Design of Eiffel Programs: Qualitative Evaluation Using the MOOD Metrics*, in: R. Ege (Ed.), Proc. of 20th International Conference on Technology of Object-Oriented Languages and Systems (TOOLS’96 USA), Zenodo, Santa Barbara, CA, USA, 1996, pp. —. doi:10.5281/zenodo.1216932.

[31] A. H. Watson, T. J. McCabe, D. R. Wallace, *Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric*, Tech. rep., NIST (1996).

[32] F. Brito e Abreu, *Using OCL to Formalize Object-Oriented Metrics Definitions*, Tech. Rep. ES007/2001, INESC (May 2001).

[33] G. Botterweck, C. Werner (Eds.), *Mastering Scale and Complexity in Software Reuse*, Vol. 10221 LNCS, Springer Verlag, 2017. doi:10.1007/978-3-319-56856-0.

[34] J. Cardoso, J. Mendling, G. Neumann, H. A. Reijers, *A Discourse on Complexity of Process Models*, in: International Conference on Business Process Management, Springer, 2006, pp. 117–128.

[35] I. Vanderfeesten, J. Cardoso, J. Mendling, H. A. Reijers, W. Van Der Aalst, *Quality Metrics for Business Process Models*, Tech. rep., Technische Universiteit Eindhoven (2007).

[36] T. Menzies, T. Zimmermann, *Software Analytics: What’s Next?*, IEEE Software Engineering (2018) 64–70.

[37] D. Zhang, S. Han, Y. Dang, J.-G. Lou, H. Zhang, M. Research Asia, T. Xie, *Software Analytics in Practice*, IEEE Software (2013) 30–37.

[38] F. Akiyama, *An Example of Software System Debugging*, in: C. V. Freiman, J. E. Griffith, J. L. Rosenfeld (Eds.), IFIP Congress (1), North-Holland, 1971, pp. 353–359.

[39] T. J. McCabe, *A Complexity Measure*, IEEE Transactions on Software Engineering SE-2 (4) (1976) 308–320. doi:10.1109/TSE.1976.233837.

[40] T. Hariprasad, G. Vidhyagaran, K. Seenu, C. Thirumalai, *Software Complexity Analysis Using Halstead Metrics*, in: Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017, Vol. 2018-January, Institute of Electrical and Electronics Engineers Inc., 2018, pp. 1109–1113. doi:10.1109/ICOEI.2017.8300883.

[41] B. Curtis, S. B. Sheppard, P. Milliman, *Third Time Charm: Stronger Prediction of Programmer Performance by Software Complexity Metrics*, in: Proceedings of the 4th International Conference on Software Engineering, ICSE ’79, IEEE Press, 1979, pp. 356–360.

[42] B. W. Boehm, *Software Engineering Economics*, 1st Edition, Prentice Hall PTR, Upper Saddle River, NJ, United States, 1981.

[43] S. Henry, D. Kafura, *Software Structure Metrics Based on Information Flow*, IEEE Trans. Softw. Eng. 7 (5) (1981) 510–518. doi:10.1109/TSE.1981.231113.

[44] R. P. L. Buse, T. Zimmermann, *Analytics for Software Development*, Tech. rep., Microsoft Research (2010).

[45] P. Mayer, M. Kirsch, M. A. Le, *On Multi-Language Software Development, Cross-Language Links, and Accompanying Tools: A Survey of Professional Software Developers*, Journal of Software Engineering Research and Development 5 (2017) 1. doi:10.1186/s40411-017-0035-z.

[46] H. Henriques, H. Lourenço, V. Amaral, M. Goulão, *Improving the Developer Experience with a Low-Code Process Modelling Language*, in: Proceedings - 21st ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2018, Association for Computing Machinery, Inc, 2018, pp. 200–210. doi:10.1145/3239372.3239387.

[47] M. Oltrogge, E. Derr, C. Stransky, Y. Acar, S. Fahl, C. Rossow, G. Pellegrino, S. Bugiel, M. Backes, *The Rise of the Citizen Developer: Assessing the Security Impact of Online App Generators*, in: 2018 IEEE Symposium on Security and Privacy (SP), 2018, pp. 634–647. doi:10.1109/SP.2018.00005.

[48] M. Niazi, S. Mahmood, M. Alshayeb, M. R. Riaz, K. Faisal, N. Cerpa, S. U. Khan, I. Richardson, *Challenges of Project Management in Global Software Development: A Client-Vendor Analysis*, Information and Software Technology 80 (C) (2016) 1–19. doi:10.1016/j.infsof.2016.08.002.

[49] L. Cruz, R. Abreu, D. Lo, *To the Attention of Mobile Software Developers: Guess What, Test Your App!*, Empirical Software Engineering 24 (2019) 2438–2468. doi:10.1007/s10664-019-09701-0.

[50] J. Herbsleb, *Building a Socio-Technical Theory of Coordination: Why and How (Outstanding Research Award)*, in: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016, Association for Computing Machinery, New York, NY, USA, 2016, pp. 2–10. doi:10.1145/2950290.2994160.

[51] W. Poncin, A. Serebrenik, M. V. D. Brand, *Process Mining Software Repositories*, 2011 15th European Conference on Software Maintenance and Reengineering (2011) 5–14. doi:10.1109/CSMR.2011.5.

[52] V. A. Rubin, I. Lomazova, W. M. P. v. d. Aalst, *Agile Development with Software Process Mining*, Proceedings of the 2014 International Conference on Software and System Process - ICSSP 2014 (2014) 70–74. doi:10.1145/2600821.2600842.

[53] V. A. Rubin, A. A. Mitsyuk, I. A. Lomazova, W. M. P. van der Aalst, *Process Mining Can Be Applied to Software Too!*, Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement - ESEM ’14 (2014) 1–8. doi:10.1145/2652524.2652583.

[54] C. Ioannou, A. Burattin, B. Weber, *Mining Developers’ Workflows from IDE Usage*, in: Lecture Notes in Business Information Processing, Vol. 316, Springer, 2018, pp. 167–179. doi:10.1007/978-3-319-92898-2_14.

[55] M. Mittal, A. Sureka, *MIMANSA: Process Mining Software Repositories from Student Projects in an Undergraduate Software Engineering Course Categories and Subject Descriptors*, Software Engineering Education and Training — ICSE 2014 (2014) 344–353.

[56] M. Yan, X. Xia, D. Lo, A. E. Hassan, S. Li, *Characterizing and Identifying Reverted Commits*, Empirical Software Engineering 24 (4) (2019) 2171–2208. doi:10.1007/s10664-019-09688-8.

[57] S. Hassan, C. Tantithamthavorn, C. P. Bezemer, A. E. Hassan, *Studying the Dialogue Between Users and Developers of Free Apps in the Google Play Store*, Empirical Software Engineering 23 (3) (2018) 1275–1312. doi:10.1007/s10664-017-9538-9.

[58] L. Bao, Z. Xing, X. Xia, D. Lo, A. E. Hassan, *Inference of Development Activities from Interaction with Uninstrumented Applications*, Empirical Software Engineering 23 (3) (2018) 1313–1351. doi:10.1007/s10664-017-9547-8.

[59] K. Damevski, D. C. Shepherd, J. Schneider, L. Pollock, *Mining Software Repositories for Insights into Developer Behavior*, in: Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015, ACM, Bergamo, Italy, 2015, pp. 1–11. doi:10.1145/2786805.2786815.