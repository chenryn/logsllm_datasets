### Empty Test Cases and Design Issues in AIOps Projects

The third most frequently ignored rule category, design, pertains to the poor design of software. Examples within this category include unstable tests, duplicated string literals, and the use of randomized data in test cases. Common issues such as naming conventions, unused code, and poor design suggest that developers often write AIOps code in an ad hoc manner, which hinders the reusability and maintainability of their projects. By paying more attention to these details, the quality of AIOps solutions can be significantly improved. For instance, three out of the top-ten violated rules are related to naming conventions. Adhering to these conventions can substantially reduce the number of issues.

Moreover, since many of the rules (8 out of the top-10) and rule categories (9 out of the top-10) are common in both AIOps and ML projects, AIOps projects can benefit from the quality assessment and assurance tools and techniques developed for ML systems. These points are discussed in more detail in Section 4.

### Summary of RQ3

Although AIOps projects have a sufficient amount of comments compared to ML and General baselines, they exhibit poorer quality in terms of various metrics, such as bugs, code smells, and technical debt. Code smells are the most prevalent type of issue in AIOps projects. The top-three violated rule categories are naming conventions, unused code, and poor design. Additionally, there is a notable similarity in the violated rules between AIOps and ML projects. Future efforts should focus on reducing these issues and improving the quality of AIOps projects, such as by developing tools to fix bugs and code smells or by establishing coding guidelines. Reducing the ad hoc nature of the code will also enhance reusability and maintainability.

### Discussion

#### Natural Language Processing (NLP) Techniques in AIOps

NLP techniques could receive more attention in AIOps in the coming years. Our results in Section 3.2 indicate that only 2% of the studied projects use NLP techniques. However, approximately one-third of the projects use logs as input data. Prior research has shown that logs can be represented as natural language text because they are generated by logging statements written by humans (Hindle et al., 2016). Other studies have demonstrated that software systems are even more predictable and repetitive than human languages like English (Allamanis and Sutton, 2014; Tu et al., 2014). Therefore, AIOps projects could benefit from leveraging NLP techniques to analyze and model input data such as logs.

#### Increasing Automation and Reducing Human Interventions

More attention should be given to increasing automation and reducing human interventions in AIOps solutions. Most of the studied projects aim to detect and predict anomalies, monitor, and analyze the root causes of failures. However, all these systems require human intervention when the goal is achieved. For example, when an anomaly is detected, an operator must decide the next action. As shown in Figure 5, only 1% of projects have self-healing capabilities, meaning the system can make necessary changes without human intervention. We believe AIOps solutions should move towards greater automation, detecting and resolving incidents autonomously.

#### Paying Attention to Simple Details

Paying attention to simple details can significantly improve the quality of AIOps solutions. In Section 3.3.3, we identified the most common issues in AIOps projects. Some of the most violated rules, such as python:S3776 (cognitive complexity of functions) and python:S1192 (duplication of string literals), are challenging and time-consuming to fix, requiring structural changes in the code. On the other hand, some issues are easy to handle. Three out of the ten most violated rules are related to naming conventions, and adhering to these conventions can greatly reduce the number of issues. Additionally, rules such as python:S2208 (using wildcard imports) and python:S1481 (removing unused variables) are quick to identify and easy to fix. Identifying the most common issues helps practitioners and researchers become aware of them and take measures to reduce them.

#### Utilizing Quality Assurance Tools and Techniques from ML

Most of the tools and techniques used for the quality assurance of machine learning systems can also be applied to AIOps solutions. Our results show that a high proportion of issues in ML and AIOps projects are the same (80% of the most violated rules and 90% of the most violated rule categories are shared). Furthermore, the quality assurance of ML systems has received significant attention in recent years, with various approaches developed to maintain the quality of ML-based systems (Braiek and Khomh, 2022; Nakajima, 2018; Nikanjam et al., 2021, 2022; Poth et al., 2020; Tambon et al., 2023). Therefore, we believe that the tools and techniques developed for quality assurance in ML systems can be effectively utilized in AIOps projects. Applying these tools and techniques may help reduce the quality issues mentioned in Section 3.3.3.

### Threats to Validity

#### External Validity

In this work, we identified and studied a set of AIOps projects on GitHub. These projects may not cover all AIOps projects on GitHub, those hosted on other platforms, or private projects. To maximize our coverage and avoid false positives, we combined automated search, keyword expansion, manual verification, and filtering. Two authors carefully examined each candidate project, and the third author resolved any disagreements. Future work can leverage our replication package to extend the study by analyzing more AIOps projects.

We also collected the ML baseline using the keywords "machine learning" and "deep learning." However, these keywords may not capture all machine learning repositories on GitHub. Future work can expand our results by analyzing more projects extracted from a broader range of keywords.

#### Internal Validity

We studied the code quality metrics of AIOps projects using a variety of metrics (e.g., code smells). However, these metrics may not accurately represent the quality of the projects. To mitigate this threat, we used a diverse set of metrics that represent different aspects of each project. These metrics have been used in other studies to measure code quality (Businge et al., 2019; Lenarduzzi et al., 2019; Tan et al., 2018).

#### Construct Validity

To expand our set of AIOps projects, we performed pattern mining and selected 4 out of 194 pairs of keywords. These keywords were chosen through a systematic process and based on discussions among the authors to reduce bias. All keywords are among the most frequently used.

To answer RQ2, we used qualitative analysis to categorize the input data, analysis techniques, and goals of each AIOps project. Our results may be biased by personal deductions, as with any qualitative study. To mitigate this, two authors performed the qualitative analysis carefully and followed a 5-step process, achieving a Cohen’s kappa value of 0.81, indicating strong and reliable agreement.

For RQ3, we relied on the code issues detected by SonarQube. We are aware that SonarQube might have false positives or false negatives. However, SonarQube claims to have zero false-positives for code smells and bugs. We chose SonarQube because it is one of the most widely used tools for analyzing code quality (Lenarduzzi et al., 2017, 2018).

### Related Work

This work studies AIOps projects on GitHub and analyzes their quality using SonarQube. We discuss related work in the following three areas:

#### AIOps Solutions

In recent years, with the emergence of AIOps, numerous studies have been conducted in this field. Prior works have proposed various solutions for different problems, including anomaly detection (He et al., 2018; Li et al., 2014), failure prediction (Li et al., 2020; Lin et al., 2018; Zhao et al., 2021, 2020), task and job failures (El-Sayed et al., 2017; Gao et al., 2020; Islam and Manivannan, 2017; Rosà et al., 2015), ticket management (Xue et al., 2016, 2018), self-healing (Ding et al., 2014; Lou et al., 2013, 2017), and issue diagnosis (Luo et al., 2014). For example, Gao et al. (2020) attempted to predict task failures in cloud data centers, achieving 93% accuracy. Similarly, Islam et al. (Islam and Manivannan, 2017) characterized Google cluster trace failures and used LSTM to predict task failures. There are also studies focusing on hardware failures (Botezatu et al., 2016; Mahdisoltani et al., 2017; Pinheiro et al., 2007; Schroeder and Gibson, 2007; Xu et al., 2018), performing large-scale studies on disk failures. Unlike these studies, our work focuses on understanding the practices and characteristics of real-world AIOps projects on GitHub.

#### Characterizing GitHub Projects

As the largest hosting service for open-source software, GitHub has been extensively studied. Some studies focus on finding the most prominent repositories (Dabic et al., 2021; Kalliamvakou et al., 2014, 2016). Many studies have used GitHub to mine software repositories (Businge et al., 2019; Coelho et al., 2018; Guzman et al., 2014; Horschig et al., 2018; Kallis et al., 2021; Lopes et al., 2017; Manes and Baysal, 2021; Subramanian et al., 2020; Vadlamani and Baysal, 2020; Wessel et al., 2018). For example, Vadlamani et al. (Vadlamani and Baysal, 2020), along with Subramanian et al. (Subramanian et al., 2020) and Horschig et al. (Horschig et al., 2018), characterize the developers of open-source software. Kallis et al. (Kallis et al., 2021) predict the issue types of projects. More similar to our study, Ghrairi et al. (Ghrairi et al., 2018) study the state of Virtual Reality (VR) projects on GitHub. Coppola et al. (Coppola et al., 2019) characterize the popularity of Kotlin in Android projects by analyzing 1,232 applications on GitHub. To our knowledge, no studies have been conducted specifically on AIOps. Our work is the first to use GitHub to characterize and analyze AIOps projects.

#### Analyzing Code Quality Using SonarQube

SonarQube is one of the most popular static code analysis tools used in academia (Lenarduzzi et al., 2017, 2018) and industry (Vassallo et al., 2020). Previous research has analyzed the code quality of open-source software projects using SonarQube. Businge et al. (Businge et al., 2019) used SonarQube to find the number of bugs in 119 applications. Tan et al. (Tan et al., 2018) studied 9 Apache software systems written in Python to investigate their technical debt. Lenarduzzi et al. (Lenarduzzi et al., 2019) investigated the technical debt of 33 Apache systems written in Java, finding that the amount of code smell is much higher than bugs or vulnerabilities. They also reported that code smells with major severity take the longest time to fix. Compared to these studies, our paper analyzes the quality of 6,101 projects, making our results more robust and confident.

### Conclusion and Future Work

This work studies the characteristics of AIOps projects on GitHub and compares them with two baselines (ML and General projects). We combine quantitative and qualitative analyses to understand the current state of AIOps solutions. Specifically, we illustrate the state of AIOps projects on GitHub in RQ1, determine the most common input data, techniques, and goals of AIOps solutions in RQ2, and identify the most common issues in terms of code quality in RQ3. We observe that AIOps projects are relatively new and growing rapidly, but their quality is poorer compared to the baselines. Furthermore, we uncover common patterns in the input data, analysis techniques, and goals of AIOps projects. Practitioners and researchers can learn from these patterns and adopt AIOps solutions that are optimal for their specific application scenarios. Our findings highlight future efforts to improve AIOps practices, such as paying more attention to weak aspects (e.g., self-healing) and reducing the ad hoc nature of coding to enhance AIOps quality.

Our study can be extended in several ways. First, we focused on publicly available AIOps projects on GitHub. However, there may be differences between open-source and closed-source projects implemented for private datasets in companies. Thus, one extension could be to gather and analyze industrial AIOps solutions and report any similarities and differences with GitHub projects. Second, based on our results regarding inputs, techniques, and goals, future work could develop a pipeline to automate or augment the development of AIOps solutions. Finally, we discussed the possibility of adopting quality assurance techniques from machine learning systems into AIOps. Future work could implement these techniques for AIOps solutions and report their effects on code quality.

### References

- Allamanis M, Sutton C (2014) Mining idioms from source code. In: Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, pp 472–483
- Artstein R, Poesio M (2008) Inter-coder agreement for computational linguistics. Computational Linguistics 34(4):555–596
- Basili VR, Selby RW, Hutchens DH (1986) Experimentation in software engineering. IEEE Transactions on Software Engineering (7):733–743
- Botezatu MM, Giurgiu I, Bogojeska J, Wiesmann D (2016) Predicting disk replacement towards reliable data centers. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp 39–48
- Braiek HB, Khomh F (2022) Testing feedforward neural networks training programs. ACM Trans Softw Eng Methodol DOI 10.1145/3529318, URL https://doi.org/10.1145/3529318
- Businge J, Openja M, Kavaler D, Bainomugisha E, Khomh F, Filkov V (2019) Studying Android app popularity by cross-linking GitHub and Google Play Store. In: 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER), IEEE, pp 287–297
- Chen Z, Cao Y, Liu Y, Wang H, Xie T, Liu X (2020) A comprehensive study on challenges in deploying deep learning-based software. In: Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp 750–762
- Chen Z, Liu J, Gu W, Su Y, Lyu MR (2021) Experience report: deep learning-based system log analysis for anomaly detection. arXiv preprint arXiv:210705908