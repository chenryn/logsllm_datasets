### Vulnerability Assessment

In addition to code coverage, we also evaluate the scanners' effectiveness in detecting vulnerabilities. This includes assessing the number of vulnerabilities they can identify and the number of false positives they generate. Our study focuses on both reflected and stored Cross-Site Scripting (XSS) vulnerabilities.

To evaluate the vulnerability detection capabilities of the scanners, we collect and process all reported vulnerabilities. We follow a two-step process:
1. **Manual Analysis**: We manually verify if the reported vulnerabilities can be reproduced or if they should be considered false positives.
2. **Clustering**: We cluster similar vulnerability reports into unique vulnerabilities to ensure a fair comparison between different reporting mechanisms. This is necessary because some applications, like SCARF, can generate an infinite number of vulnerabilities by dynamically adding new input fields. These should be clustered together.

Classifying the uniqueness of vulnerabilities is challenging. Our goal is to cluster each injection so that it corresponds to a unique line of code on the server. For example, if a form has multiple fields that are all stored using the same SQL query, these should count as one injection. The rationale is that fixing this would only require the developer to change one line in the server code. Similarly, for reflected injections, we cluster parameters of the same request together. We manually inspect the web application source code for each true-positive vulnerability to determine if they should be clustered.

### Scanners

We compare our scanner, Black Widow, with Wget [21] for code coverage reference and six state-of-the-art open-source web vulnerability scanners from both academia and the web security community: Arachni [18], Enemy of the State [13], j ¨Ak [8], Skipfish [22], w3af [16], and ZAP [17].

- **Academic Scanners**: Enemy of the State and j ¨Ak are included as they are state-of-the-art academic blackbox scanners.
- **Benchmark Scanners**: Skipfish, Wget, and w3af are included as they serve as good benchmarks when comparing with previous studies [13], [8].
- **Modern Open-Source Scanners**: Arachni and ZAP are modern open-source scanners used in recent studies [23].

While a pure crawler with JavaScript capabilities, such as CrawlJAX [9], could serve as a good coverage reference, this paper focuses on coverage compared to other vulnerability scanners. We still include Wget for comparison with previous studies.

We configure the scanners with the correct credentials for the web application. When this is not possible, we change the default credentials of the application to match the scanner’s default values. Since the scanners have different capabilities, we try to configure them as similarly as possible, including activating crawling components (both static and dynamic) and all types of XSS vulnerability detection.

### Time Performance

Comparing the time performance between scanners is non-trivial due to differences in programming languages and execution models (some are sequential while others run in parallel). Additionally, some older scanners need to be run in virtual machines (VMs) for compatibility reasons. To avoid infinite scans, we limit each scanner to run for a maximum of eight hours.

### Web Applications

To ensure the scanners can handle different types of web applications, we test them on ten different applications. The applications range from reference applications used in previous studies to newer production-grade applications. Each application runs in a VM that we can reset between runs to improve consistency.

The applications are divided into two sets:
- **Reference Applications with Known Vulnerabilities**:
  - phpBB (2.0.23)
  - SCARF (2007)
  - Vanilla (2.0.17.10)
  - WackoPicko (2018)

- **Modern Production-Grade Applications**:
  - Drupal (8.6.15)
  - HotCRP (2.102)
  - Joomla (3.9.6)
  - osCommerce (2.3.4.1)
  - PrestaShop (1.7.5.1)
  - WordPress (5.1)

### Code Coverage Results

This section presents the code coverage in each web application by all the crawlers. Table I shows the number of unique lines of code covered by each scanner.

| Scanner | Application 1 | Application 2 | Application 3 | ... | Application 10 |
|---------|---------------|---------------|---------------|-----|----------------|
| Black Widow | ... | ... | ... | ... | ... |
| Wget | ... | ... | ... | ... | ... |
| Arachni | ... | ... | ... | ... | ... |
| Enemy of the State | ... | ... | ... | ... | ... |
| j ¨Ak | ... | ... | ... | ... | ... |
| Skipfish | ... | ... | ... | ... | ... |
| w3af | ... | ... | ... | ... | ... |
| ZAP | ... | ... | ... | ... | ... |

The table provides a comprehensive overview of the code coverage achieved by each scanner across the ten web applications.