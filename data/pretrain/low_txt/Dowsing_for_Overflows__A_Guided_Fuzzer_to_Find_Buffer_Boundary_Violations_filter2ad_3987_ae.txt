### Enhanced Results and Comparative Analysis

The observed results can be attributed to the simplicity of our test suite, which allowed execution paths to quickly reach the vulnerability conditions. In contrast, more complex applications involve intensive input processing, diverting symbolic execution away from the critical code regions. In all these scenarios, Dowser consistently identifies bugs at a significantly faster rate. Even when considering the 15-minute tests conducted by higher-ranking analysis groups, Dowser provides a substantial improvement over existing systems.

### Related Work

Dowser is a guided fuzzer that integrates knowledge from multiple domains. This section situates our system within the context of existing approaches, starting with the scoring function and selection of code fragments. We then discuss traditional fuzzing, dynamic taint analysis in fuzzing, and finally, whitebox fuzzing and symbolic execution.

#### Software Complexity Metrics

Numerous studies have shown that software complexity metrics are positively correlated with defect density or security vulnerabilities [29, 35, 16, 44, 35, 32]. However, Nagappan et al. [29] argue that no single set of metrics fits all projects, while Zimmermann et al. [44] emphasize the need for metrics that exploit the unique characteristics of vulnerabilities, such as buffer overflows or integer overruns. These approaches generally consider a broad class of post-release defects or security vulnerabilities, using generic measurements like the number of basic blocks in a function's control flow graph, the number of global or local variables read or written, and the maximum nesting level of if or while statements. Dowser, on the other hand, focuses on a narrow group of security vulnerabilities, specifically buffer overflows. Our scoring function is tailored to reflect the complexity of pointer manipulation instructions, making it unique in this respect.

#### Traditional Fuzzing

Software fuzzing gained prominence in the 1990s when Miller et al. [25] demonstrated how random inputs could crash 25-33% of UNIX utilities. More advanced fuzzers, such as Spike [39] and SNOOZE [5], generate deliberately malformed inputs. Later fuzzers, aimed at deeper bugs, often use input grammars (e.g., Kaksonen [20] and [40]). DeMott [13] provides a comprehensive survey of fuzz testing tools. As noted by Godefroid et al. [18], traditional fuzzers are useful but typically find only shallow bugs.

#### Application of Dynamic Taint Analysis (DTA) to Fuzzing

BuzzFuzz [15] uses DTA to identify regions of seed input files that influence values used at library calls. They focus on library calls because they are often developed by different people and may lack a perfect API description. BuzzFuzz does not use symbolic execution, relying solely on DTA to ensure the correct input format. Unlike Dowser, it ignores implicit flows, making it unable to find bugs like the one in nginx (Figure 1). Additionally, Dowser is more selective in applying DTA, explicitly choosing complex code fragments, which is challenging to do with library calls alone.

TaintScope [42] also uses DTA to select fields of the input seed that influence security-sensitive points, such as system/library calls. It can identify and bypass checksum checks. Like BuzzFuzz, it differs from Dowser by ignoring implicit flows and focusing only on library calls. TaintScope operates at the binary level rather than the source.

#### Symbolic-Execution-Based Fuzzing

Recent interest in whitebox fuzzing, symbolic execution, concolic execution, and constraint solving has led to the development of systems like EXE [8], KLEE [7], CUTE [33], DART [17], SAGE [18], and the work by Moser et al. [28]. For example, Microsoft’s SAGE starts with a well-formed input and symbolically executes the program to explore all feasible execution paths, checking security properties using AppVerifier. These systems substitute some program inputs with symbolic values, gather input constraints, and generate new inputs to exercise different paths. While powerful, they struggle with scalability, especially with many loop-based array accesses, due to the rapid growth in the number of paths.

Zesti [24] takes a different approach by symbolically executing existing regression tests to check if they can trigger vulnerable conditions by slightly modifying the test input. This technique scales better and is useful for finding bugs near existing test suites but is less effective for bugs far from these paths. For instance, a generic input exercising the vulnerable loop in Figure 1 has a URI of the form "///{arbitrary characters}", and the shortest input triggering the bug is "///../". When fed with "///abc", Zesti does not find the bug. Instead, it requires an input closer to the vulnerability condition, such as "///..{an arbitrary character}". Dowser, however, finds the bug with the generic input.

SmartFuzz [27] focuses on integer bugs, using symbolic execution to construct test cases that trigger arithmetic overflows, non-value-preserving width conversions, or dangerous signed/unsigned conversions. In contrast, Dowser targets the more common and harder-to-find case of buffer overflows. Babić et al. [4] guide symbolic execution to potentially vulnerable program points detected with static analysis. However, their interprocedural, context- and flow-sensitive static analysis does not scale well to real-world programs, and their experimental results are limited to short traces.

### Conclusion

Dowser is a guided fuzzer that combines static analysis, dynamic taint analysis, and symbolic execution to find buffer overflow vulnerabilities deep in a program's logic. It first determines 'interesting' array accesses, ranks them by complexity, and uses taint analysis to identify influencing inputs. It then makes these bytes symbolic for subsequent symbolic execution, selecting paths most likely to lead to overflows. Each step contains novel contributions, resulting in a practical and scalable fuzzing approach that can find complex bugs in real applications, which would be hard or impossible to find with existing techniques. Moreover, Dowser introduces a novel 'spot-check' approach to finding buffer overflows in real software.

### Acknowledgment

This work is supported by the European Research Council through project ERC-2010-StG 259108-ROSETTA, the EU FP7 SysSec Network of Excellence, and the Microsoft Research PhD Scholarship Programme through the project MRL 2011-049. The authors would like to thank Bartek Knapik for his help in designing the statistical evaluation.

### References

[1] CVE-2009-2629: Buffer underflow vulnerability in nginx. http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2009-2629, 2009.
[2] ACZEL, A. D., AND SOUNDERPANDIAN, J. Complete Business Statistics, sixth ed. McGraw-Hill, 2006.
[3] AKRITIDIS, P., CADAR, C., RAICIU, C., COSTA, M., AND CASTRO, M. Preventing memory error exploits with WIT. In Proceedings of the 2008 IEEE Symposium on Security and Privacy (2008), S&P’08.
[4] BABIĆ, D., MARTIGNONI, L., MCCAMANT, S., AND SONG, D. Statically-directed dynamic automated test generation. In Proceedings of the 2011 International Symposium on Software Testing and Analysis (2011), ISSTA’11.
[5] BANKS, G., COVA, M., FELMETSGER, V., ALMEROTH, K., KEMMERER, R., AND VIGNA, G. SNOOZE: toward a stateful network protocol fuzZEr. In Proceedings of the 9th international conference on Information Security (2006), ISC’06.
[6] BAO, T., ZHENG, Y., LIN, Z., ZHANG, X., AND XU, D. Strict control dependence and its effect on dynamic information flow analyses. In Proceedings of the 19th International Symposium on Software testing and analysis (2010), ISSTA’10.
[7] CADAR, C., DUNBAR, D., AND ENGLER, D. KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs. In Proceedings of the 8th USENIX Symposium on Operating Systems Design and Implementation (2008), OSDI’08.
[8] CADAR, C., GANESH, V., PAWLOWSKI, P. M., DILL, D. L., AND ENGLER, D. R. EXE: Automatically generating inputs of death. In CCS ’06: Proceedings of the 13th ACM conference on Computer and communications security (2006).
[9] CAVALLARO, L., SAXENA, P., AND SEKAR, R. On the Limits of Information Flow Techniques for Malware Analysis and Containment. In Proceedings of the Fifth Conference on Detection of Intrusions and Malware & Vulnerability Assessment (2008), DIMVA’08.
[10] CHIPOUNOV, V., KUZNETSOV, V., AND CANDEA, G. S2E: A platform for in vivo multi-path analysis of software systems. In Proceedings of the 16th Intl. Conference on Architectural Support for Programming Languages and Operating Systems (2011), ASPLOS’11.
[11] COWAN, C., PU, C., MAIER, D., HINTONY, H., WALPOLE, J., BAKKE, P., BEATTIE, S., GRIER, A., WAGLE, P., AND ZHANG, Q. StackGuard: Automatic Adaptive Detection and Prevention of Buffer-Overflow Attacks. In Proceedings of the 7th USENIX Security Symposium (1998), SSYM’98.
[14] FERRANTE, J., OTTENSTEIN, K. J., AND WARREN, J. D. The program dependence graph and its use in optimization. ACM Trans. Program. Lang. Syst. 9 (1997), 319–349.
[15] GANESH, V., LEEK, T., AND RINARD, M. Taint-based directed whitebox fuzzing. In Proceedings of the 31st International Conference on Software Engineering (2009), ICSE’09.
[16] GEGICK, M., WILLIAMS, L., OSBORNE, J., AND VOUK, M. Prioritizing software security fortification through code-level metrics. In Proc. of the 4th ACM workshop on Quality of protection (Oct. 2008), QoP’08, ACM Press.
[17] GODEFROID, P., KLARLUND, N., AND SEN, K. DART: directed automated random testing. In Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation (2005), PLDI’05.
[18] GODEFROID, P., LEVIN, M. Y., AND MOLNAR, D. A. Automated Whitebox Fuzz Testing. In Proceedings of the 15th Annual Network and Distributed System Security Symposium (2008), NDSS’08.
[19] GODEFROID, P., AND LUCHAUP, D. Automatic partial loop summarization in dynamic test generation. In Proceedings of the 2011 International Symposium on Software Testing and Analysis (2011), ISSTA’11.
[20] KAKSONEN, R. A functional method for assessing protocol implementation security. Tech. Rep. 448, VTT, 2001.
[21] KANG, M. G., MCCAMANT, S., POOSANKAM, P., AND SONG, D. DTA++: Dynamic taint analysis with targeted control-flow propagation. In Proceedings of the 18th Annual Network and Distributed System Security Symposium (2011), NDSS’11.
[22] KHURSHID, S., PĂSĂREANU, C. S., AND VISSER, W. Generalized symbolic execution for model checking and testing. In Proceedings of the 9th international conference on Tools and algorithms for the construction and analysis of systems (2003), TACAS’03.
[23] LATTNER, C., AND ADVE, V. LLVM: A compilation framework for lifelong program analysis & transformation. In Proceedings of the international symposium on Code generation and optimization (2004), CGO’04.
[24] MARINESCU, P. D., AND CADAR, C. make test-zesti: a symbolic execution solution for improving regression testing. In Proc. of the 2012 International Conference on Software Engineering (June 2012), ICSE’12, pp. 716–726.
[25] MILLER, B. P., FREDRIKSEN, L., AND SO, B. An empirical study of the reliability of UNIX utilities. Commun. ACM 33 (Dec 1990), 32–44.
[26] MITRE. Common Vulnerabilities and Exposures (CVE). http://cve.mitre.org/, 2011.
[27] MOLNAR, D., LI, X. C., AND WAGNER, D. A. Dynamic test generation to find integer bugs in x86 binary Linux programs. In Proceedings of the 18th conference on USENIX security symposium (2009), SSYM’09.
[28] MOSER, A., KRUEGEL, C., AND KIRDA, E. Exploring multiple execution paths for malware analysis. In Proceedings of the 2007 IEEE Symposium on Security and Privacy (2007), SP’07, IEEE Computer Society.
[29] NAGAPPAN, N., BALL, T., AND ZELLER, A. Mining metrics to predict component failures. In Proceedings of the 28th international conference on Software engineering (2006), ICSE’06.
[30] NETHERCOTE, N., AND SEWARD, J. Valgrind: A Framework for Heavyweight Dynamic Binary Instrumentation. In Proceedings of the Third International ACM SIGPLAN/SIGOPS Conference on Virtual Execution Environments (2007), VEE’07.
[31] NEWSOME, J., AND SONG, D. Dynamic taint analysis: Automatic detection, analysis, and signature generation of exploit attacks on commodity software. In Proceedings of the Network and Distributed Systems Security Symposium (2005), NDSS’05.
[32] NGUYEN, V. H., AND TRAN, L. M. S. Predicting vulnerable software components with dependency graphs. In Proc. of the 6th International Workshop on Security Measurements and Metrics (Sept. 2010), MetriSec’10, ACM Press.
[33] SEN, K., MARINOV, D., AND AGHA, G. CUTE: a concolic unit testing engine for C. In Proceedings of the 10th European software engineering conference held jointly with 13th ACM SIGSOFT international symposium on Foundations of software engineering (2005), ESEC/FSE-13.
[34] SEREBRYANY, K., BRUENING, D., POTAPENKO, A., AND VYUKOV, D. AddressSanitizer: A fast address sanity checker. In Proceedings of USENIX Annual Technical Conference (2012).
[35] SHIN, Y., AND WILLIAMS, L. An initial study on the use of execution complexity metrics as indicators of software vulnerabilities. In Proceedings of the 7th International Workshop on Software Engineering for Secure Systems (2011), SESS’11.
[36] SLOWINSKA, A., AND BOS, H. Pointless tainting?: evaluating the practicality of pointer tainting. In EuroSys ’09: Proceedings of the 4th ACM European conference on Computer systems (2009).
[37] SLOWINSKA, A., STANCESCU, T., AND BOS, H. Body Armor for Binaries: preventing buffer overflows without recompilation. In Proceedings of USENIX Annual Technical Conference (2012).
[38] SOTIROV, A. Modern exploitation and memory protection bypasses. Invited talk, USENIX Security 2009. http://www.usenix.org/events/sec09/tech/slides/sotirov.pdf, 2009.
[39] SPIKE. http://www.immunitysec.com/resources-freesoftware.shtml.
[40] SUTTON, M., GREENE, A., AND AMINI, P. Fuzzing: Brute Force Vulnerability Discovery. Addison-Wesley Professional, 2007.
[41] VAN DER VEEN, V., DUTT-SHARMA, N., CAVALLARO, L., AND BOS, H. Memory Errors: The Past, the Present, and the Future. In Proceedings of The 15th International Symposium on Research in Attacks, Intrusions and Defenses (2012), RAID’12.
[42] WANG, T., WEI, T., GU, G., AND ZOU, W. TaintScope: A Checksum-Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection. In Proceedings of the 31st IEEE Symposium on Security and Privacy (2010), SP’10.
[43] WILLIAMS, N., MARRE, B., AND MOUY, P. On-the-Fly Generation of K-Path Tests for C Functions. In Proceedings of the 19th IEEE international conference on Automated software engineering (2004), ASE’04.
[44] ZIMMERMANN, T., NAGAPPAN, N., AND WILLIAMS, L. Searching for a Needle in a Haystack: Predicting Security Vulnerabilities for Windows Vista. In Proc. of the 3rd International Conference on Software Testing, Verification and Validation (Apr. 2010), ICST’10.
[45] ZITSER, M., LIPPMANN, R., AND LEEK, T. Testing static analysis tools using exploitable buffer overflows from open source code. In Proc. of the 12th ACM SIGSOFT twelfth international symposium on Foundations of software engineering (Nov. 2004), SIGSOFT ’04/FSE-12.