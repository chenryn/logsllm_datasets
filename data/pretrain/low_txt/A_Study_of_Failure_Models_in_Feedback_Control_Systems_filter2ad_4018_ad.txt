### 4.2. The Fail-Bounded Approach

From the previous results, the types of faults that can cause the pendulum to collapse are:
- Errors affecting the program code
- Errors affecting data that is not periodically recalculated (constants)

Since the code in this type of controller is usually stored in ROM and/or protected by a signature and regularly verified by diagnostic code, it is reasonable to expect that such errors will not go undetected in real applications. The experiments show that even if corrupted code is executed during one iteration, if the corruption is detected within a few control cycles, any erroneous output produced would be harmless to the process. Since the diagnostic code can easily verify the check code of the program every few iterations, these errors are of no substantial consequence.

Thus, the most critical part to be protected is the permanent data used by the program—data that is not recalculated in all iterations or at least not frequently. While the constants used by the application can also be stored in ROM, they may be copied to RAM during execution, for example, for performance reasons. Other permanent data is only collected or calculated during the controller/process setup phase, such as calibration data. Therefore, in the Fail-Bounded model, we have protected all this data with checksums to enable the detection of almost all potentially serious errors.

However, we cannot assume that these methods have 100% effectiveness, nor that there are no other sources of errors that can lead to the collapse of the pendulum. We need some form of end-to-end protection [15], which is exactly what well-designed assertions can provide. To fully implement the Fail-Bounded model, we still need to decide on the appropriate assertions. These should allow non-critical errors to pass but filter out serious ones, thus providing a compromise between the Fail-Arbitrary and Fail-Silent models.

The usual approach to defining assertions is to look at the control algorithm and derive a simpler expression that detects gross deviations from the correct result. However, in many control systems, the control equation is only a couple of high-level instructions long, making it difficult to define a "simpler" expression. Additionally, although the equation itself is derived from an analytical analysis of the system, the exact values of several parameters are experimentally tuned, making it more challenging to derive and tune a good assertion. A significant additional hurdle is that the assertions should not react to an isolated wrong output (as seen in our experiments, arbitrarily large output errors can occur as long as they do not last long) but to a series of significantly wrong outputs. Considering time in this manner is not simple.

To solve this problem, we can take a radically different approach: instead of looking at the stream of outputs from the controller, we should look at the controlled system. The robust assertions underlying the concept of fail-boundedness should be applied to the system as a whole (controller + process) rather than solely to the controller. The best indicator of the quality of control is the behavior of the controlled system itself. We just need to define an acceptable interval around the set-point, such that whenever the pendulum leaves this interval, it is flagged as an error. This view is more related to the concept of "quality of service" rather than to the more traditional "consistency checks." In fact, the consistency of the process is checked rather than the consistency of the outputs produced, and time is inherently taken into account because only lasting perturbations will result in the system leaving that interval. This approach is general and can be applied to almost every feedback control system: every application's state-space has an "envelope" of execution stability that can be used to deduce the quality/effectiveness of the control applied. In fact, this quality of service is the metric used in every optimizing control technique. Again, we are using knowledge from the analysis of the control problem to improve dependability.

For the experiments described next, the bounds we set in the input assertions were chosen based on observations of the system behavior and not derived by any rigorous method, which should be the subject of another study. Specifically, if the rod angle or the cart position of the inverted pendulum exceeds 60% of the crash angle or position (defined in section 3.2), the assertion signals an error.

The results from the injection of a new series of 597 faults inducing permanent errors solely in constant data used by the algorithm are presented in Table 5.

| Controller Behavior | Detected Error | Crash | Correct Output | Undet. Wrong Output |
|-------------------|---------------|-------|----------------|---------------------|
| 98.8%             | 0%            | 0%    | 7              |

**Table 5. Behavior of the Fail-Bounded controllers to faults injected in constant data (70 and no. of cases)**

The pendulum did not fall down for the 1.2% of wrong outputs generated. Of the 590 detected errors, 12 were detected only by the assertions; all others were detected by the simple checksums used. This confirms that a careful protection of these structures can detect the occurrence of the most dangerous errors for this kind of system. If better check codes were used, the figure of 1.2% of undetected wrong outputs could certainly be lowered. It is also relevant to note that there were no vanished errors or crashes since only pure data errors were injected in these experiments. There were three false alarms, meaning that an error was detected, but the controller output was equal to the output of the reference controller. This was likely a case where the checksums themselves were corrupted, but not the data.

It must be stressed that the Fail-Arbitrary and Fail-Bounded figures were achieved with a simplex computer without any structural redundancy, as opposed to the Fail-Silent version, which used two machines and voting. This means that it is possible to achieve, for this setup and fault model, higher availability using only one controller than using two, because the Fail-Silent model leads to many unnecessary stops of the system. The Fail-Bounded model thus retains the advantage of low redundancy of the Fail-Arbitrary model while not losing control of the system.

### 5. Conclusions and Future Work

In this paper, we present a study showing that the feedback used in most continuous control systems to compensate for external disturbances can also be used to compensate for errors in the controller itself. We have not considered permanent faults, which could lead to more frequent loss of control. However, we observed that whenever permanent errors are generated (the faults change the program code or permanent data), a quick diagnosis can provide a disciplined shutdown or even restart the system in time to recover control. Permanent faults are relatively easy to detect by means of a watchdog timer, in case of a crash, or by periodic diagnostic routines that use up the application idle time.

Another interesting conclusion of this study is that, in feedback systems like the one studied, the Fail-Silent model is clearly inappropriate because it flags failure situations where the system suffers no negative impact. Essentially, the Fail-Silent model lacks the notion of time—a single erroneous output is not significant, only a sequence of erroneous outputs is—and fails to take into account the fact that the natural inertia of the controlled system filters out short-lived disturbances. We show that the Fail-Bounded model, with assertions based on the behavior of the controlled system, is a much better failure model for these systems compared to both Fail-Silent and Fail-Arbitrary. This conclusion cannot be generalized for every class of applications.

It is fair to say that there is a characteristic of feedback control systems that has not been sufficiently addressed in this study, which may impact the conclusion about the inadequacy of the Fail-Silent model: the level of "memory" of the control algorithm. Since the inverted pendulum is a "fast" system, only the inputs of the present and previous iterations are needed to calculate the next output. In other "slower" systems (for instance, the control of temperature in industrial boilers), which use e.g., PI (Proportional Integrative) algorithms, the system integrates the previous history of the controlled system and uses that integration in the calculation of the next output. In those systems, the consequences of a short-lived perturbation will last longer. It can also be argued that since the system is also slower, these perturbations will have no consequences. Further study is needed to clarify this point.

Another contribution of this study is a different way of defining effective assertions. Instead of analyzing the outputs of the controller, the behavior of the controlled system should be taken into account. While it is not possible to fully characterize the best assertions for every application, we claim that truly effective assertions should not check the outputs of the controller per se but instead the quality of service delivered by the controller. This approach encompasses the sensors, controller, actuators, and the application. In feedback control applications, it makes no sense to consider these components separately; they should be viewed as a whole, as the feedback control equations consider the full control loop.

In our opinion, another very interesting conclusion can be drawn from this study regarding real-time scheduling. Many scheduling algorithms have been proposed with the aim that no hard deadline should be missed, lest some catastrophe may happen. The observations presented in this paper show that for the type of systems studied, neither the deadlines are so hard nor errors have dramatic consequences as long as they are corrected in subsequent iterations of the control loop. These results are not valid only in rare circumstances; on the contrary, the inverted pendulum is one of the most time-critical control applications. These observations confirm a concept well known in industry, that of grace-time, i.e., the time that an application can run without control, exhibiting no significant or even null consequences. A more thorough study is required to fully characterize the number of deadlines that can be lost versus the errors present in the control loop, and different scheduling algorithms should be developed to take that into account. We have done some work in that direction in the past [16].

There is a final area of open research that we would like to mention. It is not clear whether these or similar conclusions also apply to discrete systems, as they are not so forgiving of short-lived disturbances. This is another point of research currently underway.

### 6. Acknowledgements

We acknowledge the fruitful discussions with the members of the Control System group of CISUC at the University of Coimbra, namely Jorge Henriques and Alberto Cardoso, and thank Professor António Dourado for letting us use the inverted pendulum testbed used in these experiments.

### References

[1] A. Avizienis, “Building Dependable Systems: How to Keep Up with Complexity” Special Issue of the FTCS-25, Pasadena-CA, 1995, pp. 4-14.

[2] J. Carreira, H. Madeira, J. G. Silva, “Xception: Software Fault Injection and Monitoring in Processor Functional Units”, DCCA ‘95, Urbana, Champaign-USA, 1995, pp. 135-149.

[3] Chillarege, R., and N. S. Bowen, “Understanding Large System Failures: A Fault Injection Experiment”, FTCS-19, 1989.

[4] J. C. Cunha, M. Z. Rela, J. G. Silva, “Can Software-Implemented Fault-Injection be used on Real-Time Systems?”, EDCC-3, Prague, Czech Republic, September 1999.

[5] M. Hiller, “Executable Assertions for Detecting Data Errors in Embedded Control Systems”, DSN’2000, New York, June 2000, pp. 24-33.

[6] J. C. Laprie, “Dependability: Basic Concepts and Terminology”, Springer-Verlag, 1991.

[7] M. Pease, R. Shostak, L. Lamport, “Reaching Agreement in the presence of faults”, Journal of the ACM, 25, 1980, pp. 228-234.

[8] D. Powell, P. Verissimo, G. Bonn, F. Waeselynck, and D. Seaton, “The Delta-4 Approach to Dependability in Open Distributed Computing Systems”, FTCS-18, Tokyo, 1988.

[9] B. Randell, “System Structure for Software Fault-Tolerance”, IEEE Trans. on Software Engineering, SE-1(2), 1975, pp. 220-232.

[10] M. Rela, H. Madeira, J.G. Silva, “Experimental Evaluation of the Fail-Silent Behavior of Programs with Consistency Checks”, FTCS-26, Sendai, Japan, 1996.

[11] L. Sha, “Dependable System Upgrade”, RTSS’98, Madrid, Spain, December 1998, pp. 440-448.

[12] J. G. Silva, P. Prata, M. Rela, H. Madeira, “Practical Issues in the Use of ABFT and a New Failure Model”, FTCS-28, Munich, Germany, 1998, pp. 26-35.

[13] “PS600 Laboratory Experiment Inverted Pendulum - Manual” - Amira GmbH, Germany, 1996.

[14] http://www.smxinfo.com

[15] J. H. Saltzer, D. P. Reed, D. D. Clark, “End-To-End Arguments in System Design”, ACM Transactions in Computer Systems, Vol 2, N. 4, November 1984, pp. 277-288.

[16] A. P. Magalhães, J. G. Silva, “Stabilizing Pre-Run-Time Schedules with the Help of Grace Time”, Real-Time Systems, 17(1), July 1999, pp. 65-86.