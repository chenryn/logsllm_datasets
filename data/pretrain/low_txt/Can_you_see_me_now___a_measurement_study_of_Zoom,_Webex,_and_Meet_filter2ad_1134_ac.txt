### Experimental Setup and Observations

In our experiments, we collected 700-800 latency measurements from each of six virtual machines (VMs) for a specific meeting host. We conducted these experiments on Zoom, Webex, and Google Meet. In a second set of experiments, we used seven VMs deployed in Europe, as detailed in Table 3, and repeated the experiments with meeting hosts in UK-west and Switzerland.

#### Service Endpoint Interactions

We observed that the multi-user sessions in this experiment were relayed through platform-operated service endpoints with designated fixed port numbers: UDP/8801 for Zoom, UDP/9000 for Webex, and UDP/19305 for Meet. Figure 3 compares the three videoconferencing systems in terms of how their clients interact with these service endpoints for content streaming, based on traffic traces.

- **Zoom and Webex**: A single service endpoint is designated for each meeting session, and all participants send or receive streaming data via this endpoint.
- **Meet**: Each client connects to a separate, geographically close-by endpoint, and meeting sessions are relayed among these multiple distinct endpoints.

The number of distinct service endpoints encountered by a client varies significantly across platforms. For example, out of 20 videoconferencing sessions:
- **Zoom**: A client encounters, on average, 20 endpoints.
- **Webex**: A client encounters, on average, 19.5 endpoints.
- **Meet**: A client encounters, on average, 1.8 endpoints.

On Zoom and Webex, service endpoints almost always change (with different IP addresses) across different sessions. On Meet, a client tends to stick with one or two endpoints across sessions.

#### Streaming Lag Analysis

Figures 4–7 plot the cumulative distribution functions (CDFs) of streaming lag experienced by clients in four different scenarios. In Figures 4 and 5, we consider videoconferencing sessions among seven US-based clients (including a meeting host), where the host is located in either US-east or US-west. In Figures 6 and 7, we set up videoconferencing sessions among seven clients in Europe, with a meeting host in either UK or Switzerland.

##### US-based Videoconferencing
- **US-east Host (Figure 4)**: Across all three platforms, streaming lag increases as clients are further away from US-east, with US-west clients experiencing the most lags (about 30 ms higher than the US-east client). This suggests that streaming is relayed via servers in US-east, where the meeting host resides. This is confirmed by Figure 8, which shows RTTs between clients and service endpoints. On Zoom and Webex, RTTs measured by US-east users are much lower than those by US-west users. On Meet, RTTs are uniform across clients due to its distributed service endpoint architecture.
- **US-west Host (Figure 5)**: Geographic locality plays a similar role with Zoom and Meet, where the farthest clients in US-east experience the worst lags. For Webex, the worst streaming lag is experienced by another user in US-west. According to the RTTs collected for Webex (Figure 9b), its service endpoints seem to be provisioned on the east-side of the US even when sessions are created in US-west, causing detours via US-east. The lag distributions for US-west-based sessions are shifted by 30 ms from the US-east-based counterparts (Figures 4b and 5b).

One unexpected observation is that Meet sessions exhibit the worst lag despite having the lowest RTTs. This might be due to the fact that Meet sessions are relayed via multiple service endpoints, unlike Zoom and Webex. Additionally, although Meet’s infrastructure appears to be more distributed, the total aggregate server capacity at each location may be smaller, leading to more load variation.

##### Non-US-based Videoconferencing
- **Europe (Figures 6 and 7)**: When sessions are set up among clients in Europe, Zoom and Webex clients experience much higher lags than Meet users. Compared to sessions created in US-east, clients in Europe experience 55–75 ms and 45–65 ms higher median lags on Zoom and Webex, respectively. The reported RTTs (Figures 10 and 11) show that clients closer to the east-coast of the US (e.g., UK and Ireland) have lower RTTs than those in central Europe (e.g., Germany and Switzerland). These observations suggest that the service infrastructures used are located in the US.

Comparing Zoom and Webex, RTTs to service endpoints on Zoom vary widely across different sessions, suggesting regional load balancing within the US. In Webex, RTTs consistently remain close to trans-Atlantic RTTs, indicating that non-US sessions are relayed via its infrastructure in US-east. For Meet, its distributed service endpoints allow clients in Europe to enjoy comparable stream lags without any artificial detour. The lower streaming lag in Europe may be due to smaller end-to-end latency among clients connected via service endpoints.

### User-Perceived Video Quality

Next, we evaluated user-perceived quality of videoconferencing. We prepared two distinct video feeds with 640×480 resolution:
- **Low-motion feed**: Capturing the upper body of a single person talking with occasional hand gestures in an indoor environment.
- **High-motion feed**: A tour guide feed with dynamically moving objects and scene changes.

On each videoconferencing system, we used a designated meeting host VM to create 10 five-minute long sessions, injecting the low-/high-motion videos in an alternating fashion (hence two sets of five sessions). In each session, \( N \) clients joined and rendered the received video feed in full screen mode while their desktop screens were recorded locally. We used PulseAudio as the audio backend and set the video/audio codec to H.24 (30 fps) and AAC (128 Kbps), respectively. We repeated the whole experiment as we varied \( N \) from one to five.

#### US-based Videoconferencing
- **Quality Metrics**: We compared the quality of video streaming for these VMs in terms of three QoE metrics (PSNR, SSIM, and VIFp) as the number of users in a session increased. The height of bars in Figure 12 indicates average QoE values across all sessions, with error bars representing standard deviations.
- **Low-Motion vs. High-Motion Feeds**: Comparing Figures 12a–12c against Figures 12d–12f, low-motion sessions experience less quality degradation than high-motion sessions. The decrease in QoE values between low-motion and high-motion sessions (Figure 14) is significant enough to downgrade mean opinion score (MOS) ratings by one level. On Webex, QoE degradation in high-motion scenarios tends to become more severe with more users, whereas no such consistent pattern is observed in Zoom and Meet.
- **Data Rates**: The QoE results from low-motion sessions (Figures 12a–12c) show a non-negligible QoE drop between \( N=2 \) and \( N>2 \) on Meet. The data rate for two-user sessions (1.6–2.0 Mbps) is significantly higher than other multi-user sessions (0.4–0.6 Mbps) (Figure 15). This higher traffic rate with \( N=2 \) helps with the QoE of low-motion sessions but does not contribute much to the QoE of high-motion sessions.

Among the three, Webex exhibits the most stable QoE across different scenarios.

#### Video Quality Comparison
We used the VQMT tool to compute objective QoE metrics, including PSNR, SSIM, and VIFp. To avoid partial occlusion inside the video viewing area, we prepared video feeds with enough padding (Figure 13). Post-processing steps included cropping out the surrounding padding, resizing video frames, and synchronizing the start/end time of original/recorded videos with millisecond-level precision.

### Conclusion
Our study provides insights into the performance and quality of videoconferencing systems across different geographic regions and under varying conditions. The findings highlight the importance of service endpoint architecture and geographic proximity in determining streaming lag and user-perceived quality.