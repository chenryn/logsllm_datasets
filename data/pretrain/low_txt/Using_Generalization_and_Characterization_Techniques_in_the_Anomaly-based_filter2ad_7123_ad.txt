### 7. Evaluation

The system was evaluated based on its false positive rate, its ability to correctly group and classify anomalies, and its real-time detection performance on web request logs. All experiments were conducted on a Pentium IV 1.8 GHz machine with 1 GB of RDRAM.

#### 7.1. False Positive Rate

To evaluate the false positive rate of the anomaly detector, datasets from two universities, TU Vienna and UCSB, were analyzed. A client was written to replay the requests to a honeypot web server, while a misuse detection system monitored the traffic between the client and server. All requests corresponding to reported attacks were removed from the dataset. Additionally, since many of the attacks were intended for Microsoft IIS while the datasets were generated by an Apache web server, requests for non-existent documents were also removed.

The detection system was configured with an initially empty anomaly signature set and default learning, detection, and similarity thresholds. The learning phase was performed over the first 1,000 examples of a specific web application attribute, after which the system switched to detection mode. During detection mode, any alerts reported by the system were flagged as false positives, assuming the dataset was attack-free. The results are summarized in Table 1.

**Table 1. False Positive Results**

| Data Set   | Queries     | False Positives | False Positive Rate | Groups | Grouped False Positive Rate |
|------------|-------------|-----------------|---------------------|--------|-----------------------------|
| TU Vienna  | 737,626     | 14              | 1.90 × 10<sup>-5</sup> | 2      | 3.00 × 10<sup>-6</sup>       |
| UCSB       | 35,261      | 513             | 1.45 × 10<sup>-2</sup> | 3      | 8.50 × 10<sup>-5</sup>       |

During the analysis of the TU Vienna dataset, the detection system produced 14 alerts out of 737,626 queries, resulting in a very low false positive rate. This indicates that the anomaly detection models accurately captured the "normal" behavior of attribute values during the learning phase. The addition of the anomaly generalization and aggregation components further improved this by collapsing the 14 alerts into 2 groups. Upon examination, it was found that each group comprised related alerts. For example, one group involved repeated access to an IMAP mailbox through the imp webmail application, which had not been observed during the learning phase. The token finder generated an alert, and the resulting anomaly signature grouped these alerts logically. The second group involved developers passing invalid values to an attribute during test invocations, detected by the attribute length model, which grouped subsequent variations with the first instance.

The results for the UCSB dataset were more dramatic. The detection system reported 513 alerts out of 35,261 queries, resulting in a much higher false positive rate. However, due to generalization and aggregation, the 513 alerts were partitioned into 3 groups. Manual inspection showed that, similar to the TU Vienna dataset, the groups were composed of related alerts. One group involved anomalous queries to the whois.pl user lookup script, with the name attribute value "teacher+assistant++advisor," possibly due to a bad hyperlink reference. The character distribution model detected an anomalous number of "a" characters. Another group had a similar issue with the name argument value "dean+of+computer+science," where the character distribution detected an anomalous number of "e" characters. The final group involved alerts on the optional "showphone" argument, which the token finder correctly identified as anomalous.

#### 7.2. Performance

The performance of the detection system was evaluated in terms of processing time and memory usage when run on attack-free datasets from TU Vienna and UCSB. These metrics are crucial for real-world applicability, as the system should ideally operate in real-time on hardware available to most web site operators. The same parameters used for the false positive evaluation were applied. Ten runs were performed for each dataset, and the elapsed times were averaged. The results are shown in Table 3.

**Table 3. Detection Performance Results (Time)**

| Data Set   | Requests    | Request Rate (req/sec) | Elapsed Analysis Time (sec) | Analysis Rate (req/sec) |
|------------|-------------|------------------------|-----------------------------|-------------------------|
| TU Vienna  | 737,626     | 0.107095               | 934                         | 788.06                  |
| UCSB       | 35,261      | 0.001360               | 64                          | 550.95                  |

For both datasets, the detection system maintained a processing rate significantly higher than the rate of client requests logged by the web server. For the TU Vienna dataset, the request analysis was performed approximately 7,000 times faster than the actual request logging. This suggests that the detection system is capable of performing real-time analysis for many sites.

In addition to CPU usage, the memory utilization of the system was also analyzed. The results showed that the system did not require substantial memory resources once the profiles were established. Detailed memory usage data is not provided here due to space constraints.

### 8. Conclusions and Future Work

This paper presents an approach that addresses the limitations of anomaly-based intrusion detection systems by using generalization and characterization techniques. Generalization creates a more abstract description of an anomaly, enabling the grouping of similar attacks. Characterization infers the class of attack associated with a group of anomalies. These techniques reduce the time required by administrators to make decisions about the nature and criticality of anomalies, and assist application developers in identifying previously unknown vulnerabilities.

One potential drawback is that if an attack is grouped with false positives, the real attack may be dropped by the system administrator, leading to a false negative. We developed a system that implements anomaly signature generation and attack class inference, and tested it on real-world data from two universities. The results show that the proposed techniques can correctly generalize and characterize attacks, significantly reducing the effort needed to analyze the output of the intrusion detection system.

Future research will explore extending these techniques to other domains, such as the arguments of system calls issued by critical applications. We will also investigate improving the attack inference technique, possibly by using more sophisticated heuristics or different decision models, and explore whether attack characterization can be expressed as a Bayesian network. Finally, we plan to evaluate the system using alternative metrics to increase the precision of our characterization of its effectiveness in reducing the effective false positive rate.

### Acknowledgments

This research was supported by the Army Research Office under agreement DAAD19-01-1-0484 and by the National Science Foundation under grants CCR-0238492 and CCR-0524853.

### References

[1] M. Almgren, H. Debar, and M. Dacier. A Lightweight Tool for Detecting Web Server Attacks. In Proceedings of the ISOC Symposium on Network and Distributed Systems Security, San Diego, CA, February 2000.

[2] M. Almgren and U. Lindqvist. Application-Integrated Data Collection for Security Monitoring. [Proceedings details]

[3] C. Warrender, S. Forrest, and B.A. Pearlmutter. Detecting Intrusions Using System Calls: Alternative Data Models. In IEEE Symposium on Security and Privacy, pages 133–145, 1999.

[4] K. Coar and D. Robinson. The WWW Common Gateway Interface, Version 1.1. Internet Draft, June 1999.

[5] Common Vulnerabilities and Exposures. http://www.cve.mitre.org/, 2005.

[6] D.E. Denning. An Intrusion Detection Model. IEEE Transactions on Software Engineering, 13(2):222–232, February 1987.

[7] S. Forrest. A Sense of Self for UNIX Processes. In Proceedings of the IEEE Symposium on Security and Privacy, pages 120–128, Oakland, CA, May 1996.

[8] A.K. Ghosh, J. Wanken, and F. Charron. Detecting Anomalous and Unknown Intrusions Against Programs. In Proceedings of the Annual Computer Security Application Conference (ACSAC’98), pages 259–267, Scottsdale, AZ, December 1998.

[9] C. Ko, M. Ruschitzka, and K. Levitt. Execution Monitoring of Security-Critical Programs in Distributed Systems: A Specification-Based Approach. In Proceedings of the 1997 IEEE Symposium on Security and Privacy, pages 175–187, Oakland, CA, May 1997.

[10] C. Kruegel, T. Toth, and E. Kirda. Service-Specific Anomaly Detection for Network Intrusion Detection. In Symposium on Applied Computing (SAC). ACM Scientific Press, March 2002.

[11] C. Kruegel and G. Vigna. Anomaly Detection of Web-Based Attacks. In Proceedings of the 10th ACM Conference on Computer and Communication Security (CCS ’03), pages 251–261, Washington, DC, October 2003. ACM Press.

[12] W. Lee, S. Stolfo, and P. Chan. Learning Patterns from Unix Process Execution Traces for Intrusion Detection. In Proceedings of the AAAI Workshop: AI Approaches to Fraud Detection and Risk Management, July 1997.

[13] M. Mahoney and P. Chan. Learning Nonstationary Models of Normal Network Traffic for Detecting Novel Attacks. In Proceedings of the 8th International Conference on Knowledge Discovery and Data Mining, pages 376–385, 2002.

[14] V. Paxson. Bro: A System for Detecting Network Intruders in Real-Time. In Proceedings of the 7th USENIX Security Symposium, San Antonio, TX, January 1998.

[15] L. Portnoy, E. Eskin, and S. Stolfo. Intrusion Detection with Unlabeled Data Using Clustering. In Proceedings of ACM CSS Workshop on Data Mining Applied to Security, Philadelphia, PA, November 2001.

[16] M. Roesch. Snort - Lightweight Intrusion Detection for Networks. In Proceedings of the USENIX LISA ’99 Conference, Seattle, WA, November 1999.

[17] A. Stolcke and S. Omohundro. Inducing Probabilistic Grammars by Bayesian Model Merging. In Conference on Grammatical Inference, 1994.

[18] K.M.C. Tan, K.S. Killourhy, and R.A. Maxion. Undermining an Anomaly-Based Intrusion Detection System Using Common Exploits. In Proceedings of the 5th International Symposium on Recent Advances in Intrusion Detection, pages 54–73, Zurich, Switzerland, October 2002.

[19] E. Tombini, H. Debar, L. Me, and M. Ducasse. A Serial Combination of Anomaly and Misuse IDSes Applied to HTTP Traffic. In Proceedings of the Twentieth Annual Computer Security Applications Conference, Tucson, Arizona, December 2004.

[20] Thomas Toth and Christopher Kruegel. Accurate Buffer Overflow Detection via Abstract Payload Execution. In 5th Symposium on Recent Advances in Intrusion Detection (RAID), 2002.

[21] G. Vigna, W. Robertson, V. Kher, and R.A. Kemmerer. A Stateful Intrusion Detection System for World-Wide Web Servers. In Proceedings of the Annual Computer Security Applications Conference (ACSAC 2003), pages 34–43, Las Vegas, NV, December 2003.

[22] Giovanni Vigna, William Robertson, and Davide Balzarotti. Testing Network-Based Intrusion Detection Signatures Using Mutant Exploits. In 11th ACM Conference on Computer and Communications Security (CCS), 2004.

[23] D. Wagner and P. Soto. Mimicry Attacks on Host-Based Intrusion Detection Systems. In Proceedings of the 9th ACM Conference on Computer and Communications Security, pages 255–264, Washington DC, USA, November 2002.