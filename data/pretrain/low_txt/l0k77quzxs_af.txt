### Threats to Validity

#### External Validity
In this study, we evaluate Logram on 16 log datasets from an existing benchmark [21]. Logram achieves a parsing accuracy higher than 0.9 on approximately half of these datasets. However, we cannot guarantee that Logram will achieve similar high accuracy on other log datasets not included in our evaluation. Despite this, our evaluation on logs from 16 different systems across various domains (e.g., big data applications and operating systems) demonstrates that Logram achieves comparable accuracy to the best existing log parsing approaches, with significantly faster processing speeds. Future work could focus on improving Logram's performance on a wider variety of log data.

#### Internal Validity
Logram leverages n-grams for log parsing, a technique typically used for modeling natural languages or source code written by humans. Logs, however, are generated by machines and contain both static and dynamic information, which differ from human-generated text. Nonetheless, our results show that n-grams can effectively distinguish between static and dynamic elements in log messages. Future research may explore the use of n-grams in other log-related analyses. Additionally, we use an automated approach to determine the threshold for identifying static and dynamic tokens. While this method is efficient, it may not always generate optimal thresholds. Further optimization of these thresholds could potentially enhance the accuracy of our approach.

#### Construct Validity
In our evaluation, we compare Logram with six other log parsing approaches. There are additional log parsing methods, such as LKE [8], that were not included in our comparison. We limited our evaluation to five existing approaches due to the significant manual effort required to verify the parsing accuracy of each method. Our primary goal was not to provide a comprehensive benchmark but to propose and evaluate an innovative and promising log parsing approach. Nevertheless, our results, based on a recent benchmark [21], show that Logram outperforms the best-performing log parsing approaches in terms of both accuracy and speed.

### Conclusion
In this work, we introduce Logram, an automated log parsing approach that utilizes n-gram dictionaries to parse log data efficiently. The nature of n-gram dictionaries allows for parallel construction and online updates without compromising parsing accuracy, making Logram suitable for log streaming scenarios. Through an evaluation on 16 public log datasets, we demonstrate that Logram achieves high accuracy and efficiency while maintaining stability and scalability. Specifically, Logram surpasses state-of-the-art log parsing approaches in both efficiency and accuracy. Additionally, we show that Logram supports online parsing, achieving similar results and efficiency to offline parsing when logs are continuously generated as a stream. This is the first work to apply n-grams in log analysis, successfully leveraging the unique characteristics of logs. Logram has the potential to benefit future research and practical applications that rely on automated log parsing for log analysis.

### References
[1] T. Barik, R. DeLine, S. M. Drucker, and D. Fisher, “The bones of the system: a case study of logging and telemetry at Microsoft,” in Proceedings of the 38th International Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016 - Companion Volume, 2016, pp. 92–101.
[2] J. Cito, P. Leitner, T. Fritz, and H. C. Gall, “The making of cloud applications: an empirical study on software development for the cloud,” in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015, Bergamo, Italy, August 30 - September 4, 2015, 2015, pp. 393–403.
[3] A. J. Oliner and J. Stearley, “What supercomputers say: A study of five system logs,” in The 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2007, 25-28 June 2007, Edinburgh, UK, Proceedings, 2007, pp. 575–584.
[4] B. Schroeder and G. A. Gibson, “Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you?” in 5th USENIX Conference on File and Storage Technologies, FAST 2007, February 13-16, 2007, San Jose, CA, USA, 2007, pp. 1–16.
[5] W. Xu, L. Huang, A. Fox, D. A. Patterson, and M. I. Jordan, “Detecting large-scale system problems by mining console logs,” in Proceedings of the 22nd ACM Symposium on Operating Systems Principles 2009, SOSP 2009, Big Sky, Montana, USA, October 11-14, 2009, 2009, pp. 117–132.
[6] ——, “Online system problem detection by mining patterns of console logs,” in ICDM 2009, The Ninth IEEE International Conference on Data Mining, Miami, Florida, USA, 6-9 December 2009, 2009, pp. 588–597.
[7] J. Lou, Q. Fu, S. Yang, Y. Xu, and J. Li, “Mining invariants from console logs for system problem detection,” in 2010 USENIX Annual Technical Conference, Boston, MA, USA, June 23-25, 2010, 2010.
[8] Q. Fu, J. Lou, Y. Wang, and J. Li, “Execution anomaly detection in distributed systems through unstructured log analysis,” in ICDM 2009, The Ninth IEEE International Conference on Data Mining, Miami, Florida, USA, 6-9 December 2009, 2009, pp. 149–158.
[9] Z. M. Jiang, A. E. Hassan, G. Hamann, and P. Flora, “Automatic identification of load testing problems,” in 24th IEEE International Conference on Software Maintenance (ICSM 2008), September 28 - October 4, 2008, Beijing, China, 2008, pp. 307–316.
[10] Q. Fu, J. Lou, Q. Lin, R. Ding, D. Zhang, and T. Xie, “Contextual analysis of program logs for understanding system behaviors,” in Proceedings of the 10th Working Conference on Mining Software Repositories, MSR ’13, San Francisco, CA, USA, May 18-19, 2013, 2013, pp. 397–400.
[11] K. Nagaraj, C. E. Killian, and J. Neville, “Structured comparative analysis of systems logs to diagnose performance problems,” in Proceedings of the 9th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2012, San Jose, CA, USA, April 25-27, 2012, 2012, pp. 353–366.
[12] M. Chow, D. Meisner, J. Flinn, D. Peek, and T. F. Wenisch, “The mystery machine: End-to-end performance analysis of large-scale internet services,” in 11th USENIX Symposium on Operating Systems Design and Implementation, OSDI ’14, Broomfield, CO, USA, October 6-8, 2014, 2014, pp. 217–231.
[13] W. Shang, Z. M. Jiang, H. Hemmati, B. Adams, A. E. Hassan, and P. Martin, “Assisting developers of big data analytics applications when deploying on Hadoop clouds,” in 35th International Conference on Software Engineering, ICSE ’13, San Francisco, CA, USA, May 18-26, 2013, 2013, pp. 402–411.
[14] Y. Dang, Q. Lin, and P. Huang, “AIOps: Real-world challenges and research innovations,” in Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019, 2019, pp. 4–5.
[15] Q. Lin, K. Hsieh, Y. Dang, H. Zhang, K. Sui, Y. Xu, J. Lou, C. Li, Y. Wu, R. Yao, M. Chintalapati, and D. Zhang, “Predicting node failure in cloud service systems,” in Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018, 2018, pp. 480–490.
[16] S. He, Q. Lin, J. Lou, H. Zhang, M. R. Lyu, and D. Zhang, “Identifying impactful service system problems via log analysis,” in Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018, 2018, pp. 60–70.
[17] N. El-Sayed, H. Zhu, and B. Schroeder, “Learning from failure across multiple clusters: A trace-driven approach to understanding, predicting, and mitigating job terminations,” in 37th IEEE International Conference on Distributed Computing Systems, ICDCS 2017, Atlanta, GA, USA, June 5-8, 2017, 2017, pp. 1333–1344.
[18] P. Huang, C. Guo, J. R. Lorch, L. Zhou, and Y. Dang, “Capturing and enhancing in situ system observability for failure detection,” in 13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018, Carlsbad, CA, USA, October 8-10, 2018, 2018, pp. 1–16.
[19] P. He, J. Zhu, S. He, J. Li, and M. R. Lyu, “An evaluation study on log parsing and its use in log mining,” in 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), June 2016, pp. 654–661.
[20] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu, “Tools and benchmarks for automated log parsing,” in Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice, ICSE (SEIP) 2019, Montreal, QC, Canada, May 25-31, 2019, 2019, pp. 121–130.
[21] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, “Drain: An online log parsing approach with fixed depth tree,” in 2017 IEEE International Conference on Web Services, ICWS 2017, Honolulu, HI, USA, June 25-30, 2017, 2017, pp. 33–40.
[22] Z. M. Jiang, A. E. Hassan, G. Hamann, and P. Flora, “An automated approach for abstracting execution logs to execution events,” Journal of Software Maintenance, vol. 20, no. 4, pp. 249–267, 2008.
[23] W. Shang, Z. M. Jiang, B. Adams, A. E. Hassan, M. W. Godfrey, M. Nasser, and P. Flora, “An exploratory study of the evolution of communicated information about the execution of large software systems,” Journal of Software: Evolution and Process, vol. 26, no. 1, pp. 3–26, 2014.
[24] D. Yuan, S. Park, and Y. Zhou, “Characterizing logging practices in open-source software,” in 34th International Conference on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland, 2012, pp. 102–112.
[25] B. Chen and Z. M. J. Jiang, “Characterizing logging practices in Java-based open source software projects - a replication study in Apache Software Foundation,” Empirical Software Engineering, vol. 22, no. 1, pp. 330–374, 2017.
[26] M. Lemoudden and B. E. Ouahidi, “Managing cloud-generated logs using big data technologies,” in International Conference on Wireless Networks and Mobile Communications, WINCOM 2015, Marrakech, Morocco, October 20-23, 2015, 2015, pp. 1–7.
[27] H. Li, T. P. Chen, A. E. Hassan, M. N. Nasser, and P. Flora, “Adopting autonomic computing capabilities in existing large-scale systems: an industrial experience report,” in Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice, ICSE (SEIP) 2018, Gothenburg, Sweden, May 27 - June 03, 2018, 2018, pp. 1–10.
[28] H. Li, W. Shang, Y. Zou, and A. E. Hassan, “Towards just-in-time suggestions for log changes,” Empirical Software Engineering, vol. 22, no. 4, pp. 1831–1865, 2017.
[29] W. B. Cavnar, J. M. Trenkle et al., “N-gram-based text categorization,” in Proceedings of the 3rd annual symposium on document analysis and information retrieval, SDAIR ’94, vol. 161175. Citeseer, 1994.
[30] M. Siu and M. Ostendorf, “Variable n-grams and extensions for conversational speech language modeling,” IEEE Trans. Speech and Audio Processing, vol. 8, no. 1, pp. 63–75, 2000.
[31] S. Nessa, M. Abedin, W. E. Wong, L. Khan, and Y. Qi, “Software fault localization using n-gram analysis,” in Wireless Algorithms, Systems, and Applications, Third International Conference, WASA 2008, Dallas, TX, USA, October 26-28, 2008. Proceedings, 2008, pp. 548–559.
[32] A. Tomovic, P. Janicic, and V. Keselj, “n-gram-based classification and unsupervised hierarchical clustering of genome sequences,” Computer Methods and Programs in Biomedicine, vol. 81, no. 2, pp. 137–153, 2006.
[33] C. Lin and E. H. Hovy, “Automatic evaluation of summaries using n-gram co-occurrence statistics,” in Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL 2003, Edmonton, Canada, May 27- June 1, 2003, 2003.
[34] P. F. Brown, V. J. D. Pietra, P. V. de Souza, J. C. Lai, and R. L. Mercer, “Class-based n-gram models of natural language,” Computational Linguistics, vol. 18, no. 4, pp. 467–479, 1992.
[35] E. Charniak, Statistical Language Learning. MIT Press, 1996.
[36] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, “On the naturalness of software,” in Proceedings of the 34th International Conference on Software Engineering, ser. ICSE’12, 2012, pp. 837–847.
[37] M. Rahman, D. Palani, and P. C. Rigby, “Natural software revisited,” in Proceedings of the 41st International Conference on Software Engineering, ser. ICSE ’19. Piscataway, NJ, USA: IEEE Press, 2019, pp. 37–48.
[38] T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, “A statistical semantic language model for source code,” in Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2013. New York, NY, USA: ACM, 2013, pp. 532–542.
[39] S. Bird, E. Klein, and E. Loper, Natural Language Processing with Python. O’Reilly, 2009.
[40] R. Vaarandi, “Simple event correlator for real-time security log monitoring,” Hakin9 Magazine, vol. 1, no. 6, pp. 28–39, 2006.
[41] C. V. Damasio, P. Fröhlich, W. Nejdl, L. M. Pereira, and M. Schroeder, “Using extended logic programming for alarm-correlation in cellular phone networks,” Applied Intelligence, vol. 17, no. 2, pp. 187–202, 2002.
[42] S. E. Hansen and E. T. Atkins, “Automated system monitoring and notification with swatch.” in LISA, vol. 93, 1993, pp. 145–152.
[43] R. Ramati, “A beginner’s guide to Logstash Grok,” https://logz.io/blog/logstash-grok, (Accessed on 08/14/2019).