### Table VI: Performance of TailBench Applications Without Net_uk Failure (Latencies in Milliseconds)

| Application | 99th Percentile | Mean | 95th Percentile |
|-------------|-----------------|------|-----------------|
| Xen         | 879.11          | 457.61 | 475.37          |
| FG FT       | 901.99          | 460.20 | 491.58          |
| CG FT       | 900.12          | 461.09 | 489.03          |
| TFD-Xen     | 889.91          | 461.18 | 489.10          |
| Xoar        | 6616.44         | 8.6   | 4.35            |
| 99th        | 1820.84         | 10.1  | 5.56            |
| 1711.11     | 1977.15         | 11.9  | 5.96            |
| 1792.04     | 1963.50         | 9.12  | 4.98            |
| 1701.43     | 1911.33         | 1.79  | 4.2             |
| 9026.7      | 2.11            | 494.30 | 39.567          |
| 95th        | 475.37          | 2.12  | 40.78           |
| 491.58      | 2.11            | 490.12 | 41.3            |
| 489.03      | 3.42            | 493.55 | 40.44           |
| 489.10      | 476.20          | 8695  |                 |
| 99th        | 65.642          | 7.6   | 7.6             |
| 72.44       | 1.7             | 1.7   | 1.7             |
| 72.12       | 8.21            | 1.92  | 1.92            |
| 73.19       | 7.98            | 1.9   | 1.9             |
| mean        | 457.61          | 8.11  | 543.9           |
| 460.20      | 2526            | 9603  |                 |

### Discussion

In the context of virtualized environments, particularly those utilizing a privileged virtual machine (pVM) for hosting critical components such as XenStore or the toolstack, failures can lead to significant downtime and full recovery for all other components. To mitigate this, one approach is to replicate the state of the XenStore in a dedicated virtual machine (dVM) and make XenStore and VM management operations transactional using a log stored in the dVM. However, the mechanisms for enforcing consistency and availability in the presence of concurrent failures are not detailed.

The evaluation of this approach focuses on its resilience against synthetic fault injection, but no detailed performance measurements (with or without failures) are provided, and the code of the prototype is not available. Our work builds on this approach by:
1. Applying fault tolerance techniques at a finer granularity.
2. Exploring the ramifications of interdependencies between services.
3. Providing a detailed performance evaluation.

### Hypervisor Resilience

Several works have focused on improving the resilience of the hypervisor. For instance, ReHype [8], [30], [31] is a Xen-based system that leverages microreboot techniques to recover from hypervisor failures without stopping or resetting the state of the VMs. NiLyHype [9] improves on ReHype by replacing the microreboot approach with a microreset technique, which resets a software component rather than performing a full reboot, thereby reducing recovery latency. TinyChecker [10] uses nested virtualization to provide resilience against crashes and state corruption in the main hypervisor. Shi et al. [34] proposed a modular, "deconstructed" design for Xen to thwart security attacks, focusing on the redesign of the Xen hypervisor rather than the pVM. These works do not consider the services hosted in the pVM and are mostly orthogonal to our work.

Our contribution leverages these results to improve the fault tolerance of the pVM components. The FTXen project [35] aims to harden the Xen hypervisor layer to withstand hardware failures on "relaxed" CPU cores. In contrast, our work focuses on the resilience of pVM components on current hardware, with a homogeneous/symmetric fault model with respect to CPU cores.

Recent projects aim to support live reboot and/or upgrade of VMMs without disrupting the VMs [36], [37]. These techniques focus on code updates for improving the safety and security of the hypervisor component, making them orthogonal to our contribution. This trend highlights the importance of improving the resilience of the remaining components, i.e., the pVM services.

### Conclusion

VMMs remain a key building block for cloud computing, and many are based on a pVM-based design. We have highlighted that, in this design, the pVM component has become the main weakness in terms of fault tolerance compared to the bare metal hypervisor component. Existing solutions only tackle a limited set of pVM services and often require long failure detection/recovery times and significant performance overheads. To the best of our knowledge, our contribution is the first to propose and empirically demonstrate a complete approach that achieves both high resilience and low overhead. Our approach currently relies on manual tuning of some important parameters, but we envision that recent works could help manage them in a more automated and robust way [38]. Future work will focus on tuning and optimizing resource allocation for disaggregated pVM components, extending from existing techniques for a monolithic pVM design [27].

### References

[1] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I. Pratt, and A. Warfield, “Xen and the Art of Virtualization,” SIGOPS Oper. Syst. Rev., vol. 37, no. 5, pp. 164–177, Oct. 2003. [Online]. Available: http://tiny.cc/5xt4nz

[2] K. Fraser, S. Hand, R. Neugebauer, I. Pratt, A. Warfield, and M. Williamson, “Safe Hardware Access with the Xen Virtual Machine Monitor,” in In Proceedings of the 1st Workshop on Operating System and Architectural Support for the on-demand IT InfraStructure (OASIS), 2004.

[3] S. Spector, “Why Xen?” 2009. [Online]. Available: http://www-archive.xenproject.org/files/Marketing/WhyXen.pdf

[4] H. Jo, H. Kim, J. Jang, J. Lee, and S. Maeng, “Transparent fault tolerance of device drivers for virtual machines,” IEEE Transactions on Computers, vol. 59, no. 11, pp. 1466–1479, Nov 2010.

[5] P. Colp, M. Nanavati, J. Zhu, W. Aiello, G. Coker, T. Deegan, P. Loscocco, and A. Warfield, “Breaking Up is Hard to Do: Security and Functionality in a Commodity Hypervisor,” in Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles, ser. SOSP ’11. New York, NY, USA: ACM, 2011, pp. 189–202. [Online]. Available: http://doi.acm.org/10.1145/2043556.2043575

[6] H. Kasture and D. Sanchez, “TailBench: a benchmark suite and evaluation methodology for latency-critical applications,” IEEE International Symposium on Workload Characterization (IISWC), 2016. [Online]. Available: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7581261&isnumber=7581253

[7] D. J. Scales, M. Nelson, and G. Venkitachalam, “The Design of a Practical System for Fault-tolerant Virtual Machines,” SIGOPS Oper. Syst. Rev., vol. 44, no. 4, pp. 30–39, Dec. 2010. [Online]. Available: http://doi.acm.org/10.1145/1899928.1899932

[8] M. Le and Y. Tamir, “ReHype: Enabling VM Survival Across Hypervisor Failures,” in Proceedings of the 7th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, ser. VEE ’11. New York, NY, USA: ACM, 2011, pp. 63–74. [Online]. Available: http://doi.acm.org/10.1145/1952682.1952692

[9] D. Zhou and Y. Tamir, “Fast Hypervisor Recovery Without Reboot,” in 2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), June 2018, pp. 115–126.

[10] C. Tan, Y. Xia, H. Chen, and B. Zang, “TinyChecker: Transparent protection of VMs against hypervisor failures with nested virtualization,” in IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN 2012), June 2012, pp. 1–6.

[11] The Linux Foundation, “Xen Project.” [Online]. Available: https://xenproject.org

[12] A. Madhavapeddy and D. J. Scott, “Unikernels: The Rise of the Virtual Library Operating System,” Commun. ACM, vol. 57, no. 1, pp. 61–69, Jan. 2014. [Online]. Available: http://doi.acm.org/10.1145/2541883.2541895

[13] M. M. Swift, M. Annamalai, B. N. Bershad, and H. M. Levy, “Recovering Device Drivers,” ACM Trans. Comput. Syst., vol. 24, no. 4, pp. 333–360, Nov. 2006. [Online]. Available: http://doi.acm.org/10.1145/1189256.1189257

[14] “Xl.” [Online]. Available: https://wiki.xenproject.org/wiki/XL

[15] “Mirage OS.” [Online]. Available: https://mirage.io

[16] O. Mutlu, “The RowHammer Problem and Other Issues We May Face As Memory Becomes Denser,” in Proceedings of the Conference on Design, Automation & Test in Europe, ser. DATE ’17. 3001 Leuven, Belgium, Belgium: European Design and Automation Association, 2017, pp. 1116–1121. [Online]. Available: http://dl.acm.org/citation.cfm?id=3130379.3130643

[17] L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos, “Exploiting Correcting Codes: On the Effectiveness of ECC Memory Against Rowhammer Attacks,” in S&P, May 2019, best Practical Paper Award, Pwnie Award Nomination for Most Innovative Research. [Online]. Available: http://tiny.cc/trt4nz

[18] “etcd.” [Online]. Available: https://etcd.io

[19] “Exploring performance of etcd, zookeeper and consul consistent key-value datastores.” [Online]. Available: http://tiny.cc/8hu4nz

[20] D. Ongaro and J. Ousterhout, “In Search of an Understandable Consensus Algorithm,” in 2014 USENIX Annual Technical Conference (USENIX ATC 14). Philadelphia, PA: USENIX Association, Jun. 2014, pp. 305–319. [Online]. Available: http://tiny.cc/wku4nz

[21] “MiniOS.” [Online]. Available: https://github.com/mirage/mini-os

[22] M. M. Swift, S. Martin, H. M. Levy, and S. J. Eggers, “Nooks: An architecture for reliable device drivers,” in Proceedings of the 10th Workshop on ACM SIGOPS European Workshop, ser. EW 10. New York, NY, USA: ACM, 2002, pp. 102–107. [Online]. Available: http://doi.acm.org/10.1145/1133373.1133393

[23] V. Narayanan, A. Balasubramanian, C. Jacobsen, S. Spall, S. Bauer, M. Quigley, A. Hussain, A. Younis, J. Shen, M. Bhattacharyya, and A. Burtsev, “LXDs: Towards Isolation of Kernel Subsystems,” in 2019 USENIX Annual Technical Conference (USENIX ATC 19). Renton, WA: USENIX Association, Jul. 2019, pp. 269–284. [Online]. Available: https://www.usenix.org/conference/atc19/presentation/narayanan

[24] “Xen Security Modules.” [Online]. Available: http://tiny.cc/zdu4nz

[25] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul, C. Limpach, I. Pratt, and A. Warfield, “Live migration of virtual machines,” in Proceedings of the 2nd Conference on Symposium on Networked Systems Design and Implementation, ser. NSDI’05. USA: USENIX Association, 2005, p. 273–286.

[26] “ApacheBench.” [Online]. Available: http://tiny.cc/anu4nz

[27] D. Mvondo, B. Teabe, A. Tchana, D. Hagimont, and N. De Palma, “Closer: A New Design Principle for the Privileged Virtual Machine OS,” in 2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS), 2019, pp. 49–60.

[28] D. G. Murray, G. Milos, and S. Hand, “Improving Xen Security Through Disaggregation,” in Proceedings of the Fourth ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, ser. VEE ’08. New York, NY, USA: ACM, 2008, pp. 151–160. [Online]. Available: http://doi.acm.org/10.1145/1346256.1346278

[29] M. Le and T. Yuval, “Resilient Virtualized Systems Using ReHype,” UCLA Computer Science Department, Tech. Rep. 140019, October 2014.

[30] M. Le, “Resilient Virtualized Systems,” UCLA Computer Science Department, Ph.D. thesis 140007, March 2014.

[31] M. Le and Y. Tamir, “Applying Microreboot to System Software,” in 2012 IEEE Sixth International Conference on Software Security and Reliability, June 2012, pp. 11–20.

[32] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox, “Microreboot — A Technique for Cheap Recovery,” in Proceedings of the 6th Conference on Symposium on Opearting Systems Design & Implementation - Volume 6, ser. OSDI’04. Berkeley, CA, USA: USENIX Association, 2004, pp. 3–3. [Online]. Available: http://dl.acm.org/citation.cfm?id=1251254.1251257

[33] A. Depoutovitch and M. Stumm, “Otherworld: Giving Applications a Chance to Survive OS Kernel Crashes,” in Proceedings of the 5th European Conference on Computer Systems, ser. EuroSys ’10. New York, NY, USA: ACM, 2010, pp. 181–194. [Online]. Available: http://doi.acm.org/10.1145/1755913.1755933

[34] L. Shi, Y. Wu, Y. Xia, N. Dautenhahn, H. Chen, B. Zang, and J. Li, “Deconstructing Xen,” in 24th Annual Network and Distributed System Security Symposium, NDSS 2017, San Diego, California, USA, February 26 - March 1, 2017. [Online]. Available: http://tiny.cc/j3t4nz

[35] X. Jin, S. Park, T. Sheng, R. Chen, Z. Shan, and Y. Zhou, “FTXen: Making hypervisor resilient to hardware faults on relaxed cores,” in 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), Feb 2015, pp. 451–462.

[36] S. Doddamani, P. Sinha, H. Lu, T.-H. K. Cheng, H. H. Bagdi, and K. Gopalan, “Fast and Live Hypervisor Replacement,” in Proceedings of the 15th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, ser. VEE 2019. New York, NY, USA: ACM, 2019, pp. 45–58. [Online]. Available: http://doi.acm.org/10.1145/3313808.3313821

[37] X. Zhang, X. Zheng, Z. Wang, Q. Li, J. Fu, Y. Zhang, and Y. Shen, “Fast and Scalable VMM Live Upgrade in Large Cloud Infrastructure,” in Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS ’19. New York, NY, USA: ACM, 2019, pp. 93–105. [Online]. Available: http://doi.acm.org/10.1145/3297858.3304034

[38] S. Wang, C. Li, H. Hoffmann, S. Lu, W. Sentosa, and A. I. Kistijantoro, “Understanding and auto-adjusting performance-sensitive configurations,” in Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS ’18. New York, NY, USA: Association for Computing Machinery, 2018, p. 154–168. [Online]. Available: https://doi.org/10.1145/3173162.3173206

**Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 11:32:03 UTC from IEEE Xplore. Restrictions apply.**