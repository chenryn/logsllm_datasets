### Flow Table and Packet Matching

The system incorporates a 32K-entry SRAM for storing flow table entries. Each incoming packet's header is matched against 10 fields in the Ethernet, IP, and TCP/UDP headers to find a match in the two hardware flow tables. Each entry in the TCAM (Ternary Content-Addressable Memory) and SRAM is associated with an action, such as forwarding the packet to an output port or to the switch software. TCAM entries can include "don't care" bits, while SRAM matches must be exact.

### System Architecture

PortLand intercepts all ARP (Address Resolution Protocol) requests and IGMP (Internet Group Management Protocol) join requests at the edge switch and forwards them to the local switch software module running on the PC hosting each NetFPGA. The local switch module interacts with the OpenFlow fabric manager to resolve ARP requests and manage forwarding tables for multicast sessions. For new flows, the first few packets will miss in the hardware flow tables and will be forwarded to the local switch module. The switch module uses ECMP (Equal-Cost Multi-Path) style hashing to choose among available forwarding paths and inserts a new flow table entry matching the flow.

Upon receiving failure and recovery notifications from the fabric manager, each switch recalculates global connectivity and modifies the appropriate forwarding entries for affected flows. The OpenFlow fabric manager monitors the liveness of each switch module and updates its fault matrix accordingly. Switches also send keepalives to their immediate neighbors every 10ms. If no keepalive is received after 50ms, they assume a link failure and update the fabric manager.

Figure 7 illustrates the system architecture. OpenFlow switch modules run locally on each switch, and the fabric manager transmits control updates using OpenFlow messages. In our testbed, a separate control network supports communication between the fabric manager and local switch modules. It is possible to run the fabric manager as a separate host on the data plane and communicate in-band. The cost and wiring for a separate lower-speed control network are modest. For a 2,880-switch data center with k = 48, fewer than 100 low-cost, low-speed switches should suffice to provide control plane functionality. The real question is whether the benefits of such a dedicated network justify the additional complexity and management overhead.

### State Requirements

Table 2 summarizes the state maintained locally at each switch and the fabric manager. Here:
- \( k \) = Number of ports on the switches
- \( m \) = Number of local multicast groups
- \( p \) = Number of multicast groups active in the system

| State                  | Switch        | Fabric Manager |
|------------------------|---------------|----------------|
| Connectivity Matrix    | \( O(k^{3/2}) \) | \( O(k^{3/2}) \) |
| Multicast Flows        | \( O(m) \)     | \( O(p) \)      |
| IP → PMAC Mappings    | \( O(k/2) \)   | \( O(k^{3/4}) \) |

### Evaluation

#### Convergence Time with Increasing Faults

We measured the convergence time for a UDP flow while introducing a varying number of random link failures. A sender transmits packets at 250Mbps to a receiver in a separate pod. If at least one of the failures affects the default path, we measured the total time required to re-establish communication. Figure 8 plots the average convergence time across 20 runs as a function of the number of randomly-induced failures. The total convergence time begins at about 65ms for a single failure and increases slowly with the number of failures due to additional processing time.

#### TCP Convergence

We repeated the same experiment for TCP communication. We monitored network activity using tcpdump at the sender while injecting a link failure along the path between the sender and receiver. As shown in Figure 9, convergence for TCP flows takes longer than for UDP, despite the same steps being taken in the underlying network. This discrepancy occurs because TCP loses an entire window worth of data, causing it to fall back to the retransmission timer, with TCP’s RTOmin set to 200ms in our system. By the time the first retransmission takes place, connectivity has already been re-established in the underlying network.

#### Multicast Convergence

We further measured the time required to designate a new core when one of the subscribers of a multicast group loses connectivity to the current core. For this experiment, we used the same configuration as in Figure 5. The sender transmits a multicast flow to a group consisting of 3 subscribers, augmenting each packet with a sequence number. As shown in Figure 10, 4.5 seconds into the experiment, we inject two failures, causing one of the receivers to lose connectivity. After 110ms, connectivity is restored. During this time, individual switches detect the failures and notify the fabric manager, which in turn reconfigures the appropriate switch forwarding tables.

#### Scalability

One concern regarding PortLand design is the scalability of the fabric manager for larger topologies. Since we do not have a prototype at scale, we use measurements from our existing system to project the requirements of larger systems. Figure 11 shows the amount of ARP control traffic the fabric manager would be expected to handle as a function of overall cluster size. We considered cases where each host transmitted 25, 50, and 100 ARP requests/sec to the fabric manager. Even 25 ARPs/sec is likely to be extreme in today’s data center environments, especially considering the presence of a local ARP cache with a typical 60-second timeout. In a data center with each of the 27,648 hosts transmitting 100 ARPs per second, the fabric manager must handle a manageable 376Mbits/s of control traffic. More challenging is the CPU time required to handle each request. Our measurements indicate approximately 25 µs of time per request in our non-optimized implementation. Fortunately, the work is highly parallelizable, making it amenable to deployment on multiple cores and multiple hardware thread contexts per core. Figure 12 shows the CPU requirements for the fabric manager as a function of the number of hosts in the data center generating different numbers of ARPs/sec. For the highest levels of ARPs/sec and large data centers, the required level of parallelism to keep up with the ARP workload will be approximately 70 independent cores. This is beyond the capacity of a single modern machine but represents a significant number of ARP misses/second. Further, it should be possible to move the fabric manager to a small-scale cluster (e.g., four machines) if absolutely necessary when very high frequency of ARP requests is anticipated.

#### VM Migration

Finally, we evaluate PortLand’s ability to support virtual machine migration. In this experiment, a sender transmits data at 150 Mbps to a virtual machine (hosted on Xen) running on a physical machine in one pod. We then migrate the virtual machine to a physical machine in another pod. On migration, the host transmits a gratuitous ARP with its new MAC address, which is in turn forwarded to all hosts communicating with that VM by the previous egress switch. The communication is not at line-rate (1 Gbps) since we use software MAC layer rewriting capability provided by OpenFlow to support PMAC and AMAC translation at edge switches. This introduces additional per-packet processing latency. Existing commercial switches have MAC layer rewriting support directly in hardware [2].

Figure 13 plots the results of the experiment with measured TCP rate for both state transfer and flow transfer (measured at the sender) on the y-axis as a function of time progressing on the x-axis. We see that 5+ seconds into the experiment, the throughput of the TCP flow drops below the peak rate as the state of the VM begins to migrate to a new physical machine. During migration, there are short time periods (200-600ms) during which the throughput of the flow drops to near zero (not visible in the graph due to the scale). Communication resumes with the VM at full speed after approximately 32 seconds (dominated by the time to complete VM state transfer).

### Conclusions

The goal of this work is to explore the extent to which entire data center networks may be treated as a single plug-and-play fabric. Modern data centers may contain 100,000 hosts and employ virtual machine multiplexing that results in millions of unique addressable end hosts. Efficiency, fault tolerance, flexibility, and manageability are all significant concerns with general-purpose Ethernet and IP-based protocols. In this paper, we present PortLand, a set of Ethernet-compatible routing, forwarding, and address resolution protocols specifically tailored for data center deployments. It is our hope that through protocols like PortLand, data center networks can become more flexible, efficient, and fault-tolerant.

### References

[1] Cisco Data Center Infrastructure 2.5 Design Guide.
www.cisco.com/application/pdf/en/us/guest/netsol/ns107/c649/ccmigration_09186a008073377d.pdf.

[2] Configuring IP Unicast Layer 3 Switching on Supervisor Engine 2. www.cisco.com/en/US/docs/routers/7600/ios/12.1E/configuration/guide/cef.html.

[3] Inside Microsoft’s $550 Million Mega Data Centers. www.informationweek.com/news/hardware/data_centers/showArticle.jhtml?articleID=208403723.

[4] OpenFlow. www.openflowswitch.org/.

[5] OSPF Design Guide. www.ciscosystems.com/en/US/tech/tk365/technologies_white_paper09186a0080094e9e.shtml.

[6] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity Data Center Network Architecture. In SIGCOMM ’08: Proceedings of the ACM SIGCOMM 2008 conference on Data communication, pages 63–74, New York, NY, USA, 2008. ACM.

[7] M. Caesar, D. Caldwell, N. Feamster, J. Rexford, A. Shaikh, and J. van der Merwe. Design and Implementation of a Routing Control Platform. In USENIX Symposium on Networked Systems Design & Implementation, 2005.

[8] M. Caesar, M. Castro, E. B. Nightingale, G. O, and A. Rowstron. Virtual Ring Routing: Network Routing Inspired by DHTs. In Proceedings of ACM SIGCOMM, 2006.

[9] M. Caesar, T. Condie, J. Kannan, K. Lakshminarayanan, I. Stoica, and S. Shenker. ROFL: Routing on Flat Labels. In Proceedings of ACM SIGCOMM, 2006.

[10] M. C. Changhoon Kim and J. Rexford. Floodless in SEATTLE: A Scalable Ethernet Architecture for Large Enterprises. In SIGCOMM ’08: Proceedings of the ACM SIGCOMM 2008 conference on Data communication, 2008.

[11] C. Clark, K. Fraser, S. Hand, J. G. H. E. J. C. Limpach, I. Pratt, and A. Warfield. Live Migration of Virtual Machines. In USENIX Symposium on Networked Systems Design & Implementation, 2005.

[12] J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. In OSDI’04: Proceedings of the 6th conference on Symposium on Operating Systems Design & Implementation, pages 10–10, Berkeley, CA, USA, 2004. USENIX Association.

[13] S. Ghemawat, H. Gobioff, and S.-T. Leung. The Google File System. ACM SIGOPS Operating Systems Review, 37(5), 2003.

[14] A. Greenberg, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. Towards a Next Generation Data Center Architecture: Scalability and Commoditization. In PRESTO ’08: Proceedings of the ACM Workshop on Programmable Routers for Extensible Services of Tomorrow, pages 57–62, New York, NY, USA, 2008. ACM.

[15] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu. DCell: A Scalable and Fault-tolerant Network Structure for Data Centers. In Proceedings of the ACM SIGCOMM 2008 conference on Data communication, pages 75–86, New York, NY, USA, 2008. ACM.

[16] C. Hopps. Analysis of an Equal-Cost Multi-Path Algorithm. RFC 2992, Internet Engineering Task Force, 2000.

[17] K. Lakshminarayanan, M. Caesar, M. Rangan, T. Anderson, S. Shenker, I. Stoica, and H. Luo. Achieving Convergence-Free Routing Using Failure-Carrying Packets. In Proceedings of ACM SIGCOMM, 2007.

[18] C. E. Leiserson. Fat-Trees: Universal Networks for Hardware-Efficient Supercomputing. IEEE Transactions on Computers, 34(10):892–901, 1985.

[19] J. W. Lockwood, N. McKeown, G. Watson, G. Gibb, P. Hartke, J. Naous, R. Raghuraman, and J. Luo. NetFPGA–An Open Platform for Gigabit-Rate Network Switching and Routing. In Proceedings of the 2007 IEEE International Conference on Microelectronic Systems Education, pages 160–161, Washington, DC, USA, 2007. IEEE Computer Society.

[20] R. Moskowitz and P. Nikander. Host Identity Protocol (HIP) Architecture. RFC 4423 (Proposed Standard), 2006.

[21] J. Moy. OSPF Version 2. RFC 2328, Internet Engineering Task Force, 1998.

[22] A. Myers, T. S. E. Ng, and H. Zhang. Rethinking the Service Model: Scaling Ethernet to a Million Nodes. In ACM HotNets-III, 2004.

[23] L. S. C. of the IEEE Computer Society. IEEE Standard for Local and Metropolitan Area Networks, Common Specifications Part 3: Media Access Control (MAC) Bridges Amendment 2: Rapid Reconfiguration, June 2001.

[24] R. Perlman, D. Eastlake, D. G. Dutt, S. Gai, and A. Ghanwani. Rbridges: Base Protocol Specification. Technical report, Internet Engineering Task Force, 2009.

[25] T. L. Rodeheffer, C. A. Thekkath, and D. C. Anderson. SmartBridge: A Scalable Bridge Architecture. In Proceedings of ACM SIGCOMM, 2001.

[26] M. D. Schroeder, A. D. Birrell, M. Burrows, H. Murray, R. M. Needham, T. L. Rodeheffer, E. H. Satterthwaite, and C. P. Thacker. Autonet: A High-Speed, Self-Configuring Local Area Network Using Point-to-Point Links. In IEEE Journal On Selected Areas in Communications, 1991.

[27] M. Scott and J. Crowcroft. MOOSE: Addressing the Scalability of Ethernet. In EuroSys Poster session, 2008.

[28] J. Touch and R. Perlman. Transparent Interconnection of Lots of Links (TRILL): Problem and Applicability Statement, 2009.

### Appendix A: Loop-Free Proof

A fat-tree network topology has many physical loops, which can easily lead to forwarding loops given some combination of forwarding rules present in the switches. However, physical loops in data center networks are desirable and provide many benefits such as increased network bisection bandwidth and fault tolerance. Traditional Ethernet uses a minimum spanning tree to prevent forwarding loops at the cost of decreased bisection bandwidth and fault tolerance.

Here we show that fat trees can be constrained in such a way as to prevent forwarding loops without requiring an explicit spanning tree. This constraint is simple, stateless, local to an individual switch, and uniform across all switches in the fat tree.

**Constraint 1:** A switch must never forward a packet out along an upward-facing port when the ingress port for that packet is also an upward-facing port.

**Theorem 1:** When all switches satisfy Constraint 1 (C1), a fat tree will contain no forwarding loops.

**Proof:** C1 prevents traffic from changing direction more than once. It imposes the logical notion of up-packets and down-packets. Up-packets may travel only upward through the tree, whereas down-packets may travel only downward. C1 effectively allows a switch to perform a one-time conversion of an up-packet to a down-packet. There is no provision for converting a down-packet to an up-packet. For a switch to receive the same packet from the same ingress port more than once, the packet would need to change its direction at least twice while routed through the tree topology. However, this is not possible since there is no mechanism for converting a down-packet to an up-packet, something that would be required for at least one of these changes in direction.