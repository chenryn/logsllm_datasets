# Quantifying Information Leaks in Outbound Web Traffic

**Authors:**
- Kevin Borders, Web Tap Security, Inc., Ann Arbor, MI
- Atul Prakash, University of Michigan, Ann Arbor, MI

**Conference:**
2009 30th IEEE Symposium on Security and Privacy

## Abstract
As the Internet expands and network bandwidth continues to increase, administrators face the challenge of keeping confidential information from leaving their networks. The volume of today's network traffic makes manual inspection impractical. In response, researchers have developed data loss prevention (DLP) systems that check outgoing traffic for known sensitive information. While these systems can stop naive adversaries, they cannot identify encrypted or obfuscated information leaks, leaving a high-capacity channel for data exfiltration.

We present an approach to quantify the capacity for information leaks in network traffic. Instead of detecting the presence of sensitive data—an impossible task in the general case—our goal is to measure and constrain its maximum volume. By leveraging the fact that most network traffic is repetitive or determined by external factors like protocol specifications, we can isolate and quantify true information flow. This paper introduces measurement algorithms for the Hypertext Transfer Protocol (HTTP), the primary protocol for web browsing. When applied to real web browsing traffic, our algorithms were able to discount 98.5% of measured bytes, effectively isolating information leaks.

## 1. Introduction
Network-based information leaks pose a significant threat to confidentiality. They are the primary means by which hackers extract data from compromised computers and can also serve as a conduit for insider leaks. According to a 2007 CSI/FBI survey, insider threats are the most prevalent security issue for organizations [17]. The large volume of legitimate network traffic makes it easy for attackers to blend in with normal activity, making leak prevention challenging. For example, a single computer browsing a social networking site for 30 minutes can generate over 1.3 MB of legitimate request data, equivalent to about 195,000 credit card numbers. Manually analyzing network traffic for leaks is both unreasonably expensive and error-prone. Limiting network traffic based on raw byte count would only help stop large information leaks.

To address the threat of network-based information leaks, researchers have developed DLP systems [18, 24]. These systems search outbound network traffic for known sensitive information, such as credit card and social security numbers. Some DLP systems even catalog sensitive documents and look for excerpts in outbound traffic. While effective at stopping accidental and plain-text leaks, DLP systems cannot detect obfuscated information flows, leaving an open channel for data exfiltration.

We introduce a new approach to precisely quantify information leak capacity in network traffic. Rather than searching for known sensitive data, we aim to measure and constrain its maximum volume. This research addresses the threat of a hacker or malicious insider extracting sensitive information from a network by hiding it in the noise of normal outbound traffic. For web traffic, this often involves stashing bytes in paths or header fields within seemingly benign requests. To combat this, we exploit the fact that a large portion of legitimate network traffic is repetitive or constrained by protocol specifications. By ignoring this fixed data, we can isolate real information leaving a network, regardless of data hiding techniques.

The leak measurement techniques presented here focus on HTTP, the main protocol for web browsing. These techniques leverage HTTP interactions with HTML documents and JavaScript code to quantify information leak capacity. The basic idea is to compute the expected content of HTTP requests using only externally available information, including previous network requests, server responses, and protocol specifications. The amount of unconstrained outbound bandwidth is then the edit distance between actual and expected requests, plus timing information. Given correct assumptions about timing channel characteristics, these results may overestimate but will never underestimate the true size of information leaks, serving as a tight upper bound on information leakage.

### Figure 1: Graph of Outbound Web Traffic
Figure 1 illustrates the benefit of precise leak quantification. The graphs show bandwidth from legitimate web browsing over a one-day period in black. A 100 KB information leak was inserted into the traffic and can be seen in a lighter color. This leak was deliberately inserted in short bursts to resemble legitimate web traffic and avoid detection methods that look at request regularity [3]. The left graph shows raw request bandwidth, where the leak is barely noticeable and easily blends in with normal activity. After running the same traffic through our unconstrained bandwidth measurement engine, the leak stands out dramatically from normal traffic. More accurate traffic measurement does not completely prevent information leaks from slipping by undetected; it only makes it possible to identify smaller leaks. Our analysis techniques force a leak that would normally blend in with a week’s worth of traffic to be spread out over an entire year.

### Evaluation
We evaluated our leak measurement techniques on real browsing data from 10 users over 30 days, which included over 500,000 requests. The results were compared to a simple calculation described in prior research [3] and incremental gzip compression [8]. The average request size using our leak measurement techniques was 15.8 bytes, 1.6% of the raw byte count. The average size for gzip was 132 bytes, and for the simple measurement was 243 bytes. Our experiments show that our approach is an order of magnitude better than traditional gzip compression.

### Focus on HTTP
This work focuses specifically on analyzing leaks in HTTP traffic for several reasons. First, HTTP is the primary protocol for web browsing and accounts for a large portion of overall traffic. Many networks, particularly those prioritizing confidentiality, will only allow outbound HTTP traffic and block everything else by forcing all traffic through a proxy server. In this scenario, HTTP is the only option for directly leaking data. Another reason for focusing on HTTP is that a high percentage of its request data can be filtered out by eliminating repeated and constrained values.

### Future Work
The principles used to measure leaks in HTTP traffic are likely to work for other protocols as well. Binary protocols for instant messaging, secure shell access, and domain name resolution all contain a number of fixed and repeated values. Correlation between protocols may enable filtering of DNS lookups. Extending a similar methodology to outbound SMTP (e-mail) traffic is more challenging due to the free-form nature of e-mails. However, the unconstrained data in e-mails is usually text, for which there are well-known methods of determining information content [21], or file attachments. These attachments, written in specific file formats, could be analyzed similarly to HTTP. Researchers have already examined ways of identifying information hidden in files with steganography by looking for additional unexpected entropy [2]. Further investigation of leak measurement techniques for file attachments and other protocols is future work.

### Encrypted Traffic
The measurement techniques in this paper do not provide an unconstrained bandwidth measurement for fully encrypted traffic. If a hacker tries to hide or tunnel encrypted data in an unencrypted protocol, it can be measured. All networks that allow outbound encrypted traffic must deal with this limitation.

---

**Note:** This document is authorized for limited use by Tsinghua University. Downloaded on March 19, 2021, at 03:15:14 UTC from IEEE Xplore. Restrictions apply.