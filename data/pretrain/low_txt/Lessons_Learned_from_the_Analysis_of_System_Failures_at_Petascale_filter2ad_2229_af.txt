### Analysis of System-Wide Outages in Blue Waters

#### Causes and Repair Times

In this section, we analyze the causes and repair times for system-wide outages (interruption failure category in Table III) in the Blue Waters supercomputing system.

#### Basic Statistics

Table IX presents the statistics for 39 system-wide outages documented in the failure reports. On average, a system-wide outage occurred every 159.22 hours. In 90% of these cases, the system was restored to full capacity within 3.28 hours from the start of the outage, with a Mean Time to Repair (MTTR) of 5.12 hours. The rapid resolution of system-wide outages contributed to a system availability of 0.9688.

#### Distribution of Time Between Outages

Figure 5 illustrates the Probability Density Function (PDF) and Cumulative Distribution Function (CDF) of the time intervals between system-wide outages (SWOs). The exponential, Weibull, and lognormal distributions all achieved an acceptable goodness of fit (p-value less than 0.1), as reported in Table X. Notably, the good fit for the lognormal distribution suggests a hazard rate that initially increases and then decreases (shape parameter > 1), indicating that a system-wide outage might be influenced by the preceding one. However, the reports document only one instance where a system-wide outage was attributed to the same cause as the preceding one, which occurred 3 hours and 51 minutes earlier (both outages were associated with Lustre). After each system-wide repair, the system is rebooted and cleared of any remaining issues, which contributes to the good fit of the exponential distribution in Figure 5.

#### Root Causes of System-Wide Outages

Approximately 74.4% (29 out of 39) of the system-wide outages were caused by software issues. Figure 6(a) breaks down the system-wide outages across three categories: hardware, software, and environmental. Hardware issues contributed to 15.4% of system-wide outages (6 out of 39), while environmental issues (primarily power failures) caused 10.2% (4 out of 39). Network-related root causes led to system-wide outages in 2 cases, representing 0.6% of recorded network failures. These failures occurred when the network became partitioned and was unable to route around failed nodes due to a lack of redundant paths.

#### Specific Software Issues

Lustre failures accounted for 46% of SWOs (18 out of 39, or about 62% of all software-related SWOs). Figure 7 details the types of Lustre failures that can escalate to system-wide outages. Key observations include:
- Approximately 20% of Lustre failures (18 out of 104) cascade into system-wide outages.
- For about half (47%) of these cases, a failover failure was identified as the direct cause of the SWO.

Single-node failures led to SWOs in 0.7% (6 out of 831 cases) of instances, primarily due to an inability to reroute network traffic around the failed nodes. Data indicates that node failures due to hardware problems become critical when they affect Gemini routing or access to the Lustre file system. The full 3D torus topology effectively provides multiple redundant paths, and link degradation mechanisms and distributed routing tables ensure protection against transient errors, such as routing table corruption.

#### Gemini Network Routing

The Gemini network uses dimension-ordered routing. In an error-free network, a packet first travels along the X dimension to the destination node’s X coordinate, then in the Y dimension to the Y coordinate, and finally in the Z dimension to the final destination. If the standard X-Y-Z ordering fails, Gemini attempts alternative routes until a valid path is found. Certain failure patterns, such as multiple errors in the same loop along the Z dimension, can leave Gemini unable to find a valid route. While such failures are rare, they require the longest recovery time, with an MTTR of 8.1 hours.

#### Case Studies of Routing Problems

The failure reports document three cases of system-wide outages due to routing problems:
1. A failover procedure mishap during a warm swap of a failed service, causing a system partition.
2. A partial failure of the Gemini network that did not trigger the link recovery procedure, leading to job aborts due to lack of communication.
3. A crash of the Gemini routing algorithm, causing routes to hang. System logs revealed a high rate of misrouted packets and routing data corruption within a 1-hour window before the incident.

### Related Work

Several studies have evaluated large-scale systems, addressing various issues such as basic error characteristics, modeling and evaluation, failure prediction, and proactive checkpointing. This study is the first to characterize the failures and related root causes of a sustained petascale system. Following a process similar to [4], our analysis is based on manual failure reports produced by NCSA technical staff managing Blue Waters, supplemented by system logs for deeper analysis and characterization of hardware resiliency.

Previous research has classified failures by their root cause, such as hardware and software. Hardware failures contribute 6% of total failures, while software failures account for 42% [24]. Other studies identify hardware as the primary cause of node failures, with percentages ranging from 30% to over 60%, and software percentages ranging from 5% to 24%. Our work demonstrates significant technological improvements in hardware resiliency features, showing that the majority of system outages in Blue Waters are due to software errors (74.4%), while hardware issues contribute to single node/blade downtime in 99.4% of documented failures.

### Conclusions

This paper presents an in-depth study of manual failure reports collected over 261 days for a large-scale computing system, marking the first failure study of a sustained petaflop system. The overall failure characterization indicates that software failures are the major contributor to total repair time (53%), although they represent only 20% of the total number of failures. More importantly, software is the primary cause (74.4%) of system-wide outages. The analysis highlights the inadequacy of mechanisms behind complex failover operations, such as those employed in the Lustre file system. Despite its complexity, hardware in Blue Waters is highly resilient, with error-correcting codes (including Chipkill) providing extremely high coverage (99.997%) for memory and processor errors. Future work will involve collecting data from similar systems and conducting a detailed analysis of Lustre error resiliency at different scales, as well as examining automatically collected system error logs and workload data to gain further insights into the impact of errors and failures on user computations.

### Acknowledgements

This work is partially supported by the National Science Foundation grants CNS 10-18503 CISE, Air Force Research Laboratory, and the Air Force Office of Scientific Research under agreement No. FA8750-11-2-0084. Additional support comes from an IBM faculty award and a grant from Infosys Ltd. This research is part of the Blue Waters sustained-petascale computing project, supported by the National Science Foundation (award number ACI 1238993) and the state of Illinois. Blue Waters is a joint effort of the University of Illinois at Urbana-Champaign and its National Center for Supercomputing Applications. The work of Fabio Baccanico was supported by the "Programma di scambi Internazionali con Università ed Istituti di Ricerca straniera per la mobilità di breve durata di docenti, ricercatori e studiosi" at Federico II University of Naples. We also acknowledge the anonymous reviewers and Cray engineers for their comments, and Jenny Applequist for her careful reading of the manuscript.

### References

[1] R. K. Sahoo, A. Sivasubramaniam, M. S. Squillante, and Y. Zhang. Failure data analysis of a large-scale heterogeneous server environment. In DSN ’04: Proc. of the 2004 Int. Conference on Dependable Systems and Networks, pages 772–781, 2004.

[2] Y. Liang, A. Sivasubramaniam, J. Moreira, Y. Zhang, R.K. Sahoo, and M. Jette. Filtering failure logs for a bluegene/l prototype. In DSN ’05: Proc. of the 2005 Int. Conference on Dependable Systems and Networks, pages 476–485, 2005.

[3] Y. Liang, Y. Zhang, M. Jette, Anand Sivasubramaniam, and R. Sahoo. Bluegene/l failure analysis and prediction models. In Dependable Systems and Networks, 2006. DSN 2006. International Conference on, pages 425–434, 2006.

[4] B. Schroeder and G.A. Gibson. A large-scale study of failures in high-performance computing systems. Dependable and Secure Computing, IEEE Transactions on, 7(4):337–350, 2010.

[5] B. Schroeder and G. A. Gibson. Disk failures in the real world: what does an mttf of 1,000,000 hours mean to you? In Proceedings of the 5th USENIX conference on File and Storage Technologies, FAST ’07, Berkeley, CA, USA, 2007. USENIX Association.

[6] A. Oliner and J. Stearley. What supercomputers say: A study of five system logs. Dependable Systems and Networks, 2007. DSN ’07. 37th Annual IEEE/IFIP Int. Conference on, pages 575–584, June 2007.

[7] C. Di Martino, M. Cinque, and D. Cotroneo. Assessing time coalescence techniques for the analysis of supercomputer logs. In In Proc. of 42nd Annual IEEE/IFIP Int. Conf. on Dependable Systems and Networks (DSN), 2012, pages 1–12, 2012.

[8] A. Pecchia, d. Cotroneo, Z. Kalbarczyk, and R. K. Iyer. Improving log-based field failure data analysis of multi-node computing systems. In Proceedings of the 2011 IEEE/IFIP 41st International Conference on Dependable Systems&Networks, DSN ’11, pages 97–108, Washington, DC, USA, 2011. IEEE Computer Society.

[9] B. Schroeder, E. Pinheiro, and W. Weber. DRAM errors in the wild: a large-scale field study. SIGMETRICS Perform. Eval. Rev., 37(1):193–204, June 2009.

[10] A. Gainaru, F. Cappello, M. Snir, and W. Kramer. Fault prediction under the microscope: A closer look into HPC systems. In High Performance Computing, Networking, Storage and Analysis (SC), 2012 International Conference for, pages 1–11, 2012.

[11] AMD Inc. BIOS and kernel developers guide, for AMD family 16h.

[12] T. Dell. IBM Microelectronics Division. A white paper on the benefits of Chipkill-Correct ECC for PC server main memory, 1997.

[13] M. Karo, R. Lagerstrom, M. Kohnke, and C. Albing. The application level placement scheduler. In Cray User Group - CUG, 2008.

[14] http://www.adaptivecomputing.com/products/hpc-products/moab-hpc-suite-enterprise-edition.

[15] http://www.cray.com/Products/Storage/Sonexion/Specifications.aspx.

[16] J. Stearley, R. Ballance, and L. Bauman. A state-machine approach to disambiguating supercomputer event logs. In proc. of Workshop on Managing System Automatically and Dynamically 2, 155-192, Berkeley, CA, 2012. USENIX.

[17] V. Sridharan, J. Stearley, N. DeBardeleben, S. Blanchard, and S. Gurumurthi. Feng shui of supercomputer memory: Positional effects in DRAM and SRAM faults. In Proceedings of SC13: International Conference for High Performance Computing, Networking, Storage and Analysis, SC ’13, pages 22:1–22:11, New York, NY, USA, 2013. ACM.

[18] http://www.olcf.ornl.gov/titan/, number 2 on top500.org.

[19] M. R. Lyu, editor. Handbook of software reliability engineering. McGraw-Hill, Hightstown, NJ, USA, 1996.

[20] E. Heien, D. Kondo, A. Gainaru, A. LaPine, W. Kramer, and F. Cappello. Modeling and tolerating heterogeneous failures in large parallel systems. In Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis, SC ’11, pages 45:1–45:11, New York, NY, USA, 2011. ACM.

[21] C. Di Martino. One size does not fit all: Clustering supercomputer failures using a multiple time window approach. In Julian Martin Kunkel, Thomas Ludwig, and Hans Werner Meuer, editors, International Supercomputing Conference - Supercomputing, volume 7905 of Lecture Notes in Computer Science, pages 302–316. Springer Berlin Heidelberg, 2013.

[22] Xin Chen, Charng-Da Lu, and K. Pattabiraman. Predicting job completion times using system logs in supercomputing clusters. In Dependable Systems and Networks Workshop (DSN-W), 2013 43rd Annual IEEE/IFIP Conference on, pages 1–8, June 2013.

[23] A. Gainaru, F. Cappello, and W. Kramer. Taming of the shrew: Modeling the normal and faulty behavior of large-scale HPC systems. In Parallel Distributed Processing Symposium (IPDPS), 2012 IEEE 26th International, pages 1168–1179, 2012.

[24] D. Oppenheimer and D. A. Patterson. Studying and using failure data from large-scale internet services. In Proceedings of the 10th workshop on ACM SIGOPS European workshop, EW 10, pages 255–258, New York, NY, USA, 2002. ACM.