### Evaluation and Ranking of Anomaly Detectors

The 14 anomaly detectors were evaluated and ranked from best to worst, with standard deviations provided in parentheses. The top-performing detectors, which are not significantly worse than the best-performing detector, are indicated in bold.

### Performance Metrics

To evaluate the detectors, we used two performance metrics: equal-error rate (EER) and zero-miss false-alarm rate (ZMFAR). 

- **Equal-Error Rate (EER):** This is the error rate when the threshold is set such that the miss rate and false-alarm rate are equal. This measure was used by Kang et al. [11].
- **Zero-Miss False-Alarm Rate (ZMFAR):** This is the false-alarm rate when the threshold is set to minimize the false-alarm rate under the constraint that the miss rate is zero. This measure was used in earlier studies [4, 21].

Both EER and ZMFAR are error rates, where lower values indicate better performance. For each of the 714 combinations of detector and subject, we generated an ROC curve and calculated these two measures.

### Results and Analysis

#### Table 2: Average Error Rates
Table 2 shows the average EER and ZMFAR for each of the 14 detectors over all subjects. The detectors are rank-ordered from best to worst, with separate rankings for each performance measure.

Since all anomaly detectors were evaluated using the same data, under the same conditions, and using the same procedures, any differences in performance can be attributed to the detectors themselves rather than to experimental conditions.

#### Statistical Analysis
To determine the top-performing detectors, we conducted 13 hypothesis tests, comparing the performance of the best-performing detector to each of the other 13 detectors. Each test's null hypothesis is that the best-performing detector is no better than the other detector (a one-sided test). If the null hypothesis cannot be rejected, the detector is considered a top performer.

Cho et al. [4] used a paired t-test for this purpose, but our data showed non-normal distributions. Therefore, we used the non-parametric Wilcoxon signed-rank test, which is more robust for non-normal distributions. We applied a Bonferroni correction for multiple testing, using a significance level of α = 0.05.

### Detector Performance Comparison

- **Best Equal-Error Rate (EER):** 0.096, achieved by the Manhattan (scaled) detector [1]. Other top-performing detectors include the Nearest Neighbor (Mahalanobis) and Outlier Count (z-score) detectors.
- **Best Zero-Miss False-Alarm Rate (ZMFAR):** 0.468, achieved by the Nearest Neighbor (Mahalanobis) detector [4]. Other top-performing detectors include the classical Mahalanobis, Mahalanobis (normed), and SVM (one-class) detectors.

None of the performance measures meet the 0.001% miss rate and 1% false-alarm rate required by the European standard for access-control systems [3]. This suggests that further progress is needed before keystroke dynamics can be reliably used for access control.

### Shared Strategies Among Top-Performing Detectors

Top-performing detectors often employ some form of scaling of timing features, such as using Mahalanobis distance instead of Euclidean distance. This accounts for the different variability in timing features, leading to better performance.

The Nearest Neighbor (Mahalanobis) detector is the only top-performer according to both performance measures, suggesting its robustness to different threshold-selection procedures.

### Discussion and Future Work

This study is the first to evaluate 14 anomaly detectors for keystroke dynamics on an equal basis. Our results highlight the value of shared data and consistent evaluation methodologies. We invite researchers to use our data and methods to evaluate other detectors and implementations.

#### Shared Data and Methods
While the comparison of specific detectors is interesting, the availability of shared data and methods is even more valuable. We encourage researchers to use our data and methods to make valid comparisons with our results. To our knowledge, Montalvão et al. [15] are the only other researchers to make keystroke data publicly available. We welcome the trend of making diverse public data sets available, as it will boost progress in this field.

#### Extensions to the Evaluation Method
Our data can be used beyond comparing anomaly detectors. For example, researchers can evaluate the impact of different decisions, such as the number of training samples or the type of impostors used. We ask that any extensions be explicitly and carefully described to avoid confusion.

#### Detector Variability Across Data Sets
Minor differences in detectors, data, and evaluation methods can cause major swings in performance. Shared data and common evaluation procedures are critical for reliable assessment and comparison.

### Summary and Conclusion

This work provides a comprehensive evaluation of 14 anomaly detectors for keystroke dynamics, establishing which detectors have the lowest error rates on our data. We provide a data set and evaluation methodology that can be used by the community to assess new detectors and report comparative results.

### Acknowledgments

We thank Patricia Loring for running the experiments, Rachel Krishnaswami for her insightful comments, and several anonymous reviewers for their helpful feedback. This work was supported by the National Science Foundation and the Army Research Office.

### References

[1] L. C. F. Araújo, L. H. R. Sucupira, M. G. Lizárraga, L. L. Ling, and J. B. T. Yabu-uti. User authentication through typing biometrics features. In Proceedings of the 1st International Conference on Biometric Authentication (ICBA), volume 3071 of Lecture Notes in Computer Science, pages 694–700. Springer-Verlag, Berlin, 2004.

[2] S. Bleha, C. Slivinsky, and B. Hussien. Computer-access security systems using keystroke dynamics. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(12):1217–1222, 1990.

[3] CENELEC. European Standard EN 50133-1: Alarm systems. Access control systems for use in security applications. Part 1: System requirements, 2002. Standard Number EN 50133-1:1996/A1:2002, Technical Body CLC/TC 79, European Committee for Electrotechnical Standardization (CENELEC).

[4] S. Cho, C. Han, D. H. Han, and H. Kim. Web-based keystroke dynamics identity verification using neural network. Journal of Organizational Computing and Electronic Commerce, 10(4):295–307, 2000.

[5] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons, Inc., second edition, 2001.

[6] G. Forsen, M. Nelson, and R. Staron, Jr. Personal attributes authentication techniques. Technical Report RADC-TR-77-333, Rome Air Development Center, October 1977.

[7] R. S. Gaines, W. Lisowski, S. J. Press, and N. Shapiro. Authentication by keystroke timing: Some preliminary results. Technical Report R-2526-NSF, RAND Corporation, May 1980.

[8] S. Haider, A. Abbas, and A. K. Zaidi. A multi-technique approach for user identification through keystroke dynamics. IEEE International Conference on Systems, Man and Cybernetics, pages 1336–1341, 2000.

[9] B. Hwang and S. Cho. Characteristics of auto-associative MLP as a novelty detector. In Proceedings of the IEEE International Joint Conference on Neural Networks, volume 5, pages 3086–3091, 10–16 July, 1999, Washington, DC, 1999.

[10] R. Joyce and G. Gupta. Identity authentication based on keystroke latencies. Communications of the ACM, 33(2):168–176, 1990.

[11] P. Kang, S. Hwang, and S. Cho. Continual retraining of keystroke dynamics based authenticator. In Proceedings of the 2nd International Conference on Biometrics (ICB’07), pages 1203–1211. Springer-Verlag Berlin Heidelberg, 2007.

[12] H.-j. Lee and S. Cho. Retraining a keystroke dynamics-based authenticator with impostor patterns. Computers & Security, 26(4):300–310, 2007.

[13] Microsoft. Password checker, 2008. http://www.microsoft.com/protect/yourself/password/checker.mspx.

[14] R. G. Miller, Jr. Simultaneous Statistical Inference. Springer-Verlag, New York, second edition, 1981.

[15] J. Montalvão, C. A. S. Almeida, and E. O. Freire. Equalization of keystroke timing histograms for improved identification performance. In 2006 International Telecommunications Symposium, pages 560–565, September 3–6, 2006, Fortaleza, Brazil, 2006.

[16] PC Tools. Security guide for windows—random password generator, 2008. http://www.pctools.com/guides/password/.

[17] A. Peacock, X. Ke, and M. Wilkerson. Typing patterns: A key to user identification. IEEE Security and Privacy, 2(5):40–47, 2004.

[18] R Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2008.

[19] D. J. Sheskin. Handbook of Parametric and Nonparametric Statistical Procedures. Chapman & Hall/CRC, fourth edition, 2007.

[20] J. A. Swets and R. M. Pickett. Evaluation of Diagnostic Systems: Methods from Signal Detection Theory. Academic Press, New York, 1982.

[21] E. Yu and S. Cho. GA-SVM wrapper approach for feature subset selection in keystroke dynamics identity verification. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pages 2253–2257. IEEE Press, 2003.