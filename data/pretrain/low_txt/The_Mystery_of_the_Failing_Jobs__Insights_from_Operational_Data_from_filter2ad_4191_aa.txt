# The Mystery of the Failing Jobs: Insights from Operational Data from Two University-Wide Computing Systems

**Authors:**
- Rakesh Kumar
- Saurabh Jha
- Ashraf Mahgoub
- Rajesh Kalyanam
- Stephen L. Harrell
- Xiaohui Carol Song
- Zbigniew Kalbarczyk
- William T. Kramer
- Ravishankar K. Iyer
- Saurabh Bagchi

**Affiliations:**
- 1: Microsoft, [EMAIL]
- 2: Purdue University, {amahgoub, rkalyana, slh, cxsong, sbagchi}@purdue.edu
- 3: University of Illinois at Urbana-Champaign, {sjha8, kalbarcz, wtkramer, rkiyer}@illinois.edu

**Abstract:**
Node downtime and job failures in high-performance computing (HPC) clusters result in wasted resources and user dissatisfaction. This paper analyzes node and job failures in two university-wide computing clusters at Tier 1 US research universities. We examined approximately 3.0 million job execution records from System A and 2.2 million from System B, using data from accounting logs, resource usage (memory, I/O, network), and node failure data. We observed various correlations between failures and resource usage, and proposed a job failure prediction model to trigger event-driven checkpointing and avoid wasted work. Additionally, we present user history-based resource usage and runtime prediction models, which can help mitigate system-related issues such as contention and improve quality of service by reducing mean queue time. As a proof of concept, we simulated an easy backfill scheduler using one of these models, i.e., runtime, and demonstrated improvements in terms of lower mean queue time. Based on our observations, we provide generalizable insights for cluster management to enhance reliability.

**Index Terms:**
- HPC
- Production failure data
- Data analytics
- Compute clusters

## I. Introduction

"THE PHOENIX MUST BURN TO EMERGE."
â€” Janet Fitch

Large-scale HPC systems are prevalent in academic, industrial, and governmental settings for compute-intensive applications. These systems solve problems that would take millennia on personal computers but managing such large shared resources is challenging. Administrators must balance requirements from diverse users. Large, focused organizations often centralize resources, managed through a central IT organization, funded by federal agencies like the National Science Foundation in the US. Researchers write grant proposals to access compute time on these systems, such as Comet at UC San Diego, Blue Waters at UIUC, and Frontera at UT Austin.

Another trend in university IT acquisition is the community cluster model, where research groups buy nodes and hardware in a central cluster, managed by the central IT organization. This model allows flexible usage policies, enabling partners to access their purchased capacity and use more resources when other groups' nodes are idle. This results in opportunistic use for end-users and higher resource utilization for cluster managers. Examples include Purdue's program since 2006, providing 431M CPU hours in 2018, and similar programs at the Universities of Rochester, Delaware, and Texas at Austin.

This paper studies the reliability of jobs running on two clusters following these operational models: System A and System B, at Purdue University and the University of Illinois at Urbana-Champaign, respectively. Table I summarizes the data analyzed for both clusters.

**Table I: Summary of Data Analyzed (All Production Jobs) for the Two University-Wide Clusters**

| Computing Cluster | Duration          | # Jobs | # Single | # Multi | Total | # Unique Users |
|-------------------|-------------------|--------|----------|---------|-------|----------------|
| System A          | Mar 2015 - Jun 2017 | 2,908k | 1,125k (38.7%, 15.8%) | 28k (1.0%, 1.9%) | 1,153k (39.7%, 17.7%) | 1,348k (46.3%, 18.4%) | 407k (14.0%, 63.9%) | 1,755k (60.3%, 82.3%) | 617 |
| System B          | Feb - Jun 2017    | 2,219k | 1,640k (73.9%, 5.4%) | 580k (26.1%, 94.6%) | 2,219k (100%) | 467 |

## II. Key Observations and Recommendations

**Table II: Key Observations and Recommendations for the Two University-Wide Computing Clusters, System A and System B**

| Observations                                                                 | Recommendations                                                                                                       |
|------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|
| O1: Local I/O-related failure rates in System A rise with low utilization (3-6 MB/s). Remote I/O-related failures in System B occur at 46 MB/s. | R1: (P) Monitor I/O utilization and identify contention thresholds to take proactive measures.                     |
| O2: Job failure rates and runtime durations show opposite trends in System A and B. | R2: (P) Use job runtime duration as a feature for failure prediction in System B. (U) Test new jobs in test environments. |
| O3: Significant out-of-memory exceptions in System A even with ample free memory. | R3: (P) Use user history-based memory usage prediction for scheduling. (ii) Monitor and take preemptive actions.   |
| O4: System A has 53% system-related failures, while System B has only 4%. User-related failures are significant in both. | R4: (P) Use resiliency features and dedicated administrators. (U) Use static analysis tools and small-scale testing.  |
| O5: Contention for remote resources is dominant in non-shared environments, while local contention is dominant in shared environments. | R5: (P) Use user-based resource usage prediction and adopt resource isolation technologies. (U) Use dynamic reconfiguration. |
| O6: Significant fraction of jobs hit walltime in both systems.                | R6: (P) Provide extra cycles for checkpointing and use dynamic checkpointing frequency.                              |

## III. System Details

**Table III: System Specifications**

| Unit              | Compute Nodes | Cores | Accelerators | Period       | # Jobs  |
|-------------------|---------------|-------|--------------|--------------|---------|
| System A          | 580           | 4,640 | 1,160 Xeon Phi | Mar 2015 - Jun 2017 | 3.0M    |
| System B          | 396,000       | 4,229 GPU | Feb - Jun 2017 | 2.2M         | 26% multi-node |

The paper is structured as follows:
- **Section II:** Provides details of the two systems.
- **Section III:** Describes the data sources.
- **Section IV:** Analyzes job failure categories.
- **Section V:** Examines the impact of resource usage on job failures and presents resource usage prediction models.
- **Section VI:** Demonstrates the applications of failure and runtime prediction models.
- **Discussion:** Threats to validity, related work, and conclusions.

This comprehensive analysis provides actionable insights for improving the reliability and efficiency of HPC clusters.