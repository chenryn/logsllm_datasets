### Optimized Text

Bro, when operating with a limited state cache, requires approximately 2.2 times more processing time compared to its operation with an unlimited DFA (Deterministic Finite Automaton) state memory. Initially, it might seem that the performance discrepancy is due to the re-computation of states previously expired from the limited cache. However, further experiments with effectively infinite cache sizes indicate that the performance decrease is primarily due to the additional overhead associated with maintaining the cache.

While this overhead appears significant, it is unclear whether the space savings provided by the cache are necessary in operational use. For the analyzed trace, only 2,669 DFA states were computed, totaling around 10MB. In a typical day of operation at a university gateway, the number of states quickly rises to about 2,500 in the first hour and then gradually increases to just over 4,000 by the end of the day.

A remaining question is whether an attacker could generate traffic specifically designed to enlarge the DFAs, potentially through a "state-holding" attack on the IDS. Additional research is needed to evaluate this threat.

When comparing the alerts generated by Snort and the signature matches reported by Bro for the USB-Full dataset, we find a high degree of agreement. The main difference lies in the reporting mechanism. Bro reports all matching signatures, but each one only once per connection, similar to the approach suggested in [10]. Snort, on the other hand, reports the first matching signature for each packet, independent of the connection. This makes direct comparison challenging. To address this, we compare connections where at least one match is generated by either system. For USB-Full, Bro generates 2,065 matches across 1,313 connections, while Snort reports 4,147 alerts. When counting each alert only once per connection, Snort produces 1,320 alerts across 1,305 connections. There are 1,296 connections where both systems generate at least one alert, and 17 (9) connections where Bro (Snort) reports a match but not Snort (Bro).

Analyzing individual signatures, Bro misses 10 matches that Snort detects. Five of these are caused by Snort ID #1013 (WEB-IIS fpcount access). The corresponding connections contain several requests, but an idle time greater than the defined inactivity timeout of 30 seconds, causing Bro to flush the state before encountering the match. Conversely, Bro reports 41 signature matches for connections where Snort does not report anything. Thirty-seven of these are Web signatures, and the discrepancies are due to differences in TCP stream semantics. Bro and Snort have slightly different definitions of when a session is established, and the semantic differences between stream-wise and packet-wise matching, as discussed in §4.2, cause some of the additional alerts.

**Figure 7: Run-time Comparison on 550MHz Pentium-3**
- **Runtime for USB-Full on Pentium-3**
  - Bro w/o state cache
  - Bro w/ state cache
  - Snort
  - Snort [FV01]
  - Snort patched

We conducted similar measurements with LBL-Web. Due to space constraints, the corresponding plot is omitted. For this trace, the original Snort takes 440 CPU seconds, while Bro without (with) a limited state cache needs 280 (328) CPU seconds. The modified Snort by us requires only 164 CPU seconds. While this suggests room for improvement in some of Bro’s internal data structures, Bro’s matcher still compares favorably to the typical Snort configuration. For this trace, Bro (Snort) reports 2,764 (2,049) matches in total. If we count Snort’s alerts only once per connection, there are 1,472 of them. There are 1,395 connections where both systems report at least one alert. For 133 (69) connections, Bro (Snort) reports a match but Snort (Bro) does not. Analyzing individual signatures, Bro misses 73 of Snort’s alerts. Twenty-five of these are matches of Snort signature #1287 (WEB-IIS scripts access), all caused by the same host. The reason is missing packets in the trace, which prevent TCP stream reassembly by Bro. Another 19 are due to signature #1287 (CodeRed v2 root.exe access), caused by premature server-side resets, which Bro correctly identifies as the end of the corresponding connections, while Snort continues matching on the traffic sent by the client. Bro reports 186 signature matches for connections where Snort does not report a match at all. Sixty-eight of these connections simultaneously trigger three signatures (#1002, #1113, #1287). Forty-six are due to simultaneous matches of signatures #1087 and #1242. Some of these discrepancies are due to missing SYN-packets, leading to different interpretations of established sessions by Snort and Bro, and thus different matches.

### Conclusions

In this work, we introduce the concept of contextual signatures as an enhancement to traditional string-based signature matching used by NIDS. Contextual signatures augment the matching process with both low-level context, using regular expressions instead of fixed strings, and high-level context, leveraging the rich, additional semantic context provided by Bro’s protocol analysis and scripting language.

By integrating the new signature engine into Bro’s event-based architecture, we achieve several major improvements over other signature-based NIDSs like Snort, which often generate a large number of alerts. By interpreting a signature match as an event rather than an alert, we can leverage Bro’s context and state-management mechanisms to improve alert quality. We demonstrated several examples of this approach, including matching requests with replies, recognizing exploit scans, using vulnerability profiles, and defining dependencies between signatures to model multi-connection attacks. Additionally, by converting Snort’s freely available signature set into Bro’s language, we build upon existing community efforts.

For a baseline evaluation, we compared our signature engine with Snort in terms of runtime performance and generated alerts using the signature set archived at [2]. During this process, we encountered several general problems when comparing NIDSs, such as differing internal semantics, incompatible tuning options, the difficulty of creating representative input, and extreme sensitivity to hardware specifics. These challenges require caution in interpreting comparisons between NIDSs.

Based on this work, we are now deploying Bro’s contextual signatures in various educational, research, and commercial environments. Our work has been integrated into version 0.8 of the Bro distribution, freely available at [5].

### Acknowledgments

We would like to thank the Lawrence Berkeley National Laboratory (LBL), Berkeley, USA; the National Energy Research Scientific Computing Center (NERSC), Berkeley, USA; and Saarland University, Germany. We are grateful to Anja Feldmann for making this work possible. Finally, we thank the anonymous reviewers for their valuable suggestions.

### References

[1] arachNIDS. http://whitehats.com/ids/.
[2] Web archive of versions of software and signatures used in this paper. http://www.net.in.tum.de/˜robin/ccs03.
[3] S. Axelsson. The base-rate fallacy and the difficulty of intrusion detection. ACM Transactions on Information and System Security, 3(3):186–205, August 2000.
[4] R. G. Bace. Intrusion Detection. Macmillan Technical Publishing, Indianapolis, IN, USA, 2000.
[5] Bro: A System for Detecting Network Intruders in Real-Time. http://www.icir.org/vern/bro-info.html.
[6] Bugtraq. http://www.securityfocus.com/bid/1187.
[7] CERT Advisory CA-2002-27 Apache/mod ssl Worm. http://www.cert.org/advisories/CA-2002-27.html.
[8] C. J. Coit, S. Staniford, and J. McAlerney. Towards Faster Pattern Matching for Intrusion Detection or Exceeding the Speed of Snort. In Proc. 2nd DARPA Information Survivability Conference and Exposition, June 2001.
[9] Common Vulnerabilities and Exposures. http://www.cve.mitre.org.
[10] H. Debar and B. Morin. Evaluation of the Diagnostic Capabilities of Commercial Intrusion Detection Systems. In Proc. Recent Advances in Intrusion Detection, number 2516 in Lecture Notes in Computer Science. Springer-Verlag, 2002.
[11] R. F. et. al. Hypertext transfer protocol – http/1.1. Request for Comments 2616, June 1999.
[12] M. Fisk and G. Varghese. Fast Content-Based Packet Handling for Intrusion Detection. Technical Report CS2001-0670, UC San Diego, May 2001.
[13] Fyodor. Remote OS detection via TCP/IP Stack Finger Printing. Phrack Magazine, 8(54), 1998.
[14] J. Haines, L. Rossey, R. Lippmann, and R. Cunnigham. Extending the 1999 Evaluation. In Proc. 2nd DARPA Information Survivability Conference and Exposition, June 2001.
[15] M. Hall and K. Wiley. Capacity Verification for High Speed Network Intrusion Detection Systems. In Proc. Recent Advances in Intrusion Detection, number 2516 in Lecture Notes in Computer Science. Springer-Verlag, 2002.
[16] M. Handley, C. Kreibich, and V. Paxson. Network intrusion detection: Evasion, traffic normalization, and end-to-end protocol semantics. In Proc. 10th USENIX Security Symposium, Washington, D.C., August 2001.
[17] J. Heering, P. Klint, and J. Rekers. Incremental generation of lexical scanners. ACM Transactions on Programming Languages and Systems (TOPLAS), 14(4):490–520, 1992.
[18] J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Languages, and Computation. Addison Wesley, 1979.
[19] K. Jackson. Intrusion detection system product survey. Technical Report LA-UR-99-3883, Los Alamos National Laboratory, June 1999.
[20] U. Lindqvist and P. A. Porras. Detecting computer and network misuse through the production-based expert system toolset (P-BEST). In Proc. IEEE Symposium on Security and Privacy. IEEE Computer Society Press, May 1999.
[21] R. Lippmann, R. K. Cunningham, D. J. Fried, I. Graf, K. R. Kendall, S. E. Webster, and M. A. Zissman. Results of the 1998 DARPA Offline Intrusion Detection Evaluation. In Proc. Recent Advances in Intrusion Detection, 1999.
[22] R. Lippmann, J. W. Haines, D. J. Fried, J. Korba, and K. Das. The 1999 DARPA off-line intrusion detection evaluation. Computer Networks, 34(4):579–595, October 2000.
[23] R. Lippmann, S. Webster, and D. Stetson. The Effect of Identifying Vulnerabilities and Patching Software on the Utility of Network Intrusion Detection. In Proc. Recent Advances in Intrusion Detection, number 2516 in Lecture Notes in Computer Science. Springer-Verlag, 2002.
[24] J. McHugh. Testing Intrusion detection systems: A critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln Laboratory. ACM Transactions on Information and System Security, 3(4):262–294, November 2000.
[25] V. Paxson. Bro: A system for detecting network intruders in real-time. Computer Networks, 31(23–24):2435–2463, 1999.
[26] P. A. Porras and P. G. Neumann. EMERALD: Event monitoring enabling responses to anomalous live disturbances. In National Information Systems Security Conference, Baltimore, MD, October 1997.
[27] T. H. Ptacek and T. N. Newsham. Insertion, evasion, and denial of service: Eluding network intrusion detection. Technical report, Secure Networks, Inc., January 1998.
[28] M. J. Ranum, K. Landfield, M. Stolarchuk, M. Sienkiewicz, A. Lambeth, and E. Wall. Implementing a generalized tool for network monitoring. In Proc. 11th Systems Administration Conference (LISA), 1997.
[29] M. Roesch. Snort: Lightweight intrusion detection for networks. In Proc. 13th Systems Administration Conference (LISA), pages 229–238. USENIX Association, November 1999.
[30] R. Sekar and P. Uppuluri. Synthesizing fast intrusion prevention/detection systems from high-level specifications. In Proc. 8th USENIX Security Symposium. USENIX Association, August 1999.
[31] U. Shankar and V. Paxson. Active Mapping: Resisting NIDS Evasion Without Altering Traffic. In Proc. IEEE Symposium on Security and Privacy, 2003.
[32] Steven T. Eckmann. Translating Snort rules to STATL scenarios. In Proc. Recent Advances in Intrusion Detection, October 2001.
[33] tcpdump. http://www.tcpdump.org.
[34] Valgrind. http://developer.kde.org/˜sewardj.
[35] G. Vigna, S. Eckmann, and R. Kemmerer. The STAT Tool Suite. In Proc. 1st DARPA Information Survivability Conference and Exposition, Hilton Head, South Carolina, January 2000. IEEE Computer Society Press.
[36] G. Vigna and R. A. Kemmerer. Netstat: A network-based intrusion detection system. Journal of Computer Security, 7(1):37–71, 1999.
[37] Whisker. http://www.wiretrip.net/rfp.