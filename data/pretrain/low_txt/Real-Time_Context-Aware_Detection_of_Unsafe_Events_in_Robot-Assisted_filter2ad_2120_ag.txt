### Healthcare Services at Home for the Elderly

[56] demonstrated that incorporating the notion of context and typical behavior of services leads to improved detection accuracy over traditional mechanisms in critical service-oriented architectures. [57] provided a framework for context-based detection of network intrusions by integrating protocol context and byte sequences. Our work shares similarities with these approaches by incorporating context to enhance anomaly detection, but it leverages deep learning for real-time context inference and anomaly detection in robotic surgery.

### Threats to Validity

Our solution relies on the accuracy and generalizability of Deep Neural Networks (DNNs) for detecting the operational context and context-specific errors. While DNNs have been successful across many domains, slight perturbations in input data, caused by environmental noise [58] or adversarial attacks [59], can lead to high-confidence misclassifications. However, most proposed adversarial examples target image-based classification systems. Our safety monitoring system is based on kinematics samples, and we use computer vision only for orthogonal labeling of failures. Future work will focus on robustness analysis and design of our ML-based safety monitor against both accidental and malicious perturbations.

Additionally, the performance of supervised learning models heavily depends on accurate labeling of the operational context (surgical gestures) and context-specific anomalies (erroneous gestures). Our labeling of erroneous gestures in the JIGSAWS dataset was based on human annotations of the corresponding videos. We labeled any gesture as erroneous if it contained an anomaly, even if the error did not occur at the beginning of the gesture. Future work will aim to automate the labeling of trajectory data from real surgical tasks, similar to our automated labeling of the Block Transfer task on the RAVEN II robot using video data, for more precise localization of errors.

### Conclusion

We presented an end-to-end safety monitoring system for real-time, context-aware identification of erroneous gestures in robotic surgery. Preliminary results show the promise of our kinematics-only based solution in timely and accurate detection of unsafe events, even when vision data might be unavailable or sub-optimal. Experimental results validate the need for context-aware monitoring and suggest that some surgical gestures have similar error patterns, potentially allowing for better monitoring when considered as a sequence. Our results also indicate the potential for early detection and prevention of these unsafe events, which could be further enhanced with larger training datasets and extended context semantics using vision or other sensing modalities. Future work will focus on generalizing our solution to a wider set of realistic surgical gestures and tasks with a larger number of trials. We also plan to improve the accuracy and timeliness of our safety monitoring system to effectively prevent safety-critical events during surgery.

### References

[1] Intuitive Surgical, Inc., "2017 Annual Report," http://www.annualreports.com/HostedData/AnnualReportArchive/i/NASDAQ_ISRG_2017.pdf.

[2] G.-Z. Yang, J. Cambias, K. Cleary, E. Daimler, J. Drake, P. E. Dupont, N. Hata, P. Kazanzides, S. Martel, R. V. Patel et al., "Medical robotics—regulatory, ethical, and legal considerations for increasing levels of autonomy," Sci. Robot, vol. 2, no. 4, p. 8638, 2017.

[3] H. Alemzadeh, J. Raman, N. Leveson, Z. Kalbarczyk, and R. K. Iyer, "Adverse events in robotic surgery: a retrospective study of 14 years of FDA data," PloS one, vol. 11, no. 4, p. e0151470, 2016.

[4] E. Rajih, C. Tholomier, B. Cormier, V. Samouelian, T. Warkus, M. Liberman, H. Widmer, J.-B. Lattouf, A. M. Alenizi, M. Meskawi et al., "Error reporting from the da Vinci surgical system in robotic surgery: A Canadian multispecialty experience at a single academic centre," Canadian Urological Association Journal, vol. 11, no. 5, p. E197, 2017.

[5] T. Bonaci, J. Herron, T. Yusuf, J. Yan, T. Kohno, and H. J. Chizeck, "To make a robot secure: An experimental analysis of cyber security threats against teleoperated surgical robots," arXiv preprint arXiv:1504.04339, 2015.

[6] H. Alemzadeh, D. Chen, X. Li, T. Kesavadas, Z. T. Kalbarczyk, and R. K. Iyer, "Targeted attacks on teleoperated surgical robots: dynamic model-based detection and mitigation," in Dependable Systems and Networks (DSN), 2016 46th Annual IEEE/IFIP International Conference on. IEEE, 2016, pp. 395–406.

[7] H. Alemzadeh, D. Chen, A. Lewis, Z. Kalbarczyk, J. Raman, N. Leveson, and R. Iyer, "Systems-theoretic safety assessment of robotic telesurgical systems," in International conference on computer safety, reliability, and security. Springer, 2014, pp. 213–227.

[8] T. R. Eubanks, R. H. Clements, D. Pohl, N. Williams, D. C. Schaad, S. Horgan, and C. Pellegrini, "An objective scoring system for laparoscopic cholecystectomy," Journal of the American College of Surgeons, vol. 189, no. 6, pp. 566–574, 1999.

[9] O. Elhage, B. Challacombe, A. Shortland, and P. Dasgupta, "An assessment of the physical impact of complex surgical tasks on surgeon errors and discomfort: a comparison between robot-assisted, laparoscopic, and open approaches," BJU international, vol. 115, no. 2, pp. 274–281, 2015.

[10] P. Joice, G. Hanna, and A. Cuschieri, "Errors enacted during endoscopic surgery - a human reliability analysis," Applied ergonomics, vol. 29, no. 6, pp. 409–414, 1998.

[11] N. Leveson, Engineering a safer world: Systems thinking applied to safety. MIT press, 2011.

[12] B. Hannaford, J. Rosen, D. W. Friedman, H. King, P. Roan, L. Cheng, D. Glozman, J. Ma, S. N. Kosari, and L. White, "Raven-II: an open platform for surgical robotics research," IEEE Transactions on Biomedical Engineering, vol. 60, no. 4, pp. 954–959, 2012.

[13] R. P. Goldberg, M. Hanuschik, H. Hazebrouck, P. Millman, D. Kapoor, J. Zabinski, D. Robinson, D. Weir, and S. J. Brogna, "Ergonomic surgeon control console in robotic surgical systems," Feb. 21 2012, US Patent 8,120,301.

[14] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Y. Ng, "ROS: an open-source robot operating system," in ICRA workshop on open source software, vol. 3, no. 3. Kobe, Japan, 2009, p. 5.

[15] N. Koenig and A. Howard, "Design and use paradigms for Gazebo, an open-source multi-robot simulator," in 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566), vol. 3. IEEE, 2004, pp. 2149–2154.

[16] P. Kazanzides, Z. Chen, A. Deguet, G. S. Fischer, R. H. Taylor, and S. P. DiMaio, "An open-source research kit for the da Vinci® surgical system," in 2014 IEEE international conference on robotics and automation (ICRA). IEEE, 2014, pp. 6434–6439.

[17] E. M. Bonrath, N. J. Dedy, B. Zevin, and T. P. Grantcharov, "Defining technical errors in laparoscopic surgery: a systematic review," Surgical endoscopy, vol. 27, no. 8, pp. 2678–2691, 2013.

[18] E. Bonrath, B. Zevin, N. Dedy, and T. Grantcharov, "Error rating tool to identify and analyze technical errors and events in laparoscopic surgery," British Journal of Surgery, vol. 100, no. 8, pp. 1080–1088, 2013.

[19] M. S. Yasar, D. Evans, and H. Alemzadeh, "Context-aware monitoring in robotic surgery," in 2019 International Symposium on Medical Robotics (ISMR). IEEE, 2019, pp. 1–7.

[20] D. Neumuth, F. Loebe, H. Herre, and T. Neumuth, "Modeling surgical processes: A four-level translational approach," Artificial intelligence in medicine, vol. 51, no. 3, pp. 147–161, 2011.

[21] J. Rosen, J. D. Brown, L. Chang, M. N. Sinanan, and B. Hannaford, "Generalized approach for modeling minimally invasive surgery as a stochastic process using a discrete Markov model," IEEE Transactions on Biomedical engineering, vol. 53, no. 3, pp. 399–413, 2006.

[22] J. H. Peters, G. M. Fried, L. L. Swanstrom, N. J. Soper, L. F. Sillin, B. Schirmer, K. Hoffman, S. F. Committee et al., "Development and validation of a comprehensive program of education and assessment of the basic fundamentals of laparoscopic surgery," Surgery, vol. 135, no. 1, pp. 21–27, 2004.

[23] Y. Gao, S. S. Vedula, C. E. Reiley, N. Ahmidi, B. Varadarajan, H. C. Lin, L. Tao, L. Zappella, B. Béjar, D. D. Yuh et al., "JHU-ISIS Gesture and Skill Assessment Working Set (JIGSAWS): A surgical activity dataset for human motion modeling," in MICCAI Workshop: M2CAI, vol. 3, 2014, p. 3.

[24] C. E. Reiley and G. D. Hager, "Decomposition of robotic surgical tasks: an analysis of subtasks and their correlation to skill," in M2CAI workshop. MICCAI, London, 2009.

[25] A. Zia, A. Hung, I. Essa, and A. Jarc, "Surgical activity recognition in robot-assisted radical prostatectomy using deep learning," in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2018, pp. 273–280.

[26] A. J. Hung, J. Chen, Z. Che, T. Nilanon, A. Jarc, M. Titus, P. J. Oh, I. S. Gill, and Y. Liu, "Utilizing machine learning and automated performance metrics to evaluate robot-assisted radical prostatectomy performance and predict outcomes," Journal of endourology, vol. 32, no. 5, pp. 438–444, 2018.

[27] A. Zia, L. Guo, L. Zhou, I. Essa, and A. Jarc, "Novel evaluation of surgical activity recognition models using task-based efficiency metrics," International journal of computer assisted radiology and surgery, pp. 1–9, 2019.

[28] N. Ahmidi, P. Poddar, J. D. Jones, S. S. Vedula, L. Ishii, G. D. Hager, and M. Ishii, "Automated objective surgical skill assessment in the operating room from unstructured tool motion in septoplasty," International journal of computer assisted radiology and surgery, vol. 10, no. 6, pp. 981–991, 2015.

[29] M. J. Fard, S. Ameri, R. Darin Ellis, R. B. Chinnam, A. K. Pandya, and M. D. Klein, "Automated robot-assisted surgical skill evaluation: Predictive analytics approach," The International Journal of Medical Robotics and Computer Assisted Surgery, vol. 14, no. 1, p. e1850, 2018.

[30] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, "Evaluating surgical skills from kinematic data using convolutional neural networks," in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2018, pp. 214–221.

[31] K. Moorthy, Y. Munz, A. Dosis, F. Bello, A. Chang, and A. Darzi, "Bimodal assessment of laparoscopic suturing skills," Surgical Endoscopy and Other Interventional Techniques, vol. 18, no. 11, pp. 1608–1612, 2004.

[32] M. R. Kwaan, D. M. Studdert, M. J. Zinner, and A. A. Gawande, "Incidence, patterns, and prevention of wrong-site surgery," Archives of surgery, vol. 141, no. 4, pp. 353–358, 2006.

[33] Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," nature, vol. 521, no. 7553, p. 436, 2015.

[34] S. Krishnan, A. Garg, S. Patil, C. Lea, G. Hager, P. Abbeel, and K. Goldberg, "Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning," in Robotics Research. Springer, 2018, pp. 91–110.

[35] J. Lin, "Divergence measures based on the Shannon entropy," IEEE Transactions on Information theory, vol. 37, no. 1, pp. 145–151, 1991.

[36] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[37] M. Hermans and B. Schrauwen, "Training and analysing deep recurrent neural networks," in Advances in neural information processing systems, 2013, pp. 190–198.

[38] V. Nair and G. E. Hinton, "Rectified linear units improve restricted Boltzmann machines," in Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 807–814.

[39] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.

[40] F. Chollet et al., "Keras," 2015.

[41] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al., "TensorFlow: A system for large-scale machine learning," in 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 2016, pp. 265–283.

[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al., "Scikit-learn: Machine learning in Python," Journal of machine learning research, vol. 12, no. Oct, pp. 2825–2830, 2011.

[43] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, "Image quality assessment: from error visibility to structural similarity," IEEE transactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.

[44] C. Lea, G. D. Hager, and R. Vidal, "An improved model for segmentation and recognition of fine-grained activities with application to surgical training tasks," in 2015 IEEE winter conference on applications of computer vision. IEEE, 2015, pp. 1123–1129.

[45] S. Sefati, N. J. Cowan, and R. Vidal, "Learning shared, discriminative dictionaries for surgical gesture segmentation and classification," in MICCAI Workshop: M2CAI, vol. 4, 2015.

[46] M. Y. Jung, R. H. Taylor, and P. Kazanzides, "Safety design view: A conceptual framework for systematic understanding of safety features of medical robot systems," in 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 1883–1888.

[47] C. He, N. Patel, I. Iordachita, and M. Kobilarov, "Enabling technology for safe robot-assisted retinal surgery: Early warning for unsafe scleral force," in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 3889–3894.

[48] K. Coble, W. Wang, B. Chu, and Z. Li, "Secure software attestation for military telesurgical robot systems," in 2010-MILCOM 2010 MILITARY COMMUNICATIONS CONFERENCE. IEEE, 2010, pp. 965–970.

[49] M. E. Tozal, Y. Wang, E. Al-Shaer, K. Sarac, B. Thuraisingham, and B.-T. Chu, "On secure and resilient telesurgery communications over unreliable networks," in 2011 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). IEEE, 2011, pp. 714–719.

[50] G. S. Lee and B. Thuraisingham, "Cyberphysical systems security applied to telesurgical robotics," Computer Standards & Interfaces, vol. 34, no. 1, pp. 225–229, 2012.

[51] H. C. Lin, I. Shafran, D. Yuh, and G. D. Hager, "Towards automatic skill evaluation: Detection and segmentation of robot-assisted surgical motions," Computer Aided Surgery, vol. 11, no. 5, pp. 220–230, 2006.

[52] R. DiPietro, C. Lea, A. Malpani, N. Ahmidi, S. S. Vedula, G. I. Lee, M. R. Lee, and G. D. Hager, "Recognizing surgical activities with recurrent neural networks," in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2016, pp. 551–558.

[53] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, "Rethinking the inception architecture for computer vision," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826.

[54] D. Katić, A.-L. Wekerle, J. Görtler, P. Spengler, S. Bodenstedt, S. Röhl, S. Suwelack, H. G. Kenngott, M. Wagner, B. P. Müller-Stich et al., "Context-aware augmented reality in laparoscopic surgery," Computerized Medical Imaging and Graphics, vol. 37, no. 2, pp. 174–182, 2013.

[55] B. Yuan and J. Herbert, "Context-aware hybrid reasoning framework for pervasive healthcare," Personal and ubiquitous computing, vol. 18, no. 4, pp. 865–881, 2014.

[56] T. Zoppi, A. Ceccarelli, and A. Bondavalli, "Context-awareness to improve anomaly detection in dynamic service oriented architectures," in International Conference on Computer Safety, Reliability, and Security. Springer, 2016, pp. 145–158.

[57] P. Duessel, C. Gehl, U. Flegel, S. Dietrich, and M. Meier, "Detecting zero-day attacks using context-aware anomaly detection at the application-layer," International Journal of Information Security, vol. 16, no. 5, pp. 475–490, 2017.

[58] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, "Improving the robustness of deep neural networks via stability training," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 4480–4488.

[59] I. J. Goodfellow, J. Shlens, and C. Szegedy, "Explaining and harnessing adversarial examples," arXiv preprint arXiv:1412.6572, 2014.

Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 11:23:52 UTC from IEEE Xplore. Restrictions apply.