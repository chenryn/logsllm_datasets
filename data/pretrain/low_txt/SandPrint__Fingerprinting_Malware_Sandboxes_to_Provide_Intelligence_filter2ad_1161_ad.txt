### Sandboxes and Classifier Development

By gathering information from sandboxes, we can build a classifier that accurately distinguishes between a user's personal computer (PC) and a security appliance. With insider knowledge of these appliances, an advanced attacker could refine the classifier to evade detection stealthily.

### 7. Discussion and Limitations

This section discusses the ethical implications and potential limitations of our work. As part of our ethical considerations, we also describe the responsible disclosure process through which we informed sandbox and appliance operators about our findings.

#### 7.1 Ethical Considerations

Our research may appear to be offensive as it reveals fingerprints of malware sandboxes that adversaries can use to evade them. However, the information we present can be obtained by anyone using our simple fingerprinting method. We consider this information to be public knowledge. To maintain confidentiality, we only present data in aggregated form and do not disclose any internal details of specific sandboxes.

Using our insights, sandbox operators can aim to implement more stealthy analysis systems. For example, we have shown that periodically updating features inherent to the sandbox snapshot can improve security. While it will always be possible to find artifacts that identify individual sandboxes, it is significantly harder to build a classifier that works for all sandboxes, especially if more people randomize their characteristics. Our research highlights which features are particularly characteristic of sandboxes, providing operators with guidance on where to improve the stealthiness of their systems.

#### 7.2 Responsible Disclosure

Organizations developing sandboxes and/or appliances are directly affected by our research results. Therefore, we considered them the primary target of our responsible disclosure process. We contacted these organizations 90 days before the publication date, detailing the proposed attack and providing suggestions on how to protect against potential adversaries in the future. We used direct contacts whenever possible and available. If we did not receive a response after two weeks, we attempted to contact the organization again, using alternative communication channels if necessary. If there was no response after four weeks, we contacted the national Computer Emergency Response Teams (CERTs) in the same country as the affected organization to notify them via a trusted intermediary.

We provided each organization with an executive summary of our research results and a full description of our methodology (i.e., a pre-print version of this paper). We highlighted the implications of our work for the future operations of the sandbox and/or appliance. We also specified our contact details, including physical address, phone number, and the email address of a representative for the research activities. We allowed the organizations to download the latest version of SandPrint and its source code, which is helpful for building protection mechanisms against similar sandbox-evasive programs. We removed all organizations' names when referring to individual sandboxes/services.

#### 7.3 Isolated Sandboxes

Most sandboxes allowed the program under analysis to communicate over the Internet, whereas nine services and all three appliances did not. To some extent, we could extract features of isolated sandboxes (the appliances) by encoding the features into events of the analysis report. However, this requires access to the isolated sandboxes, which may be difficult for an attacker to obtain. Note that our sandbox classification did not use features that depend on the network configuration. In principle, our classification results should generalize to non-connected sandboxes. Although we cannot rule out the possibility that there are non-connected sandboxes for which our classifier would perform poorly, the successful detection of appliances supports our claim.

Due to our assumption of Internet-connected sandboxes, the number of in-the-wild sandboxes is likely higher than our findings suggest. However, our analyses are based on a statistically significant set of sandboxes, including those of the most popular analysis services.

### 8. Related Work

#### Evasion Techniques

With the increasing popularity of sandboxes, malware authors are seeking ways to evade sandbox analysis. Egele et al. provide an overview of sandbox implementations [24]. Most sandboxes use virtual machine (VM) technology or CPU emulators, which facilitate the parallel analysis of multiple samples. Studies show how to distinguish between a real machine and a virtual environment. RedPill [47] determines whether it is executed on VMware using the `sidt` instruction. Many other detection methods have been developed for VMware [29, 43], QEMU [22, 29, 40, 43], and BOCHS [34, 40]. There are also detection methods for emulation-based Android sandboxes [27, 42, 54].

The fundamental difference between our approach and these techniques is that we do not aim to detect virtualization or emulation, as VMs and sandboxes are not equivalent. Additionally, as shown with Pafish, most of these checks are not stealthy, whereas our approach even managed to detect security appliances without triggering alerts. It is also likely that our approach could work for bare-metal sandboxes, which conceptually share many sandbox-inherent features with traditional sandboxes, except for the absence of virtualization and emulation.

The work closest to our approach is by Maier et al. [33]. They gathered several features about Android sandboxes and showed that Android malware can bypass existing sandboxes using these fingerprints. However, they did not perform automated clustering and classification of sandbox-inherent features, nor did they test their approach against security appliances. Furthermore, their feature selection is very specific to smartphones, making some features, such as "the device needs at least n saved WiFi networks" or "the device must have a paired Bluetooth device," less applicable in our non-mobile context. We also use some similar features like special hardware artifacts or system uptime. Regarding sandboxes for Windows malware, Yoshioka et al. [56] clustered and detected sandboxes by their external IP addresses. Inspired by these works, we performed a more detailed study, collecting 25 features and identifying 76 sandboxes using an unsupervised machine learning technique.

#### Transparent Sandboxes

Seeing the threat of VM evasion, researchers started to explore transparent sandboxes that are stealthy against detection. Vasudevan et al. proposed Cobra [53], the first analysis system countering anti-analysis techniques. Dinaburg et al. proposed Ether [23], a transparent sandbox using hardware virtualization extensions such as Intel VT. These systems focus on concealing the existence of analysis mechanisms from malware. Pek et al. introduced a timing-based detection mechanism to detect Ether [41]. As we have shown, the majority of sandboxes, including VT-based sandboxes, are susceptible to evasion due to sandbox-inherent features.

Kirat et al. proposed using actual hardware to analyze malware [9, 31]. The proposed system, called BareBox, is based on a fast and rebootless system restore technique. Since the system executes malware on real hardware, it is not vulnerable to VM/emulation-based detection attacks. However, as it is snapshot-based, it falls for the methods described in Section 5.

### 9. Conclusion

Our real-world malware sandbox investigations have shown that it is quite straightforward to fingerprint malware sandboxes. We identified 76 sandboxes by uploading a measurement binary to 20 services, all of which can be rather trivially detected and evaded based on sandbox-inherent characteristics. Our findings also suggest that detecting and evading malware appliances is similarly possible. This calls into question how we can protect against the threat of sandbox evasion in the future and should serve as a warning for sandbox operators to inform them about threats that may already be silently misused by malware.

### Acknowledgements

We would like to thank the anonymous reviewers for their valuable comments. Special thanks go to our shepherd Michael Bailey, who supported us during the process of finalizing the paper. This work was supported by the MEXT Program for Promoting Reform of National Universities and by the German Federal Ministry of Education and Research (BMBF) through funding for the Center for IT-Security, Privacy and Accountability (CISPA) and for the BMBF project 13N13250.

### Appendix

See Figure 3.

### References

1. Amnpardaz Sandbox - File Analyzer. http://jevereg.amnpardaz.com/
2. Anubis: Malware Analysis for Unknown Binaries. https://anubis.iseclab.org/
3. Bkav - Scan virus online. http://quetvirus.vn/default.aspx?lang=en
4. bochs: The Open Source IA-32 Emulation Project. http://bochs.sourceforge.net
5. Dr. Web Online Check. http://online.drweb.com/?lng=en
6. FortiGuard Center. Online Virus Scanner. http://www.fortiguard.com/virusscanner
7. Garyâ€™s Hood. Online Virus Scanner. http://www.garyshood.com/virus/
8. Malwr - Malware Analysis by Cuckoo Sandbox. https://malwr.com/
9. NVMTrace: Proof-of-concept Automated Baremetal Malware Analysis Framework. https://code.google.com/p/nvmtrace/
10. Oracle VM VirtualBox. https://www.virtualbox.org
11. #totalhash. https://totalhash.cymru.com/upload/
12. http://www.Vicheck.ca
13. Virusblokada. http://anti-virus.by/en/index.shtml
14. VirusTotal - Free Online Virus, Malware and URL Scanner. https://www.virustotal.com/en/
15. VMware. http://www.vmware.com/
16. Bayer, U., Milani Comparetti, P., Hlauschek, C., Kruegel, C., Kirda, E.: Scalable, behavior-based malware clustering. In: Network and Distributed System Security Symposium (NDSS) (2009)
17. Bellard, F.: QEMU, a fast and portable dynamic translator. In: Proceedings of the Annual Conference on USENIX Annual Technical Conference, ATEC 2005 (2005)
18. Brengel, M., Backes, M., Rossow, C.: Detecting hardware-assisted virtualization. In: Caballero, J., Zurutuza, U., RodrÃ­guez, R.J. (eds.) DIMVA 2016. LNCS, vol. 9721, pp. 207â€“227. Springer, Heidelberg (2016). doi:10.1007/978-3-319-40667-1_11
19. Caballero, J., Grier, C., Kreibich, C., Paxson, V.: Measuring pay-per-install: the commoditization of malware distribution. In: USENIX Security (2011)
20. Comodo. Comodo Instant Malware Analysis. http://camas.comodo.com/
21. Cristianini, N., Shawe-Taylor, J.: An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, Cambridge (2000)
22. DEXLabs. Detecting Android Sandboxes (2012). http://www.dexlabs.org/blog/btdetect
23. Dinaburg, A., Royal, P., Sharif, M., Ether, L.W.: Malware analysis via hardware virtualization extensions. In: Proceedings of the 15th ACM Conference on Computer and Communications Security, CCS 2008 (2008)
24. Egele, M., Scholte, T., Kirda, E., Kruegel, C.: A survey on automated dynamic malware-analysis techniques and tools. ACM Comput. Surv. 44, 2 (2008)
25. F-Secure. Sample Analysis System. https://analysis.f-secure.com/portal/login.html
26. Freiling, F.C., Holz, T., Wicherski, G.: Botnet tracking: exploring a root-cause methodology to prevent distributed denial-of-service attacks. In: di Vimercati, S.C., Syverson, P.F., Gollmann, D. (eds.) ESORICS 2005. LNCS, vol. 3679, pp. 319â€“335. Springer, Heidelberg (2005)
27. Jing, Y., Zhao, Z., Ahn, G.-J., Hu, H.: Morpheus: automatically generating heuristics to detect android emulators. In: Proceedings of the 30th Annual Computer Security Applications Conference, ACSAC 2014 (2014)
28. Jotti. Jottiâ€™s Malware Scan. http://virusscan.jotti.org/en
29. Jung, P.: Bypassing Sandboxes for Fun. https://www.botconf.eu/wp-content/uploads/2014/12/2014-2.7-Bypassing-Sandboxes-for-Fun.pdf
30. Kirat, D., Vigna, G., Kruegel, C.: Barecloud: bare-metal analysis-based evasive malware detection. In: Proceedings of the 23rd USENIX Conference on Security Symposium, SEC 2014 (2014)
31. Kirati, D., Vigna, G., Kruegel, C.: BareBox: efficient malware analysis on bare-metal. In: Proceedings of the 27th Annual Computer Security Applications Conference, ACSAC 2011 (2011)
32. Lanzi, A., Balzarotti, D., Kruegel, C., Christodorescu, M., Kirda, E.: AccessMiner: using system-centric models for malware protection. In: Proceedings of the 17th ACM Conference on Computer and Communications Security, CCS 2010 (2010)
33. Maier, D., MÃ¼ller, T., Protsenko, M.: Divide-and-Conquer: why android malware cannot be stopped. In: Proceedings of the 2014 Ninth International Conference on Availability, Reliability and Security, ARES 2014 (2014)
34. Martignoni, L., Paleari, R., Roglia, G.F., Bruschi, D.: Testing CPU emulators. In: Proceedings of the Eighteenth International Symposium on Software Testing and Analysis, ISSTA 2009 (2009)
35. Microsoft. Submit a sample - Microsoft Malware Protection Center. https://www.microsoft.com/security/portal/submission/submit.aspx
36. Neugschwandtner, M., Comparetti, P.M., Platzer, C.: Detecting malwareâ€™s failover C&C strategies with squeeze. In: Proceedings of the 27th Annual Computer Security Applications Conference, ACSAC 2011 (2011)
37. Neuner, S., van der Veen, V., Lindorfer, M., Huber, M., Merzdovnik, G., Mulazzani, M., Weippl, E.: Enter Sandbox: Android Sandbox Comparison (2015). http://arxiv.org/ftp/arxiv/papers/1410/1410.7749.pdf
38. OPSWAT. Metascan Online: Free File Scanning with Multiple Antivirus Engines. https://www.metascan-online.com/#!/scan-file
39. Pa, Y.M.P., Suzuki, S., Yoshioka, K., Matsumoto, T., Kasama, T., Rossow, C.: IoT-POT: analysing the rise of IoT compromises. In: Proceedings of the 9th USENIX Workshop on Offensive Technologies, WOOT (2015)
40. Paleari, R., Martignoni, L., Roglia, G.F., Bruschi, D.A.: Fistful of red-pills: how to automatically generate procedures to detect CPU emulators. In: Proceedings of the 3rd USENIX Conference on Offensive Technologies, WOOT 2009 (2009)
41. PÃ©k, G., BencsÃ¡th, B., ButtyÃ¡n, L.: nEther: in-guest detection of out-of-the-guest malware analyzers. In: Proceedings of the Fourth European Workshop on System Security, EUROSEC 2011 (2011)
42. Petsas, T., Voyatzis, G., Athanasopoulos, E., Polychronakis, M., Ioannidis, S.: Rage against the virtual machine: hindering dynamic analysis of android malware. In: Proceedings of the Seventh European Workshop on System Security, EuroSec 2014 (2014)
43. Raffetseder, T., Kruegel, C., Kirda, E.: Detecting system emulators. In: Garay, J.A., Lenstra, A.K., Mambo, M., Peralta, R. (eds.) ISC 2007. LNCS, vol. 4779, pp. 1â€“18. Springer, Heidelberg (2007)
44. Rieck, K., Schwenk, G., Limmer, T., Holz, T., Laskov, P.: Botzilla: detecting the phoning home of malicious software. In: Proceedings of the 2010 ACM Symposium on Applied Computing (ACSAC 2010) (2010)
45. Rieck, K., Trinius, P., Willems, C., Holz, T.: Automatic analysis of malware behavior using machine learning. J. Comput. Secur. 19(4), 639â€“668 (2009)
46. Rossow, C., Dietrich, C., Bos, H.: Large-scale analysis of malware downloaders. In: Flegel, U., Markatos, E., Robertson, W. (eds.) DIMVA 2012. LNCS, vol. 7591, pp. 42â€“61. Springer, Heidelberg (2013)
47. Rutkowska, J.: Red Pill... Or How To Detect VMM Using (Almost) One CPU Instruction (2004). http://www.securiteam.com/securityreviews/6Z00H20BQS.html
48. Payload Security: Free Automated Malware Analysis Service. https://www.hybrid-analysis.com/
49. Payload Security: Blog (2015). http://www.pandasecurity.com/mediacenter/press-releases/pandalabs-neutralized-75-million-new-malware-samples-2014-twice-many-2013/
50. ThreatTrack Security: Free Online Malware Analysis. http://www.threattracksecurity.com/resources/sandbox-malware-analysis.aspx
51. Symantec. Internet Security Threat Report 04/2015 (2015). http://www.symantec.com/de/de/security_response/publications/threatreport.jsp
52. ThreatExpert. http://www.threatexpert.com/submit.aspx
53. Vasudevan, A., Yerraballi, R.: Cobra: fine-grained malware analysis using stealth localized-executions. In: Proceedings of the 2006 IEEE Symposium on Security and Privacy, S&P 2006 (2006)
54. Vidas, T., Christin, N.: Evading android runtime analysis via sandbox detection. In: Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security, ASIA CCS 2014 (2014)
55. VirSCAN.org. Free Multi-Engine Online Virus Scanner. http://www.virscan.org/
56. Yoshioka, K., Hosobuchi, Y., Orii, T., Matsumoto, T.: Your sandbox is blinded: impact of decoy injection to public malware analysis systems. J. Inf. Process. 52, 3 (2011)