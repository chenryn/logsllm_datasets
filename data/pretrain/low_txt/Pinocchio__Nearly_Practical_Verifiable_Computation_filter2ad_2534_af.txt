### Performance Metrics and Reductions

- 12.5ms
- 319.1µs
- 12.2ms
- 55.9MB
- 640B
- 288B

**Reduction:**
- 61%
- -2%
- 83%
- 65%
- 59%
- 64%
- 70%
- 60%
- 18%
- 30%
- 18%
- 47%
- 0%
- 18%

### Figure 9: Improving GGPR [30]

**Performance for the multivariate polynomial application:**
Pinocchio’s high-level operations are 2.6x, 2.8x, and 1.2x faster than the original. (N = 10, σ ≤ 3%).

### Impact of Our Optimizations

In Figure 9, we break down Pinocchio’s protocol overhead for the large multivariate polynomial example application to better identify the major bottlenecks. For comparison, we also measure the performance of our implementation of GGPR’s scheme [30], using the same underlying cryptographic and polynomial libraries.

The results indicate that our protocol improvements had a significant impact. KeyGen and Compute are more than twice as fast, and even verification is 18% faster. Of Pinocchio’s remaining KeyGen overhead, the majority comes from encoding the evaluations of the QAP’s polynomials in the generator’s exponent. For Compute, the multi-exponentiation required to compute the QAP’s polynomials in the exponent still dominates, but the overhead of solving for h(x) is non-trivial as well.

### Size Reductions

Pinocchio also drastically reduces the size of the evaluation key and even manages to reduce the size of GGPR’s already svelte 9-element proof to 8 elements.

### 5.5 QSPs versus QAPs

To confirm our theoretical prediction that QAPs would outperform QSPs (§3.2), we compared the two on our SHA-1 application, which performs numerous bitwise operations, and hence should favor QSPs. The resulting QSP’s size was 38.6× that of the QAP, and the degree was 55.5× as large. Not surprisingly, the QSP’s KeyGen took 35.2× and Compute took 55.4× as long as those of the QAP; the verification times were comparable.

### 6 Related Work

Much of the prior work in this area focuses on verifying specific functions via auditing or special properties of the functions [2–6]. Other systems rely on replication, assuming failures are uncorrelated [1, 7, 8, 55]. A large body of work verifies computation by assuming the worker employs secure hardware [9–15].

While the theory and cryptography community has long studied the problem of general-purpose proof systems [16–23], until recently, this work was largely regarded as highly impractical, to the point where no one bothered to implement it. Much of this work [16–20, 56] relied on Probabilistically Checkable Proofs (PCPs), which offer impressive theoretical performance but can take trillions of years to verify in practice [27]. Other work [22, 23] relies on fully-homomorphic encryption (FHE) [24], which, despite continuing advances [53], remains highly impractical.

Recently, security and systems researchers have started to develop techniques to make theoretical cryptographic protocols practical. Secure multiparty computation, for example, has seen tremendous progress [57–59]. However, since the primary focus is on secrecy, not outsourcing, both parties typically perform work equal to evaluating the function.

With regard to implementing verified computation, in the last year, two parallel efforts have emerged, both using optimized PCP-based arguments. One effort [25, 26] builds on the PCP-based arguments of Goldwasser et al. [20] (GKR). They target a streaming setting where the client cannot store all of the data it wishes to compute over; the system currently requires the function computed to be highly parallelizable. On the plus side, it does not require cryptography and is secure against computationally unbounded adversaries.

Setty et al. produced a second line of PCP-based systems called Pepper [27] and Ginger [28]. They build on a particular type of PCP called a linear PCP [52], in which the proof can be represented as a linear function. This allows the worker to use a linearly-homomorphic encryption scheme to create a commitment to its proof while relying only on standard cryptographic assumptions. Through a combination of theoretical and systems-level improvements, this work made tremendous progress in making PCP-based systems practical. Indeed, for applications that can tolerate large batch sizes, the amortized costs of verification can be quite low.

A few downsides remain, however. Because the work builds on the Hadamard PCP [56], the setup time, network overhead, and the prover’s work are quadratic in the size of the original computation, unless the protocol is hand-tailored. To achieve efficiency, the verifier must outsource computations in batches, which means it cannot verify the results until the full batch returns. The scheme is designated verifier, meaning that third parties cannot verify the results of outsourced computations without sharing the client’s secret key, and hence opening the possibility for fraud. The scheme also does not support zero-knowledge proofs.

Concurrent work [60] also builds on the quadratic programs of Gennaro et al. [30]. They observe that QAPs can be viewed as linear PCPs and hence can fit into Ginger’s cryptographic framework [28]. Their work shows worker computation improvements similar to those of Pinocchio. Additional concurrent work [61] adapts GKR’s protocol to the batching model and develops a compiler that chooses amongst three PCP-based backends. Both systems retain PCPs and Ginger’s cryptographic protocol, so they rely on simpler cryptographic assumptions than Pinocchio, but they must still batch computations to obtain an efficient verifier. They also remain designated verifier and do not support zero-knowledge proofs.

Previous systems either did not offer a compiler [25–27], or compiled from a subset of an academic language, SFDL [28, 60]. In contrast, we compile from a subset of C, which should ease the development burden for verifying computation.

Several systems provide compilers for zero-knowledge (ZK) proofs [62–64]. Both the systems of Almeida et al. [62] and Meiklejohn et al. [63] adopt an approach based on Σ-protocols [65]. The former provides functionality for proving knowledge in arbitrary groups, AND and OR compositions, and linear relations. The latter focuses on functionalities for cryptographic protocols, e.g., e-cash, blind signatures, or verifiable encryption. The compiler of Backes et al. [64] uses Groth-Sahai ZK proofs [66] and handles logical formulas. Rial and Danezis [32] propose a system for privacy-preserving smart metering in which clients use a ZK protocol to prove correctness of the billing computation they perform on meter readings. In general, these systems are likely to exhibit better performance than Pinocchio for their particular subset of functionality, but they do not possess the same level of efficient generality.

### 7 Conclusion and Future Work

We have presented Pinocchio, a system for public verifiable computing. Pinocchio uses quadratic programs, a new method for encoding computation, combined with a highly efficient cryptographic protocol to achieve both asymptotic and concrete efficiency. Pinocchio produces 288-byte proofs, regardless of the size of the computation, and the proofs can be verified rapidly, typically in tens of milliseconds, beating native execution in several cases. This represents five to seven orders of magnitude performance improvement over prior work. The worker also produces the proof 19-60× faster.

Pinocchio even slashes the cost of its underlying protocol, cutting the cost of both key and proof generation by more than 60%. The end result is a natural cryptographic protocol for efficiently signing computations. Combined with a compiler for real C programs, Pinocchio brings verifiable computation much closer to practicality.

Nonetheless, gaps still remain. We hope that additional theoretic improvements, combined with efforts to expand our toolchain, e.g., to support floating point or parallel execution (via standard techniques [25, 28, 43]), will continue to advance us towards truly practical verifiable computing.

### Acknowledgements

The authors gratefully thank: Peter Montgomery, Michael Naehrig, and Patrick Pierola for assisting us with the cryptographic library used by Pinocchio; Chris Hawblitzel for his sage guidance on compiler development; Rosario Gennaro for valuable discussions; and the anonymous reviewers for their helpful comments. Mariana Raykova was supported by NSF Grant No. 1017660.

### References

[1] D. P. Anderson, J. Cobb, E. Korpela, M. Lebofsky, and D. Werthimer, “SETI@Home: An experiment in public-resource computing,” Communications of the ACM, vol. 45, no. 11, 2002.
[2] F. Monrose, P. Wyckoff, and A. Rubin, “Distributed execution with remote audit,” in Proc. of ISOC NDSS, 1999.
[3] P. Golle and S. G. Stubblebine, “Secure distributed computing in a commercial environment,” in Proc. of Financial Cryptography, 2002.
[4] W. Du and M. T. Goodrich, “Searching for high-value rare events with uncheatable grid computing,” in ACNS, 2005.
[5] P. Golle and I. Mironov, “Uncheatable distributed computations,” in Proc. of CT-RSA, 2001.
[6] R. Sion, “Query execution assurance for outsourced databases,” in The Very Large Databases Conference (VLDB), 2005.
[7] M. Castro and B. Liskov, “Practical Byzantine fault tolerance and proactive recovery,” ACM Trans. on Comp. Sys., vol. 20, no. 4, 2002.
[8] B. Carbunar and R. Sion, “Uncheatable reputation for distributed computation markets,” in Financial Cryptography, 2006.
[9] R. Sailer, X. Zhang, T. Jaeger, and L. van Doorn, “Design and implementation of a TCG-based integrity measurement architecture,” in Proc. of the USENIX Security, 2004.
[10] L. Chen, R. Landfermann, H. Löh, M. Rohe, A.-R. Sadeghi, and C. Stüble, “A protocol for property-based attestation,” in Proc. of the ACM Workshop on Scalable Trusted Computing (STC), 2006.
[11] B. Parno, J. M. McCune, and A. Perrig, Bootstrapping Trust in Modern Computers. Springer, 2011.
[12] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. VanDoorn, and P. Khosla, “Pioneer: Verifying integrity and guaranteeing execution of code on legacy platforms,” in Proc. of the ACM SOSP, 2005.
[13] R. B. Lee, P. Kwan, J. P. McGregor, J. Dwoskin, and Z. Wang, “Architecture for protecting critical secrets in microprocessors,” in Proc. of the International Symposium on Computer Architecture (ISCA), 2005.
[14] D. Lie, C. A. Thekkath, M. Mitchell, P. Lincoln, D. Boneh, J. C. Mitchell, and M. Horowitz, “Architectural support for copy and tamper resistant software,” in Proc. of the ACM ASPLOS, 2000.
[15] A.-R. Sadeghi, T. Schneider, and M. Winandy, “Token-based cloud computing: secure outsourcing of data and arbitrary computations with lower latency,” in TRUST, 2010.
[16] S. Goldwasser, S. Micali, and C. Rackoff, “The knowledge complexity of interactive proof systems,” SIAM J. Comput., vol. 18, no. 1, 1989.
[17] S. Arora and S. Safra, “Probabilistic checking of proofs: A new characterization of NP,” J. ACM, vol. 45, no. 1, pp. 70–122, 1998.
[18] J. Kilian, “A note on efficient zero-knowledge proofs and arguments (extended abstract),” in STOC, 1992.
[19] S. Micali, “Computationally sound proofs,” SIAM J. Comput., vol. 30, no. 4, pp. 1253–1298, 2000. Extended abstract in FOCS ’94.
[20] S. Goldwasser, Y. T. Kalai, and G. N. Rothblum, “Delegating computation: Interactive proofs for muggles,” in STOC, 2008.
[21] J. Groth, “Short pairing-based non-interactive zero-knowledge arguments,” in ASIACRYPT, 2010.
[22] R. Gennaro, C. Gentry, and B. Parno, “Non-interactive verifiable computing: Outsourcing computation to untrusted workers,” 2010.
[23] K.-M. Chung, Y. T. Kalai, and S. P. Vadhan, “Improved delegation of computation using fully homomorphic encryption,” in CRYPTO, 2010.
[24] C. Gentry, A fully homomorphic encryption scheme. PhD thesis, Stanford University, 2009. crypto.stanford.edu/craig.
[25] J. Thaler, M. Roberts, M. Mitzenmacher, and H. Pfister, “Verifiable computation with massively parallel interactive proofs,” in USENIX HotCloud Workshop, 2012.
[26] G. Cormode, M. Mitzenmacher, and J. Thaler, “Practical verified computation with streaming interactive proofs,” in ITCS, 2012.
[27] S. Setty, R. McPherson, A. J. Blumberg, and M. Walfish, “Making argument systems for outsourced computation practical (sometimes),” in Proceedings of the ISOC NDSS, 2012.
[28] S. Setty, V. Vu, N. Panpalia, B. Braun, A. J. Blumberg, and M. Walfish, “Taking proof-based verified computation a few steps closer to practicality,” in Proc. of USENIX Security, 2012.
[29] B. Parno, M. Raykova, and V. Vaikuntanathan, “How to delegate and verify in public: Verifiable computation from attribute-based encryption,” in IACR Theory of Cryptography Conference (TCC), 2012.
[30] R. Gennaro, C. Gentry, B. Parno, and M. Raykova, “Quadratic span programs and succinct NIZKs without PCPs,” in EUROCRYPT, 2013. Originally published as Cryptology ePrint Archive, Report 2012/215.
[31] M. Blum, A. D. Santis, S. Micali, and G. Persiano, “Noninteractive zero-knowledge,” SIAM J. on Computing, vol. 20, no. 6, 1991.
[32] A. Rial and G. Danezis, “Privacy-preserving smart metering,” in Proc. of the ACM WPES, 2011.
[33] N. Pippenger and M. J. Fischer, “Relations among complexity measures,” J. ACM, vol. 26, no. 2, 1979.
[34] D. Boneh and M. Franklin, “Identity-based encryption from the Weil pairing,” Proceedings of IACR CRYPTO, 2001.
[35] D. Boneh, X. Boyen, and E.-J. Goh, “Hierarchical identity based encryption with constant size ciphertext,” in EUROCRYPT, 2005.
[36] D. Boneh, C. Gentry, and B. Waters, “Collusion resistant broadcast encryption with short ciphertexts and private keys,” in CRYPTO, 2005.
[37] M. Naor, “On cryptographic assumptions and challenges,” in Proceedings of IACR CRYPTO, 2003.
[38] M. Bellare and A. Palacio, “The knowledge-of-exponent assumptions and 3-round zero-knowledge protocols,” in CRYPTO, 2004.
[39] C. Gentry and D. Wichs, “Separating succinct non-interactive arguments from all falsifiable assumptions,” in STOC, 2011.
[40] “Verifiable computation: Pinocchio.” http://research.microsoft.com/verifcomp/, Mar. 2013.
[41] I. Damgård, “Towards practical public key systems secure against chosen ciphertext attacks,” in IACR CRYPTO, 1991.
[42] D. A. Wheeler, “SLOCCount.” http://www.dwheeler.com/sloccount/.
[43] M. Aliasgari, M. Blanton, Y. Zhang, and A. Steele, “Secure computation on floating point numbers,” in Proc. of ISOC NDSS, 2013.
[44] A. Holzer, M. Franz, S. Katzenbeisser, and H. Veith, “Secure two-party computations in ANSI C,” in Proc. of ACM CCS, 2012.
[45] M. Naehrig, R. Niederhagen, and P. Schwabe, “New software speed records for cryptographic pairings,” in Proc. of LATINCRYPT, 2010.
[46] P. S. L. M. Barreto and M. Naehrig, “Pairing-friendly elliptic curves of prime order,” in Selected Areas in Cryptography (SAC), 2006.
[47] N. Pippenger, “On the evaluation of powers and related problems (preliminary version),” in Proc. of FOCS, 1976.
[48] A. Aho, J. Hopcroft, and J. Ullman, The Design and Analysis of Computer Algorithms. Addison-Wesley, 1974.
[49] G. Adomavicius and A. Tuzhilin, “Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions,” Trans. Knowledge and Data Engineering, vol. 17, no. 6, 2005.
[50] D. A. Wolf-Gladrow, Lattice-Gas Cellular Automata and Lattice Boltzmann Models: An Introduction. Springer, 2005.
[51] R. Motwani and P. Raghavan, Randomized Algorithms. Cambridge University Press, 1995.
[52] Y. Ishai, E. Kushilevitz, and R. Ostrovsky, “Efficient arguments without short PCPs,” in IEEE Conference on Computational Complexity, 2007.
[53] C. Gentry, S. Halevi, and N. Smart, “Homomorphic evaluation of the AES circuit,” in Proceedings of CRYPTO, 2012.
[54] Y. Chen and R. Sion, “To cloud or not to cloud? Musings on costs and viability,” in Proc. of the ACM Symposium on Cloud Computing, 2011.
[55] G. O. Karame, M. Strasser, and S. Capkun, “Secure remote execution of sequential computations,” in Intl. Conf. on Information and Communications Security, 2009.
[56] S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy, “Proof verification and the hardness of approximation problems,” J. ACM, vol. 45, no. 3, 1998.
[57] D. Malkhi, N. Nisan, B. Pinkas, and Y. Sella, “Fairplay—a secure two-party computation system,” in Proc. of USENIX Security, 2004.
[58] Y. Huang, D. Evans, J. Katz, and L. Malka, “Faster secure two-party computation using garbled circuits,” in USENIX Security, 2011.