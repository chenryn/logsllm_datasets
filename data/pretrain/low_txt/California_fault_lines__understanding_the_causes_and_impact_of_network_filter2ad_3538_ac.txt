### IS Considerations for Repaired Links

When a link is repaired, the IS (Information System) considers it down until a configurable hold-down timer expires. This process is dynamic but should create biases of similar magnitude.

### Ambiguity in Determining Link Age

Another ambiguity arises when trying to pinpoint the "age" of each link to allow for the calculation of annualized statistics. The natural definition of age is the time between when a link was added to the network and when it was removed. However, this definition faces minor issues:

1. **Left Censoring**: Some links are added to the network before our syslog data begins.
2. **Right Censoring**: Some links continue to operate after our syslog data ends or during periods where syslog data is missing.

To address these issues:
- We do not allow any links to be added to the network before the syslog data starts.
- We remove all links from the network after the syslog data ends.
- We ignore any operational time during the periods where syslog data is missing.

A second version of this problem is the granularity with which router configuration files are maintained. Since interfaces are not tagged with creation or removal times, we rely on the first and last configuration file that contains a valid interface description. Unfortunately, configuration updates are logged periodically rather than instantaneously, leading to potential inaccuracies in the timing of link additions and removals.

### 5.2 Internal Consistency

Given that our data is historical and the CENIC network operators did not collect additional logs, we use two qualitatively different approaches to validate the data:

1. **Cross-Validation**: We cross-validate the records we have. Inconsistencies or disagreements between syslog and operational email announcements increase the likelihood of error. While the lack of inconsistency does not guarantee correctness, it helps quantify the degree of inconsistency and provides an approximate upper bound on the accuracy of our approach.
2. **External Validation**: Certain failures may be externally visible, allowing us to leverage logs collected by third parties.

#### Administrator Notices

We use administrator notices (Section 4.2.3) to validate the event history reconstructed from the syslog archive. We label failures with causes when available, particularly if there is an announcement related to the specific failure. Only a small subset of link failures is discussed by the operators on the email list. We check whether the reconstructed event history also records the corresponding events.

Ideally, we would confirm that each of the 3,505 distinct events mentioned in an administrative announcement appears in the log. Due to the difficulties in extracting precise details from free-form email messages, matching must be done manually. We verify a random subset of the events. Of the 35 (roughly 1%) events inspected, only one could not be matched to a corresponding set of failures in the event history, indicating 97% accuracy.

### 5.3 Externally Visible Events

In a well-designed network, most failures are masked by redundant links and protocols. However, catastrophic failures that result in network partitions are observable. The CENIC networks are connected to the larger Internet, making such partitions observable from the commercial Internet.

#### 5.3.1 CAIDA Ark/Skitter Traceroute

One direct method to ascertain whether a link is down is to attempt to use it. Most commercial network operators conduct periodic active end-to-end probes. CAIDA’s Ark (né Skitter) project conducts sporadic traceroutes to numerous destinations throughout the Internet from various traceroute servers. Occasionally, Skitter probes destinations within the CENIC network. We validate our failure log by comparing the success or failure of the Skitter probe to our event records. For all successful Skitter probes, we verify that all traversed links were "up" at the time of the probe according to our failure log. Conversely, if a Skitter probe fails, we verify that either the probe failed before reaching or after passing through the CENIC network, or the link leaving the last successful hop was "down" at the time of the probe.

CAIDA provided us with Skitter traceroute data covering six months (January–June 2007) of our study, amounting to over four gigabytes of compressed data. From this, we extracted 75,493,637 probes directed at 301 distinct destinations within the CENIC network from 17 different traceroute servers, covering 131 links and 584 distinct paths. The outcome of each of these 75 million Skitter probes was consistent with the link states reflected in our event history. Unfortunately, none of the Skitter probes failed within the CENIC network itself, meaning while the log is consistent with the Skitter data, Skitter does not positively confirm any failure events in the log.

#### 5.3.2 Route Views BGP Archive

Unlike traceroute, BGP listeners passively receive reachability information. The University of Oregon’s Route Views project has deployed ten BGP listeners worldwide to collect BGP updates, making their logs publicly available. The main challenge with BGP data is its coarse granularity, as BGP speaks in terms of networks or IP prefixes rather than individual layer-3 links. Hence, a BGP listener will only detect when an entire network becomes unreachable.

For the CENIC network, multiple core routers in multiple cities would need to fail simultaneously to partition the core network. We do not observe such partitions in the CENIC core network during our study. However, most customer sites, which have only one or two links to the CENIC core, can become partitioned if all links fail. Such events are infrequent but do occur.

We identified the IP prefixes for 60 distinct networks (i.e., customer sites) served by CENIC. We can only use BGP to validate a subset of these sites because CENIC does not withdraw prefixes of customers residing in CENIC address space. We identified 19 customer sites in the CENIC failure logs that have their own autonomous system (AS) and for which CENIC generates BGP withdraw messages. We identify network partitions for these sites by searching for multi-link failure events involving all of a CPE router's links to CENIC. We declare such customer sites isolated for the duration of the failure. One issue is that some customers may be multi-homed, meaning they have access links to networks other than CENIC. In such cases, we might assert that a site is isolated when it is only suffering degraded service. We have not, however, uncovered any evidence of such sites in our logs or interactions with CENIC operators.

The geographically closest Route Views BGP listener to the CENIC network is housed at the Peering and Internet Exchange (PAIX) in Palo Alto, California. The BGP listener’s network (AS6447) does not directly peer with the CENIC network (AS2152), but it does peer with several ASes that directly peer with CENIC. To accommodate BGP convergence idiosyncrasies, we declare a CENIC site isolated according to BGP if at least four peer ASes withdraw all of the site’s prefixes. We also observe instances where two or three ASes withdraw all of a site’s prefixes, but several other ASes advertise multiple paths of monotonically increasing length to a site’s prefixes. We refer to this as a BGP path change. While isolation is a strong proof of network partition, BGP path change events are also likely due to externally visible failures within the CENIC network and are useful for validating our error log.

Of the 147 isolating events in our event history that should be visible in BGP, we were able to match 51 to complete BGP isolations. If we consider BGP path changes, we confirm 105 of the 147 events. Notably, of the remaining 42 events, 23 pertain to a single link, possibly backed up by a statically configured link not reflected in our IS-IS dataset.

### 6. Analysis

By applying the methodology described in the previous sections to the CENIC syslogs and operational announcement email logs, we obtain over half-a-decade worth of failure data for a moderately sized production network. We can ask several fundamental questions about the operation of a real network:

- How often do failures occur? How long do they last?
- What are the causes of failures? Are some types of links more failure-prone than others?
- What is the impact of link failures? Does the network adapt without significant service disruption?

While we make no claims regarding the generality of our results, we believe that a study of this scope and duration is unprecedented in the literature and that our findings are likely representative of a larger class of educational and research networks.

### 6.1 Event History at a Glance

Figure 6 shows the reconstructed event history at a glance. Links are ordered lexicographically along the y-axis. Each failure is represented by a single point on the plot, located according to the start of the event. Two aspects bear note:

- **Vertical Banding**: Several vertical bands are apparent, corresponding to system-wide events. For example, the band in September 2005 labeled V1 is a network-wide IS-IS configuration change requiring a router restart. Another band in March 2007 (labeled V2) is the result of a network-wide software upgrade. The third band, V3, occurs in February 2009 as a network-wide configuration change in preparation for IPv6.
- **Horizontal Banding**: Figure 6 also contains several horizontal segments. The nearly solid segment labeled H1 corresponds to a series of failures on a link between a core router and a County of Education office. The segment is made up of many short failures happening only a few times a day. After at least one unsuccessful attempt to diagnose the problem, the cause was ultimately found to be faulty hardware. The horizontal segment labeled H2 corresponds to a RIV-SAC link between two HPR routers. Between July 2006 and January 2007, this link experienced over 33,000 short-duration failures. While the initial cause was a fiber cut, the repair process damaged an optical device leading to instability that was difficult to diagnose. Because this single flapping event accounts for 93% of all link failures in the HPR network, we remove it from the data set to avoid skewing further analyses.

### 6.2 Aggregate Statistics

We begin our analysis by computing aggregate statistics about the frequency and duration of failures on a per-link basis, both in terms of individual failure events and cumulative link downtime. Table 4 shows the average, median, and 95th percentile of each distribution. For all annualized statistics, we excluded links in operation fewer than 30 days because of their inflated variance.

#### 6.2.1 Failure Rate

Perhaps the most natural first question is, "How many failures are there?" Figure 7 shows the cumulative distribution function (CDF) of the number of failures per link per year. We compute the number of failures per year for each link by dividing the number of failures by the lifetime of the link (excluding links in operation for less than 30 days).

In the DC network, most links experience few failures, as expected in a production network. The CPE network, consisting of access links and routers on customer premises, is somewhat less reliable, with a median annual failure rate of 20.5 failures per link. The HPR network experienced considerably more failures.