### Table 5: Performance Metrics for MNIST with Decreased Fraction of Malicious Data

| Fraction of Malicious Users (%) | DR (%) | SR (%) | AD (%) |
|---------------------------------|--------|--------|--------|
| 20                              | 100    | 38     | 2      |
| 30                              | 100    | 69     | 5      |

### Table 6: Performance Metrics for GTSRB with Decreased Fraction of Malicious Data

| Fraction of Malicious Data (%) | DR (%) | SR (%) | AD (%) |
|--------------------------------|--------|--------|--------|
| 50                             | 100    | 16     | 2      |
| 70                             | 90     | 22     | 4      |
| 90                             | 100    | 32     | 5      |
| 100                            | 70     | 13     | 2      |

### 7. Related Work

#### Collaborative Learning
Various studies have highlighted the benefits of collaborative learning in machine learning algorithms [28, 45]. With the advent of deep learning, researchers have proposed collaborative settings for deep learning models. For example, Wang et al. used a hierarchical Bayesian model to show that collaborative deep learning significantly improves recommendation systems [43]. Xu et al. leveraged data locality to separate learning tasks among users, ensuring that only the final result is shared [44]. To enhance privacy, Pathak et al. aggregated classifiers trained locally by different participants using differential privacy techniques [35]. Shokri et al. also used differential privacy to design a deep learning model that supports collaboration while preserving user privacy [39]. Our work is inspired by these indirect collaborative deep learning models.

#### Adversarial Learning
Adversarial learning has been a long-standing research topic [14, 20, 22, 27]. Researchers categorize adversarial attacks into two types: causative (or poisoning) attacks and exploratory attacks. In poisoning attacks, adversaries corrupt the training data, while in exploratory attacks, they modify input samples to bypass the trained classifier. Deep learning systems are vulnerable to both types of attacks. Papernot et al. provided algorithms to generate adversarial samples that cause misclassification [34]. Goodfellow et al. used gradients to create adversarial samples [21]. Papernot et al. also introduced an auxiliary model-based attack algorithm [33]. While previous works focused on exploratory attacks, the emerging collaborative learning technique opens deep learning systems to poisoning attacks. Although poisoning attacks are well understood in traditional machine learning, their impact on deep learning systems remains underexplored. To our knowledge, this is the first work to study the efficacy of poisoning attacks in deep learning and propose a robust defense mechanism.

#### Defense
Defenses against poisoning attacks have been studied in the context of collaborative recommendation systems [15, 19, 36]. Biggio et al. examined the effectiveness of bagging ensembles in defending against poisoning attacks in spam filters and intrusion detection systems [15]. Previous work has also considered data sanitization to mitigate the negative impact of poisoned training data [29, 32]. Muhlenbach et al. filtered out samples that did not match their neighbors' classes, requiring access to the raw training dataset [30]. These defenses are primarily designed for traditional machine learning algorithms like k-nearest neighbors (KNN) and support vector machines (SVM). Defending against poisoning attacks in deep learning systems, where the final model aggregates parameters from multiple nodes, is less explored. In this work, we demonstrate that detecting poisoned datasets based on anomalous parameter distributions is an effective and promising solution.

### 8. Conclusion
In this paper, we investigate the impact of targeted poisoning attacks on deep learning systems using the MNIST and GTSRB datasets in an indirect collaborative learning setting. We show that even with limited control over the training data, attackers can effectively poison the system. To counter these attacks, we propose AUROR, a statistical defense that exploits the fact that malicious users can only poison their own data without knowledge of others' data. Our evaluation confirms that AUROR is a promising defense against poisoning attacks in indirect collaborative learning.

### 9. Acknowledgements
We thank the anonymous reviewers for their valuable feedback. We also extend our gratitude to Shweta Shinde, Viswesh Narayanan, and Yaoqi Jia for their insightful discussions and feedback on an early version of this paper. This work was supported by the Ministry of Education, Singapore under Grant No. R-252-000-560-112 and a university research grant from Intel. The opinions expressed in this work are solely those of the authors.

### 10. References
[1] Batman V Superman caught purchasing fake ratings on IMDB. http://www.bleachbypass.com/batman-v-superman-fake-imdb-ratings/
[2] Facebook’s Moments app uses artificial intelligence. http://money.cnn.com/2015/06/15/technology/facebook-moments-ai/
[3] Facebook’s Virtual Assistant ‘M’ Is Super Smart. It’s Also Probably a Human. http://recode.net/2015/11/03/facebooks-virtual-assistant-m-is-super-smart-its-also-probably-a-human/
[4] How ‘Deep Learning’ Works at Apple, Beyond. https://www.theinformation.com/How-Deep-Learning-Works-at-Apple-Beyond
[5] Improving Photo Search: A Step Across the Semantic Gap. http://googleresearch.blogspot.sg/2013/06/improving-photo-search-step-across.html
[6] Making Cortana smarter: how machine learning is becoming more dynamic. http://www.techradar.com/sg/news/
[7] Meet The Guy Who Helped Google Beat Apple’s Siri. http://www.forbes.com/sites/roberthof/2013/05/01/meet-the-guy-who-helped-google-beat-apples-siri/#7c3a2bda56cb
[8] Personalized Recommendations Frequently Asked Questions. http://www.imdb.com/help/show_leaf?personalrecommendations
[9] Spam filter. https://gmail.googleblog.com/2015/07/the-mail-you-want-not-spam-you-dont.html
[10] 'The Interview' Now Has a Perfect 10 Rating on IMDb. http://motherboard.vice.com/read/the-interview-has-a-perfect-10-on-imdb
[11] The mail you want, not the spam you don’t. https://gmail.googleblog.com/2015/07/the-mail-you-want-not-spam-you-dont.html
[12] Theano Package. https://github.com/Theano/Theano
[13] A. Anjos and S. Marcel. Counter-measures to photo attacks in face recognition: a public database and a baseline. In International Joint Conference on Biometrics (IJCB), pages 1–7. IEEE, 2011.
[14] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can machine learning be secure? In Symposium on Information, Computer and Communications Security, pages 16–25. ACM, 2006.
[15] B. Biggio, I. Corona, G. Fumera, G. Giacinto, and F. Roli. Bagging classifiers for fighting poisoning attacks in adversarial classification tasks. In Multiple Classifier Systems, pages 350–359. Springer, 2011.
[16] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases, pages 387–402. Springer, 2013.
[17] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In Proceedings of the 29th International Conference on Machine Learning, 2012.
[18] L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of the 19th International Conference on Computational Statistics, pages 177–186. Springer, 2010.
[19] Y. Cao and J. Yang. Towards making systems forget with machine unlearning. In Symposium on Security and Privacy, pages 463–480. IEEE, 2015.
[20] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al. Adversarial classification. In Proceedings of the 10th SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 99–108. ACM, 2004.
[21] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Proceedings of the 3rd International Conference on Learning Representations, 2015.
[22] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar. Adversarial machine learning. In Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence, pages 43–58. ACM, 2011.
[23] W. Jung, S. Kim, and S. Choi. Poster: Deep learning for zero-day flash malware detection. In 36th IEEE Symposium on Security and Privacy, 2015.
[24] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, volume 86, pages 2278–2324. IEEE, 1998.
[25] Y. Li, R. Ma, and R. Jiao. A hybrid malicious code detection method based on deep learning. In International Journal of Security and Its Applications, volume 9, pages 205–216, 2015.
[26] G. Linden, B. Smith, and J. York. Amazon.com recommendations: Item-to-item collaborative filtering. In Internet Computing, IEEE, volume 7, pages 76–80. IEEE, 2003.
[27] D. Lowd and C. Meek. Adversarial learning. In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, pages 641–647. ACM, 2005.
[28] L. Melis, G. Danezis, and E. De Cristofaro. Efficient private statistics with succinct sketches. In Proceedings of the 23rd Network and Distributed System Security Symposium, 2015.
[29] M. Mozaffari-Kermani, S. Sur-Kolay, A. Raghunathan, and N. K. Jha. Systematic poisoning attacks on and defenses for machine learning in healthcare. In IEEE Journal of Biomedical and Health Informatics, volume 19, pages 1893–1905. IEEE, 2015.
[30] F. Muhlenbach, S. Lallich, and D. A. Zighed. Identifying and handling mislabeled instances. In Journal of Intelligent Information Systems, volume 22, pages 89–109. Springer, 2004.
[31] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. In 29th IEEE Symposium on Security and Privacy, pages 111–125. IEEE, 2008.
[32] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. Rubinstein, U. Saini, C. Sutton, J. Tygar, and K. Xia. Misleading learners: Co-opting your spam filter. In Machine Learning in Cyber Trust, pages 17–51. Springer, 2009.
[33] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Berkay Celik, and A. Swami. Practical black-box attacks against deep learning systems using adversarial examples. In arXiv preprint arXiv:1602.02697, 2016.
[34] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy, pages 372–387. IEEE, 2016.
[35] M. Pathak, S. Rane, and B. Raj. Multiparty differential privacy via aggregation of locally trained classifiers. In Advances in Neural Information Processing Systems, pages 1876–1884, 2010.
[36] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. Tygar. Antidote: Understanding and defending against poisoning of anomaly detectors. In Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement Conference, pages 1–14. ACM, 2009.
[37] D. Rumelhart, G. Hinton, and R. Williams. Learning internal representations by error propagation. In Neurocomputing: Foundations of Research, pages 673–695. MIT Press, 1988.
[38] J. Schmidhuber. Deep learning in neural networks: An overview. In Neural Networks, volume 61, pages 85–117. Elsevier, 2015.
[39] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1310–1321. ACM, 2015.
[40] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. In Neural Networks, volume 32, pages 323–332. Elsevier, 2012.
[41] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In Proceedings of the International Conference on Learning Representations, 2014.
[42] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao. Man vs. machine: Practical adversarial detection of malicious crowdsourcing workers. In Proceedings of the 23rd USENIX Conference on Security Symposium, pages 239–254, 2014.
[43] H. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep learning for recommender systems. In Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1235–1244. ACM, 2015.
[44] K. Xu, H. Ding, L. Guo, and Y. Fang. A secure collaborative machine learning framework based on data locality. In 2015 IEEE Global Communications Conference (GLOBECOM), pages 1–5. IEEE, 2015.
[45] K. Xu, H. Yue, L. Guo, Y. Guo, and Y. Fang. Privacy-preserving machine learning algorithms for big data systems. In Distributed Computing Systems (ICDCS), 2015 IEEE 35th International Conference on, pages 318–327. IEEE, 2015.
[46] W. Xu, Y. Qi, and D. Evans. Automatically evading classifiers. In Proceedings of the 2016 Network and Distributed Systems Symposium, 2016.