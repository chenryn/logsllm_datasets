### VI. Experimental Evaluation

**Figure 6: Elements of Workflow Diagram**

**Figure 7: Experimental Evaluation. Test Initial Data Set.**

To ensure the reliability and robustness of the visualization component, a manual analysis of the generated schemas and triggered rules was conducted. The manual review confirmed that the order of rule implementation displayed on the visualization chart corresponds to the order in which the rules are executed to build the schemas. Additionally, the generated schemas were found to be valid according to the technical requirements and provided data.

After thorough testing, the visualization mechanism can replace the manual inspection of schemas with a simple check of the rule implementation order based on the visualization. During the experimental evaluation process, the system was provided with sets of input optical parameters for which the expected synthesis results were already known. This ensured that the correct schemas were generated and that the visualization component displayed the charts with the correct rule invocation order.

#### Example of Synthesis Process

One of the synthesis processes is presented here as an example. An input form for a photographic lens subsystem (Figure 7) was filled with an initial set of valid test data (Table I), and a request to the server was sent by clicking the "Synthesis" button. Based on the inserted facts, a total of 21 rules were triggered (12 from the generation package, 6 from the corrective package, and 1 from the basic, fast, and wide-angle packages). A set of 12 structural schemas was produced (Figure 8).

**Table I: Initial Data Set for Evaluation**
- **Aperture Speed:** 1.8
- **Angular Field:** 84°
- **Focal Length:** 4.5 mm
- **Image Quality:** Geometrically limited
- **Back Focal Distance:** 1 mm
- **Entrance Pupil Position:** Forward
- **Spectral Range:** 450–600 nm

The generated schemas were found to be valid for the provided technical requirements, and the rule invocation order, as seen in the visualization chart, was correct (Figure 9). Other experiments with structural schema synthesis and visualization also showed satisfactory results, confirming the feasibility of the component.

**Figure 8: Produced Schemas. Not all 12 schemas are presented.**

**Figure 9: Experimental Evaluation. Visualization of Synthesis Based on Real-World Parameters (Cut for Size Reasons)**

### G. Examples of Visualization

For testing and clarity, a synthesis result for another set of input values was visualized. The following example displays a synthesis process for a simplified, random input not related to real-world applications, to simplify the produced diagram so that the visualization chart can be presented full-sized and uncut. The initial set of input data (Table II) was chosen to meet simplicity requirements.

**Table II: Initial Data Set for Simplified Diagram Example**
- **Aperture Speed:** 1
- **Angular Field:** 2°
- **Focal Length:** 1 mm
- **Image Quality:** Geometrically limited
- **Back Focal Distance:** 1 mm
- **Entrance Pupil Position:** Forward
- **Spectral Range:** 1–2 nm

Based on the test initial data, a diagram was built (Figure 10). As shown, the first rules are displayed in the order of their invocation time. Next, the facts are displayed in ascending order by insertion time. Finally, the relationship between rules and facts is drawn (arcs with arrows).

**Figure 10: Example 1: Visualization of Simplified Synthesis Based on Non-Real-World Data**

### V. Conclusion & Future Work

Expert systems are complex artifacts that are difficult to develop and test. This paper presents the technical aspects of OSYST, an environment for automated structural synthesis of optical systems (photo-objectives), and the process of its validation and verification, particularly focusing on reasoning visualization. The analysis of the system revealed a drawback in the validation process, which was overcome by introducing a visualization component for tracking working memory processes, primarily rule invocation and operations on facts.

In this study, we demonstrated that the new visualization component provides experts with an additional means of inspecting the knowledge engine's workflow. The more ways an alpha-tester has to assess rule executions, the more thorough and efficient the validation process becomes. Focusing on the rule firing order rather than comparing synthesized schemas with expected schemas simplifies the overall validation and verification process.

Although the visualization component designed is applicable for validation purposes, further revisions are desirable. A scaling mechanism is needed to facilitate the review of large diagrams, and more information on the fired rules would be beneficial. More research and development are necessary to expand the functionality of the component, making it a powerful debugging tool.

Future work should focus on improving the expert system in general and introducing new components and tools, in particular.

### References

[1] S.H. Liao, “Expert system methodologies and applications: a decade review from 1995 to 2004,” *Expert Systems with Applications*, vol. 28(1), 2005, pp. 93-103.

[2] M. Huettig, G. Busher, T. Menzel, W. Scheppach, F. Puppe, H.P. Buscher, “A Diagnostic Expert System for Structured Reports, Quality Assessment, and Training of Residents in Sonography,” *Medizinische Klinik*, vol. 99, 2004, pp. 117-122.

[3] T. Padma, P. Balasubramanie, “Knowledge-based decision support system to assist work-related risk analysis in musculoskeletal disorders,” *Knowledge-Based Systems*, vol. 22, 2009, pp. 72-78.

[4] R.K. Lindsay, B.G. Buchanan, E.A. Feigenbaum, J. Lederberg, “DENDRAL: a case study of the first expert system for scientific hypothesis formation,” *Artificial Intelligence*, vol. 61, 1993, pp. 209-261.

[5] J. Baumeister, “Advanced Empirical Testing,” *Knowledge-Based Systems*, vol. 24(1), 2011, pp. 83-94.

[6] D. Mouromtsev, I. Livshits, M. Kolchin, “Knowledge-based engineering system for structural optical design,” *Frontiers in Artificial Intelligence and Applications*, vol. 246, 2012, pp. 254-272.

[7] R. Knauf, A.J. Gonzalez, K.P. Jantke, “Validating rule-based systems: a complete methodology,” in *Proc. IEEE SMC’99 Conf.*, 1999, pp. 744-749.

[8] J. Vanthienen, C. Mues, A. Aerts, “An illustration of verification and validation in the modeling phase of KBS development,” *Data and Knowledge Engineering*, vol. 27, 1998, pp. 337-352.

[9] A. Preece, “Evaluating Verification and Validation Methods in Knowledge Engineering,” in R. Roy (ed.), *Micro-Level Knowledge Management*, Morgan-Kaufman, 2001, pp. 123-145.

[10] M. Tavana, “Knowledge-Based Expert System Development and Validation with Petri Nets,” *Journal of Information & Knowledge Management*, vol. 7(1), 2008, pp. 37-46.

[11] J. Baumeister, J. Reutelshoefer, F. Puppe, “KnowWe: a Semantic Wiki for knowledge engineering,” *Applied Intelligence*, vol. 35(3), 2011, pp. 323-344.

[12] B.J. Wielinga, A.T. Schreiber, J.A. Breuker, “KADS: a modeling approach to knowledge engineering,” *Knowledge Acquisition - Special issue on the KADS approach to knowledge engineering*, vol. 4, 1992, pp. 5-53.

[13] N. Prat, J. Akoka, I. Comyn-Wattiau, “An MDA approach to knowledge engineering,” *Expert Systems with Applications: An International Journal*, vol. 39(12), 2012, pp. 10420-10437.

[14] O. Cair, S. Guardati, “The KAMET II methodology: Knowledge acquisition, knowledge modeling, and knowledge generation,” *Expert Systems with Applications: An International Journal*, vol. 39(9), 2012, pp. 8108-8114.

[15] M. Freiling, J. Alexander, S. Messick, S. Rehfuss, S. Shulman, “Starting a Knowledge Engineering Project: A Step-by-Step Approach,” *AI Magazine*, vol. 6(3), 1985.