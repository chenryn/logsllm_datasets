### Incentivizing Server Updates and Preventing Downgrade Attacks

To ensure that servers do not lag behind clients in deploying newer versions, the initial version requested by the client and the list of versions supported by the server are both fed into the key-derivation function at both ends. This process generates the final keys and helps prevent downgrade attacks.

### 3.2 Stream Multiplexing

Applications often multiplex units of data within TCP's single-bytestream abstraction. To avoid head-of-line blocking due to TCP's sequential delivery, QUIC supports multiple streams within a connection. This ensures that a lost UDP packet only affects the streams whose data was carried in that packet, allowing subsequent data on other streams to continue being reassembled and delivered to the application.

QUIC streams provide a reliable, bidirectional bytestream. They can be used for framing application messages of arbitrary size—up to \(2^{64}\) bytes can be transferred on a single stream. However, they are lightweight enough to allow a new stream for each small message. Streams are identified by stream IDs, which are statically allocated as odd IDs for client-initiated streams and even IDs for server-initiated streams to avoid collisions. Stream creation is implicit when sending the first bytes on an unused stream, and stream closing is indicated by setting a "FIN" bit on the last stream frame. If either the sender or the receiver determines that the data on a stream is no longer needed, the stream can be canceled without tearing down the entire QUIC connection. Although streams are reliable, QUIC does not retransmit data for a canceled stream.

A QUIC packet consists of a common header followed by one or more frames. Stream multiplexing is implemented by encapsulating stream data in one or more stream frames, with a single QUIC packet capable of carrying stream frames from multiple streams.

The rate at which a QUIC endpoint can send data is always limited. The endpoint must decide how to divide available bandwidth between multiple streams. In our implementation, QUIC relies on HTTP/2 stream priorities [8] to schedule writes.

### 3.3 Authentication and Encryption

With the exception of a few early handshake packets and reset packets, QUIC packets are fully authenticated and mostly encrypted. Figure 5 illustrates the structure of a QUIC packet. The parts of the QUIC packet header outside the cover of encryption are required for routing or for decrypting the packet: Flags, Connection ID, Version Number, Diversification Nonce, and Packet Number. Flags encode the presence of the Connection ID field and the length of the Packet Number field, and must be visible to read subsequent fields. The Connection ID serves routing and identification purposes, used by load balancers to direct traffic to the correct server and by the server to locate connection state. The version number and diversification nonce fields are only present in early packets. The server generates the diversification nonce and sends it to the client in the SHLO packet to add entropy to key generation. Both endpoints use the packet number as a per-packet nonce, necessary for authenticating and decrypting packets. The packet number is placed outside of encryption to support decryption of out-of-order packets, similar to DTLS [56].

Any information sent in unencrypted handshake packets, such as in the Version Negotiation packet, is included in the derivation of the final connection keys. In-network tampering of these handshake packets causes the final connection keys to differ at the peers, leading to connection failure without successful decryption of any application data. Reset packets, sent by a server that does not have state for the connection (e.g., due to a routing change or server restart), are sent unencrypted and unauthenticated.

### 3.4 Loss Recovery

TCP sequence numbers facilitate reliability and represent the order in which bytes are to be delivered at the receiver. This conflation causes the "retransmission ambiguity" problem, as a retransmitted TCP segment carries the same sequence numbers as the original packet [39, 64]. The receiver of a TCP ACK cannot determine whether the ACK was sent for the original transmission or a retransmission, and the loss of a retransmitted segment is commonly detected via an expensive timeout. Each QUIC packet carries a new packet number, including those carrying retransmitted data. This design obviates the need for a separate mechanism to distinguish the ACK of a retransmission from that of an original transmission, thus avoiding TCP’s retransmission ambiguity problem. Stream offsets in stream frames are used for delivery ordering, separating the two functions that TCP conflates. The packet number represents an explicit time-ordering, enabling simpler and more accurate loss detection than in TCP.

QUIC acknowledgments explicitly encode the delay between the receipt of a packet and its acknowledgment being sent. Together with monotonically increasing packet numbers, this allows for precise network round-trip time (RTT) estimation, which aids in loss detection. Accurate RTT estimation also benefits delay-sensing congestion controllers such as BBR [10] and PCC [16]. QUIC’s acknowledgments support up to 256 ACK blocks, making QUIC more resilient to reordering and loss than TCP with SACK [46]. Consequently, QUIC can keep more bytes on the wire in the presence of reordering or loss.

These differences between QUIC and TCP allowed us to build simpler and more effective mechanisms for QUIC. Further details on these mechanisms can be found in the Internet-draft on QUIC loss detection [33].

### 3.5 Flow Control

When an application reads data slowly from QUIC’s receive buffers, flow control limits the buffer size that the receiver must maintain. A slowly draining stream can consume the entire connection’s receive buffer, blocking the sender from sending data on other streams. QUIC mitigates this potential head-of-line blocking among streams by limiting the buffer that a single stream can consume. QUIC employs both connection-level flow control, which limits the aggregate buffer that a sender can consume at the receiver across all streams, and stream-level flow control, which limits the buffer that a sender can consume on any given stream.

Similar to HTTP/2 [8], QUIC uses credit-based flow control. A QUIC receiver advertises the absolute byte offset within each stream up to which the receiver is willing to receive data. As data is sent, received, and delivered on a particular stream, the receiver periodically sends window update frames that increase the advertised offset limit for that stream, allowing the peer to send more data. Connection-level flow control works similarly to stream-level flow control, but the bytes delivered and the highest received offset are aggregated across all streams.

Our implementation uses a connection-level window that is substantially larger than the stream-level window to allow multiple concurrent streams to make progress. Our implementation also uses flow control window auto-tuning, similar to common TCP implementations (see [1] for details).

### 3.6 Congestion Control

The QUIC protocol does not rely on a specific congestion control algorithm and has a pluggable interface for experimentation. In our deployment, both TCP and QUIC use Cubic [26] as the congestion controller, with one notable difference. For video playback on both desktop and mobile devices, non-QUIC clients use two TCP connections to the video server to fetch video and audio data. The connections are not designated as audio or video; each chunk of audio and video arbitrarily uses one of the two connections. Since the audio and video streams are sent over two streams in a single QUIC connection, QUIC uses a variant of mulTCP [14] for Cubic during the congestion avoidance phase to achieve parity in flow-fairness with the use of TCP.

### 3.7 NAT Rebinding and Connection Migration

QUIC connections are identified by a 64-bit Connection ID. This enables connections to survive changes to the client’s IP and port, which can be caused by NAT timeout and rebinding (more aggressive for UDP than for TCP [27]) or by the client changing network connectivity to a new IP address. While QUIC endpoints use the Connection ID to identify connections, client-initiated connection migration is still a work in progress with limited deployment.

### 3.8 QUIC Discovery for HTTPS

A client does not know a priori whether a given server supports QUIC. When a client makes an HTTP request to an origin for the first time, it sends the request over TLS/TCP. Our servers advertise QUIC support by including an "Alt-Svc" header in their HTTP responses [48]. This header tells the client that connections to the origin may be attempted using QUIC. The client can then attempt to use QUIC in subsequent requests to the same origin.

On a subsequent HTTP request to the same origin, the client races a QUIC and a TLS/TCP connection, but prefers the QUIC connection by delaying the TLS/TCP connection by up to 300 ms. Whichever protocol successfully establishes a connection first is used for that request. If QUIC is blocked on the path or if the QUIC handshake packet is larger than the path’s MTU, the QUIC handshake fails, and the client uses the fallback TLS/TCP connection.

### 3.9 Open-Source Implementation

Our implementation of QUIC is available as part of the open-source Chromium project [1]. This implementation is shared code, used by Chrome and other clients such as YouTube, and also by Google servers, albeit with additional Google-internal hooks and protections. The source code is in C++ and includes substantial unit and end-to-end testing. The implementation includes a test server and a test client for experimentation, though they are not tuned for production-level performance.

### 4. Experimentation Framework

Our development of the QUIC protocol relies heavily on continual Internet-scale experimentation to examine the value of various features and to tune parameters. In this section, we describe the experimentation frameworks in Chrome and our server fleet, which allow us to experiment safely with QUIC.

We drove QUIC experimentation by implementing it in Chrome, which has a strong experimentation and analysis framework that allows new features to be A/B tested and evaluated before full launch. Chrome’s experimentation framework pseudo-randomly assigns clients to experiments and exports a wide range of metrics, from HTTP error rates to transport handshake latency. Clients opted into statistics gathering report their statistics along with a list of their assigned experiments, enabling us to slice metrics by experiment. This framework also allows us to rapidly disable any experiment, protecting users from problematic experiments.

We used this framework to help evolve QUIC rapidly, steering its design according to continuous feedback based on data collected at the full scale of Chrome’s deployment. Monitoring a broad array of metrics makes it possible to guard against regressions and to avoid imposing undue risks that might otherwise result from rapid evolution. As discussed in Section 5, this framework allowed us to contain the impact of occasional mistakes. Perhaps more importantly, QUIC had the luxury of being able to directly link experiments into analytics of the application services using those connections. For instance, QUIC experimental results might be presented in terms of familiar metrics for a transport, such as packet retransmission rate, but the results were also quantified by user- and application-centric performance metrics, such as web search response times or rebuffer rate for video playbacks. Through small but repeatable improvements and rapid iteration, the QUIC project has been able to establish and sustain an appreciable and steady trajectory of cumulative performance gains.

We added QUIC support to our mobile video (YouTube) and search (Google Search) apps as well. These clients have similar experimentation frameworks that we use for deploying QUIC and measuring its performance.

Google’s server fleet consists of thousands of machines distributed globally, within data centers and ISP networks. These front-end servers terminate incoming TLS/TCP and QUIC connections for all our services and perform load-balancing across internal application servers. We have the ability to toggle features on and off on each server, allowing us to rapidly disable broken or buggy features. This mechanism enabled us to perform controlled experiments with QUIC globally while severely limiting the risk of large-scale outages induced by these experiments. Our servers report performance data related to current and historic QUIC connections. This data is collected by a centralized monitoring system that aggregates it and provides visualizations and alerts.

### 5. Internet-Scale Deployment

The experimentation framework described in Section 4 enabled safe global deployment of QUIC to our users. We first present QUIC’s deployment timeline. We then describe the evolution of one of several metrics that we monitored carefully as we deployed QUIC globally.

#### 5.1 The Road to Deployment

QUIC support was added to Chrome in June 2013. It was enabled via an optional command-line flag, so usage was effectively limited to the QUIC development team. In early 2014, we were confident in QUIC’s stability and turned it on via Chrome’s experimentation framework for a tiny fraction (< 0.025%) of users. As QUIC proved to be performant and safe, this fraction was increased. As of January 2017, QUIC is turned on for almost all users of Chrome and the Android YouTube app.

Simultaneously developing and deploying a new secure protocol has its difficulties, and it has not been completely smooth sailing. Figure 2 shows QUIC traffic to our services from February 2015 to December 2016. We now describe the two notable events seen in the graph.

**Unencrypted data in 0-RTT requests:** In December 2015, we discovered a vulnerability in our implementation of the QUIC handshake. The vulnerability was traced to a bug in the client code, which could result in 0-RTT requests being sent unencrypted in an exceedingly rare corner case. Our immediate response was to disable QUIC globally at our servers, using the feature toggle mechanism described in Section 4. This turndown can be seen as the drop to zero in Figures 6 and 14. The bug was fixed, and QUIC traffic was restored as updated clients were rolled out.

**Increasing QUIC on mobile:** A substantial fraction of our users access our services through mobile phones, often using dedicated applications (apps). The majority of our mobile users perform searches via our mobile search app, and the majority of mobile video playbacks are performed through the YouTube app. The YouTube app started using QUIC in September 2016, doubling the percentage of Google’s egress traffic over QUIC, from 15% to over 30%.

#### 5.2 Monitoring Metrics: Search Latency

Our server infrastructure gathers performance data exported by front-end servers and aggregates them with service-specific metrics gathered by the server and clients, providing visualizations and alerts. We will use Search Latency as an example of such a metric. Search Latency is defined as the delay between when a user enters a search term into the client and when all the search-result content is generated and delivered to the client, including images and embedded content. We analyze the evolution of Search Latency improvement for users in the QUIC experiment versus those using TLS/TCP over an 18-month period, as shown in Figure 6.

Over the 18-month period shown in Figure 6, there are two notable regressions and one improvement. The first regression started in July 2015 (labeled '1' in Figure 6) and lasted for about 5 months. This regression was attributed to changes in our serving infrastructure and a client configuration bug, both of which inadvertently caused a large number of clients in the QUIC experiment to gradually stop speaking QUIC.

The second regression in December 2015 lines up with the complete disabling of QUIC described in Section 5.1. When QUIC was re-enabled in February 2016 (labeled '2' in Figure 6), the client configuration bug had been fixed, and Search Latency improved, albeit not to the same extent as earlier, since the infrastructure changes had not been resolved.

We take a slight detour to describe restricted edge locations (RELs) and UDP proxying. A large number of our servers are deployed inside ISPs, and we refer to these as RELs. For technical, commercial, and other considerations, RELs do not terminate TLS sessions to a number of domains. For these domains, RELs therefore