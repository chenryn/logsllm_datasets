### 30-Hour TPC-W Workload Used in the Case Study

The 30-hour TPC-W workload used in the case study includes the following components:

1. **Browsing Mix**: The number of EBs (End-Boxes) is set to 200, 400, 600, 800, and 1000 respectively.
2. **Non-Stationary Transaction Mix with Background CPU Process**: To validate the algorithm's ability to detect performance anomalies, a special workload was generated. This workload includes a non-stationary transaction mix and an additional CPU process that consumes a random amount of CPU in the background.
3. **Shopping Mix**: The number of EBs is set to 200, 400, 600, 800, and 1000 respectively.
4. **Ordering Mix**: The number of EBs is set to 200, 400, 600, 800, and 1000 respectively.
5. **Non-Stationary TPC-W Transaction Mix**: As described above.

### Validation of Algorithm for Detecting Application Changes

To validate whether the algorithm can automatically recognize application changes, the source code of the "Home" transaction (the 8th transaction) in TPC-W was modified by inserting a controlled CPU loop. This increased the service time of the "Home" transaction by 5 ms and 10 ms in two separate experiments. These modifications were tested using the non-stationary TPC-W transaction mix.

### Approach Validation

We applied our online regression-based algorithm to the 30-hour workload shown in Figure 9. We experimented with two values of allowable error: \( E_{\text{allow}} = 3\% \) and \( E_{\text{allow}} = 1\% \) to demonstrate the impact of error setting and highlight the importance of tuning this value.

- **With \( E_{\text{allow}} = 3\% \)**: The algorithm correctly identified 4 major model changes, as shown in Figure 10. For the second segment, there were 42 model changes (not shown in the figure for simplicity), with the maximum segment being 5 epochs. The algorithm accurately detected that the entire second segment was anomalous and performed model reconciliation for the consecutive segments around the anomalous segment, as shown in Figure 11. Finally, the algorithm correctly raised alarms on the application change when the regression model changed and could not be reconciled (last two segments in Figure 11).

- **With \( E_{\text{allow}} = 1\% \)**: The algorithm identified 6 major model changes, including the 4 model changes shown in Figure 10, plus 2 extra segments at timestamps 790 and 1030, which correspond to workload changes and are false alarms. It is important to use an appropriate error setting to minimize the number of false alarms. The application signature can be used during the allowable error tuning to gain insight into whether the model change is due to an application change or a workload change.

### Power of Regression-Based Approach

The regression-based approach is sensitive and accurate in detecting differences in the CPU consumption model of application transactions. However, it cannot identify the specific transactions responsible for these resource consumption differences. To complement the regression-based approach and identify the transactions causing the model change, we use the application performance signature. Comparing the new application signature against the old one allows efficient detection of transactions with performance changes, as shown in Figure 12.

### Related Work

Applications built using Web services can span multiple computers, operating systems, languages, and enterprises, making it challenging to measure application availability and performance. Several commercial tools and research projects address this challenge:

- **Commercial Tools**: Tools like HP BTO product suite, IBM Tivoli, CA Wily Introscope, and Symantec use fixed or statistical baseline-guided threshold setting, adaptive threshold setting, and change detection combined with statistical baselining and thresholding. While these tools provide detailed information on current transaction latencies, they offer limited information on the causes of observed latencies and cannot directly detect performance changes in updated or modified applications.
  
- **Research Projects**: Projects like Pinpoint, Magpie, and Cohen et al. use statistical techniques to identify sources of high latency, capture resource demands, and model performance problems. Magpie and Cohen et al. are most closely related to our approach, but they focus on more sophisticated tracing infrastructure and rare anomalies. Our work aims to detect performance changes caused by application modifications and software updates, independent of workload conditions.

### Conclusion and Future Work

The three-tier architecture paradigm is now an industry standard for building enterprise client-server applications. When a new application update is introduced or unexpected performance problems are observed, it is crucial to distinguish between performance issues caused by high workload and those caused by errors or inefficiencies in the upgraded software.

In this work, we propose an integrated framework of measurement and system modeling techniques for anomaly detection and analysis of essential performance changes in application behavior. Our solution combines a regression-based transaction model and an application performance signature. The regression-based algorithm accurately detects changes in CPU consumption and raises alarms, while the application performance signature identifies the specific transactions causing the model change.

While this paper focuses on CPU consumption, both the regression method and the application performance signature can be extended to evaluate memory usage and detect memory leaks. We plan to explore this in future work.

### Acknowledgements

We would like to thank our HP colleagues from the Diagnostics team: Brent Enck, Dave Gershon, Anupriya Ramraj, and Glenna Mayo for their help and useful discussions during this work.

### References

[1] M. Aguilera, J. Mogul, J. Wiener, P. Reynolds, and A. Muthitacharoen. Performance debugging for distributed systems of black boxes. Proc. of the 19th ACM SOSP'2003.

[2] P. Barham, A. Donnelly, R. Isaacs, R. Mortier. Using Magpie for request extraction and workload modelling. Proc of the 6th Symposium OSDI'2004.

[3] BMC ProactiveNet. www.bmc.com/

[4] M. Chen, A. Accardi, E. Kiciman, J. Lloyd, D. Patterson, A. Fox, and E. Brewer. Path-based failure and evolution management. Proc. of the 1st Symposium NSDI'04.

[5] L. Cherkasova, M. Karlsson. Dynamics and Evolution of Web Sites: Analysis, Metrics and Design Issues. In Proc. of the 6-th International Symposium on Computers and Communications (ISCC'01), 2001.

[6] L. Cherkasova, Y. Fu, W. Tang, A. Vahdat: Measuring and Characterizing End-to-End Internet Service Performance. Journal ACM/IEEE Transactions on Internet Technology, (TOIT), November, 2003.

[7] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly, A. Fox. Capturing, Indexing, Clustering, and Retrieving System History. Proc. of the 20th ACM Symposium SOSP'2005.

[8] CA Wily Introscope. www.wilytech.com.

[9] F. Douglis and A. Feldmann. Rate of change and other metrics: a live study of the world wide web. In USENIX Symposium on Internet Technologies and Systems, 1997.

[10] N. R. Draper and H. Smith. Applied Regression Analysis. John Wiley & Sons, 1998.

[11] IBM Corporation. Tivoli Web Management Solutions, http://www.tivoli.com/products/demos/twsm.html.

[12] Indicative Co. www.indicative.com/product/End-to-End.pdf

[13] Mercury Diagnostics. www.mercury.com/us/products/diagnostics/

[14] N. Mi, L. Cherkasova, K. Ozonat, J. Symons, and E. Smirni. Analysis of Application Performance and Its Change via Representative Application Signatures. Will appear in NOMS'2008.

[15] http://www.netuitive.com/

[16] Quest Software Inc. Performasure. http://java.quest.com/performasure

[17] C. Stewart, T. Kelly, A. Zhang. Exploiting nonstationarity for performance prediction. Proc. of the EuroSys'2007.

[18] Symantec [3: Application Performance Management http://www.symantec.com/business/products/

[19] TPC-W Benchmark. URL http://www.tpc.org

[20] Q. Zhang, L. Cherkasova, and E. Smirni: A Regression-Based Analytic Model for Dynamic Resource Provisioning of Multi-Tier Applications. Proc. of the 4th IEEE International Conference on Autonomic Computing (ICAC'2007), 2007.

[21] Q. Zhang, L. Cherkasova, G. Mathews, W. Greene, and E. Smirni: R-Capriccio: A Capacity Planning and Anomaly Detection Tool for Enterprise Services with Live Workloads. Proc. of the ACM/IFIP/uSENIX 8th International Middleware Conference (Middleware' 2007), 2007.

1-4244-2398-9/08/$20.00 Â©2008 IEEE
461
DSN 2008: Cherkasova et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021 at 13:22:14 UTC from IEEE Xplore. Restrictions apply.