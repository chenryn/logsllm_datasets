### Formal Specification and Analysis of TLS 1.2 and 1.3

Formally specifying cryptographic protocols can reveal flaws that might otherwise go unnoticed. For instance, the implementation of TLS 1.2 with verified cryptographic security by Bhargavan et al. [70] uncovered new alert fragmentation and fingerprinting attacks, leading to the discovery of the Triple Handshake attack [72]. Similarly, the symbolic analysis of TLS 1.3 draft 10 using Tamarin by Cremers et al. [2] revealed a potential attack allowing an adversary to impersonate a client during a PSK-resumption handshake, which was subsequently fixed in draft 11. Bhargavan et al. [3] used ProVerif to analyze TLS 1.3 and discovered a new attack on 0-RTT client authentication, which was addressed in draft 13. Cremers et al. [4] also conducted a symbolic analysis of draft 21 using Tamarin, uncovering unexpected behavior that inhibited certain strong authentication guarantees. In nearly all cases, these discoveries led to protocol improvements and clarified the documentation of security guarantees.

### Lessons from TLS 1.3 Standardization

**Lesson 1: Cryptographic Protocol Designs are Moving Targets; Machine-Checked Proofs Can Be More Easily Updated.**

The TLS 1.3 specification was a rapidly evolving target, with significant changes made regularly across 28 drafts. As changes were introduced, previous analyses often became obsolete within a few months, necessitating new analyses and proofs. One key advantage of machine-checked analyses and proofs over manual ones is their ability to be more easily and reliably updated as the protocol evolves [2]–[4]. Additionally, machine-checked analyses help ensure that new flaws are not introduced as components are modified.

**Lesson 2: Standardization Processes Can Facilitate Analysis by Embracing Minor Changes That Simplify Security Arguments and Help Modular Reasoning.**

Unlike other protocol standards, the TLS 1.3 design incorporated many suggestions from the academic community. These included not only security fixes but also changes aimed at simplifying security proofs and automated analysis. For example, modifications to the key schedule improved key separation, making modular proofs simpler. A consistent tagging scheme and the inclusion of more transcript information in exchanges also simplified consistency proofs. These changes had a negligible impact on performance and made it feasible to analyze such a complex protocol.

### Concluding Remarks

#### Recommendations to Authors

**Clarity of Trust Assumptions:**
We recommend that authors clearly delineate between trusted and untrusted parts of their artifacts. In some papers, this distinction is not always clear, leading to potentially hazy or exaggerated claims. While it can be challenging, especially when multiple tools are used, transparency and clarity in trust assumptions are crucial for progress. Beringer et al. [173] provide an excellent example of how to clearly define these distinctions. Critics should also understand that trust assumptions are often necessary for any meaningful progress.

**Use of Metrics:**
Metrics are useful for tracking progress over time when used appropriately. The HACL* [5] study effectively uses metrics, reporting proof-to-code ratios and person efforts for various primitives. Although these are crude proxies, they demonstrate, for example, that code involving bignums requires more verification effort in F*. Even if crude, appropriate metrics are better than none for advancing the field. However, inappropriate use of metrics, particularly in horizontal comparisons across disparate tools, can be misleading. Such comparisons must be done with care due to the non-trivial nature of modeling problems identically.

#### Recommendations to Tool Developers

While verified cryptography is still in its early stages, a major challenge is maintaining computer-aided cryptography artifacts. These tools are constantly evolving, often in non-backwards-compatible ways. When changes occur, we must either allow the artifacts (e.g., machine-checked proofs) to become stale or invest significant human effort to keep them up to date. Given that cryptography is a moving target, even verified implementations and their proofs will require updates, whether to add functionality or to patch new vulnerabilities. We hope to see more interplay between proof engineering research [184], [185] and computer-aided cryptography research in the coming years.

#### Recommendations to Standardization Bodies

Given the benefits observed in the TLS 1.3 standardization effort, we believe that computer-aided cryptography should play a crucial role in standardization processes [186]. Traditionally, cryptographic standards are written in a combination of prose, formulas, and pseudocode, and can change drastically between drafts. Besides ensuring the correctness of the cryptography, standards must also focus on clarity, ease of implementation, and interoperability. Standardization processes can be long and arduous, and even successful ones leave room for errors due to the gap between standards and implementations.

Security proofs can be a double-edged sword in standardization. Proposals supported by handwritten security arguments often cannot be reasonably audited. A plausible claim with an unauditable proof should not be considered more reliable than simply stating the claim, as the latter does not create a false sense of security. Examples include the Dual EC pseudo-random generator [187] and the recent attacks against the AES-OCB2 ISO standard [189].

To address these challenges, we advocate the use of computer-aided cryptography to formally certify compliance with standards and facilitate the role of auditors and evaluators. This allows discussions to focus on security claims rather than the validity of supporting arguments. The current NIST post-quantum standardization effort [190] presents an excellent opportunity to implement our recommendations, and we encourage the computer-aided cryptography community to engage in this process.

### Acknowledgments

We thank the anonymous reviewers for their valuable suggestions; Jason Gross, Boris K¨opf, Steve Kremer, Peter Schwabe, and Alwen Tiu for feedback on earlier drafts of the paper; and Tiago Oliveira for help setting up Jasmin and benchmarks.

Manuel Barbosa's work was supported by National Funds through the Portuguese Foundation for Science and Technology (FCT) under project PTDC/CCI-INF/31698/2017. Gilles Barthe's work was supported by the Office of Naval Research (ONR) under project N00014-15-1-2750. Karthik Bhargavan's work was supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement no. 683032 - CIRCUS). Bruno Blanchet's work was supported by the French National Research Agency (ANR) under project TECAP (decision no. ANR-17-CE39-0004-03). Kevin Liao's work was supported by the National Science Foundation (NSF) through a Graduate Research Fellowship. Bryan Parno's work was supported by a gift from Bosch, a fellowship from the Alfred P. Sloan Foundation, the NSF under Grant No. 1801369, and the Department of the Navy, Office of Naval Research under Grant No. N00014-18-1-2892.

### References

[1] A. Delignat-Lavaud, C. Fournet, M. Kohlweiss, J. Protzenko, A. Rastogi, N. Swamy, S. Z. B´eguelin, K. Bhargavan, J. Pan, and J. K. Zinzindohoue, “Implementing and proving the TLS 1.3 record layer,” in IEEE Symposium on Security and Privacy (S&P). IEEE Computer Society, 2017, pp. 463–482.

[2] C. Cremers, M. Horvat, S. Scott, and T. van der Merwe, “Automated analysis and verification of TLS 1.3: 0-rtt, resumption and delayed authentication,” in IEEE Symposium on Security and Privacy (S&P). IEEE Computer Society, 2016, pp. 470–485.

[3] K. Bhargavan, B. Blanchet, and N. Kobeissi, “Verified models and reference implementations for the TLS 1.3 standard candidate,” in IEEE Symposium on Security and Privacy (S&P). IEEE Computer Society, 2017, pp. 483–502.

[4] C. Cremers, M. Horvat, J. Hoyland, S. Scott, and T. van der Merwe, “A comprehensive symbolic analysis of TLS 1.3,” in ACM Conference on Computer and Communications Security (CCS). ACM, 2017, pp. 1773–1788.

[5] J. K. Zinzindohoué, K. Bhargavan, J. Protzenko, and B. Beurdouche, “HACL*: A verified modern cryptographic library,” in ACM Conference on Computer and Communications Security (CCS). ACM, 2017, pp. 1789–1806.

[6] A. Erbsen, J. Philipoom, J. Gross, R. Sloan, and A. Chlipala, “Simple high-level code for cryptographic arithmetic - with proofs, without compromises,” in IEEE Symposium on Security and Privacy (S&P). IEEE, 2019, pp. 1202–1219.

[7] J. Protzenko, B. Parno, A. Fromherz, C. Hawblitzel, M. Polubelova, K. Bhargavan, B. Beurdouche, J. Choi, A. Delignat-Lavaud, C. Fournet, N. Kulatova, T. Ramananandro, A. Rastogi, N. Swamy, C. Wintersteiger, and S. Zanella-Beguelin, “EverCrypt: A fast, verified, cross-platform cryptographic provider,” in IEEE Symposium on Security and Privacy (S&P). IEEE, 2020.

[8] M. Bellare and P. Rogaway, “The security of triple encryption and a framework for code-based game-playing proofs,” in Annual International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), ser. LNCS, vol. 4004. Springer, 2006, pp. 409–426.

[9] R. Canetti, “Universally composable security: A new paradigm for cryptographic protocols,” in IEEE Annual Symposium on Foundations of Computer Science (FOCS). IEEE Computer Society, 2001, pp. 136–145.

[10] S. Halevi, “A plausible approach to computer-aided cryptographic proofs,” IACR Cryptology ePrint Archive, vol. 2005, p. 181, 2005.

[11] K. G. Paterson and G. J. Watson, “Plaintext-dependent decryption: A formal security treatment of SSH-CTR,” in Annual International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), ser. LNCS, vol. 6110. Springer, 2010, pp. 345–361.

[12] A. Boldyreva, J. P. Degabriele, K. G. Paterson, and M. Stam, “Security of symmetric encryption in the presence of ciphertext fragmentation,” in Annual International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), ser. LNCS, vol. 7237. Springer, 2012, pp. 682–699.

[13] J. P. Degabriele, K. G. Paterson, and G. J. Watson, “Provable security in the real world,” IEEE Security & Privacy, vol. 9, no. 3, pp. 33–41, 2011.

[14] V. Shoup, “Sequences of games: a tool for taming complexity in security proofs,” IACR Cryptology ePrint Archive, vol. 2004, p. 332, 2004. [Online]. Available: http://eprint.iacr.org/2004/332

[15] Y. Lindell, “How to simulate it - A tutorial on the simulation proof technique,” in Tutorials on the Foundations of Cryptography. Springer International Publishing, 2017, pp. 277–346.

[16] S. F. Doghmi, J. D. Guttman, and F. J. Thayer, “Searching for shapes in cryptographic protocols,” in International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS), ser. LNCS, vol. 4424. Springer, 2007, pp. 523–537.

[17] J. Bengtson, K. Bhargavan, C. Fournet, A. D. Gordon, and S. Maffeis, “Refinement types for secure implementations,” ACM Trans. Program. Lang. Syst., vol. 33, no. 2, pp. 8:1–8:45, 2011.

[18] M. Backes, C. Hritcu, and M. Maffei, “Union, intersection and refinement types and reasoning about type disjointness for secure protocol implementations,” J. Comput. Secur., vol. 22, no. 2, pp. 301–353, Mar. 2014.

[19] S. Escobar, C. A. Meadows, and J. Meseguer, “Maude-npa: Cryptographic protocol analysis modulo equational properties,” in Foundations of Security Analysis and Design (FOSAD), ser. LNCS, vol. 5705. Springer, 2007, pp. 1–50.

[20] B. Blanchet, “Modeling and verifying security protocols with the applied pi calculus and ProVerif,” Foundations and Trends in Privacy and Security, vol. 1, no. 1–2, pp. 1–135, Oct. 2016.

[21] K. Bhargavan, C. Fournet, A. D. Gordon, and S. Tse, “Verified interoperable implementations of security protocols,” ACM Transactions on Programming Languages and Systems, vol. 31, no. 1, 2008.

[22] V. Cheval, V. Cortier, and M. Turuani, “A little more conversation, a little less action, a lot more satisfaction: Global states in ProVerif,” in IEEE Computer Security Foundations Symposium (CSF). IEEE Computer Society, 2018, pp. 344–358.

[23] D. L. Li and A. Tiu, “Combining ProVerif and automated theorem provers for security protocol verification,” in International Conference on Automated Deduction (CADE), ser. LNCS, vol. 11716. Springer, 2019, pp. 354–365.

[24] M. Arapinis, E. Ritter, and M. D. Ryan, “StatVerif: Verification of stateful processes,” in IEEE Computer Security Foundations Symposium (CSF). IEEE Computer Society, 2011, pp. 33–47.

[25] C. J. F. Cremers, “The Scyther tool: Verification, falsification, and analysis of security protocols,” in International Conference on Computer-Aided Verification (CAV), ser. LNCS, vol. 5123. Springer, 2008, pp. 414–418.

[26] S. Meier, C. J. F. Cremers, and D. A. Basin, “Strong invariants for the efficient construction of machine-checked protocol security proofs,” in IEEE Computer Security Foundations Symposium (CSF). IEEE Computer Society, 2010, pp. 231–245.

[27] S. Meier, B. Schmidt, C. Cremers, and D. A. Basin, “The TAMARIN prover for the symbolic analysis of security protocols,” in International Conference on Computer-Aided Verification (CAV), ser. LNCS, vol. 8044. Springer, 2013, pp. 696–701.

[28] S. Kremer and R. Künne