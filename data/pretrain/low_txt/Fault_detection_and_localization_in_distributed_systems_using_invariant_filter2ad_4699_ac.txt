Certainly! Here is a revised and optimized version of the provided text, with improved clarity, coherence, and professionalism:

---

**Introduction to Methodology and Case Study**

The decision to inject faults into a separate dataset was influenced by the lack of ground truth. In two other similar datasets, we observed broken invariants even without injecting anomalies. Without ground truth, it is unclear whether these broken invariants are due to actual anomalies or measurement noise. This methodology allows us to clearly explain why an invariant is broken, thereby stress-testing various aspects of our algorithm. A real-world case study is discussed in Section VII-A.

**Anomaly Injection and Evaluation**

We injected several instances of three types of anomalies into multiple metrics. We first present the results for the noise-free case, where only the injected anomalies cause broken invariants, followed by the results for the noisy case.

### Anomalies without Noise

To inject spikes, we selected three metrics: m1 with a high degree of 16 (i.e., many invariant relationships), m2 with a moderate degree of 7, and m3 with only one invariant relationship. We randomly injected 10 spikes into each metric. Table I shows the accuracy of nodeScore alone and in combination with neighborScore, i.e., the two variants of the spatial approach—spatial avg. takes an average of the two scores, and spatial rank combines their rankings.

A spike anomaly in m1 at time t leads to broken invariants during the interval [t, t+4]. Table I indicates the number of time points (out of 4) for which metric m1 is ranked highest by each algorithm. All three methods can localize the anomaly to metric m1 for all 10 instances. In two cases (t = 146 and t = 274), m1 does not get the top rank because metric m2 is also affected by a spike anomaly and is ranked first. Similar results were observed for spike anomalies in m2 and m3. It is noteworthy that the highest anomaly score does not always occur at the time of injection; for m1, there is a shift by 2 samples. Our temporal algorithm localizes the spike anomaly to both the metric m1 and the time point of injection, except for cases where two metrics are affected. The temporal algorithm achieves this after observing all broken invariants over a window of samples, e.g., for an anomaly at t = 146, it considers the interval [146, 149]. This highlights a trade-off between our spatial and temporal approaches: the spatial approach can raise early alarms but may not pinpoint the exact time of the fault, while the temporal approach provides more accurate time localization but with a delay.

### Anomalies with Noise

To simulate measurement noise, we randomly select a few metrics and mark a certain fraction of their invariants as broken, ensuring these metrics are different from those with injected anomalies. Thus, we have two categories of broken invariants: those due to anomalies and those due to noise, referred to as noisy broken invariants.

For the spike anomaly case, we added noisy broken invariants for three metrics, considering scenarios where 10% and 50% of a metric’s invariants are broken due to noise. Our metric for comparing different methods is their precision when recall is 1. Each method assigns a score to each metric with broken invariants (higher score indicates more abnormality) and ranks them in descending order. In practice, operators review the list of top-ranked metrics until they identify the abnormal metric. The ideal scenario is when the anomalous metric is ranked first. For a method that ranks the anomalous metric kth, the recall is 1 and the precision is 1/k.

Table II shows the average precision across 10 instances of spike anomalies in metric m1 for different methods. Qualitatively similar results were obtained for other metrics, but these are omitted due to space constraints. nodeScore performs the worst because it evaluates metrics in isolation and cannot distinguish between invariants broken due to anomalies and noise. This situation worsens with increased noise impact, as the precision for nodeScore drops to 0.46 and 0.4 for 10% and 50% noisy broken invariants, respectively. In contrast, the invariant graph-based spatial algorithms, spatial avg. and spatial rank, maintain reasonable precision even at 50% noisy broken invariants, with spatial avg. outperforming spatial rank. This result aligns with Lee’s observation in document retrieval that combining scores yields better performance than combining ranks. Our spatial algorithms are more robust to noise because they combine the "view" at a node (nodeScore) with the view across its one-hop neighborhood (neighborScore), reducing the likelihood of metrics affected by random noise being ranked as abnormal.

Our temporal approach does not perform as well as the spatial algorithms, primarily due to incorrect time localization of the anomaly. For example, for a spike anomaly at t = 272 with noisy broken invariants, t = 275 has the highest anomaly score, resulting in a precision of 0.1. In contrast, if t = 272 is identified as the onset of the anomaly, the precision is 1. The results in Table II for the temporal approach are averaged across all time points with broken invariants for a fair comparison with nodeScore and spatial algorithms. These results highlight a significant challenge in using our temporal approach: it is effective if the time of the anomaly is known, but less so with noise. However, the temporal approach does not require very precise time localization to be effective. For instance, the spike anomaly at t = 272 causes broken invariants during [272, 275], and even with noisy broken invariants, the precision of the temporal algorithm, averaged over [272, 274], is 0.75 (dropping below 0.6 when t = 275 is included). Improving the robustness of our temporal algorithm is part of our future work.

### Case Study

We applied our spatial and temporal fault localization algorithms to analyze real-world data from 20 sensors containing faults. The dataset is divided into training and test sets, each consisting of one time series with 2800 samples per sensor. The training set is free of anomalies, while the test set contains unknown anomalies. We extracted 39 invariants from the training dataset, and Figure 1(a) shows the invariant graph. For the test data, we observed three intervals with multiple broken invariants, as shown in Table III, along with the maximum number of broken invariants and the number of associated metrics. For the instance in row 1 of Table III, metrics m1, m2, and m3 have broken invariants. Their nodeScores are 0.28, 0.17, and 0.17, respectively (m1 has 2 broken invariants, while m2 and m3 have one each). While none of the metrics have high nodeScores, m1 has a high neighborScore of 1 (m2 and m3 have neighborScores of 0). Therefore, the spatial algorithms—spatial avg. and spatial rank—identify m1 as the most abnormal metric. Further evidence for a fault in m1 comes from the consistent high ranking of m1 in the other two instances (rows 2 and 3 in Table III), despite the presence of new metrics and broken invariants. Figure 1(b) shows the broken invariants at one of the time points from row 2 in Table III. Manual inspection of m1 samples during these periods revealed an average value of 7.2, 17.6, and 16.2 compared to 4.9 for the entire time series, indicating a shift-by-constant type of fault. This detection, localization, and characterization of the fault were verified with the customer who provided the dataset.

**Related Work**

We briefly discuss work on rank aggregation and graph-based anomaly detection that informed our algorithms for fault localization using SIAT.

**Rank Aggregation**

Research in information retrieval and metasearch for the web [10, 4, 15] inspired our spatial algorithms. Global ranking problems also arise in recommendation systems and voting, often involving the computation of a global rank using quantitative or qualitative preferences. With quantitative scores, combination methods similar to our metaranking approach and others used in information retrieval [15] can be applied. Ranking algorithms based on qualitative preferences [3] are not directly applicable to SIAT, which uses quantitative scores.

**Graph-Based Anomaly Detection**

Noble et al. propose a method for detecting anomalous subgraphs in a given graph [17], useful for detecting insider threats [11] and software bugs [16]. Our neighborScore is computed based on the subgraph of one-hop neighbors of a node, but our goal is to identify abnormal nodes, not subgraphs.

**Conclusion**

We presented an overview of our tool SIAT for modeling dependencies in time series monitoring data from distributed systems and described real-world problems addressed using it. We introduced two algorithms—one based on invariant graphs and the other on temporal patterns—to solve the metric ranking and noise reduction challenges encountered in real-world data analysis with SIAT. A key contribution of this paper is showing that by using pairwise invariant relationships among time series monitoring data, we can transform the task of fault/anomaly localization in distributed systems into a metric ranking problem on invariant graphs in the presence of noise.

**References**

[1] GE Intelligent Platforms. http://www.ge-ip.com.
[2] Splunk Inc. http://www.splunk.com.
[3] A. Ammar and D. Shah. Efficient Rank Aggregation Using Partial Data. In SIGMETRICS, 2012.
[4] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic Combination of Multiple Ranked Retrieval Systems. In SIGIR, 1994.
[5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly Detection: A Survey. ACM Computing Surveys, 14(3), 2009.
[6] R. N. Charette. This Car Runs on Code. IEEE Spectrum, Feb. 2009.
[7] H. Chen, H. Cheng, G. Jiang, and K. Yoshihira. Exploiting Local and Global Invariants for the Management of Large-Scale Information Systems. In ICDM, 2008.
[8] H. Chen, G. Jiang, K. Yoshihira, and A. Saxena. Invariants Based Failure Diagnosis in Distributed Computing Systems. In IEEE SRDS, 2010.
[9] M. Ding, H. Chen, A. B. Sharma, K. Yoshihira, and G. Jiang. A Data Analytic Engine Towards Self-Management of Cyber-Physical Systems. In ICDCS Workshop on Cyber-Physical Networked Systems, 2013.
[10] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank Aggregation Methods for the Web. In WWW, 2001.
[11] W. Eberle and L. B. Holder. Applying Graph-Based Anomaly Detection Approaches to the Discovery of Insider Threats. In IEEE Intelligence and Security Informatics, 2009.
[12] G. Box, G. M. Jenkins, and G. Reinsel. Time Series Analysis: Forecasting and Control. John Wiley & Sons, Inc., 2008.
[13] G. Jiang, H. Chen, and K. Yoshihira. Discovering Likely Invariants of Distributed Transaction Systems for Autonomic System Management. In ICAC, 2006.
[14] G. Jiang, H. Chen, and K. Yoshihira. Efficient and Scalable Algorithms for Inferring Invariants in Distributed Systems. IEEE Transactions on Knowledge and Data Engineering, 19(11):1508–1523, 2007.
[15] J. H. Lee. Analyses of Multiple Evidence Combination. In SIGIR, 1997.
[16] C. Liu, X. Yan, H. Yu, J. Han, and P. S. Yu. Mining Behavior Graphs for Backtrace of Noncrashing Bugs. In ICDM, 2005.
[17] C. Noble and D. Cook. Graph-Based Anomaly Detection. In KDD, 2003.
[18] A. B. Sharma, L. Golubchik, and R. Govindan. Sensor Data Faults: Detection Methods and Prevalence in Real-World Datasets. Trans. on Sensor Networks, August 2010.

---

This revision aims to make the text more structured, clear, and professional, while maintaining the original content and intent.