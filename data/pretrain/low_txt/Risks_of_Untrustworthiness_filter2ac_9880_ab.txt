### Masking the Reality of Near-Meltdown

The incident in question involved temperatures exceeding 4,000 degrees, a near-meltdown that was not immediately apparent.

### Medical Care and Software Failures

One of the most frequently cited examples of a software failure in a medical device is the Therac-25 accelerator. This device resulted in several deaths due to a race condition in a non-atomic transaction when switching from high-intensity research mode to low-intensity therapeutic mode [10]. In the earlier model, the Therac-20, a physical interlock was present in the hardware, but it was mistakenly assumed to have been implemented in software for the Therac-25. At the other end of the technology spectrum, a heart-monitoring device with a standard electrical-socket wall plug instead of a jack came loose. A cleaning person instinctively plugged it into the wall socket rather than the monitor, resulting in the patient's electrocution. Recent reports also highlight new cases of operations on the wrong patient due to mistaken or misinterpreted computer data, erroneous test results, and mode-change faults in glucose-monitoring devices, among others.

### Safety Risks in Untrustworthy Systems

For these and many other application areas, the safety risks associated with untrustworthy systems are significant. Additionally, risks can arise in supposedly safe systems concerning security, privacy, reliability, system survivability, and graceful degradation. Developing safe systems requires much greater care than is typically devoted to standard software. For instance, the work by Leveson [7, 8, 9] provides serious approaches to enhancing safety, which could inspire R&D in trustworthiness for secure applications.

### Continued Occurrence of Safety-Related Accidents

Safety-related accidents continue to occur, particularly in air, rail, and medical applications. These incidents are often caused by hardware/software malfunctions and errors by controllers, pilots, and operators. In hindsight, some of these accidents could have been prevented with better human interfaces, cross-checking, adequate staffing, preventive diagnostics and maintenance, training, and pervasive oversight.

### Unsecure Systems and Security Vulnerabilities

Next, we consider the problems of unsecure systems, drawing insights from the previous sections. Documenting historical and recent security vulnerabilities and their exploitations is essential, even if it may seem repetitive to some. Common risks include penetrations by outsiders, misuse by insiders, spam and phishing attacks, and distributed denial-of-service (DDoS) attacks. Vulnerable applications range from financial systems with potential for fraud and undetected errors to databases with opportunities for identity theft and privacy violations. Critical national infrastructures and electronic voting systems, which require system integrity and voter privacy, are also at risk.

### Program Flaws and Security Failures

Buffer overflows, bounds checks, type mismatches, and other program flaws continue to cause security failures. Numerous efforts have been made to provide taxonomies for such problems [1, 6, 18, 22], as well as tools like static analysis to detect characteristic flaws. However, disciplined software development and systematic use of analysis tools are required. There is a significant need for well-designed and well-implemented trustworthy systems that can meet a broad set of security requirements. Historically, system integrity has often been subordinated to confidentiality, and accountability has remained largely overlooked. Preventing denials of service is still widely ignored.

### Total-System Perspective

From a total-system perspective, it would be highly desirable for systems designed for security to also satisfy other requirements for trustworthiness. For example, a system that is secure but unreliable may no longer be secure when it becomes unreliable. Similarly, a system that is not predictably survivable under certain environmental disruptions may become unsecure when transformed into fail-safe or degraded operation. The concept of fail-secure systems presents significant challenges.

### Conclusions

Given the broad scope of the problems discussed, including diverse causes and effects, far-reaching measures are needed to prevent or detect the likelihood of risks related to untrustworthiness. Prevention and remediation must encompass a better understanding of the full range of requirements and better system architectures that explicitly address those trustworthiness requirements with appropriate assurance, usability, operation, and maintainability. Greater attention needs to be devoted to the software engineering disciplines for implementing trustworthy applications, either based on underlying trustworthy infrastructures or able to architecturally surmount some untrustworthiness in subsystems.

Additionally, these concepts must be integrated into the practice of developers and operational staff through enlightened education and training. Corporate altruism and intelligent government action could also be valuable. Trustworthiness demands a pervasive sense of systems in their entirety, considering long-term risks in the global context of all relevant applications relative to the totality of all relevant requirements. Life-critical and supposedly secure systems should be held to higher standards than conventional software, although criteria and evaluations tend to be less rigorous. Myopia is dangerous with respect to trustworthiness, and a massive culture shift is needed to proactively develop and compositionally evaluate systems in their entirety to assure their operational configurations and usability.

### Acknowledgments

The author thanks Douglas Maughan, who sponsored reference [14] when he was a Program Manager at the Defense Advanced Research Projects Agency (DARPA). Many of the concepts discussed there on how to develop composable high-assurance trustworthy systems and networks are relevant to avoiding risks such as those discussed here. This paper was prepared in part under National Science Foundation Grant Number 0524111.

### References

[1] R.P. Abbott et al. Security analysis and enhancements of computer operating systems. Technical report, National Bureau of Standards, 1974. Order No. S-413558-74.
[2] K. Borg. Re: LA power outages. ACM Forum, Risks, 24(39), 22 August 2006. http://catless.ncl.ac.uk/Risks/24.39.html#subj8.
[3] R.J. Feiertag and P.G. Neumann. The foundations of a Provably Secure Operating System (PSOS). In Proceedings of the National Computer Conference, pages 329–334. AFIPS Press, 1979. http://www.csl.sri.com/neumann/psos.pdf.
[4] D. Ford. Three Mile Island: Thirty Minutes to Meltdown. Viking Press, 1982. Sensor-related quote reproduced in ACM SIGSOFT Software Engineering Notes, 11, 3, 9–10, July 1986.
[5] J. Garman. The bug heard 'round the world. ACM SIGSOFT Software Engineering Notes, 6(5):3–10, October 1981.
[6] C.E. Landwehr, A.R. Bull, J.P. McDermott, and W.S. Choi. A taxonomy of computer program security flaws, with examples. Technical report, Center for Secure Information Technology, Information Technology Division, Naval Research Laboratory, Washington, D.C., November 1993.
[7] N.G. Leveson. Safeware: System Safety and Computers. Addison-Wesley, Reading, Massachusetts, 1995.
[8] N.G. Leveson. A new accident model for engineering safer systems. Safety Science (Elsevier), 42(4):237–270, April 2004.
[9] N.G. Leveson. A systems-theoretic approach to safety in software-intensive systems. IEEE Trans. on Dependable and Secure Computing, 1(1), January 2005.
[10] N.G. Leveson and C. Turner. An investigation of the Therac-25 accidents. Computer, pages 18–41, July 1993.
[11] R. Mercuri. Electronic Vote Tabulation Checks and Balances. PhD thesis, Department of Computer Science, University of Pennsylvania, 2001. http://www.notablesoftware.com/evote.html.
[12] P.G. Neumann. Illustrative risks to the public in the use of computer systems and related technology. Technical report, Computer Science Laboratory, SRI International, Menlo Park, California. Updated regularly at http://www.csl.sri.com/neumann/illustrative.html; also in .ps and .pdf form for printing in a denser format.
[13] P.G. Neumann. Computer-Related Risks. ACM Press, New York, and Addison-Wesley, Reading, Massachusetts, 1995.
[14] P.G. Neumann. Principled assuredly trustworthy composable architectures. Technical report, Computer Science Laboratory, SRI International, Menlo Park, California, December 2004. http://www.csl.sri.com/neumann/chats4.html, .pdf, and .ps.
[15] P.G. Neumann. System and network trustworthiness in perspective. In Proceedings of the Thirteenth ACM Conference on Computer and Communications Security (CCS), Alexandria, Virginia, November 2006.
[16] P.G. Neumann, R.S. Boyer, R.J. Feiertag, K.N. Levitt, and L. Robinson. A Provably Secure Operating System: The system, its applications, and proofs. Technical report, Computer Science Laboratory, SRI International, Menlo Park, California, May 1980. 2nd edition, Report CSL-116.
[17] P.G. Neumann and R.J. Feiertag. PSOS revisited. In Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003), Classic Papers section, pages 208–216, Las Vegas, Nevada, December 2003. IEEE Computer Society. http://www.acsac.org/ and http://www.csl.sri.com/neumann/psos03.pdf.
[18] P.G. Neumann and D.B. Parker. A summary of computer misuse techniques. In Proceedings of the Twelfth National Computer Security Conference, pages 396–407, Baltimore, Maryland, 10–13 October 1989. NIST/NCSC.
[19] L. Robinson and K.N. Levitt. Proof techniques for hierarchically structured programs. Communications of the ACM, 20(4):271–283, April 1977.
[20] E. Rosen. Vulnerabilities of network control protocols. ACM SIGSOFT Software Engineering Notes, 6(1):6–8, January 1981.
[21] A. Rubin. Brave New Ballot. Random House, 2006.
[22] K. Tsikpenyuk, B. Chess, and G. McGraw. Seven pernicious kingdoms: A taxonomy of software security errors. IEEE Security and Privacy, 3(6), November-December 2005.