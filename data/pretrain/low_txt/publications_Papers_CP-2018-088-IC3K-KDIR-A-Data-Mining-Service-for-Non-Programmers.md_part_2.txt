### Summary of Workflow Tasks and Netflix Conductor Technology

The six types of tasks that can be used in the workflow, sent by users, are as follows:

1. **Dataset Input**: A unique task where the user specifies the dataset to use. The user can also choose to remove features during this step.
2. **Validation Procedure**: Contains tasks that specify a method to be used in the creation of the model.

By using Netflix Conductor technology, we can organize these tasks in a specific sequence. Data Science services can pull the scheduled tasks and work on them in parallel and independently, following a competing consumers pattern (Hohpe and Woolf, 2003). Netflix Conductor ensures that tasks appearing ahead in a workflow's path are executed only after the prior tasks have been completed.

### Data Science Services and Communication

Data Science Services are multiple fine-grained services/workers that handle specific data science tasks pulled from the Conductor Service. These services share files (e.g., datasets, models) between them by writing and reading to/from the Network File System (NFS).

Communication between all the services in the architecture is performed using the HTTP protocol, mainly through REST APIs. All the services can be scaled out independently.

### Example of Workflow Translation

To better understand how individual data science tasks are processed in the system, Figure 4 presents an example of a translation from a sequential workflow sent by the user (on the left) to its representation in Netflix Conductor (on the right). This translation abstracts users from the complexity of creating complex workflows, which is an advantage over other systems such as Azure ML Studio, as mentioned above.

**Figure 4: Example of a Data Mining Workflow Translation**

- **Sequential Workflow Sent by User:**
  - Dataset Input
  - Split Dataset (Train-Test)
  - Feature Scaling
  - SVM Creation
  - Feature Scaling
  - SVM Prediction
  - Classification Performance Calculation

- **Netflix Conductor Representation:**
  - Split Dataset (split original data into training and test sets)
  - Feature Scaling (applied to the training set)
  - SVM Creation (applied to the processed training set)
  - Feature Scaling (applied to the testing set and using info from the previous feature scaling task)
  - SVM Prediction (applied to the processed test set and using the model created before)
  - Classification Performance Calculation

### Experiments

#### 4.1 Usability Tests

##### 4.1.1 Setup

Usability tests played a crucial role in evaluating the prototype and validating the paradigm of constructing data mining processes using sequential tasks. The tests involved users executing a few exercises using the interface and providing feedback. This feedback was used to evaluate the user experience, the usability of the interface, and the value provided to them, thereby validating the concept.

We divided the users into two types:
- **Type A**: Users with no experience with data mining systems and no knowledge of data mining or programming languages (8 users).
- **Type B**: Users with experience in data mining systems (mainly Orange), with knowledge in data mining but without programming skills (11 users).

##### 4.1.2 Results

The usability tests started with a quick overview of the platform and its functionalities, which took less than 3 minutes. After this introduction and question answering, we gave the users a script with a few exercises estimated to be solved in less than 20 minutes. In the end, we provided a questionnaire for the users to fill out about their experience and their thoughts on the relevance of the system.

To keep the tests simple, we asked the users to perform six exercises using the iris flower dataset (Anderson, 1936). The exercises were simple and interconnected, giving the user a sense of progress during execution.

Briefly, the exercises were:
1. Scale the attributes of the dataset between the values 0 and 1.
2. Create an SVM model and use the hold-out procedure to assess the model performance. Also, verify the accuracy and F-measure of the produced model.
3. Same exercise as before, including a feature scaling operation before model creation.
4. Perform feature selection using the ReliefF algorithm and different numbers of features to see which attributes would have the most predictive capabilities.
5. Build an SVM model preceded by feature scaling using the two best features discovered in the previous exercise and use cross-validation to validate the model.

After performing the tests, we asked the users to fill out a questionnaire, which allowed us to gauge their satisfaction with the interface, their experience using the tool, and whether they found it useful. Each statement could be answered as: totally disagree, disagree, indifferent, agree, and totally agree. To analyze the average response and standard deviation, we converted the answers to numbers, where 1 translates to "totally disagree" and 5 to "totally agree."

As seen in Figure 5, the values are all above average. The most satisfactory results were that users found the interface easy to use, they would recommend it to colleagues, and they would use it again to solve related problems. The attractiveness of the interface, although very positive, scored lower than the other metrics, which was expected since this is a prototype and that part was not a priority. The results from Type A users were lower than those from Type B, indicating that users with no experience (Type A) had more difficulty using the interface, which was expected. Surprisingly, they found it easier to find the required functionalities and the design simpler to understand.

**Figure 5: Average and Standard Deviation of the Users’ Responses**

#### 4.2 Computational Performance Tests

Basic preliminary computational performance tests were conducted to assess how the system would behave with the current architecture. We executed tests using two randomly generated numerical datasets with a binary response class: Dataset 1 containing 10,000 rows and 1,001 columns (34.2 MB) and Dataset 2 with 20,000 rows and 1,001 columns (68.4 MB).

Using each dataset, we created 10 times a Naïve Bayes model and evaluated its classification performance using 10-fold cross-validation. As a baseline, we performed the same experiments with H2O deployed in an equal cluster.

The results can be seen in Figure 6. It can be seen that our system is slower in the preliminary tests, but this was expected, as we are storing intermediate results in a centralized disk using NFS, while H2O stores them in memory. We will address this issue in the future.

**Figure 6: Tests Performed with Our System and H2O**

### Conclusion

We presented a service for non-programmers to perform data mining experiments employing good machine learning/data mining practices. We prototyped a cloud application following a microservices architecture with an interface aimed at achieving high usability metrics.

### Acknowledgements

This work was carried out under the project PTDC/EEI-ESS/1189/2014 Data Science for Non-Programmers, supported by COMPETE 2020, Portugal 2020-POCI, UE-FEDER, and FCT.

### References

- Anderson, E. (1936). The species problem in Iris. Annals of the Missouri Botanical Garden, 23:457–509.
- Cao, L. (2017). Data science: A comprehensive overview. ACM Comput. Surv., 50(3):43:1–43:42.
- Cawley, G. C., & Talbot, N. L. (2010). On over-fitting in model selection and subsequent selection bias in performance evaluation. Journal of Machine Learning Research, 11(Jul):2079–2107.
- Han, J., Pei, J., & Kamber, M. (2011). Data mining: Concepts and techniques. Elsevier.
- Hastie, T., Tibshirani, R., & Friedman, J. (2001). The Elements of Statistical Learning. Springer Series in Statistics. Springer New York Inc., New York, NY, USA.
- Henke, N., Bughin, J., Chui, M., Manyika, J., Saleh, T., Wiseman, B., & Sethupathy, G. (2016). The age of analytics: Competing in a data-driven world. McKinsey Global Institute, 4.
- Hohpe, G., & Woolf, B. (2003). Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
- Kranjc, J., Ora, R., Podpean, V., Lavra, N., & Robnik-Sikonja, M. (2017). Clowdflows: Online workflows for distributed big data mining. Future Generation Computer Systems, 68:38–58.
- Medvedev, V., Kurasova, O., Bernataviien, J., Treigys, P., Marcinkeviius, V., & Dzemyda, G. (2017). A new web-based solution for modelling data mining processes. Simulation Modelling Practice and Theory, 76:34–46. High-Performance Modelling and Simulation for Big Data Applications.
- Miller, S., & Hughes, D. (2017). The quant crunch: How the demand for data science skills is disrupting the job market. Burning Glass Technologies.
- Nielsen, J. (1994). Enhancing the explanatory power of usability heuristics. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '94, pages 152–158, New York, NY, USA. ACM.