### 3. Rendering Quality and Browser Performance

Less popular browsers often exhibit lower rendering quality. When we limit our analysis to chunks with good performance (i.e., a frame rate greater than 1.5 frames per second) and where the player is visible (i.e., `vis = true`), the rendering quality can still be poor due to inefficiencies in the client's rendering path. Since we cannot measure the client host environment in production, we characterize clients based on their operating system (OS) and browser.

**Figure 21** illustrates the fraction of chunks requested from browsers on macOS and Windows platforms (each platform is normalized to 100%), along with the average fraction of dropped frames for each browser. Browsers with internal Flash support (e.g., Chrome) and native HLS support (e.g., Safari on macOS) outperform other browsers, some of which may run Flash as a separate process (e.g., Firefox in protected mode). Unpopular browsers (grouped as "Other") have the lowest performance. A more detailed breakdown is provided in **Figure 22**, which focuses on browsers that have processed at least 500 chunks. Yandex, Vivaldi, Opera, and Safari on Windows show significantly lower rendered frame rates compared to other browsers.

**Key Takeaways:**
- De-multiplexing, decoding, and rendering video chunks can be resource-intensive on the client machine.
- In the absence of hardware (GPU) rendering, the CPU must efficiently process frames, but the demands from other applications on the host can affect rendering quality.
- We found that video rendering requires processing time, and a video arrival rate of 1.5 frames per second can serve as a rule-of-thumb for achieving good rendering quality.
- Similar to download stack issues, rendering quality varies based on the OS and browser, with unpopular browsers generally having lower rendering quality.

### 5. Discussion

Monitoring and diagnosing large-scale content providers is challenging due to insufficient instrumentation or measurement overhead limitations. Specifically:
1. **Sub-chunk events** such as bursty losses are not captured in per-chunk measurements; capturing them would impact player performance.
2. **Smoothed Round-Trip Time (SRTT)** does not reflect the actual round-trip time at the time of measurement but rather a smoothed average. Vanilla Linux kernels only export SRTTs to userspace today. To address this, we use methods discussed in Section 4.2.
3. **Characterization of the rendering path** could be improved by capturing underlying resource utilization and environment (e.g., CPU load, GPU presence).
4. **In-network measurements** help further localization. For example, characterizing network problems (e.g., whether bandwidth is limited at the core or edge) would be possible using active probes (e.g., traceroute or ping) or in-network measurements from ISPs (e.g., link utilization). Some of these measurements may not be feasible at web scale.

### 6. Related Work

#### Video Streaming Characterization
There is extensive research on characterizing video streaming quality. [28] uses ISP packet traces to analyze video, while [36] uses CDN-side data to study content and Live vs. VoD access patterns. [22] employs client-side data and clustering to identify critical problems related to user’s ISP, CDN, or content provider. [12] characterizes popularity in user-generated content video systems.

Our work differs from previous studies by collecting and integrating fine-grained per-chunk measurements from both sides and directly instrumenting the video delivery path, including the client’s download stack and rendering path.

#### QoE Models
Studies such as [14] have shown correlations between video quality metrics and user engagement. [25] demonstrates the impact of video quality on user behavior using quasi-experiments. [31] uses network data from commercial IPTV to learn performance indicators for user QoE, while [8] uses in-network measurements to estimate QoE for mobile users. We leverage prior work on QoE models to extract metrics that matter more to clients (e.g., re-buffering and startup delay) and study their impact on performance.

#### ABR Algorithms
Bitrate adaptation algorithms have been well-studied. [15] examines the interactions between HTTP and TCP, while [9] compares different algorithms in terms of sustainability and adaptation. Various algorithms have been proposed to optimize video quality, such as rate-based adaptation in [23, 32], buffer-based approaches in [20], and hybrid models in [37]. Our work complements these studies by showing that even with an optimized ABR, corrective actions from the content provider may still be necessary.

#### Optimizing Video Quality by CDN Selection
Previous work suggests different methods for CDN selection to optimize video quality. [33] studies policies and methods used for server selection in YouTube, while [24] investigates causes of inflated latency for better CDN placement. Some studies [26, 18, 17] advocate for centralized video control planes to dynamically optimize video delivery, and [10] argues for federated and P2P CDNs based on content, regional, and temporal shifts in user behavior.

### 7. Conclusion

In this paper, we present the first web-scale end-to-end measurement study of internet video streaming to characterize problems at a large content provider’s CDN, Internet, and the client’s download and rendering paths. Instrumenting the end-to-end path provides a unique opportunity to examine multiple components during a session at per-chunk granularity, enabling the discovery of transient and persistent issues affecting video streaming. We characterize several important aspects of video streaming services, including causes of persistent problems at CDN servers, sources of high network latency, and persistent rendering issues caused by browsers. We also gain insights into the client’s download stack latency (possible only via end-to-end instrumentation) and show that the download stack can impact QoE and feed incorrect information to the ABR algorithm. We discuss the implications of our findings for content providers (e.g., pre-fetching subsequent chunks), ISPs (establishing better peering points), and ABR logic (e.g., using a priori observations about client prefixes).

### Acknowledgments

This work benefited from the contributions of many engineers from the Video Platforms Engineering group at Yahoo. We thank the group for their assistance with instrumentation and for patiently answering our questions. Special thanks to P.P.S. Narayan for supporting this project at Yahoo. This work was partially supported by the National Science Foundation grant CNS-1162112.

### References

[1] ActionScript 3.0 reference for the Adobe Flash. http://help.adobe.com/en_US/FlashPlatform/reference/actionscript/3/flash/net/FileReference.html.

[2] Apache Traffic Server. http://trafficserver.apache.org.

[3] It’s latency, stupid. https://rescomp.stanford.edu/~cheshire/rants/Latency.html.

[4] Open read retry timer. https://docs.trafficserver.apache.org/en/4.2.x/admin/http-proxy-caching.en.html#open-read-retry-timeout.

[5] Sandvine: Global Internet phenomena report 2015. https://www.sandvine.com/trends/global-internet-phenomena/.

[6] YouTube statistics. https://www.youtube.com/yt/press/statistics.html.

[7] AGGARWAL, A., SAVAGE, S., AND ANDERSON, T. Understanding the performance of TCP pacing. In IEEE INFOCOM (2000), pp. 1157–1165.

[8] AGGARWAL, V., HALEPOVIC, E., PANG, J., VENKATARAMAN, S., AND YAN, H. Prometheus: Toward quality-of-experience estimation for mobile apps from passive network measurements. In Workshop on Mobile Computing Systems and Applications (2014), pp. 18:1–18:6.

[9] AKHSHABI, S., BEGEN, A. C., AND DOVROLIS, C. An experimental evaluation of rate-adaptation algorithms in adaptive streaming over HTTP. In ACM Conference on Multimedia Systems (2011), pp. 157–168.

[10] BALACHANDRAN, A., SEKAR, V., AKELLA, A., AND SESHAN, S. Analyzing the potential benefits of CDN augmentation strategies for internet video workloads. In IMC (2013), pp. 43–56.

[11] BRESLAU, L., CAO, P., FAN, L., PHILLIPS, G., AND SHENKER, S. Web caching and Zipf-like distributions: Evidence and implications. In IEEE INFOCOM (1999), pp. 126–134.

[12] CHA, M., KWAK, H., RODRIGUEZ, P., AHN, Y.-Y., AND MOON, S. I tube, you tube, everybody tubes: Analyzing the world’s largest user generated content video system. In IMC (2007), pp. 1–14.

[13] DIMOPOULOS, G., LEONTIADIS, I., BARLET-ROS, P., PAPAGIANNAKI, K., AND STEENKISTE, P. Identifying the root cause of video streaming issues on mobile devices. In CoNext (2015).

[14] DOBRIAN, F., SEKAR, V., AWAN, A., STOICA, I., JOSEPH, D., GANJAM, A., ZHAN, J., AND ZHANG, H. Understanding the impact of video quality on user engagement. In ACM SIGCOMM (2011), pp. 362–373.

[15] ESTEBAN, J., BENNO, S. A., BECK, A., GUO, Y., HILT, V., AND RIMAC, I. Interactions between HTTP adaptive streaming and TCP. In Workshop on Network and Operating System Support for Digital Audio and Video (2012), pp. 21–26.

[16] FREEDMAN, M. J., VUTUKURU, M., FEAMSTER, N., AND BALAKRISHNAN, H. Geographic locality of IP prefixes. In IMC (2005), pp. 13–13.

[17] GANJAM, A., JIANG, J., LIU, X., SEKAR, V., SIDDIQI, F., STOICA, I., ZHAN, J., AND ZHANG, H. C3: Internet-scale control plane for video quality optimization. In USENIX NSDI (2015), pp. 131–144.

[18] GEORGOPOULOS, P., ELKHATIB, Y., BROADBENT, M., MU, M., AND RACE, N. Towards network-wide QoE fairness using OpenFlow-assisted adaptive video streaming. In ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking (2013), pp. 15–20.

[19] GHOBADI, M., CHENG, Y., JAIN, A., AND MATHIS, M. Trickle: Rate limiting YouTube video streaming. In USENIX Annual Technical Conference (2012), pp. 17–17.

[20] HUANG, T.-Y., JOHARI, R., MCKEOWN, N., TRUNNELL, M., AND WATSON, M. A buffer-based approach to rate adaptation: Evidence from a large video streaming service. In ACM SIGCOMM (2014), pp. 187–198.

[21] JAIN, M., AND DOVROLIS, C. End-to-end available bandwidth: Measurement methodology, dynamics, and relation with TCP throughput. In ACM SIGCOMM (2002), pp. 295–308.

[22] JIANG, J., SEKAR, V., STOICA, I., AND ZHANG, H. Shedding light on the structure of internet video quality problems in the wild. In CoNext (2013), pp. 357–368.

[23] JIANG, J., SEKAR, V., AND ZHANG, H. Improving fairness, efficiency, and stability in HTTP-based adaptive video streaming with FESTIVE. In CoNext (2012), pp. 97–108.

[24] KRISHNAN, R., MADHYASTHA, H. V., SRINIVASAN, S., JAIN, S., KRISHNAMURTHY, A., ANDERSON, T., AND GAO, J. Moving beyond end-to-end path information to optimize CDN performance. In IMC (2009), pp. 190–201.

[25] KRISHNAN, S. S., AND SITARAMAN, R. K. Video stream quality impacts viewer behavior: Inferring causality using quasi-experimental designs. In IMC (2012), pp. 211–224.

[26] LIU, X., DOBRIAN, F., MILNER, H., JIANG, J., SEKAR, V., STOICA, I., AND ZHANG, H. A case for a coordinated Internet video control plane. In ACM SIGCOMM (2012), pp. 359–370.

[27] PAXSON, V., AND ALLMAN, M. Computing TCP’s Retransmission Timer. RFC 2988 (Proposed Standard), 2000. Obsoleted by RFC 6298.

[28] PLISSONNEAU, L., AND BIERSACK, E. A longitudinal view of HTTP video streaming performance. In Multimedia Systems Conference (2012), pp. 203–214.

[29] POESE, I., UHLIG, S., KAAFAR, M. A., DONNET, B., AND GUEYE, B. IP geolocation databases: Unreliable? SIGCOMM Computer Communications Review, 2 (2011), 53–56.

[30] SEN, S., REXFORD, J., AND TOWSLEY, D. Proxy prefix caching for multimedia streams. In IEEE INFOCOM (1999), pp. 1310–1319.

[31] SONG, H. H., GE, Z., MAHIMKAR, A., WANG, J., YATES, J., ZHANG, Y., BASSO, A., AND CHEN, M. Q-score: Proactive service quality assessment in a large IPTV system. In IMC (2011), pp. 195–208.

[32] TIAN, G., AND LIU, Y. Towards agile and smooth video adaptation in dynamic HTTP streaming. In CoNext (2012), pp. 109–120.

[33] TORRES, R., FINAMORE, A., KIM, J. R., MELLIA, M., MUNAFO, M. M., AND RAO, S. Dissecting video server selection strategies in the YouTube CDN. In International Conference on Distributed Computing Systems (2011), pp. 248–257.

[34] WEAVER, N., KREIBICH, C., DAM, M., AND PAXSON, V. Here be web proxies. In PAM (2014), pp. 183–192.

[35] XU, X., JIANG, Y., FLACH, T., KATZ-BASSETT, E., CHOFFNES, D., AND GOVINDAN, R. Investigating Transparent Web Proxies in Cellular Networks. In Proc. of PAM (2015).

[36] YIN, H., LIU, X., QIU, F., XIA, N., LIN, C., ZHANG, H., SEKAR, V., AND MIN, G. Inside the bird’s nest: Measurements of large-scale live VoD from the 2008 Olympics. In IMC (2009), pp. 442–455.

[37] YIN, X., JINDAL, A., SEKAR, V., AND SINOPOLI, B. A control-theoretic approach for dynamic adaptive video streaming over HTTP. In ACM SIGCOMM (2015), pp. 325–338.

[38] YU, M., GREENBERG, A., MALTZ, D., REXFORD, J., YUAN, L., KANDULA, S., AND KIM, C. Profiling network performance for multi-tier data center applications. In USENIX NSDI (2011), pp. 57–70.