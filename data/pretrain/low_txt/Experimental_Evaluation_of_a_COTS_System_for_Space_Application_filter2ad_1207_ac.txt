### Effects of Application Damage and Error Propagation

#### Error Propagation for OS and Application Faults

The error propagation mechanisms for operating system (OS) and application faults are notably different. Only 1.0% of OS faults propagate to other processes (P1 and P2), while the percentage for application faults is 4.4%. Manual inspection of these faults revealed various error propagation scenarios, primarily involving memory corruption (to simulate cache faults) and corrupted parameters in OS calls. Most propagated errors were detected by other applications or the OS. Specifically, only 3 out of 975 application faults and 6 out of 1038 OS faults went undetected, causing incorrect results in other applications (P2 and P3).

#### Impact of Faults Injected While P1 Was Scheduled

- **Kernel Mode (1038 faults)**
  - System crash: 57.3%
  - Error propagation: 51.0%
- **User Mode (975 faults)**
  - Application damage: 43.4%
  - Errors detected: 29.5%
  - No impact: 12.3%
  - Other application crash: 1.2%
  - Error propagation: 1.0%
  - System crash: 4.4%

**Figure 6: Impact of faults injected while P1 was scheduled, considering error propagation.**

- **Error Propagation Details**
  - Other application crash: 2
  - Errors detected in other application (P2 or P3): 2
  - Wrong results in other applications (P2 or P3): 6
- **Most Important Types of Errors**
  - Memory corruption
  - Application level
  - OS level
  - SIGTRAP (trace mode)
  - SIGPIPE (write + no pipe read)
  - SIGSYS (bad argument to system call)

**Table 5: Error propagation details.**

- **Application Damage Details (Kernel Mode: 1038 faults, User Mode: 975 faults)**
  - Application hang: 1.0%
  - Wrong results: 1.8%
  - Errors detected: 9.5%
  - Application hang: 0.6%
  - Wrong results: 6.0%
  - Errors detected: 36.8%

**Table 6: Application damage details.**

A high percentage (43.4%) of application faults were confined to P1, indicating that LynxOS effectively confines errors to the directly affected application. Additionally, since most of these faults were detected (36.8%), it suggests that such faults can be effectively recovered using a SIFT (Software Implemented Fault Tolerance) approach.

#### Workload Termination and Correctness of Application Results

The following experiments aim to analyze the impact of faults on application termination and the correctness of application results using realistic workloads. Faults were uniformly distributed over time and processor location to emulate SEU (Single Event Upset) faults.

- **Workload Distribution**
  - **Gravity**
    - User mode: 36 (5.7%)
    - Kernel mode: 589 (94.3%)
    - Total: 625
  - **PI**
    - User mode: 626 (99.6%)
    - Kernel mode: 3 (0.4%)
    - Total: 629
  - **Matmult**
    - User mode: 1277 (99.3%)
    - Kernel mode: 9 (0.7%)
    - Total: 1286

**Table 7: Percentage of faults injected in the workloads.**

- **Key Features of the Workloads Profile**
  - **Gravity**
    - Execution time: ~1 second
    - Storing results: 24 Mbytes
  - **PI**
    - Execution time: ~17 seconds
    - Storing results: Neglected
    - Size of results: 53 bytes
  - **Matmult**
    - Execution time: ~22 seconds
    - Storing results: 24.04 Kbytes

**Table 8: Key features of the workloads profile.**

The Gravity application spends most of its time writing results to disk, leading to 94% of faults being injected into OS code. The PI and Matmult applications show different patterns, with PI having negligible I/O activity.

- **Failure Modes with Realistic Applications**
  - **PI**
    - System crash: 54.1%
    - Abnormal app. termination: 37.5%
    - Application hang: 22.4%
    - Correct results: 18.4%
    - Wrong results: 18.4%
    - No impact: 3.3%
  - **Gravity**
    - System crash: 50.4%
    - Abnormal app. termination: 17.1%
    - Application hang: 1.9%
    - Correct results: 7.9%
    - Wrong results: 0.5%
    - No impact: 0.0%
  - **Matmult**
    - System crash: 37.8%
    - Abnormal app. termination: 27.0%
    - Application hang: 3.2%
    - Correct results: 1.9%
    - Wrong results: 27.0%

**Figure 7: Failure modes with realistic applications.**

The results vary significantly across applications due to their different profiles. The PI application, which performs mostly calculations, has a high percentage (50.4%) of wrong results, indicating its vulnerability to faults. The Matmult application also shows a high percentage (27.0%) of wrong results, suggesting that computation-intensive applications require robust application-based error detection techniques.

#### Future Work

Future work will include the analysis of additional fault injection results that could not be included in this paper due to space constraints.

#### Conclusions

This study evaluated the impact of faults in a COTS system for scientific data processing in space applications. The key observations are:

- **OS Faults**: These are easier to handle as they often cause system crashes or have no visible impact. SIFT systems are well-suited for crash recovery.
- **Application Faults**: The assumption of fail-silent behavior is inadequate. More research is needed on application-based acceptance checking.
- **Computation-Intensive Applications**: These are more likely to produce wrong results than applications with frequent OS calls.
- **LynxOS Effectiveness**: LynxOS is effective in confining errors to the directly affected process, but small percentages (1% to 4.4%) of errors still propagate. Improved application-based error detection is necessary.
- **Error Detection**: Most propagated errors were detected in other applications, suggesting that SIFT techniques can be effective.
- **Robustness**: LynxOS is robust, but a small percentage of faults go undetected, indicating the need for additional wrappers to enhance robustness.

#### References

[1] R. R. Some and D. C. Ngo, “REE: A COTS-Based Fault Tolerant Parallel Processing Supercomputer for Spacecraft Onboard Scientific Data Analysis,” Proc. of the Digital Avionics System Conference, vol.2, pp.B3-1-7 -B3-1-12,1999.
[2] K. Whisnant, R. Iyer, D. Rennels, and R. Some, “An Experimental Evaluation of the REE SIFT Environment for Spaceborne Applications”, paper accepted at the International Performance and Dependability Symposium, Washington, DC, June 23rd - 26th, 2002.
[3] Critical Software, S.A., “Xception: Professional Fault Injection”, White Paper, http://www.criticalsoftware.com, 2000.
[4] J. Carreira, H. Madeira, and J. G. Silva, “Xception: Software Fault Injection and Monitoring in Processor Functional Units", IEEE Transactions on Software Engineering, vol. 24, no. 2, February 1998.
[5] J. Arlat, A. Costes, Y. Crouzet, J.-C. Laprie and D. Powell, “Fault Injection and Dependability Evaluation of Fault-Tolerant Systems”, IEEE Trans. on Computers, 42 (8), pp.913-23, August 1993.
[6] J. Karlsson, P. Folkesson, J. Arlat, Y. Crouzet, G. Leber, J. Reisinger, “Application of Three Physical Fault Injection Techniques to the Experimental Assessment of the MARS Architecture”, Proc. 5th IFIP Working Conf. on Dependable Computing for Critical App.: DCCA-5, Urbana-Champaign, IL, USA, Sept. 1995.
[7] H. Madeira and J.G. Silva, “Experimental Evaluation of the Fail-Silent Behavior in Computers Without Error Masking,” Proc. 24th Int'l Symp. Fault Tolerant Computing Systems, Austin-Texas, 1994.
[8] D. P. Siewiorek, J. J. Hudak, B.-H. Suh and Z. Segall, “Development of a Benchmark to Measure System Robustness”, in Proc. 23rd Int. Symp. on Fault-Tolerant Computing, FTCS-2, Toulouse, France, 1993.
[9] P. J. Koopman, J. Sung, C. Dingman, D. P. Siewiorek and T. Marz, “Comparing Operating Systems using Robustness Benchmarks”, in Proc. 16th Int. Symp. on Reliable Distributed Systems, SRDS-16, Durham, NC, USA, 1997.
[10] F. Salles, M. Rodríguez, J.-C. Fabre and J. Arlat, “Metakernels and Fault Containment Wrappers”, in Proc. 29th IEEE Int. Symp. on Fault-Tolerant Computing (FTCS-29), Madison, WI, USA, 1999.
[11] J.-C. Fabre, F. Salles, M. Rodríguez Moreno and J. Arlat, “Assessment of COTS Microkernels by Fault Injection”, in Proc. 7th IFIP Working Conf. on Dependable Computing for Critical Applications: DCCA-7, San Jose, CA, USA, Jan. 1999.

#### Acknowledgements

Funding for this paper was provided, in part, by the Portuguese Government/European Union through R&D Unit 326/94 (CISUC) and by the DBench project, IST 2000 - 25425 DBENCH, funded by the European Union.