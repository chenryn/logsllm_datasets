### Number of Tolerable Rack Failures
- (d) Varying Write Request Rates
- (e) Varying Rack-Level Fault Tolerance of EAR
- (f) Varying Number of Replicas

**Figure 13.** Throughput of EAR over RR.

### Experiment B.2: Impact of Parameter Choices on Encoding and Write Performance under EAR and RR
Each plot shows the normalized performance, with the simulated performance of RR being over-estimated. We create 20 encoding processes, each encoding 50 stripes. We also issue write and background traffic requests, both following a Poisson distribution with a rate of 1 request per second. Each write request writes one 64MB block, while each background traffic request generates an exponentially distributed size of data with a mean of 64MB. The ratio of cross-rack to intra-rack background traffic is set to 1:1.

We consider different parameter configurations. For each configuration, we vary one parameter and obtain the performance over 30 runs with different random seeds. The average throughput results of EAR are normalized over those of RR for both encoding and write operations, which are carried out simultaneously. The results are presented in boxplots, showing the minimum, lower quartile, median, upper quartile, maximum, and any outliers over 30 runs.

#### Figure 13(a): Results vs. k (Fixed n - k = 4)
A larger \( k \) implies less encoding redundancy and more dominant cross-rack downloads of data blocks for encoding in RR. This leads to greater performance gains for EAR. For example, when \( k = 12 \), the encoding and write throughput gains of EAR over RR are 78.7% and 36.8%, respectively.

#### Figure 13(b): Results vs. n - k (Fixed k = 10)
A larger \( n - k \) means more data redundancy (i.e., parity blocks). While EAR reduces cross-rack traffic, the gain is offset by the need to write additional parity blocks. The encoding throughput gain of EAR over RR remains stable at around 70%, but the write throughput gain drops from 33.9% to 14.1%.

#### Figure 13(c): Results vs. Link Bandwidth
When the link bandwidth is more limited, EAR shows higher performance gains. The encoding throughput gain of EAR reaches 165.2% when the link bandwidth is only 0.2 Gb/s. The write throughput gain of EAR remains at around 20%.

#### Figure 13(d): Results vs. Write Request Arrival Rate
A larger arrival rate implies less effective link bandwidth. The encoding throughput gain of EAR over RR increases to 89.1% when the write request rate grows to 4 requests/s, while the write throughput gain is between 25% and 28%.

#### Figure 13(e): Results vs. Number of Rack Failures Tolerated in EAR
By tolerating fewer rack failures, EAR can keep more data/parity blocks in one rack, further reducing cross-rack traffic. The encoding and write throughput gains of EAR over RR increase from 70.1% to 82.1% and from 26.3% to 48.3%, respectively, when the number of tolerable rack failures of EAR is reduced from four to one.

#### Figure 13(f): Results vs. Number of Replicas per Data Block
Writing more replicas implies less effective link bandwidth, but the gain of EAR is offset since RR now downloads less data for encoding. The encoding throughput gain of EAR over RR is around 70%, while the write throughput gain decreases from 34.7% to 20.5% as the number of replicas increases from two to eight.

### C. Load Balancing Analysis
One major advantage of RR is that it achieves both storage and read load balancing by distributing data over a uniformly random set of nodes. We show via Monte Carlo simulations that although EAR adds extra restrictions to the random replica placement, it still achieves a very similar degree of load balancing to RR. We focus on rack-level load balancing and examine how the replicas are distributed across racks. We consider the replica placement for a number of blocks on a CFS composed of \( R = 20 \) racks with 20 nodes each, using 3-way replication. For EAR, we choose (14, 10) erasure coding. We obtain the averaged results over 1,000 runs.

#### Experiment C.1: Storage Load Balancing
We first examine the distribution of replicas across racks. We generate the replicas for 1,000 blocks and distribute them under RR or EAR. We then count the number of replicas stored in each rack. Figure 14 shows the proportions of replicas of RR and EAR in each rack (sorted in descending order of proportions). Both RR and EAR have very similar distributions, with the proportions of blocks stored in each rack ranging from 4.1% to 5.9%.

#### Experiment C.2: Read Load Balancing
We also examine the distribution of read requests across racks. Suppose the data blocks in File F are equally likely to be read, and the read requests to a data block are equally likely to be directed to one of the racks that contain a replica of the block. We define a hotness index \( H = \max_{1 \leq i \leq 20} (L(i)) \), where \( L(i) \) denotes the proportion of read requests to Rack \( i \). Intuitively, we want \( H \) to be small to avoid hot spots. Figure 15 shows \( H \) versus the file size, which we vary from 10 to 10,000 blocks. Both RR and EAR have almost identical \( H \).

### VI. Related Work
Erasure coding in CFSes has been extensively studied. Fan et al. [12] augment HDFS with asynchronous encoding to reduce storage overhead. Zhang et al. [32] propose applying erasure coding on the write path of HDFS and study its impact on various MapReduce workloads. Li et al. [20] deploy regenerating codes [10] on HDFS to enable multiple-node failure recovery with minimum bandwidth. Silberstein et al. [29] propose lazy recovery for erasure-coded storage to reduce bandwidth due to frequent recovery executions. Li et al. [19] improve MapReduce performance on erasure-coded storage by scheduling degraded-read map tasks carefully to avoid bandwidth competition. Enterprises have also deployed erasure coding in production CFSes to reduce storage overhead, with examples including Google [13], Azure [17], and Facebook [21, 27].

Some studies propose new erasure code constructions and evaluate their applicability in CFSes. Local repairable codes reduce I/O during recovery while limiting the number of surviving nodes to be accessed. Variants of local repairable codes have been proposed and evaluated based on an HDFS simulator [23], Azure [17], and Facebook [27]. Piggybacked-RS codes [24, 25] embed parity information of one Reed-Solomon-coded stripe into that of the following stripe, reducing recovery bandwidth while maintaining the storage efficiency of Reed-Solomon codes. Facebook’s f4 [21] protects failures at different levels, including disks, nodes, and racks, by combining Reed-Solomon-coded stripes to create an additional XOR-coded stripe.

The above studies (except [32]) often assume asynchronous encoding and focus on improving the applicability of erasure coding after the replicated data has been encoded. Our work complements these studies by examining the performance and availability of the asynchronous encoding operation itself.

Replica placement in CFSes plays a critical role in both performance and reliability. By constraining the placement of block replicas to smaller groups of nodes, the block loss probability can be reduced with multiple node failures [4, 7]. Scarlett [2] alleviates hotspots by carefully storing replicas based on workload patterns. Sinbad [6] identifies the variance of link capacities in a CFS and improves write performance by avoiding storing replicas on nodes with congested links. These studies mainly focus on replication-based storage, while our work focuses on how replica placement affects the performance and reliability of asynchronous encoding.

### VII. Conclusions
Given the importance of deploying erasure coding in cluster file systems (CFSes) to reduce storage footprints, this paper studies the problem of encoding replicated data with erasure coding in CFSes. We argue that random replication (RR) brings both performance and availability issues to the subsequent encoding operation. We present encoding-aware replication (EAR) to take into account erasure coding. EAR imposes constraints on the replica layout to eliminate both cross-rack downloads and block relocation while attempting to place the replicas as uniformly random as possible. We implement EAR on Facebook’s HDFS and show its feasibility in real deployment. We conduct extensive evaluations using testbed experiments, discrete-event simulations, and load balancing analysis, and show that EAR achieves throughput gains in both write and encoding operations while preserving the even replica distribution compared to RR. In future work, we plan to study scenarios with heterogeneous workloads and hardware resources. The source code of our EAR implementation is available at http://ansrlab.cse.cuhk.edu.hk/software/ear.

### Acknowledgments
This work was supported in part by grants AoE/E-02/08 and ECS CUHK419212 from the University Grants Committee of Hong Kong.

### References
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity Data Center Network Architecture. In Proc. of ACM SIGCOMM, Aug 2008.
[2] G. Ananthanarayanan, S. Agarwal, S. Kandula, A. Greenberg, I. Stoica, D. Harlan, and E. Harris. Scarlett: Coping with Skewed Content Popularity in MapReduce Clusters. In Proc. of ACM EuroSys, Apr 2011.
[3] J. Bloemer, M. Kalfane, R. Karp, M. Karpinski, M. Luby, and D. Zuckerman. An XOR-Based Erasure-Resilient Coding Scheme. Technical Report TR-95-048, International Computer Science Institute, UC Berkeley, Aug 1995.
[4] D. Borthakur, J. Gray, J. S. Sarma, K. Muthukkaruppan, N. Spiegelberg, H. Kuang, K. Ranganathan, D. Molkov, A. Menon, S. Rash, et al. Apache Hadoop goes realtime at Facebook. In Proc. of ACM SIGMOD, Jun 2011.
[5] B. Calder, J. Wang, A. Ogus, N. Nilakantan, A. Skjolsvold, S. McKelvie, Y. Xu, S. Srivastav, J. Wu, H. Simitci, et al. Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency. In Proc. of ACM SOSP, Oct 2011.
[6] M. Chowdhury, S. Kandula, and I. Stoica. Leveraging Endpoint Flexibility in Data-Intensive Clusters. In Proc. of ACM SIGCOMM, Aug 2013.
[7] A. Cidon, S. Rumble, R. Stutsman, S. Katti, J. Ousterhout, and M. Rosenblum. Copysets: Reducing the Frequency of Data Loss in Cloud Storage. In Proc. of USENIX ATC, 2013.
[8] CSIM. http://www.mesquite.com/products/csim20.htm.
[9] J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. In Proc. of USENIX OSDI, Dec 2004.
[10] A. G. Dimakis, P. B. Godfrey, Y. Wu, M. Wainwright, and K. Ramchandran. Network Coding for Distributed Storage Systems. IEEE Trans. on Info. Theory, 56(9):4539–4551, Sep 2010.
[11] Facebook’s Hadoop. http://goo.gl/fHDloI.
[12] B. Fan, W. Tantisiriroj, and G. Gibson. Diskreduce: Replication as a Prelude to Erasure Coding in Data-Intensive Scalable Computing. Technical Report CMU-PDL-11-112, Carnegie Mellon Univsersity, Parallel Data Laboratory, Oct 2011.
[13] D. Ford, F. Labelle, F. I. Popovici, M. Stokel, V.-A. Truong, L. Barroso, C. Grimes, and S. Quinlan. Availability in Globally Distributed Storage Systems. In Proc. of USENIX OSDI, Oct 2010.
[14] S. Ghemawat, H. Gobioff, and S. Leung. The Google File System. In Proc. of ACM SOSP, Dec 2003.
[15] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A Scalable and Flexible Data Center Network. In Proc. of ACM SIGCOMM, Aug 2009.
[16] HDFS-RAID. http://wiki.apache.org/hadoop/HDFS-RAID.
[17] C. Huang, H. Simitci, Y. Xu, A. Ogus, B. Calder, P. Gopalan, J. Li, and S. Yekhanin. Erasure Coding in Windows Azure Storage. In Proc. of USENIX ATC, Jun 2012.
[18] Iperf. https://iperf.fr/.
[19] R. Li, P. P. C. Lee, and Y. Hu. Degraded-First Scheduling for MapReduce in Erasure-Coded Storage Clusters. In Proc. of IEEE/IFIP DSN, 2014.
[20] R. Li, J. Lin, and P. P. C. Lee. Enabling Concurrent Failure Recovery for Regenerating-Coding-Based Storage Systems: From Theory to Practice. IEEE Trans. on Computers, 2014.
[21] S. Muralidhar, W. Lloyd, S. Roy, C. Hill, E. Lin, W. Liu, S. Pan, S. Shankar, V. Sivakumar, L. Tang, and S. Kumar. f4: Facebook’s Warm BLOB Storage System. In Proc. of USENIX OSDI, 2014.
[22] D. Ongaro, S. M. Rumble, R. Stutsman, J. Ousterhout, and M. Rosenblum. Fast Crash Recovery in RAMCloud. In Proc. of ACM SOSP, 2011.
[23] D. Papailiopoulos, J. Luo, A. Dimakis, C. Huang, and J. Li. Simple Regenerating Codes: Network Coding for Cloud Storage. In Proc. of IEEE INFOCOM, Mar 2012.
[24] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran. A Solution to the Network Challenges of Data Recovery in Erasure-coded Distributed Storage Systems: A Study on the FacebookWarehouse Cluster. In Proc. of USENIX HotStorage, 2013.
[25] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran. A “Hitchhiker’s” Guide to Fast and Efficient Data Reconstruction in Erasure-Coded Data Centers. In Proc. of ACM SIGCOMM, 2014.
[26] I. Reed and G. Solomon. Polynomial Codes over Certain Finite Fields. Journal of the Society for Industrial and Applied Mathematics, 8(2):300–304, 1960.
[27] M. Sathiamoorthy, M. Asteris, D. Papailiopoulos, A. G. Dimakis, R. Vadali, S. Chen, and D. Borthakur. XORing Elephants: Novel Erasure Codes for Big Data. In Proc. of VLDB Endowment, pages 325–336, 2013.
[28] K. Shvachko, H. Kuang, S. Radia, and R. Chansler. The Hadoop Distributed File System. In Proc. of IEEE MSST, May 2010.
[29] M. Silberstein, L. Ganesh, Y. Wang, L. Alvizi, and M. Dahlin. Lazy Means Smart: Reducing Repair Bandwidth Costs in Erasure-coded Distributed Storage. In Proc. of ACM SYSTOR, 2014.
[30] SWIM Project. https://github.com/SWIMProjectUCB/SWIM/wiki.
[31] H. Weatherspoon and J. D. Kubiatowicz. Erasure Coding Vs. Replication: A Quantitative Comparison. In Proc. of IPTPS, Mar 2002.
[32] Z. Zhang, A. Deshpande, X. Ma, E. Thereska, and D. Narayanan. Does Erasure Coding Have a Role to Play in my Data Center? Technical Report MSR-TR-2010-52, Microsoft Research, May 2010.