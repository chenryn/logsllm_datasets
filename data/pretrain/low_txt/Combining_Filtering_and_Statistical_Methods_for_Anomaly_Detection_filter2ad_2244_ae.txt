# Anomaly Detection in Network Traffic

## 1. Introduction
This document discusses the performance of four anomaly detection schemes: basic, GLR (Generalized Likelihood Ratio), vshift, and wavelet. The study is based on both real (Abilene) and synthetic data. The parameters used to describe the anomalies include duration, volume, number of source-destination (OD) pairs, and shape.

### 1.1 Anomaly Description Parameters
- **Duration**: Possible values range from minutes to days.
- **Volume**: Additive factor (∆) and multiplicative factor (δ).
- **Number of OD Pairs**: Ranges from (1, 1) to all ODs on one link.
- **Shape**: Ramp, Exponential, Square, Step.

## 2. Performance Analysis

### 2.1 ROC Curves
The performance of the four schemes was evaluated using Receiver Operating Characteristic (ROC) curves. For the synthetic data, the basic and GLR methods performed best and equivalently. However, for the Abilene data, the GLR method did not perform well, while the vshift method was second best. This difference may be due to the statistical properties of the anomalies, such as changes in variance in addition to changes in mean.

### 2.2 False Positives and Negatives
When using marked traces, there is a risk of undetected anomalies or normal behavior being marked as anomalous. A visual inspection was conducted to remove false positives, but false negatives were not checked. Figures 4(a) and 4(b) show examples where the residual process indicates two anomalies, but the labeling procedure marks only one. This suggests that the computed false positive rate should be considered an upper bound.

## 3. Detection Time

### 3.1 Definition and Results
Detection lag is defined as the time at which a true anomaly is detected minus the time the anomaly began. The results for the Abilene and synthetic data are shown in Figure 5. The basic and GLR methods exhibit excellent detection times, with the GLR method detecting 90% of the anomalies with no lag in the Abilene data and 100% in the synthetic data. The wavelet analysis method performs less well, taking over half an hour to detect some anomalies.

## 4. Sensitivity Analysis

### 4.1 Impact of Anomaly Size
Figure 6 shows the impact of the false positive and false negative ratios as the volume of anomalies decreases. For small anomalies (10-20% increase in flow), the detection rate drops off quickly. For larger anomalies (δ ≥ 1.5), the false positive rate increases due to the spread of error inside the Kalman filter.

## 5. Conclusions

### 5.1 Key Findings
- The GLR method performs best when the anomaly causes a change in the mean.
- The vshift method performs better for the Abilene data, possibly due to changes in variance.
- The simplest method (basic) performed best across all validation tests.
- Wavelet-based methods did not perform well, raising questions about their utility in anomaly detection.

### 5.2 Future Work
- Further study of the statistical properties of anomalies.
- Development of composite methods that use multiple types of tests.
- Exploration of the conditions under which wavelet analysis is useful for anomaly detection.

## 6. Acknowledgements
We thank Anukool Lakhina for sharing labeled Abilene traces and Simon Crosby for discussions on ROC curves.

## 7. References
[1] Basseville, M., & Nikiforov, I. (1993). Detection of abrupt changes: theory and application.
[2] Egan, J. (1975). Signal Detection Theory and ROC Analysis. Academic Press.
[3] Gunnar, A., Johansson, M., & Telkamp, T. (2004). Traffic matrix estimation on a large IP backbone - a comparison on real data. In ACM IMC.
[4] Hawkins, D. M., Qqui, P., & Kang, C. W. (2003). The changepoint model for statistical process control. Journal of Quality Technology, 35(4).
[5] Hussain, A. (2005). Measurement and Spectral Analysis of Denial of Service Attacks. PhD thesis, USC.
[6] Jung, J., Krishnamurthy, B., & Rabinovich, M. (2002). Flash crowds and denial of service attacks: Characterization and implications for CDNs and web sites. In ACM WWW Conference.
[7] Kailath, T., Sayed, A. H., Hassibi, B., Sayed, A. H., & Hassibi, B. (2000). Linear Estimation. Prentice Hall.
[8] Lakhina, A., Crovella, M., & Diot, C. (2004). Characterization of network-wide anomalies in traffic flows. In ACM IMC.
[9] Lakhina, A., Crovella, M., & Diot, C. (2004). Diagnosing network-wide traffic anomalies. In ACM Sigcomm.
[10] Lakhina, A., Papagiannaki, K., Crovella, M., Diot, C., Kolaczyk, E., & Taft, N. (2004). Structural analysis of network traffic flows. In ACM Sigmetrics.
[11] Mallat, S. (1999). A Wavelet Tour of Signal Processing. Academic Press.
[12] Mirkovic, J., & Reiher, P. (2004). A taxonomy of DDoS attack and DDoS defense mechanisms. In ACM CCR.
[13] Moore, D., Voelker, G. M., & Savage, S. (2001). Inferring Internet Denial-of-Service activity. In Proceedings of the 10th USENIX Security Symposium.
[14] Sommers, J., Yegneswaran, V., & Barford, P. (2004). A framework for malicious workload generation. In IMC.
[15] Soule, A., Lakhina, A., Taft, N., Papagiannaki, K., Salamatian, K., Nucci, A., Crovella, M., & Diot, C. (2005). Traffic matrices: Balancing measurements, inference and modeling. In ACM Sigmetrics.
[16] Soule, A., Nucci, A., Cruz, R., Leonardi, E., & Taft, N. (2004). How to identify and estimate the largest traffic matrix elements in a dynamic environment. In ACM Sigmetrics.
[17] Soule, A., Salamatian, K., & Taft, N. (2005). Traffic matrix tracking using Kalman filters. ACM LSNI Workshop.
[18] Teixeira, R., Duffield, N., Rexford, J., & Roughan, M. (2005). Traffic matrix reloaded: Impact of routing changes. In PAM.
[19] Zhang, Y., Roughan, M., Duffield, N., & Greenberg, A. (2003). Fast accurate computation of large-scale IP traffic matrices from link loads. In ACM Sigmretrics.
[20] Zhang, Y., Roughan, M., Lund, C., & Donoho, D. (2003). An information-theoretic approach to traffic matrix estimation. In ACM Sigcomm.
[21] Zweig, M. H., & Campbell, G. (1993). Receiver-operating characteristic (ROC) plots: a fundamental evaluation tool in clinical medicine. In Clinical Chemistry, 93(4).

---

This optimized version aims to provide a clear, coherent, and professional presentation of the content, with improved structure and readability.