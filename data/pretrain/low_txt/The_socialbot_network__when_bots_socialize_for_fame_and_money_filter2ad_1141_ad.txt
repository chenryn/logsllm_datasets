### Figure Analysis

The first category of socialbots constitutes 86% of the total and is responsible for 70% of the overall infiltration, with each socialbot infiltrating between 0 to 50 user profiles. The second category, which makes up 10% of the socialbots, accounts for 23% of the total infiltration, with each socialbot infiltrating between 60 to 80 user profiles. The remaining 4% of the socialbots, considered as outliers, are responsible for 7% of the overall infiltration.

### Data Collection Efficiency

Since most of the infiltration was a result of coordinated efforts by the socialbots, we calculated the amount of new data a botmaster can collect per socialbot compared to public access. This is particularly useful for estimating the number of socialbots or connection requests needed to gather specific user data, such as email addresses and birth dates. Assuming a conservative operation of 25 friendship requests per socialbot per day over an 8-week period, we found that a botmaster can expect to collect an average of 175 new pieces of user data per socialbot per day on Facebook, as shown in Table 4.

### Implications on Other Systems

Operating a Socialbot Network (SbN) for an extended period is expected to lead to significantly larger infiltration, as the socialbots will spend most of their time in the propagation phase. Consequently, the neighborhood size of each socialbot will continue to grow. This has alarming implications for software systems that rely on the social graph of an Online Social Network (OSN) to provide services, such as limiting Sybil nodes in distributed systems [45] and modeling trust in socially-informed recommender systems [43].

To illustrate the importance, consider OSN-based Sybil defenses used in P2P overlay networks. These defense mechanisms typically use two types of networks that share the same nodes: the P2P network to be protected and an external social network like Facebook, which serves as a trust network [45]. To detect Sybil nodes in the P2.P network, it is often assumed that a Sybil node cannot have many connections with non-Sybil nodes in the social network. Therefore, a well-connected node in the P2P network that is loosely connected in the social network is likely a Sybil node. However, this assumption is not reliable. Our findings show that socialbots can establish numerous social connections with arbitrary OSN users. Thus, using a social network like Facebook to detect Sybil nodes in P2P networks is ineffective. This conclusion extends to all systems that make similar assumptions about OSNs.

### Related Work

The closest threat to large-scale infiltration in OSNs is the Koobface botnet [5]. Initially, Koobface compromises user accounts on OSNs and then uses these accounts to promote a provocative message with a hyperlink. The link leads to a phishing website that tricks users into installing a Flash plugin, which is actually the Koobface executable. Unlike Koobface, an SbN does not rely on hijacked profiles, as doing so requires infecting a large number of initial "zombie" machines through OSN-independent distribution channels.

Bilge et al. [6] demonstrated that most users in OSNs are not cautious when accepting connection requests. They conducted an experiment to test how willing users are to accept connection requests from forged user profiles of people already in their friendship list as confirmed contacts. They also compared this with users' responses to connection requests from strangers. Their results showed that the acceptance rate for forged profiles was always over 60%, and about 20% for fake profiles. Unlike their targeted attack, we do not expect the adversary to forge profiles, as this limits the scalability of the attack and makes it more susceptible to detection.

Other than the extensive botnet literature, much of the related work focuses on Twitter bots and various techniques to detect them. Ratkiewicz et al. [33] described the use of Twitter bots to spread misinformation during U.S. political elections. Stringhini et al. [37] analyzed the extent to which spam has entered OSNs and how spammers operate. They developed techniques to detect spammers and showed that it is possible to automatically identify the accounts they use. Grier et al. [16] found that 8% of unique URLs sent in Twitter are later blacklisted. They also described a method to identify automated Twitter accounts by analyzing the regularity of their tweet times, which proved highly accurate in identifying spammers.

### Conclusion

We evaluated the vulnerability of OSNs to large-scale infiltration by a Socialbot Network (SbN). Using Facebook as a representative OSN, we found that bots mimicking real users are effective in infiltrating Facebook on a large scale, especially when the users and bots share mutual connections. Such socialbots make it difficult for OSN security defenses, such as the Facebook Immune System, to detect or stop an SbN. This has resulted in significant privacy breaches and serious implications for other socially-informed software systems. We believe that large-scale infiltration in OSNs is just one of many future cyber threats, and defending against such threats is the first step towards maintaining a safer social web for millions of active users.

### Acknowledgments

We would like to thank San-Tsai Sun, Elizeu Santos-Neto, Albina Muslukhova, and Bader AlAhmad for their kind help and advice. We also thank Cormac Herley, Miranda Mowbray, and Adriana Iamnitchi for their feedback on an early draft of this paper. This research is partially supported through funding from the NSERC Internetworked Systems Security Network (ISSNet).

### References

[1] Facebook Open Graph Protocol. http://developers.facebook.com/docs/opengraph.
[2] Sick profile maker. http://sickmarketing.com/sick-profile-maker.
[3] Facebook Statistics. http://www.facebook.com/press, March 2011.
[4] Jet bots. http://allbots.info, 2011.
[5] J. Baltazar, J. Costoya, and R. Flores. The real face of Koobface: The largest web 2.0 botnet explained. Trend Micro Research, July 2009.
[6] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda. All your contacts are belong to us: Automated identity theft attacks on social networks. In WWW ’09: Proceedings of the 18th International Conference on World Wide Web, pages 551–560, New York, NY, USA, 2009. ACM.
[7] N. Bos, K. Karahalios, M. Musgrove-Chávez, E. S. Poole, J. C. Thomas, and S. Yardi. Research ethics in the Facebook era: Privacy, anonymity, and oversight. In CHI EA ’09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems, pages 2767–2770, New York, NY, USA, 2009. ACM.
[8] D. Boyd. Social media is here to stay... Now what? Microsoft Research Tech Fest, February 2009.
[9] G. Brown, T. Howe, M. Ihbe, A. Prakash, and K. Borders. Social networks and context-aware spam. In CSCW ’08: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pages 403–412, New York, NY, USA, 2008. ACM.
[10] Z. Coburn and G. Marra. Realboy: Believable Twitter bots. http://ca.olin.edu/2008/realboy, April 2011.
[11] J. R. Douceur. The Sybil attack. In IPTPS ’01: Revised Papers from the First International Workshop on Peer-to-Peer Systems, pages 251–260, London, UK, 2002. Springer-Verlag.
[12] M. Egele, L. Bilge, E. Kirda, and C. Kruegel. CAPTCHA smuggling: Hijacking web browsing sessions to create CAPTCHA farms. In SAC ’10: Proceedings of the 2010 ACM Symposium on Applied Computing, pages 1865–1870, New York, NY, USA, 2010. ACM.
[13] N. B. Ellison, C. Steinfield, and C. Lampe. The benefits of Facebook “friends:” Social capital and college students’ use of online social network sites. Journal of Computer-Mediated Communication, 12(4):1143–1168, July 2007.
[14] N. FitzGerald. New Facebook worm - don’t click da’ button baby! http://fitzgerald.blog.avg.com/2009/11/, November 2009.
[15] M. Gjoka, M. Kurant, C. T. Butts, and A. Markopoulou. Walking in Facebook: A case study of unbiased sampling of OSNs. In Proceedings of IEEE INFOCOM ’10, San Diego, CA, March 2010.
[16] C. Grier, K. Thomas, V. Paxson, and M. Zhang. @spam: The underground on 140 characters or less. In Proceedings of the 17th ACM Conference on Computer and Communications Security, CCS ’10, pages 27–37, New York, NY, USA, 2010. ACM.
[17] C. Herley. The plight of the targeted attacker in a world of scale. In The 9th Workshop on the Economics of Information Security (WEIS 2010), 2010.
[18] C. Hernandez-Castro and A. Ribagorda. Remotely telling humans and computers apart: An unsolved problem. In J. Camenisch and D. Kesdogan, editors, iNetSec 2009 ? Open Research Problems in Network Security, volume 309 of IFIP Advances in Information and Communication Technology, pages 9–26. Springer Boston, 2009.
[19] M. Huber, M. Mulazzani, and E. Weippl. Who on earth is Mr. Cypher? Automated friend injection attacks on social networking sites. In Proceedings of the IFIP International Information Security Conference 2010: Security & Privacy — Silver Linings in the Cloud, 2010.
[20] T. N. Jagatic, N. A. Johnson, M. Jakobsson, and F. Menczer. Social phishing. Commun. ACM, 50(10):94–100, 2007.
[21] A. M. Kaplan and M. Haenlein. Users of the world, unite! The challenges and opportunities of social media. Business Horizons, 53(1):59 – 68, 2010.
[22] J. Leskovec and E. Horvitz. Planetary-scale views on a large instant-messaging network. In Proceeding of the 17th International Conference on World Wide Web, pages 915–924, New York, NY, USA, 2008. ACM.
[23] G. Livingston. Social media: The new battleground for politics. http://mashable.com/2010/09/23/congress-battle-social-media/, September 2010.
[24] D. Misener. Rise of the socialbots: They could be influencing you online. http://www.cbc.ca/news/technology/story/2011/03/29/f-vp-misener-socialbot-armies-election.html, March 2011.
[25] A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel, and B. Bhattacharjee. Measurement and analysis of online social networks. In IMC ’07: Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement, pages 29–42, New York, NY, USA, 2007. ACM.
[26] E. Morozov. Swine flu: Twitter’s power to misinform. http://neteffect.foreignpolicy.com/posts/2009/04/25/swine_flu_twitters_power_to_misinform, April 2009.
[27] M. Motoyama, K. Levchenko, C. Kanich, D. McCoy, G. M. Voelker, and S. Savage. Re: CAPTCHAs: Understanding CAPTCHA-solving services in an economic context. In Proceedings of the 19th USENIX Conference on Security, USENIX Security’10, pages 28–28, Berkeley, CA, USA, 2010. USENIX Association.
[28] S. Nagaraja, A. Houmansadr, P. Agarwal, V. Kumar, P. Piyawongwisal, and N. Borisov. Stegobot: A covert social network botnet. In Proceedings of the Information Hiding Conference, 2011.
[29] F. Nagle and L. Singh. Can friends be trusted? Exploring privacy in online social networks. In Proceedings of the 2009 International Conference on Advances in Social Network Analysis and Mining, pages 312–315, Washington, DC, USA, 2009. IEEE Computer Society.
[30] S. Patil. Social network attacks surge. http://www.symantec.com/connect/blogs/social-network-attacks-surge, June 2011.
[31] A. Pitsillidis, K. Levchenko, C. Kreibich, C. Kanich, G. M. Voelker, V. Paxson, N. Weaver, and S. Savage. Botnet judo: Fighting spam with itself. In NDSS, 2010.
[32] A. Rapoport. Spread of information through a population with socio-structural bias: I. Assumption of transitivity. Bulletin of Mathematical Biology, 15:523–533, 1953. 10.1007/BF02476440.
[33] J. Ratkiewicz, M. Conover, M. Meiss, B. Gonçalves, S. Patil, A. Flammini, and F. Menczer. Truthy: Mapping the spread of astroturf in microblog streams. In Proceedings of the 20th international conference companion on World wide web, WWW ’11, pages 249–252, New York, NY, USA, 2011. ACM.
[34] C. P. Robert and G. Casella. Monte Carlo Statistical Methods (Springer Texts in Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005.
[35] F. Salem and R. Mourtada. Civil movements: The impact of Facebook and Twitter. The Arab Social Media Report, 1(2), 2011.
[36] T. Stein, E. Chen, and K. Mangla. Facebook immune system. In Proceedings of the 4th Workshop on Social Network Systems, SNS ’11, pages 8:1–8:8, New York, NY, USA, 2011. ACM.
[37] G. Stringhini, C. Kruegel, and G. Vigna. Detecting spammers on social networks. In Proceedings of the 26th Annual Computer Security Applications Conference, ACSAC ’10, pages 1–9, New York, NY, USA, 2010. ACM.
[38] C. Taylor. Why not call it a Facebook revolution? http://edition.cnn.com/2011/TECH/social.media/02/24/facebook.revolution/, February 2011.
[39] S. T. Tong, B. Van Der Heide, L. Langwell, and J. B. Walther. Too much of a good thing? The relationship between number of friends and interpersonal impressions on Facebook. Journal of Computer-Mediated Communication, 13(3):531–549, 2008.
[40] J. A. Vargas. Obama raised half a billion online. http://voices.washingtonpost.com/44/2008/11/obama-raised-half-a-billion-on.html, November 2008.
[41] B. Viswanath, A. Post, K. P. Gummadi, and A. Mislove. An analysis of social network-based Sybil defenses. In Proceedings of the ACM SIGCOMM 2010 conference on SIGCOMM, SIGCOMM ’10, pages 363–374, New York, NY, USA, 2010. ACM.
[42] L. von Ahn, M. Blum, N. J. Hopper, and J. Langford. CAPTCHA: Using hard AI problems for security. In E. Biham, editor, EUROCRYPT, volume 2656 of Lecture Notes in Computer Science, pages 294–311. Springer, 2003.
[43] F. Walter, S. Battiston, and F. Schweitzer. A model of a trust-based recommendation system on a social network. Autonomous Agents and Multi-Agent Systems, 16:57–74, 2008.
[44] H. Yeend. Breaking CAPTCHA without OCR. http://www.puremango.co.uk/2005/11/breaking_captcha_115/, November 2005.
[45] H. Yu, P. B. Gibbons, M. Kaminsky, and F. Xiao. SybilLimit: A near-optimal social network defense against Sybil attacks. In Proceedings of the 2008 IEEE Symposium on Security and Privacy, pages 3–17, Washington, DC, USA, 2008. IEEE Computer Society.