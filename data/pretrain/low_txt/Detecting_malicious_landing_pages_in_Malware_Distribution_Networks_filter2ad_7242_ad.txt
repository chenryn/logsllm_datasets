### 图7：字符串聚类分类器概率的直方图

图7展示了从一百万个URL样本中抽取的子样本，其字符串聚类分类器的概率分布。不同的特征可能出现在大量网页中。对于最高概率分类器方法和随机抽样实验，我们分别选择了具有最高概率的分类器和随机选取的URL。

从表中可以看出以下几点：
1. 基于分类器的方法检测到的URL比例远低于基于规则的系统。
2. 最高概率和随机抽样方法的检测率大致相当。
3. 分类器的实际性能远不如图4所显示的那样好。图4表明，可以实现低于20%的假阴性率且几乎没有假阳性。然而，表IV显示了98.4%的假阳性率：分类器在实际应用中的表现远不如训练集上的表现。我们在第五部分探讨了这种失败的原因。

### 检测重叠分析

接下来，我们试图了解基于分类器的方法是否能够检测出基于规则的方法所检测到的相同URL集合。为此，我们计算了基于规则的方法与基于分类器的方法（选择高概率URL）之间的重叠度 ˆORB,HP：

\[ \hat{O}_{RB,HP} = \frac{| RBD \cap HPD |}{| HPD |} = 70.5\% \]

其中，RBD是基于规则方法检测到的URL集合，HPD是基于高概率分类器方法检测到的URL集合。这里的“检测”指的是通过某种方法预测为恶意并随后由动态爬虫确认为恶意的URL集合。类似地，基于规则的方法与基于分类器的方法（选择随机抽样URL）之间的重叠度 ˆORB,RS 为23.1%。

从这些估计中，我们可以看出基于规则的方法倾向于选择被分类器预测为更高概率恶意的着陆页。同时，我们也确认了基于分类器的方法仅检测到基于规则方法检测到的一部分URL。

### 讨论

在前两节中，我们提出了利用规则或分类器来检测MDN着陆页的系统。表IV的结果清楚地表明，基于规则的系统在验证率方面优于分类器。这一改进主要归功于引入了假阳性修剪阶段。如图7所示，分类器预测某些页面比其他页面更有可能是恶意的。在研究开始时，我们期望这将允许我们为分类器选择一个操作阈值，以自动检测恶意网页，同时避免假阳性修剪。然而，表IV的结果未能支持这一假设。根据动态爬虫的确认，分类器的假阳性率显著高于预期，无论是在高概率还是随机抽样实验中。

分类器产生高假阳性的一个可能原因是我们的训练数据集，尤其是良性集，覆盖范围有限。例如，我们发现分类器产生的一些假阳性是由与俄罗斯搜索引擎Yandex相关的访问分析服务的字符串引起的。进一步调查后，我们确认了我们的合法网页内容没有访问该服务，但一些使用俄文URL（即.ru）的MDN却访问了该服务。这意味着，如果我们包括一个更全面的良性集（其中一些可能使用该分析服务），我们可以有效地减少分类器的假阳性。

### 可能的应用场景

第二种基于分类器的方法的可能应用场景是对检测到的网页进行排名，以便首先验证更多可能的恶意网页。目前，我们无法使用动态爬虫扫描所有由检测器检测到的URL。然而，可以考虑使用生产环境中的动态爬虫验证更有针对性的URL子集。而不是验证随机URL样本或对应于最高分类器概率的样本，可以选择包含特征选择过程中识别出的一个或多个特征的固定数量的URL（例如100个），并基于确认为恶意的着陆页百分比来验证各个特征。这种方法类似于提出的假阳性修剪策略，但不仅仅是考虑之前被动态爬虫扫描过的URL，而是验证所有可能注入的字符串。这样，该方法可以考虑以前未在检测到的网页上发现的新字符串聚类特征。

### 攻击者规避和滥用检测系统的情况

攻击者可以通过多种方式规避或滥用检测系统。例如，在基于规则检测的假阳性修剪阶段，如果60%或更多的包含某个特征的页面之前被动态爬虫扫描并检测为恶意，则保留该特征。理论上，攻击者可以通过在41%的不发动攻击的着陆页中包含该特征来滥用修剪阶段。然而，考虑到在合法着陆页中注入恶意代码的开销和难度，这种情况在实践中不太可能发生。放弃41%的注入着陆页对攻击者来说是一个巨大的损失。

此外，多态性和混淆注入也是我们系统的挑战之一。如今，攻击者倾向于通过使代码难以阅读来隐藏注入代码的真实目的。有无数种方法可以混淆一段代码，使其看起来完全不同且难以理解。我们的方案可以应对被混淆或多态的内容，但不能同时应对两者。在极端情况下，如果攻击者在每个MDN的着陆页上以不同方式混淆代码，特征选择算法将无法识别这些恶意字符串。

### 系统协同工作

我们的系统与动态和静态爬虫协同工作。爬虫面临的一些常见挑战可能会影响我们的系统。首先，动态爬虫会暴露易受攻击的组件，并检测恶意响应。因此，由于易受攻击组件的配置有限，动态爬虫可能会错过一些攻击。此外，伪装是静态和动态爬虫共同关注的问题。攻击者可以通过知道爬虫的IP地址或检测虚拟机的存在来识别爬虫，并通过不尝试利用漏洞或返回合法内容来逃避检测。

最后，我们的系统需要一个初始的MDN集合，因此可以被视为其他检测技术的补充，而不是独立系统。

### 相关工作

为了防御驱动下载攻击，研究界和工业界已经提出了许多系统。这些系统采用不同的策略来检测驱动下载攻击，包括分类恶意URL或网页内容、主动利用客户端蜜罐探测Web服务器、从单个实例或关联多个活动内的着陆页来识别攻击。我们从以下几个方面对这些系统和技术进行分类。

首先，大多数系统从着陆页开始：它们访问恶意网站，执行嵌入的脚本，并在监控操作系统可疑状态变化的同时遍历次级URL，以检测驱动下载攻击。例如，[15]、[16]、[17]、[18]等系统会动态执行网页内容，并基于签名或异常检测捕获驱动下载。PhoneyC [19] 和 WebPatrol [20] 使用基于签名的低交互蜜罐来检测恶意网站。此外，Nozzle [11] 是一种运行时监控基础设施，用于检测恶意堆喷射尝试，而JSAND [21] 则采用机器学习方法来分类恶意JavaScript。Rozzle [22] 是另一种JavaScript虚拟机，它在一个执行中探索多个执行路径，以增加触发恶意脚本的可能性。Blade [23] 利用用户行为模型进行驱动下载检测。所有这些系统都表现出良好的检测结果。然而，跟踪完整的重定向路径并在运行时监控每个脚本执行通常成本较高。此外，它们的准确性高度依赖于网页对易受攻击组件的恶意响应。

有些系统则反向工作：它们从托管恶意利用的服务器开始，逆向追踪重定向路径以发现相应的着陆页。利用动态爬虫或公共杀毒数据库贡献的驱动下载痕迹，这些系统可以发现像MDN这样的恶意软件分发网络 [4]。现有的系统 [8]、[24] 属于这一类别。WebCop [24] 使用静态超链接构建恶意软件分发网络的网页图。WebCop 的局限性在于其静态网页图和精确匹配方法，这使得它在面对网页动态变化时不够健壮。Arrow [8] 检测并生成复杂MDN中心服务器的URL签名，以检测共享这些中心服务器的更多着陆页。在本文中，我们同样采用了这种方法来发现MDN。与 [8]、[24] 不同，我们同时利用静态和动态爬虫，并专注于着陆页内容中的恶意代码而非URL。Arrow 只能找到先前被动态爬虫扫描但未检测到的额外网页。

此外，许多现有系统 [12]、[25]、[26] 采用了静态分析技术来检测恶意网页。这些系统大多探索恶意JavaScript代码中的多种特征：例如，Zozzle [12] 专注于JavaScript代码中的上下文特征以检测堆喷射。像Prophiler [25] 这样的系统不仅考虑JavaScript特征，还提取恶意页面的HTML内容和URL中的附加特征。最近的系统EvilSeed [26] 通过分析已知恶意页面的内容、DNS痕迹和链接拓扑来生成小工具，然后使用这些小工具来发现类似的页面。与蜜罐方法的动态分析相比，上述静态分析更加轻量且耗时较少。我们的工作与现有工作的检测范围和考虑的特征不同：之前的系统是针对网页中一般恶意内容的通用检测器，而我们的系统专门针对特定MDN的着陆页特征。

最后，SURF [27] 和 deSEO [28] 从黑帽搜索引擎优化（SEO）的角度研究了恶意软件分发活动。在典型的驱动下载攻击中，着陆页的一个重要功能是将流量引向利用服务器。攻击者一直在尝试各种SEO技术，以在热门查询的搜索结果中提高恶意着陆页的排名。SURF 使用从搜索-然后-访问浏览会话中提取的特征来检测来自搜索结果的恶意重定向。它可以作为动态爬虫中的扫描器工作。deSEO 则是一种基于签名的检测器，使用搜索结果中恶意URL的模式。在我们的方法中，我们使用重定向路径中的URL来发现大型恶意软件分发网络，但我们的检测器更专注于网页内容中的恶意代码。

### 结论

在本文中，我们分析了导致驱动下载攻击的恶意软件分发网络（MDN）中的着陆页。利用生产环境中动态爬虫收集的大规模驱动下载痕迹，我们展示了同一MDN内的着陆页在其导致重定向的恶意内容方面具有一定的相似性。借助多类特征选择，我们能够识别出最能区分一个MDN与其他MDN及良性网页的MDN特有特征。

我们提出并实现了两种新的解决方案来高效检测恶意着陆页。第一个基于规则的系统基于字符串聚类匹配，能够高精度地识别MDN内着陆页中的恶意注入代码。该系统生成的URL列表中有超过57%被生产环境中的动态爬虫独立验证为恶意。这一成功率在相隔五个月的两次大规模试验中保持不变。这将所研究MDN的已知足迹扩展了17%。

第二个系统实现了一个分类器，生成的URL列表中有大约1%被独立确认为恶意。与基于规则的方法相比，我们得出结论，在不使用更大的训练集的情况下，分类器在这种设置下提供的效用很小。我们将分类器系统的结果包括在论文中以供完整性。

### 致谢

作者感谢审稿人的反馈，以及Yinglian Xie和Fang Yu提供用于计算正则表达式的软件，Christian Seifert对搜索引擎和爬虫的见解，以及Sarmad Fayyaz在运行实验方面的指导。

### 参考文献

[1] C. Jackson, “Improving Browser Security Policies,” Ph.D. dissertation, Stanford University, 2009.

[4] N. Provos, P. Mavrommatis, M. Abu Rajab and F. Monrose, “All your iframes points to us,” in Proc. USENIX Security, 2008.

[5] Trustseer Security Advisory, “Flash Security Hole Advisory,” http://www.trusteer.com/files/Flash Security Hole Advisory.pdf, 2009.

[6] S. S. David Wang and G. M. Voelker, “Cloak and dagger: Dynamics of web search cloaking,” in Proc. ACM CCS, 2011.

[7] M. Roesch, “Snort - lightweight intrusion detection for networks,” in Proc. USENIX LISA, 1999.

[8] J. Zhang, C. Seifert, J. W. Stokes, and W. Lee, “Arrow: Generating signatures to detect drive-by downloads,” in Proc. WWW, 2011.

[9] G. Ball and D. Hall, “ISODATA, a novel method of data analysis and pattern classification,” DTIC Document, Tech. Rep., 1965.

[10] C. D. Manning, P. Raghavan, and H. Schtze, An Introduction to Information Retrieval. Cambridge University Press, 2009.

[11] P. Ratanaworabhan, B. Livshits, and B. Zorn, “Nozzle: A defense against heap-spraying code injection attacks,” in Proc. USENIX Security, 2009.

[12] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, “Zozzle: Low-overhead mostly static javascript malware detection,” in Proc. USENIX Security, 2011.

[13] Y. Xie, F. Yu, K. Achan, R. Panigraphy, G. Hulten and I. Osipkov, “Spamming botnets: Signatures and characteristics,” in Proc. ACM SIGCOMM, 2008.

[14] C. Bishop, Pattern Recognition and Machine Learning. Springer, 2006.

[15] A. Moshchuk, T. Bragin, S. D. Gribble, and H. M. Levy, “A crawler-based study of spyware on the web,” in Proc. NDSS, 2006.

[16] C. Seifert and R. Steenson, “Capture - honeypot client (capture-hpc),” https://projects.honeynet.org/capture-hpc, 2006.

[17] C. Seifert, R. Steenson, T. Holz, B. Yuan and M. A. Davis, “Know your enemy: Malicious web servers,” http://www.honeynet.org/papers/mws/, 2007.

[18] Y.-M. Wang, D. Beck, X. Jiang, R. Roussev, C. Verbowski, S. Chen and S. King, “Automated web patrol with strider honeymonkeys: Finding web sites that exploit browser vulnerabilities,” in Proc. NDSS, 2006.

[19] J. Nazario, “Phoneyc: A virtual client honeypot,” in Proc. USENIX LEET, 2009.

[20] K. Z. Chen, G. Gu, J. Zhuge, J. Nazario, and X. Han, “Webpatrol: automated collection and replay of web-based malware scenarios,” in Proc. ASIACCS, 2011.

[21] M. Cova, C. Kruegel and G. Vigna, “Detection and analysis of drive-by-download attacks and malicious javascript code,” in Proc. WWW, 2010.

[22] C. Kolbitsch, B. Livshits, B. Zorn, and C. Seifert, “Rozzle: De-cloaking internet malware,” in Proc. IEEE Symposium on Security and Privacy, 2012.

[23] L. Lu, V. Yegneswaran, P. Porras and W. Lee, “Blade: An attack-agnostic approach for preventing drive-by malware infections,” in Proc. ACM CCS, 2010.

[24] J. W. Stokes, R. Andersen, C. Seifert and K. Chellapilla, “Webcop: Locating neighborhoods of malware on the web,” in Proc. USENIX LEET, 2010.

[25] D. Canali, M. Cova, C. Kruegel, and G. Vigna, “Prophiler: A fast filter for the large-scale detection of malicious web pages,” in Proc. WWW, 2011.

[26] L. Invernizzi, S. Benvenuti, P. M. Comparetti, M. Cova, C. Kruegel, and G. Vigna, “Evilseed: A guided approach to finding malicious web pages,” in Proc. IEEE Symposium on Security and Privacy, 2012.

[2] R. Dhamija, J. D. Tygar, and M. Hearst, “Why phishing works,” in Proc. CHI, 2006.

[27] L. Lu, R. Perdisci, and W. Lee, “SURF: detecting and measuring search poisoning,” in Proc. ACM CCS, 2011.

[3] S. Schechter, R. Dhamija, A. Ozment, I. Fischer, “The Emperor’s New Security Indicators: An evaluation of website authentication and the effect of role playing on usability studies,” in Proc. IEEE Symposium on Security and Privacy, 2007.

[28] J. P. John, F. Yu, Y. Xie, A. Krishnamurthy, and M. Abadi, “eSEO: Combating search-result poisoning,” in Proc. USENIX Security, 2011.