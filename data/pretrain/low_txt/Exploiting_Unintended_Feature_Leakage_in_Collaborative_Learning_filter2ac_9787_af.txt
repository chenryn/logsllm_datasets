### Improved Text

It is unclear whether the model can distinguish between a training image and another image from the same MNIST class. Song et al. [55] developed an ML model that memorizes training data, which can then be extracted with black-box access to the model. Carlini et al. [6] demonstrated that deep learning-based generative sequence models trained on text data can unintentionally memorize training inputs, which can also be extracted with black-box access. They illustrated this by introducing sequences of digits into the text, which are not influenced by the relative word frequencies in the language model.

Training data that is explicitly incorporated or otherwise memorized in the model can also be leaked through model stealing attacks [41, 57, 61]. Concurrently, Ganju et al. [18] developed property inference attacks against fully connected, relatively shallow neural networks. Their focus was on the post-training, white-box release of models trained on sensitive data, as opposed to collaborative training. Unlike our attacks, the properties inferred in [18] may be correlated with the main task. The evaluation was limited to simple datasets and tasks such as MNIST, U.S. Census tabular data, and hardware performance counters with short features.

### Conclusion

In this paper, we proposed and evaluated several inference attacks against collaborative learning. These attacks enable a malicious participant to infer not only membership (i.e., the presence of exact data points in other participants’ training data) but also properties that characterize subsets of the training data and are independent of the properties that the joint model aims to capture.

Deep learning models appear to internally recognize many features of the data that are uncorrelated with the tasks they are being trained for. Consequently, model updates during collaborative learning leak information about these "unintended" features to adversarial participants. Active attacks are particularly powerful in this setting because they allow the adversary to trick the joint model into learning features of their choosing without significantly impacting the model’s performance on its main task.

Our results suggest that the leakage of unintended features exposes collaborative learning to powerful inference attacks. We also showed that defenses such as selective gradient sharing, reducing dimensionality, and dropout are not effective. This should motivate future work on better defenses. For instance, techniques that learn only the features relevant to a given task [15, 42, 43] can potentially serve as the basis for "least-privilege" collaboratively trained models. Additionally, it may be possible to detect active attacks that manipulate the model into learning extra features. Finally, it remains an open question whether participant-level differential privacy mechanisms can produce accurate models when collaborative learning involves relatively few participants.

### Acknowledgments

This research was supported in part by NSF grants 1611770 and 1704296, the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program, the Alan Turing Institute under the EPSRC grant EP/N510129/1, and a grant by Nokia Bell Labs.

### References

[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In CCS, 2016.
[2] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers. IJSN, 10(3):137–150, 2015.
[3] M. Backes, P. Berrang, M. Humbert, and P. Manoharan. Membership Privacy in MicroRNA-based Studies. In CCS, 2016.
[4] BBC. Google DeepMind NHS app test broke UK privacy law. https://bbc.in/2A1MftK, 2017.
[5] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth. Practical secure aggregation for privacy-preserving machine learning. In CCS, 2017.
[6] N. Carlini, C. Liu, J. Kos, Ú. Erlingsson, and D. Song. The Secret Sharer: Measuring unintended neural network memorization & extracting secrets. arXiv:1802.08232, 2018.
[7] C.-H. Chang, L. Rampasek, and A. Goldenberg. Dropout feature ranking for deep learning models. arXiv:1712.08645, 2017.
[8] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv:1512.01274, 2015.
[9] T. M. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. Project Adam: Building an efficient and scalable deep learning training system. In OSDI, 2014.
[10] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.
[11] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior, P. Tucker, K. Yang, Q. Le, et al. Large scale distributed deep networks. In NIPS, 2012.
[12] S. Dieleman, J. Schlüter, C. Raffel, et al. Lasagne: First release. http://dx.doi.org/10.5281/zenodo.27878, 2015.
[13] C. Dwork and M. Naor. On the difficulties of disclosure prevention in statistical databases or the case for differential privacy. Journal of Privacy and Confidentiality, 2(1):93–107, 2010.
[14] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan. Robust traceability from trace amounts. In FOCS, 2015.
[15] H. Edwards and A. Storkey. Censoring representations with an adversary. In ICLR, 2016.
[16] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In CCS, 2015.
[17] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of personalized Warfarin dosing. In USENIX Security, 2014.
[18] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov. Property inference attacks on fully connected neural networks using permutation invariant representations. In CCS, 2018.
[19] General Data Protection Regulation. https://en.wikipedia.org/wiki/General_Data_Protection_Regulation, 2018.
[20] R. C. Geyer, T. Klein, and M. Nabi. Differentially private federated learning: A client-level perspective. arXiv:1712.07557, 2017.
[21] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep Learning. MIT Press, 2016.
[22] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.
[23] J. Hamm, Y. Cao, and M. Belkin. Learning privately from multiparty data. In ICML, 2016.
[24] J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro. LOGAN: Membership inference attacks against generative models. In PETS, 2019.
[25] B. Hitaj, G. Ateniese, and F. Pérez-Cruz. Deep models under the GAN: Information leakage from collaborative deep learning. In CCS, 2017.
[26] T. K. Ho. Random decision forests. In DAR, 1995.
[27] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays. PLoS Genetics, 4(8), 2008.
[28] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07–49, University of Massachusetts, Amherst, 2007.
[29] A. Jochems, T. M. Deist, I. El Naqa, M. Kessler, C. Mayo, J. Reeves, S. Jolly, M. Matuszak, R. Ten Haken, J. van Soest, et al. Developing and validating a survival prediction model for NSCLC patients through distributed learning across 3 countries. Int J Radiat Oncol Biol Phys, 99(2):344–352, 2017.
[30] A. Jochems, T. M. Deist, J. Van Soest, M. Eble, P. Bulens, P. Coucke, W. Dries, P. Lambin, and A. Dekker. Distributed learning: Developing a predictive model based on data from multiple hospitals without data leaving the hospital–a real-life proof of concept. Radiother Oncol, 121(3):459–467, 2016.
[31] Y. Kim. Convolutional neural networks for sentence classification. arXiv:1408.5882, 2014.
[32] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 2015.
[33] Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In ICLR, 2018.
[34] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A. Gunter, and K. Chen. Understanding membership inferences on well-generalized learning models. arXiv:1802.04889, 2018.
[35] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, et al. Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017.
[36] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private language models without losing accuracy. In ICLR, 2018.
[37] P. Mohassel and Y. Zhang. SecureML: A system for scalable privacy-preserving machine learning. In S&P, 2017.
[38] P. Moritz, R. Nishihara, I. Jordan, I. Stoica, and M. I. Jordan. SparkNet: Training deep networks in Spark. arXiv:1511.06051, 2015.
[39] M. Nasr, R. Shokri, and A. Houmansadr. Machine learning with membership privacy using adversarial regularization. In CCS, 2018.
[40] H.-W. Ng and S. Winkler. A data-driven approach to cleaning large face datasets. In ICIP, 2014.
[41] S. J. Oh, M. Augustin, M. Fritz, and B. Schiele. Towards reverse-engineering black-box neural networks. In ICLR, 2018.
[42] S. A. Osia, A. S. Shamsabadi, A. Taheri, K. Katevas, S. Sajadmanesh, H. R. Rabiee, N. D. Lane, and H. Haddadi. A hybrid deep learning architecture for privacy-preserving mobile analytics. arXiv:1703.02952, 2017.
[43] S. A. Osia, A. Taheri, A. S. Shamsabadi, K. Katevas, H. Haddadi, and H. R. Rabiee. Deep private-feature extraction. TKDE, 2019.
[44] J. Pang and Y. Zhang. DeepCity: A feature learning framework for mining location check-ins. In ICWSM (Poster Papers), 2017.
[45] N. Papernot, M. Abadi, U. Erlingsson, I. Goodfellow, and K. Talwar. Semi-supervised knowledge transfer for deep learning from private training data. In ICLR, 2017.
[46] N. Papernot, S. Song, I. Mironov, A. Raghunathan, K. Talwar, and U. Erlingsson. Scalable private learning with PATE. In ICLR, 2018.
[47] M. Pathak, S. Rane, and B. Raj. Multiparty differential privacy via aggregation of locally trained classifiers. In NIPS, 2010.
[48] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. JMLR, 12, 2011.
[49] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Mori. Privacy-preserving deep learning: Revisited and enhanced. In ATIS, 2017.
[50] A. Pyrgelis, C. Troncoso, and E. De Cristofaro. Knock knock, who’s there? Membership inference on aggregate location data. In NDSS, 2018.
[51] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 2015.
[52] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In CCS, 2015.
[53] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In S&P, 2017.
[54] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
[55] C. Song, T. Ristenpart, and V. Shmatikov. Machine learning models that remember too much. In CCS, 2017.
[56] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 2014.
[57] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. Stealing machine learning models via prediction APIs. In USENIX Security, 2016.
[58] S. Truex, L. Liu, M. E. Gursoy, L. Yu, and W. Wei. Towards demystifying membership inference attacks. arXiv:1807.09173, 2018.
[59] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. JMLR, 2008.
[60] B. Verhoeven and W. Daelemans. CLiPS Stylometry Investigation (CSI) Corpus: A Dutch corpus for the detection of age, gender, personality, sentiment, and deception in text. In LREC, 2014.
[61] B. Wang and N. Z. Gong. Stealing hyperparameters in machine learning. In S&P, 2018.
[62] E. P. Xing, Q. Ho, W. Dai, J. K. Kim, J. Wei, S. Lee, X. Zheng, P. Xie, A. Kumar, and Y. Yu. Petuum: A new platform for distributed machine learning on big data. IEEE Transactions on Big Data, 2015.
[63] D. Yang, D. Zhang, L. Chen, and B. Qu. NationTelescope: Monitoring and visualizing large-scale collective behavior in LBSNs. JNCA, 55:170–180, 2015.
[64] D. Yang, D. Zhang, and B. Qu. Participatory cultural mapping based on collective behavior in location-based social networks. ACM TIST, 2015.
[65] D. Yang, D. Zhang, B. Qu, and P. Cudré-Mauroux. PrivCheck: Privacy-preserving check-in data publishing for personalized location-based services. In UbiComp, 2016.
[66] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In CSF, 2018.
[67] N. Zhang, M. Paluri, Y. Taigman, R. Fergus, and L. Bourdev. Beyond frontal faces: Improving person recognition using multiple cues. In CVPR, 2015.
[68] M. Zinkevich, M. Weimer, L. Li, and A. J. Smola. Parallelized stochastic gradient descent. In NIPS, 2010.