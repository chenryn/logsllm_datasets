### Figures and Captions

**Figure 5: VAMO vs. Majority Voting: Rand Statistic**
- (a) Flips and Missing (> 1 AV)
- (b) Only Flips
- (c) Only Missing

**Figure 6: VAMO vs. Majority Voting: Jaccard Coefficient**
- (a) Flips and Missing (> 1 AV)
- (b) Only Flips
- (c) Only Missing

**Figure 7: VAMO vs. Majority Voting: Folkes-Mallows Index**
- (a) Flips and Missing (> 1 AV)
- (b) Only Flips
- (c) Only Missing

**Figure 8: VAMO vs. Majority Voting: F1 Index**
- (a) Flips and Missing (> 1 AV)
- (b) Only Flips
- (c) Only Missing

### Section 4 and 5: Clustering and Validation

Given the dendrogram \( Y \) obtained from a third-party malware clustering system [2], we cut \( Y \) at different heights \( l_1, l_2, \ldots, l_n \), thus obtaining a sequence of different clusterings \( C(l_1), C(l_2), \ldots, C(l_n) \). For each clustering result, we used VAMO to compute a set of validity indexes (see Section 5). Table 2 summarizes our results. The first column in Table 2 reports the value of the height \( l \) at which \( Y \) is cut, while the second column reports the related number of clusters obtained from \( M \). For example, by cutting \( Y \) at height \( l = 0.5 \), \( M \) is partitioned into 187 clusters. The remaining columns represent the values of five different external cluster validity indexes (Section 3.2) measured by comparing the obtained malware clusters to VAMO’s reference clustering, as explained in Section 5.

We varied \( l \in [0, 1] \) at steps equal to 0.01 (excluding the extreme values \( l = 0 \) and \( l > 0.8 \), because they result either in one malware per cluster or artificially large clusters, respectively). In the interest of space, since the maximum value of the validity indexes is located between \( l = 0.3 \) and \( l = 0.4 \), we report the results at steps of 0.01 only within that range.

From Table 2, the best value of the cut \( l \) is 0.31, as this is the height at which three out of four external validity indexes indicate the maximum agreement between the behavior-based clusters and the AV labels generated by four different AV scanners. Put another way, VAMO’s results suggest that the AV labels provide the best explanation of the underlying malware dataset \( M \) when \( M \) is partitioned into 301 clusters by cutting \( Y \) at \( l = 0.31 \).

It is worth noting that the F1 index is the only external validity index that does not reach its maximum at \( l = 0.31 \). However, the value of 0.8436 obtained at \( l = 0.31 \) is quite close to the maximum value of 0.8502 reached at \( l = 0.33 \). This suggests that to find the best configuration parameters for the third-party malware clustering system, it may be better to consider multiple validity indexes rather than focusing solely on precision and recall (and the related F1 index), as proposed in previous work [2, 11].

### Discussion

Using AV labels to build a reference clustering has some potential limitations, even though label inconsistencies can be mitigated using VAMO. First, we must consider that the features used by AVs to characterize malware samples and assign them to a given malware family may differ from the features used by a third-party malware clustering system to measure sample similarity. For example, AV vendors often base their malware categorization on features extracted from reverse engineering the malware binaries, while behavior-based malware clustering systems leverage features related to the malware’s system [2] or network activities [15]. Different features may highlight different types of similarities in the samples. Therefore, while AV labels are a valuable point of reference, especially in the absence of a more perfect ground truth, the comparison between behavior-based malware clustering results and AV family labels should be taken with caution. A similar argument is made in [4], where the authors outline the potential pitfalls of using labeled datasets meant for training and testing supervised learning algorithms to evaluate unsupervised clustering algorithms.

Another factor to consider is that AV labels evolve over time. A malware sample \( m \) assigned by an AV to family \( f_i \) at time \( t_0 \) may be "renamed" by the same AV as belonging to a different family \( f_j \) at a future time \( t_1 > t_0 \). This is due to AV signatures being refined to reduce false positives and more specifically characterize malware samples (e.g., by assigning a previously generic label to a more specific malware family). To account for this, the historic archive of malware labels used by VAMO should be kept updated. This can be done by periodically rescanning the malware dataset or querying online services such as virustotal.com.

### Conclusion

In this paper, we presented VAMO, a novel system that provides a fully automated assessment of the quality of malware clustering results. Previous studies have proposed evaluating malware clustering results by leveraging labels assigned to malware samples by multiple AVs, but they require manual mapping between labels assigned by different AV vendors and are limited to selecting a reference subset of samples for which an agreement can be reached across a majority of AVs. Unlike previous work, VAMO does not require manual mapping between malware family labels output by different AV scanners and does not discard malware samples for which a majority voting-based consensus cannot be reached. Instead, VAMO explicitly deals with the inconsistencies typical of multiple AV labels to build a more representative reference set. Our evaluation, which includes extensive experiments in a controlled setting and a real-world application, shows that VAMO performs better than majority voting-based approaches and provides a way for malware analysts to automatically assess the quality of their malware clustering results.

### Acknowledgments

This material is based in part upon work supported by the National Science Foundation under Grant No. CNS-1149051. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.

### References

[1] M. Bailey, J. Oberheide, J. Andersen, Z. M. Mao, F. Jahanian, and J. Nazario. Automated classification and analysis of internet malware. In Recent Advances in Intrusion Detection, 2007.

[2] U. Bayer, P. Milani Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda. Scalable, behavior-based malware clustering. In Network and Distributed System Security Symposium, 2009.

[3] M. Christodorescu, S. Jha, and C. Kruegel. Mining specifications of malicious behavior. In ACM SIGSOFT symposium on the foundations of software engineering, ESEC-FSE '07, 2007.

[4] I. Färber, S. Günnemann, H. Kriegel, P. Kröger, E. Müller, E. Schubert, T. Seidl, and A. Zimek. On using class-labels in evaluation of clusterings. In MultiClust: 1st International Workshop on Discovering, Summarizing and Using Multiple Clusterings Held in Conjunction with KDD, 2010.

[5] E. B. Fowlkes and C. L. Mallows. A method for comparing two hierarchical clusterings. Journal of the American Statistical Association, 78(383):553–569, 1983.

[6] F. Guo, P. Ferrie, and T. Chiueh. A study of the packer problem and its solutions. In Recent Advances in Intrusion Detection, 2008.

[7] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. On clustering validation techniques. J. Intell. Inf. Syst., 17(2-3):107–145, 2001.

[8] X. Hu, T.-c. Chiueh, and K. G. Shin. Large-scale malware indexing using function-call graphs. In Proceedings of the 16th ACM conference on Computer and communications security, CCS '09, 2009.

[9] A. K. Jain and R. C. Dubes. Algorithms for clustering data. Prentice-Hall, Inc., 1988.

[10] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Comput. Surv., 31(3):264–323, 1999.

[11] J. Jang, D. Brumley, and S. Venkataraman. Bitshred: feature hashing malware for scalable triage and semantic analysis. In Proceedings of the 18th ACM conference on Computer and communications security, CCS '11, 2011.

[12] P. Li, L. Liu, D. Gao, and M. K. Reiter. On challenges in evaluating malware clustering. In Proceedings of the 13th international conference on Recent advances in intrusion detection, RAID'10, 2010.

[13] F. Maggi, A. Bellini, G. Salvaneschi, and S. Zanero. Finding non-trivial malware naming inconsistencies. In International Conference on Information Systems Security, ICISS'11, 2011.

[14] M. Meilă. Comparing clusterings—an information based distance. J. Multivar. Anal., 98(5):873–895, May 2007.

[15] R. Perdisci, W. Lee, and N. Feamster. Behavioral clustering of HTTP-based malware and signature generation using malicious network traces. In Proceedings of the 7th USENIX Symposium on Networked Systems Design and Implementation, NSDI'10, 2010.

[16] D. Pfitzner, R. Leibbrandt, and D. Powers. Characterization and evaluation of similarity measures for pairs of clusterings. Knowl. Inf. Syst., 19(3):361–394, May 2009.

[17] E. Rendón, I. Abundez, A. Arizmendi, and E. M. Quiroz. Internal versus external cluster validation indexes. university-pressorguk, 5(1), 2011.

[18] K. Rieck, P. Trinius, C. Willems, and T. Holz. Automatic analysis of malware behavior using machine learning. J. Comput. Secur., 19(4):639–668, Dec. 2011.

[19] Symantec. Symantec Internet Security Threat Report, Trends for April 2011.