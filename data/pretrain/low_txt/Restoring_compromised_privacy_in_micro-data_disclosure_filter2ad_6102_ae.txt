### 4. Results

**Figure 1** illustrates that the percentage of restored cases increases to approximately 50% when there are 4 Quasi-Identifiers (QI) attributes. **Figure 2** provides a similar analysis, but for the scenario where up to two additional views are used to restore privacy. In this case, **Figure 2** shows that the percentage of restored cases increases to about 60% with 4 QI attributes.

It is important to note that when privacy was not restored, it does not necessarily mean that restoration was infeasible. Restoration might be possible if more flexible additional views are disclosed or if a larger number of additional views are provided. Consequently, the percentage of restored cases out of all restorable cases is likely higher or equal to the values depicted in the figures. Although this study is preliminary, it indicates that a significant percentage of cases can be restored even with a single additional disclosed view.

### 6. Discussion

In this section, we discuss and provide further justification for the assumptions made in the adversary model. The definition of γ-Privacy is based on the assumption that an adversary will attempt to compromise an individual's privacy in a private table through a random guessing game. This assumes that the adversary collects all disclosed information, including public knowledge and generalized disclosures from the private table. Two unaddressed assumptions are: (1) the adversary may only acquire part of the published information, and (2) the adversary may obtain additional information about the privacy restoration process. We will explore how the privacy restoration process can be affected by the second assumption in two different scenarios.

#### Intention of Privacy Restoring

The adversary may become aware of our intention to restore compromised privacy. To safely restore privacy in a given micro-data disclosure problem, the decision to perform privacy restoration should not depend on the original base table (baseT). Formally, the decision should be "simulatable" to the adversary. Consider a modified example of medical information disclosure discussed in Section 1. Suppose we have a different baseT as shown in Table 9, but the Public Knowledge disclosure (Table 2) and the two generalized disclosures (Tables 3(A, B)) remain the same. In this example, the adversary might be misled by the disclosed information and conclude that Donald has a high probability of having SARS. One might think it would be wise to maintain this status. However, this is not a safe decision because if the adversary learns that we have been attempting to restore privacy, the lack of action will immediately reveal that Donald has Viral Infection. This issue can be resolved by making the privacy restoration decision independent of the original baseT.

| Name   | Sex | Age | Employer     | Condition      |
|--------|-----|-----|--------------|----------------|
| Alan   | M   | 23  | ABC, Inc.    | SARS           |
| Bob    | M   | 24  | ABC, Inc.    | SARS           |
| Clark  | M   | 25  | ABC, Inc.    | Heart Disease  |
| Donald | M   | 26  | ABC, Inc.    | Viral Infection|
| Ellen  | F   | 27  | ABC, Inc.    | SARS           |
| Fen    | F   | 28  | ABC, Inc.    | Flu            |

**Table 9: Modified Patient Information Table**

#### Preferences in Disclosure Selection

The adversary may also discover two facts: (1) when restoring compromised privacy, we may have preferences in selecting additional generalized disclosures, and (2) among all released generalized disclosures, some are used to restore privacy. In this scenario, the adversary may have an opportunity to compromise an individual's privacy immediately. For instance, consider the same example in Table 9. To restore Donald's compromised privacy, we might disclose two additional generalized disclosures: one as shown in Table 5(B) and another as shown in Table 10(A). This works under the assumptions discussed in previous sections. However, if the adversary realizes that we are trying to minimize generalization in the additional disclosures, the fact that Table 10(B) is not disclosed will immediately reveal that Clark is highly likely to have Heart Disease. This is because if Donald has Viral Infection and Clark has SARS, Table 10(B) should have been disclosed instead of Table 10(A). Therefore, under such strong assumptions of the adversary model, we must be very careful in selecting additional generalized disclosures. A complete solution for this case remains a topic for future work.

| Age        | Condition      |
|------------|----------------|
| 24 or 26   | Viral Infection|
| 24 or 26   | SARS           |

**Table 10(A): Additional Disclosure Selection**

| Age        | Condition      |
|------------|----------------|
| 25 ∼ 26    | Viral Infection|
| 25 ∼ 26    | SARS           |

**Table 10(B): Additional Disclosure Selection**

### 7. Related Work

Initial studies [1, 4, 15, 20, 21] focused on conducting data censuses while protecting the privacy of sensitive information in disclosed tables. Data swapping [14, 28, 33] and data suppression [22] were suggested to protect data but could not quantify the level of protection. The work [12] provided a formal analysis of information disclosure in data exchange. The work [30] demonstrated that publishing datasets without identifying attributes can still cause privacy breaches and proposed the concept of k-anonymity. Achieving k-anonymity with optimal data utility was proven to be NP-hard [26]. A similar measure, called blending in a crowd, was proposed by [32]. The work [34] introduced a new generalization framework based on personalized anonymity. Many works, such as [11, 29, 30, 24, 31, 19], proposed efficient algorithms for k-anonymity. The work [3] discussed the deficiencies of k-anonymity and proposed l-diversity as an alternative property for ensuring privacy in micro-data disclosure. The work [35] focused on potential information disclosure when optimization exists. It would be interesting to see how our technique can be extended using their solutions to adapt to more flexible assumptions of the adversary's knowledge.

In statistical databases [27, 15, 17], a typical problem is how to "safely" answer aggregation queries so that sensitive data on individuals is not disclosed. The works [10, 8] addressed this problem by auditing and deciding whether a new query can be answered based on the database state and previously answered queries. The works [10, 16, 18] considered the same problem in the specific settings of offline and online auditing, respectively. The work [18] also considered the knowledge contained in the decision algorithm itself.

### 8. Conclusion

In this paper, we studied the problem of restoring compromised privacy in micro-data disclosure with multiple disclosed views by disclosing more views, both in terms of theoretical foundations and practical solutions. Many research questions remain open. One direction is a more comprehensive study of heuristic algorithms for privacy restoration, focusing on their complexity and efficacy. Another direction is the notion of optimality, i.e., how to decide on the best method to restore privacy when multiple options are available. Extending our results to additional, possibly more relaxed, adversary models is also important.

### 9. Acknowledgments

This material is based upon work supported by the National Science Foundation under grants CT-20013A, CT-0716567, CT-0716323, and CT-0627493; by the Air Force Office of Scientific Research under grants FA9550-07-1-0527, FA9550-09-1-0421, and FA9550-08-1-0157; and by the Army Research Office under grant W911NF-09-01-0352. We also thank the anonymous reviewers for their comments and kind suggestions.

### 10. References

[1] A.Dobra and S.E.Feinberg. Bounding entries in multi-way contingency tables given a set of marginal totals. In Foundations of Statistical Inference: Proceedings of the Shoresh Conference 2000. Springer Verlag, 2003.

[2] R. Agrawal and R. Srikant. Privacy-preserving data mining. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 439–450, May 2000.

[3] A.Machanavajjhala, J.Gehrke, D.Kifer, and M.Venkitasubramaniam. l-diversity: Privacy beyond k-anonymity. In Proceedings of the 22nd IEEE International Conference on Data Engineering (ICDE 2006), 2006.

[4] A.Slavkovic and S.E.Feinberg. Bounds for cell entries in two-way tables given conditional relative frequencies. Privacy in Statistical Databases, 2004.

[5] A. Asuncion and D. Newman. UCI machine learning repository (Data Provider: Andras.Janosi, Hungarian Institute of Cardiology; William.Steinbrunn, University Hospital, Zurich, Switzerland; Matthias.Pfisterer, University Hospital, Basel, Switzerland; Robert.Detrano, V.A. Medical Center, Long Beach and Cleveland Clinic Foundation.), 2007.

[6] M. Bellare and P. Rogaway. Random oracles are practical: A paradigm for designing efficient protocols. In CCS, 1995.

[7] A. Bertoni, M. Goldwurm, and M. Santini. Random generation and approximate counting of ambiguously described combinatorial structures. In STACS, pages 567–580, 2000.

[8] D.P.Dobkin, A.K.Jones, and R.J.Lipton. Secure databases: Protection against user influence. ACM: Transactions on Database Systems (TODS), 4(1):76–96, 1979.

[9] M. Dyer, R. Kannan, and J. Mount. Sampling contingency tables. In CCC, pages 487–506, 1997.

[10] F.Chin. Security problems on inference control for sum, max, and min queries. J.ACM, 33(3):451–464, 1986.

[11] G.Aggarwal, T.Feder, K.Kenthapadi, R.Motwani, R.Panigrahy, D.Thomas, and A.Zhu. k-anonymity: Algorithms and hardness. Technical report, Stanford University, 2004.

[12] G.Miklau and D.Suciu. A formal analysis of information disclosure in data exchange. In SIGMOD, 2004.

[13] P. W. P. J. Grefen and R. A. d. By. A multi-set extended relational algebra - a formal approach to a practical issue. In Proceedings of the Tenth International Conference on Data Engineering, pages 80–88, 1994.

[14] G.T.Duncan and S.E.Feinberg. Obtaining information while preserving privacy: A Markov perturbation method for tabular data. In Joint Statistical Meetings. Anaheim, CA, 1997.

[15] I.P.Fellegi. On the question of statistical confidentiality. Journal of the American Statistical Association, 67(337):7–18, 1993.

[16] J.Kleinberg, C.Papadimitriou, and P.Raghavan. Auditing Boolean attributes. In PODS, 2000.

[17] J.Schorer. Identification and retrieval of personal records from a statistical bank. In Methods Info. Med., 1975.

[18] K.Kenthapadi, N.Mishra, and K.Nissim. Simulatable auditing. In PODS, 2005.

[19] K.LeFevre, D.DeWitt, and R.Ramakrishnan. Incognito: Efficient full-domain k-anonymity. In SIGMOD, 2005.

[20] L.H.Cox. Solving confidentiality protection problems in tabulations using network optimization: A network model for cell suppression in the U.S. economic censuses. In Proceedings of the International Seminar on Statistical Confidentiality, pages 229–245. International Statistical Institute, Dublin, 1982.

[21] L.H.Cox. New results in disclosure avoidance for tabulations. In International Statistical Institute Proceedings of the 46th Session, pages 83–84. Tokyo, 1987.

[22] L.H.Cox. Suppression, methodology and statistical disclosure control. Journal of the American Statistical Association, 90:1453–1462, 1995.

[23] N. Li and T. Li. t-closeness: Privacy beyond k-anonymity and l-diversity. In ICDE, 2007.

[24] L.Sweeney. k-anonymity: A model for protecting privacy. International Journal on Uncertainty, Fuzziness and Knowledge-based Systems, 10(5):557–570, 2002.

[25] M. Mether. The history of the central limit theorem. Sovelletun Matematiikan erikoistyöt, Mat-2(108), 2003.

[26] A. Meyerson and R. Williams. On the complexity of optimal k-anonymity. In ACM Symposium on Principles of Database Systems (PODS), 2004.

[27] N.R.Adam and J.C.Wortmann. Security-control methods for statistical databases: A comparative study. ACM Comput. Surv., 21(4):515–556, 1989.

[28] P.Diaconis and B.Sturmfels. Algebraic algorithms for sampling from conditional distributions. Annals of Statistics, 1:363–397, 1998.

[29] P.Samarati. Protecting respondents’ identities in microdata release. In IEEE Transactions on Knowledge and Data Engineering, pages 1010–1027, 2001.

[30] P.Samarati and L.Sweeney. Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression. Technical report, CMU, SRI, 1998.

[31] R.J.Bayardo and R.Agrawal. Data privacy through optimal k-anonymization. In ICDE-2005, 2005.

[32] S.Chawla, C.Dwork, F.McSherry, A.Smith, and H.Wee. Toward privacy in public databases. In Theory of Cryptography Conference, 2005.

[33] T.Dalenius and S.Reiss. Data swapping: A technique for disclosure control. Journal of Statistical Planning and Inference, 6:73–85, 1982.

[34] X.Xiao and Y.Tao. Personalized privacy preservation. In SIGMOD, 2006.

[35] L. Zhang, S. Jajodia, and A. Brodsky. Information disclosure under realistic assumptions: Privacy versus optimality. In ACM Conference on Computer and Communications Security (CCS) 2007.