### Optimized Text

We can move the highly efficient original OFA primitive to the preprocessing phase, similar to what we have already done for V. The goal is to batch the calls to OFA into a single round.

In summary, all primitives except OFA and B can be outsourced by providing the inputs as shares to the computation parties. These parties then compute shares of the result according to the original protocols. We emphasize two key points: First, sharing and recombining are very inexpensive operations that require no preprocessing—these operations are feasible even on highly constrained devices. Second, there is no need to involve U or S between successive executions of outsourceable primitives. The cloud peers simply retain the shared outputs and input them to the subsequent primitive, and so on. This argument forms the basis for outsourcing SHIELD's classifiers.

### Outsourcing Classifiers

Outsourcing H and A is straightforward, as all underlying primitives can be outsourced. For example, to outsource H, U creates shares \( h_{xii}^{CU} \) and \( h_{xii}^{CS} \) of \( 2(x_i) \) and sends them to CU and CS, while S does the same with the weights \( w_{j,i} \). CU and CS then compute H as described in Protocol 1 and return \( h_{c^*i}^{CU} \) and \( h_{c^*i}^{CS} \) to U and S.

Outsourcing NB requires precomputing all required shares \( h_{\hat{p}(x_i|c_j)} \) in the preprocessing phase, as the employed OFA protocol cannot be efficiently outsourced. U and S then add the derived shares locally and directly provide shares of the posteriors \( h_{\hat{p}(c_j|\mathbf{x})} \) to the computation peers, who then only compute A and provide back the shared result \( h_{c^*i} \). While this outsourcing scheme is less efficient in unburdening U and S compared to the previous two, OFA causes only very low overheads, which are feasible even on mobile devices. In contrast, NB with an underlying Gaussian distribution (Protocol 9) can be fully outsourced: U shares \( x_i \) and S shares \( \mu_i \), \( \sigma_i \), and \( 1/(\sigma_i^2) \) to CU and CS, which compute NB and G on these shares and provide back shares of the result.

Outsourcing V also requires precomputing all invocations of OFA to compute shares of the emission scores \( h_{\hat{b}_i(o_t)} \). In the outsourcing phase, U distributes \( h_{\hat{b}_i(o_t)}^U \) to CU, while S provides \( h_{\hat{b}_i(o_t)}^S \) to CS. S further provides shares \( h_{\hat{\pi}_i} \) of the prior state distribution and shares \( h_{\hat{a}_{ji}} \) of the transition scores to CU and CS. Given these shares, CU and CS can then compute V as specified in Protocol 4 (excluding the invocation of OFA). The backtracking phase cannot be outsourced (as it requires U to know each \( s^*_t \) in clear) and must be executed between U and S in the postprocessing phase.

### Evaluation of Outsourcing

We evaluate the overheads for user U on an LG Nexus 5 smartphone (Android 4, 2.26 GHz CPU, 16 GB RAM) and service S on a desktop machine (Ubuntu 14.04 LTS, Intel i7-4770S with 4 cores at 3.10 GHz, 16 GB RAM). The smartphone is connected through a 300 Mbit/s WiFi network, and the server is on a 1 Gbit/s LAN. We assume the largest problem instance considered in our previous evaluation. Table 6 summarizes the runtime and communication overheads for U (results for S are in Table 8 in Appendix C).

For H, A, and NB, outsourcing is highly efficient and feasible on constrained mobile devices. For V, runtimes for preprocessing and outsourcing are feasible on mobile devices, although the communication overheads in the preprocessing phase might strain slower networks or the user’s data plan. However, we considered the largest HMM and observation sequence from our evaluation—overheads for smaller models range only in the order of kB to a few MB. For all classifiers, we observe that the outsourcing overheads for the user are one or two orders of magnitude smaller than for the service provider (see Appendix C). This is desirable since users need to outsource more frequently than service providers, who typically host their backends in the cloud.

To put these numbers into context, we revisit the comparison against the best-performing secure ANN from related work, i.e., Gazelle [43]. In the standard setting, Gazelle outperforms A by 20× but cannot be outsourced (cf. Section 6.1). In an outsourcing setting, however, A requires 75× less computation and 5000× less communication on the client. Here, we trade a higher load on the unconstrained cloud peers for a much lower load on the constrained client, clearly demonstrating the benefits of designing secure classification protocols for outsourcing.

### Conclusion

We introduced SHIELD, an efficient framework for secure classification upon which we built four different classes of classifiers. Despite being primarily designed for generality and wide applicability, our thorough evaluation shows that SHIELD has competitive performance even compared to approaches specialized to a single classifier, such as ANNs. We noted that despite significant improvements made by SHIELD and related work, processing and, especially, communication overheads of secure classification may still overtax constrained devices and networks. As a solution, we designed SHIELD from the ground up to enable secure and efficient outsourcing to untrusted clouds. The evaluation shows that our proposed outsourcing protocols for SHIELD’s choice of classifiers are feasible even for very constrained devices. Exciting future work includes applying our results to different classifiers and use cases, as well as to the problem of secure training of the machine learning models assumed given in this work.

### Acknowledgments

This work was funded by the German Federal Ministry of Education and Research (BMBF) under funding reference number 16KIS0443. The responsibility for the content of this publication lies with the authors, who would also like to thank the German Research Foundation DFG for the kind support within the Cluster of Excellence “Integrative Production Technology for High-Wage Countries.”

### References

[1] 2015. Pfam Database, version 29.0. http://pfam.xfam.org/.

[2] Aydin Abadi, Sotirios Terzis, and Changyu Dong. 2015. O-PSI: Delegated Private Set Intersection on Outsourced Datasets. In IFIP International Information Security Conference. Springer, 3–17.

[3] Mehrdad Aliasgari and Marina Blanton. 2013. Secure Computation of Hidden Markov Models. In SECRYPT.

[33] EncryptoGroup. 2018. C++ OT extension implementation. https://github.com/encryptogroup/OTExtension.

[34] Zekeriya Erkin, Martin Franz, Jorge Guajardo, Stefan Katzenbeisser, Inald Lagendijk, and Tomas Toft. 2009. Privacy-Preserving Face Recognition. In PETS, Ian Goldberg and Mikhail J. Atallah (Eds.). LNCS, Vol. 5672. Springer, 235–253.

[35] Martin Franz. 2011. Secure Computations on Non-integer Values. Ph.D. Dissertation. Technische Universität Darmstadt.

[36] Martin Franz, Björn Deiseroth, Kay Hamacher, Somesh Jha, Stefan Katzenbeisser, and Heike Schröder. 2010. Secure Computations on Non-integer Values. In WIFS’10. IEEE.

[37] Martin Franz, Björn Deiseroth, Kay Hamacher, Somesh Jha, Stefan Katzenbeisser, and Heike Schröder. 2011. Towards Secure Bioinformatics Services (Short Paper). In FC’11. Springer.

[38] Google. 2018. Cloud Machine Learning Engine. https://cloud.google.com/ml-engine/.

[39] Thore Graepel, Kristin Lauter, and Michael Naehrig. 2012. ML Confidential: Machine Learning on Encrypted Data. In International Conference on Information Security and Cryptology. Springer, 1–21.

[40] Wilko Henecka, Ahmad-Reza Sadeghi, Thomas Schneider, Immo Wehrenberg, et al. 2010. TASTY: Tool for Automating Secure Two-Party Computations. In Proceedings of the 17th ACM Conference on Computer and Communications Security. ACM, 451–462.

[41] Yan Huang, David Evans, Jonathan Katz, and Lior Malka. 2011. Faster Secure Two-Party Computation Using Garbled Circuits. In USENIX Security. USENIX.

[42] Yuval Ishai, Joe Kilian, Kobbi Nissim, and Erez Petrank. 2003. Extending Oblivious Transfers Efficiently. In CRYPTO 2003, Dan Boneh (Ed.). LNCS, Vol. 2729. Springer, 145–161.

[43] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. 2018. Gazelle: A Low Latency Framework for Secure Neural Network Inference. CoRR abs/1801.05507 (2018). arXiv:1801.05507 http://arxiv.org/abs/1801.05507

[44] Seny Kamara, Payman Mohassel, Mariana Raykova, and Saeed Sadeghian. 2014. Scaling Private Set Intersection to Billion-Element Sets. In International Conference on Financial Cryptography and Data Security. Springer, 195–215.

[45] Liina Kamm and Jan Willemson. 2015. Secure Floating Point Arithmetic and Private Satellite Collision Analysis. International Journal of Information Security 14, 6 (2015), 531–548.

[46] Ágnes Kiss and Thomas Schneider. 2016. Valiant’s Universal Circuit is Practical. In Annual International Conference on the Theory and Applications of Cryptographic Techniques. Springer Berlin Heidelberg, 699–728.

[47] Vladimir Kolesnikov, Ahmad-Reza Sadeghi, and Thomas Schneider. 2009. Improved Garbled Circuit Building Blocks and Applications to Auctions and Computing Minima. In CANS 2009, Juan A. Garay, Atsuko Miyaji, and Akira Otsuka (Eds.). LNCS, Vol. 5888. Springer, 1–20.

[48] Vladimir Kolesnikov and Thomas Schneider. 2008. A Practical Universal Circuit Construction and Secure Evaluation of Private Functions. In International Conference on Financial Cryptography and Data Security. Springer, 83–97.

[49] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature 521, 7553 (2015), 436–444.

[50] Yann LeCun, Corinna Cortes, and Christopher Burges. 1998. The MNIST Database of Handwritten Digits. http://yann.lecun.com/exdb/mnist/.

[51] Yehuda Lindell and Benny Pinkas. 2009. Secure Multiparty Computation for Privacy-Preserving Data Mining. Journal of Privacy and Confidentiality 1, 1 (2009).

[52] Jian Liu, Mika Juuti, Yao Lu, and N. Asokan. 2017. Oblivious Neural Network Predictions via MiniONN Transformations. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS ’17). ACM, New York, NY, USA, 619–631. https://doi.org/10.1145/3133956.3134056

[53] Viktor Mayer-Schönberger and Kenneth Cukier. 2013. Big Data: A Revolution That Will Transform How We Live, Work and Think. John Murray Publishers, UK.

[54] Microsoft. 2018. Microsoft Machine Learning Services. https://azure.microsoft.com/en-us/services/machine-learning-services/.

[55] P. Mohassel and Y. Zhang. 2017. SecureML: A System for Scalable Privacy-Preserving Machine Learning. In 2017 IEEE Symposium on Security and Privacy (SP). 19–38. https://doi.org/10.1109/SP.2017.12

[56] Moni Naor and Benny Pinkas. 2005. Computationally Secure Oblivious Transfer. Journal of Cryptology 18, 1 (2005), 1–35.

[57] Manas Pathak, Shantanu Rane, Wei Sun, and Bhiksha Raj. 2011. Privacy Preserving Probabilistic Inference with Hidden Markov Models. In ICASSP’11. IEEE.

[58] Manas A. Pathak and Bhiksha Raj. 2013. Privacy-Preserving Speaker Verification and Identification Using Gaussian Mixture Models. IEEE Transactions on Audio, Speech, and Language Processing 21, 2 (2013), 397–406.

[59] Manas A. Pathak, Bhiksha Raj, S. D. Rane, and Paris Smaragdis. 2013. Privacy-Preserving Speech Processing: Cryptographic and String-Matching Frameworks Show Promise. IEEE Signal Processing Magazine 30, 2 (2013), 62–74.

[60] Benny Pinkas, Thomas Schneider, Gil Segev, and Michael Zohner. 2015. Phasing: Private Set Intersection Using Permutation-Based Hashing. In 24th USENIX Security Symposium (USENIX Security 15). 515–530.

[26] ACC Coolen. 1998. A Beginner’s Guide to the Mathematics of Neural Networks.

[4] Mehrdad Aliasgari, Marina Blanton, and Fattaneh Bayatbabolghani. 2016. Secure Computation of Hidden Markov Models and Secure Floating-Point Arithmetic in the Malicious Model. International Journal of Information Security (2016), 1–25.

[5] Mehrdad Aliasgari, Marina Blanton, Yihua Zhang, and Aaron Steele. 2013. Secure Computation on Floating Point Numbers. In NDSS.

[6] Amazon. 2018. Machine Learning on AWS. https://aws.amazon.com/machine-learning/.

[7] Ion Androutsopoulos, John Koutsias, Konstantinos V. Chandrinos, and Constantine D. Spyropoulos. 2000. An Experimental Comparison of Naive Bayesian and Keyword-Based Anti-Spam Filtering with Personal Email Messages. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’00). ACM, New York, NY, USA, 160–167. https://doi.org/10.1145/345508.345569