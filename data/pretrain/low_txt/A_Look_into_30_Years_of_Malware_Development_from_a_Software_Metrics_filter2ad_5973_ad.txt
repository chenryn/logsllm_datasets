# A Look into 30 Years of Malware Development

## Abstract
This paper presents a comprehensive study on the evolution of malware source code over the past three decades. By focusing on software metrics, we aim to quantify the properties of the code and its development process. Our analysis provides numerical evidence of the increasing complexity of malicious code and the transformation of malware production into a full-fledged engineering discipline.

## 1. Introduction
The first natural comparison in our study refers to the size of the source code. Various malware samples from 2007 onward (e.g., Zeus, KINS, Pony2, or SpyNet) have Source Lines of Code (SLOC) counts larger than those of well-known software packages such as Snort and Bash. According to the Constructive Cost Model (COCOMO), this translates into similar or greater development costs. The function point (FP) counts also show that complex malware projects are comparable in size and effort to these software packages, though they still fall short of others like Apache, IPtables, and Git.

### 1.1. Metrics Overview
- **SLOC**: Number of lines of code.
- **FP**: Function points, a measure of the functional size of the software.
- **M CR**: Maintainability Index.
- **MI**: Maintainability Indicator.
- **Cyclomatic Complexity**: A measure of the complexity of the program's control flow.

## 2. Data Analysis
### 2.1. Size and Effort
The following table summarizes the SLOC, FP, and maintainability metrics for various software and malware samples:

| Software | Version | Year | SLOC | FP | M CR | MI | Cyclomatic Complexity |
|----------|---------|------|------|----|------|----|-----------------------|
| Snort    | 2.4.19  | 2016 | 46,526 | 135.30 | 16.14 | 8.38 | 494.24 |
| Bash     | 1.6.0   | 2016 | 160,890 | 497.81 | 26.47 | 18.81 | 2,265.35 |
| Apache   | 2.8     | 2016 | 280,051 | 890.86 | 33.03 | 26.97 | 4,520.10 |
| IPtables | 4.0.1   | 2016 | 319,173 | 1,021.97 | 34.80 | 29.37 | 3,322.05 |
| Git      | 0.99.1  | 2016 | 378,246 | 1,221.45 | 37.24 | 32.80 | 4,996.44 |
| Octave   | 5.3     | 2016 | 604,398 | 1,998.02 | 44.89 | 44.51 | 11,365.09 |
| ClamAV   | 3.10    | 2016 | 714,085 | 2,380.39 | 47.98 | 49.61 | 10,669.97 |
| Cocos2d-x| 3.10    | 2016 | 851,350 | 2,863.02 | 51.47 | 55.63 | 16,566.78 |

### 2.2. Comment-to-Code Ratio and Cyclomatic Complexity
The comment-to-code ratio and cyclomatic complexity values for malware and regular software samples are very similar, with no noticeable differences. We further investigated this by computing the cyclomatic complexities at the function level for both datasets. The histograms of the obtained values, shown in Figure 6, indicate a clear positive skewness. Statistical tests (Chi-squared and two-sample Kolmogorov-Smirnov) confirm their similarity for a significance level of α = 0.05.

### 2.3. Maintainability
Up to 12 malware samples show higher maintainability indexes (MI) than the highest one for regular software (IPtables, with MI = 68.88). Generally, recent malware samples have slightly higher maintainability indexes than regular software. Notable exceptions include Cairuh and Hexbot2, which have surprisingly low values.

## 3. Discussion
### 3.1. Suitability of Our Approach
Software metrics have a long-standing tradition in software engineering but have been subject to much debate due to frequent misinterpretations and misuse. In this work, our use of software metrics aims to quantify how different properties of malware as a software artifact have evolved over time. Our focus is on the relative comparison of values between malware samples and benign programs, rather than the absolute accuracy of the metrics.

### 3.2. Limitations
Our analysis may suffer from several limitations, including the reduced number of samples in our dataset. However, we discuss 151 samples, which, to the best of our knowledge, is the largest dataset of malware source code analyzed in the literature. Another limitation is selection bias, particularly for newer and more sophisticated samples, which may not be publicly available.

### 3.3. Main Conclusions and Open Questions
In the last 30 years, the complexity of malware has increased considerably, with increments of nearly one order of magnitude per decade in aspects such as SLOC and FP counts. One question is whether this trend will continue. If so, we could soon see malware specimens with more than 1 million SLOC. Evolving into large pieces of software involves a higher amount of vulnerabilities and defects, which has already been observed and exploited. Such evolution also requires larger efforts and possibly larger development teams, which we have not examined in detail.

## 4. Related Work
While malware typically propagates as binary code, some malware families have distributed themselves as source code. Arce and Levy [10] analyzed the Slapper worm, which uploaded its source code upon compromising a host. Holz [22] described the botnet landscape, noting how the availability of source code for Agobot and SDBot led to numerous variants. Barford and Yegneswaran [11] argue for analyzing malware source code to understand the mechanisms used by malware. Other works have explored the source code of exploit kits, finding that they make use of a limited number of vulnerabilities [26, 9, 18, 17, 14].

## 5. Conclusion
In this paper, we have presented a study on the evolution of malware source code over the last decades. Our focus on software metrics provides numerical evidence of the increase in complexity suffered by malicious code and the transformation of malware production into an engineering discipline.

## Acknowledgments
We are grateful to the anonymous reviewers for their constructive feedback and insightful suggestions. This work was supported by the MINECO grant TIN2013-46469-R, the CAM grant S2013/ICE-3095, the Regional Government of Madrid through the N-GREENS Software-CM project S2013/ICE-2731, and the Spanish Government through the Dedetis Grant TIN2015-7013-R.

## References
1. CLOC - count lines of code. http://github.com/AlDanial/cloc. Accessed 22 Sep 2015
2. Eclipse metrics plugin. https://marketplace.eclipse.org/content/eclipse-metrics. Accessed 4 Apr 2016
3. Jhawk. http://www.virtualmachinery.com/jhawkprod.htm. Accessed 4 Apr 2016
4. Radon. https://pypi.python.org/pypi/radon. Accessed 4 Apr 2016
5. Symantec’s 2015 Internet Security Threat Report. https://www.symantec.com/security_response/publications/threatreport.jsp. Accessed 6 Apr 2016
6. Unified code counter. http://csse.usc.edu/ucc_wp/. Accessed 4 Apr 2016
7. Albrecht, A.J.: Measuring Application Development Productivity. In: IBM Application Development Symposium, pp. 83–92. IBM Press, October 1979
8. Albrecht, A.J., Gaffney, J.E.: Software function, source lines of code, and development effort prediction: a software science validation. IEEE Trans. Softw. Eng. 9(6), 639–648 (1983)
9. Allodi, L., Kotov, V., Massacci, F.: MalwareLab: experimentation with cybercrime attack tools. In: USENIX Workshop on Cyber Security Experimentation and Test, Washington D.C., August 2013
10. Arce, I., Levy, E.: An analysis of the slapper worm. IEEE Secur. Priv. 1(1), 82–87 (2003)
11. Barford, P., Yegneswaran, V.: An Inside Look at Botnets. In: Christodorescu, M., Jha, S., Maughan, D., Song, D., Wang, C. (eds.) Malware Detection. Advances in Information Security, vol. 27, pp. 171–191. Springer, Heidelberg (2007)
12. Boehm, B.W.: Software Engineering Economics. Prentice-Hall, Upper Saddle River (1981)
13. Caballero, J., Grier, C., Kreibich, C., Paxson, V.: Measuring pay-per-install: the commoditization of malware distribution. In: Proceedings of the 20th USENIX Conference on Security, p. 13, SEC 2011. USENIX Association, Berkeley (2011)
14. Caballero, J., Poosankam, P., McCamant, S., Babic, D., Song, D.: Input generation via decomposition and re-stitching: finding bugs in malware. In: ACM Conference on Computer and Communications Security, Chicago, IL, October 2010
15. Caliskan-Islam, A., Harang, R., Liu, A., Narayanan, A., Voss, C., Yamaguchi, F., Greenstadt, R.: De-anonymizing programmers via code stylometry. In: USENIX Security Symposium (2015)
16. Diestel, R.: Graph Theory. Graduate Texts in Mathematics, vol. 173, 4th edn. Springer, New York (2012)
17. Eshete, B., Alhuzali, A., Monshizadeh, M., Porras, P., Venkatakrishnan, V., Yegneswaran, V.: EKHunter: a counter-offensive toolkit for exploit kit infiltration. In: Network and Distributed System Security Symposium, February 2015
18. Eshete, B., Venkatakrishnan, V.N.: WebWinnow: leveraging exploit kit workflows to detect malicious URLs. In: ACM Conference on Data and Application Security and Privacy (2014)
19. Frantzeskou, G., MacDonell, S., Stamatatos, E., Gritzalis, S.: Examining the significance of high-level programming features in source code author classification. J. Syst. Softw. 81(3), 447–460 (2008). http://dx.doi.org/10.1016/j.jss.2007.03.004
20. Grier, C., Ballard, L., Caballero, J., Chachra, N., Dietrich, C.J., Levchenko, K., Mavrommatis, P., McCoy, D., Nappa, A., Pitsillidis, A., Provos, N., Rafique, M.Z., Rajab, M.A., Rossow, C., Thomas, K., Paxson, V., Savage, S., Voelker, G.M.: Manufacturing compromise: the emergence of exploit-as-a-service. In: Proceedings of the 2012 ACM Conference on Computer and Communications Security, pp. 821–832, CCS 2012. ACM, New York (2012)
21. Halstead, M.H.: Elements of Software Science (Operating and Programming Systems Series). Elsevier Science Inc., New York (1977)
22. Holz, T.: A short visit to the bot zoo. IEEE Secur. Priv. 3(3), 76–79 (2005)
23. IEEE: IEEE standard for software productivity metrics (IEEE std. 1045–1992). Technical report (1992)
24. Jones, C.: Programming Languages Table, Version 8.2. Software Productivity Research, Burlington (1996)
25. Jones, C.: Backfiring: converting lines-of-code to function points. Computer 28(11), 87–88 (1995)
26. Kotov, V., Massacci, F.: Anatomy of exploit kits. In: Jürjens, J., Livshits, B., Scandariato, R. (eds.) ESSoS 2013. LNCS, vol. 7781, pp. 181–196. Springer, Heidelberg (2013)
27. Lehman, M.M.: Laws of software evolution revisited. In: Montangero, C. (ed.) EWSPT 1996. LNCS, vol. 1149, pp. 108–124. Springer, Heidelberg (1996)
28. McCabe, T.J.: A complexity measure. In: Proceedings of the 2nd International Conference on Software Engineering, ICSE 1976, CA, USA, p. 407. IEEE Computer Society Press, Los Alamitos (1976)
29. Nguyen, V., Deeds-rubin, S., Tan, T., Boehm, B.: A SLOC counting standard. In: COCOMO II Forum 2007 (2007)
30. Oman, P., Hagemeister, J.: Metrics for assessing a software system’s maintainability. In: Proceedings of Conference on Software Maintenance, pp. 337–344 (1992)
31. Park, R.E.: Software size measurement: a framework for counting source statements. Technical report CMU/SEI-92-TR-20, ESC-TR-92-20, Software Engineering Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania 15213, September 1992
32. Panda Security: 27% of all recorded malware appeared in 2015. http://www.pandasecurity.com/mediacenter/press-releases/all-recorded-malware-appeared-in-2015. Accessed 6 Apr 2016
33. Software Engineering Institute: C4 Software Technology Reference Guide - A Prototype. Technical report CMU/SEI-97-HB-001, January 1997
34. Sommerville, I.: Software Engineering: (Update) (8th Edn.) (International Computer Science). Addison-Wesley Longman Publishing Co. Inc., Boston (2006)
35. Stringhini, G., Hohlfeld, O., Kruegel, C., Vigna, G.: The harvester, the botmaster, and the spammer: on the relations between the different actors in the spam landscape. In: Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security, pp. 353–364. ASIA CCS 2014, NY, USA. ACM, New York (2014)
36. Suarez-Tangil, G., Tapiador, J.E., Peris-Lopez, P., Ribagorda, A.: Evolution, detection and analysis of malware for smart devices. IEEE Commun. Surv. Tutorials 16(2), 961–987 (2014)
37. Thomas, K., Huang, D., Wang, D., Bursztein, E., Grier, C., Holt, T.J., Kruegel, C., McCoy, D., Savage, S., Vigna, G.: Framing dependencies introduced by underground commoditization. In: Workshop on the Economics of Information Security (2015)
38. Watson, A.H., McCabe, T.J., Wallace, D.R.: Special publication 500–235, structured testing: a software testing methodology using the cyclomatic complexity metric. In: U.S. Department of Commerce/National Institute of Standards and Technology (1996)

---

This revised version of the text is more structured, coherent, and professional, providing a clearer and more detailed overview of the study and its findings.