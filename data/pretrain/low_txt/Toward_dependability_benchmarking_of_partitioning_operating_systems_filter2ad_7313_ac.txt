### Partitioning Violations for Software Faults in µC/OS-II

#### Without Partitioning
**Table V: Partitioning Violations for Software Faults Targeting µC/OS-II Without Partitioning**

| Workloads | High Priority | Altimeter (f) | CRC32 | Low Priority | Injected | Activated | Spatial Violation | Temporal Violation | Hamdist (f) |
|-----------|---------------|---------------|-------|--------------|----------|-----------|-------------------|--------------------|-------------|
| Altimeter  | 10            | 10            | 2     | 7            | 9        | 1         | 0 (0%)            | 2 (22.22%)         | 0 (0%)      |
| CRC32     | 10            | 10            | 2     | 8            | 9        | 2         | 0 (0%)            | 2 (22.22%)         | 0 (0%)      |
| **Average** | -             | -             | -     | -            | -        | -         | 0 (0%)            | 2 (22.22%)         | 7.41%       |

#### With Secern
**Table VI: Partitioning Violations for Software Faults Targeting µC/OS-II with Secern**

| Workloads | High Priority | Altimeter (f) | CRC32 | Low Priority | Injected | Activated | Spatial Violation | Temporal Violation | Hamdist (f) |
|-----------|---------------|---------------|-------|--------------|----------|-----------|-------------------|--------------------|-------------|
| Altimeter  | 10            | 10            | 2     | 7            | 9        | 2         | 1 (14.29%)        | 1 (11.11%)         | 0 (0%)      |
| CRC32     | 10            | 10            | 2     | 8            | 9        | 2         | 1 (14.29%)        | 0 (0%)             | 0 (0%)      |
| **Average** | -             | -             | -     | -            | -        | -         | 1 (14.29%)        | 0 (0%)             | 8.47%       |

### Overall Partitioning Coverage for the Three Benchmark Targets
**Table VII: Overall Partitioning Coverage for the Three Benchmark Targets**

| Benchmark Target | Hardware Faults | Software Faults | Average |
|------------------|-----------------|-----------------|---------|
| Basic Scheduler  | 38.58%          | 92.59%          | 65.59%  |
| µC/OS-II without Partitioning | 39.09%          | 92.59%          | 65.84%  |
| µC/OS-II with Secern | 29.17%          | 95.24%          | 60.88%  |

### Discussion
One of the key aspects of a good benchmark is its ability to provide a fair comparison between different solutions. The measurements presented in the preceding section support the proposed benchmarking approach. Table VII indicates that µC/OS-II with Secern provides the strongest partitioning coverage, which is reasonable given that it is the only target equipped with partitioning mechanisms. The basic scheduler and µC/OS-II without partitioning are ranked similarly, as neither provides memory protection or other partitioning mechanisms.

#### Guidance for Development Efforts
The benchmark should also provide designers with sufficient information to guide development efforts. Although Secern provides high partitioning coverage, there is room for improvement. We repeated experiments where partitioning was violated to identify specific issues. Two problems were found in Secern:

1. **Incorrect Partition Deletion**: Secern deleted the wrong partition due to an error in the exception handler. The system call `OSTaskDel` expects the priority of the task to be deleted, but Secern was passing the task ID instead.
2. **Memory Address Mapping Issue**: Certain memory addresses were incorrectly mapped, causing values written to one address to simultaneously write to other physical addresses. This issue was traced to hardware initialization.

We modified Secern's exception handling routines to correctly pass the task priority to `OSTaskDel`. After the modification, the error handling mechanisms worked correctly in all cases except one.

#### Benchmark Measures and Properties
In addition to fairness and guidance, the benchmark must consider representativeness, repeatability, portability, non-intrusiveness, and simplicity of use. The experimental evaluation revealed that partitioning violations typically affect both temporal and logical output. Therefore, creating two measures (PCs and PCt) may be unnecessary. A simpler approach could measure partitioning coverage as a whole.

The benchmark targets without partitioning mechanisms (basic scheduler and µC/OS-II without partitioning) showed reasonably high values for PCs and PCt, indicating the role of unintentional fault tolerance. The workloads and faultloads were selected to match typical embedded systems and common fault models, ensuring representativeness.

### Conclusion
This paper presents a dependability benchmark for evaluating the robustness of partitioning mechanisms in real-time operating systems. The benchmark includes faultloads based on hardware and software faults and several workloads to exercise the benchmark targets. The results show that the benchmark can rank the three benchmark targets as expected: µC/OS-II with Secern ranked first, followed closely by the other two targets. The benchmark also identified partitioning vulnerabilities in Secern, guiding development efforts to improve partitioning coverage.

### References
[1] K. Kanoun and L. Spainhower, Eds., *Dependability Benchmarking for Computer Systems*. John Wiley & Sons, Inc., 2008.
[2] J. Rushby, “Partitioning in avionics architectures: Requirements, mechanisms, and assurance,” NASA Langley Research Center, Tech. Rep. NASA/CR-1999-209347, Jun. 1999.
[3] B. Leiner, M. Schlager, R. Obermaisser, and B. Huber, “A comparison of partitioning operating systems for integrated systems,” in *Computer Safety, Reliability, and Security, 26th International Conference, SAFECOMP 2007, Nuremberg, Germany, September 18-21, 2007, Proceedings*, ser. Lecture Notes in Computer Science, F. Saglietti and N. Oster, Eds., vol. 4680. Springer, 2007, pp. 342–355.
[4] P. Koopman, K. DeVale, and J. DeVale, *Dependability Benchmarking for Computer Systems*. John Wiley & Sons, Inc., 2008, ch. Interface Robustness Testing: Experience and Lessons Learned from the Ballista Project, pp. 201–226.
[5] A. Albinet, J. Arlat, and J.-C. Fabre, “Characterization of the impact of faulty drivers on the robustness of the Linux kernel,” in *Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN 2004)*. IEEE Computer Society, Jun.-Jul. 2004, pp. 867–876.
[6] R. K. Iyer, Z. Kalbarczyk, and W. Gu, *Dependability Benchmarking for Computer Systems*. John Wiley & Sons, Inc., 2008, ch. Benchmarking the Operating Systems Against Faults Impacting Operating System Functions, pp. 311–339.
[7] D. Skarin, R. Barbosa, and J. Karlsson, “GOOFI-2: A tool for experimental dependability assessment,” in *Proceedings of the 40th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2010)*, Jun./Jul. 2010, pp. 557–562.
[8] R. Barbosa and J. Karlsson, “Experiences from verifying a partitioning kernel using fault injection,” in *Proceedings of the 12th European Workshop on Dependable Computing (EWDC 2009)*, May 2009.
[9] J. J. Labrosse, *MicroC/OS-II: The Real-Time Kernel, 2nd ed*. CMP Books, 2002.
[10] D. Costa, R. Barbosa, R. Maia, and F. Moreira, *Dependability Benchmarking for Computer Systems*. John Wiley & Sons, Inc., 2008, ch. DeBERT – Dependability Benchmarking for Embedded Real-time Off-the-Shelf Components for Space Applications, pp. 255–283.
[11] A. Kalakech, K. Kanoun, Y. Crouzet, and J. Arlat, “Benchmarking the dependability of Windows NT4, 2000, and XP,” in *Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN 2004)*. IEEE Computer Society, Jun.-Jul. 2004, pp. 681–686.
[12] J.-C. Ruiz, P. Yuste, P. Gil, and L. Lemus, “On benchmarking the dependability of automotive engine control applications,” in *Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN 2004)*. IEEE Computer Society, Jun.-Jul. 2004, pp. 857–866.
[13] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An empirical study of operating system errors,” in *Proceedings of the 18th ACM Symposium on Operating Systems Principles (SOSP-01)*, ser. ACM SIGOPS Operating Systems Review, G. Ganger, Ed., vol. 35, 5. ACM Press, Oct. 21–24 2001, pp. 73–88.
[14] J. Dur˜aes and H. Madeira, “Definition of software fault emulation operators: A field data study,” in *Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN 2003)*, Jun. 2003, pp. 105–114.
[15] D. Skarin, R. Barbosa, and J. Karlsson, “Comparing and validating measurements of dependability attributes,” in *Proceedings of the 8th European Dependable Computing Conference (EDCC 2010)*, pp. 3–12.
[16] R. Natella and D. Cotroneo, “Emulation of transient software faults for dependability assessment: A case study,” in *Proceedings of the 8th European Dependable Computing Conference (EDCC 2010)*, Apr. 2010, pp. 23–32.
[17] H. Madeira and J. G. Silva, “Experimental evaluation of the fail-silent behavior in computers without error masking,” in *Proceedings of the 24th International Symposium on Fault-Tolerant Computing (FTCS-24)*, Jun. 1994, pp. 350–359.
[18] P. Yuste, J.-C. Ruiz-Garcia, L. Lemus, and P. J. Gil, “Non-intrusive software-implemented fault injection in embedded systems,” in *Proceedings of the First Latin-American Symposium on Dependable Computing (LADC 2003)*, ser. Lecture Notes in Computer Science, Oct. 2003, vol. 2847, pp. 23–38.