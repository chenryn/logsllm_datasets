### Table III: Anomaly Detection with Different Log Parsing Methods (16,838 Anomalies)

| Parsing Method | Parsing Accuracy | Reported Anomalies | Detected Anomalies (Percentage) | False Alarms (Percentage) |
|----------------|------------------|--------------------|---------------------------------|---------------------------|
| SLCT           | 0.83             | 18,450             | 10,935 (64%)                    | 7,515 (40%)               |
| LogSig         | 0.87             | 11,091             | 10,678 (63%)                    | 413 (3.7%)                |
| IPLoM          | 0.99             | 10,998             | 10,720 (63%)                    | 278 (2.5%)                |
| Ground Truth   | 1.00             | 11,473             | 11,195 (66%)                    | 278 (2.4%)                |

### D. RQ3: Effectiveness of Log Parsing Methods on Log Mining

To evaluate the effectiveness of log parsing methods on log mining, we used three log parsers to address the parsing challenge in a real-world anomaly detection task described in Section III-B. In this task, 16,838 anomalies were manually identified [2]. The parameters for SLCT and LogSig were re-tuned to achieve high parsing accuracy. LKE was not used due to its inability to handle large datasets (over 10 million lines) within a reasonable time. The evaluation results are presented in Table III.

- **Reported Anomalies**: The number of anomalies reported by PCA using different log parsers.
- **Detected Anomalies**: The number of true anomalies detected by PCA.
- **False Alarms**: The number of incorrectly detected anomalies.
- **Ground Truth**: The experiment using perfectly parsed results for anomaly detection.

Note that even the ground truth could not detect all anomalies due to the limitations of the PCA anomaly detection model.

From Table III, we observe that the parsing accuracy of these methods is high (at least 0.83). LogSig and IPLoM yield nearly optimal results in anomaly detection. However, not all parsing methods perform equally well. SLCT, despite having a high parsing accuracy (0.83), generates 7,515 false alarms, which significantly increases the human effort required for inspection.

**Finding 5**: Log parsing is crucial because log mining is effective only when the parsing accuracy is sufficiently high. 

From Table III, we see that the parsing accuracies of SLCT (0.83) and LogSig (0.87) are comparable. However, the performance of log mining using LogSig as the parser is significantly better than that using SLCT. The log mining task with SLCT produces 7,515 false alarms, requiring much more human inspection effort compared to LogSig, which only leads to 413 false alarms. Additionally, the log mining tasks using LogSig and IPLoM as parsers produce similar results. However, LogSig has 12% more parsing errors than IPLoM. This indicates that log mining results are sensitive to critical events, which can cause a significant performance degradation. It also suggests that f-measure, although widely used in clustering algorithm evaluation, may not be suitable for assessing the effectiveness of log parsing methods on log mining.

**Finding 6**: Log mining is sensitive to critical events. Even 4% parsing errors can cause a significant performance degradation in log mining.

### V. Discussions

#### Limitations:
1. **Dataset Diversity**: Not all datasets (two out of five) used in our evaluation are from production environments. This is primarily due to the lack of publicly available log data. We thank those who have released log data [2], [29], [28], which greatly facilitated our research. Zookeeper and HDFS, however, are popular systems used by many companies for distributed computing, and their logs are likely representative of industrial logs to some extent.
2. **Log Mining Task Diversity**: The effectiveness of log parsing methods was evaluated on an anomaly detection task, which may not generalize to other log mining tasks. This is because real-world data for other log mining tasks is scarce. However, the anomaly detection task we evaluated is well-studied and cited [2] with over 250 citations. Our results highlight the sensitivity of log mining performance to parsing errors on critical events. We plan to extend our methodology to more varied log data and log mining tasks in future work.

#### Potential Directions:
1. **Distributed Log Parsing**: Our experiments show that current log parsing methods are time-consuming with large datasets. Heuristic-based methods are fast but lack the necessary accuracy for log mining. Therefore, distributed log parsing methods are needed. Parallelizable clustering algorithms should be considered.
2. **Logging of Event IDs**: Adding event IDs to log messages during the logging process can improve log parsing. This approach is feasible since developers know exactly which event a log message corresponds to. Tools that automatically add event IDs to source code can greatly facilitate the log parsing process.

### VI. Related Work

#### Log Analysis
Logs are a critical data source for system management tasks such as anomaly detection [3], [2], program verification [5], [6], performance monitoring [8], [7], security assurance [9], [10], and failure analysis [35]. Our evaluation results underscore the importance of log parsing in enabling effective log analysis. We believe our work on log parsing will benefit future studies in log analysis.

#### Log Parsing
Log parsing has been extensively studied. Xu et al. [2] developed a highly accurate log parser based on source code analysis. However, in practice, source code is often unavailable or incomplete, especially when third-party components are involved. Data-driven approaches like SLCT [13], IPLoM [22], LKE [3], and LogSig [15] use data mining techniques to extract log message templates. Despite this, there is a lack of open-source implementations for log parsing tools. Many researchers and practitioners [5], [8], [18] have had to implement their own log parsers, which is time-consuming and redundant. Our work not only provides valuable insights into log parsing but also releases open-source tool implementations for state-of-the-art log parsing methods.

#### Empirical Studies
Empirical studies have gained significant attention in recent years, as they provide useful insights and practical suggestions for both academic researchers and industrial practitioners. Yuan et al. [38], [7] conducted a characteristic study on logging practices in open-source systems and provided actionable suggestions for improvement. Recent work [39], [40], [41] has also studied logging practices in industry. Our work is another empirical study, focusing on the evaluation of log parsing and its use in log mining.

### VII. Conclusion

Log parsing is widely used in log mining, but the lack of studies on the performance of log parsing methods often leads users to redesign specialized log parsers, which is time-consuming. In this paper, we evaluated the performance of four state-of-the-art log parsing methods through extensive experiments. We also analyzed their effectiveness on a real-world log mining task with 10 million log messages. We provided six valuable findings on parsing accuracy, efficiency, and effectiveness in log mining. Additionally, the source code of these log parsing methods is released for reuse and further study.

### ACKNOWLEDGMENT

This work was fully supported by the National Natural Science Foundation of China (Project No. 61332010), the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14205214 of the General Research Fund), and the 2015 Microsoft Research Asia Collaborative Research Program (Project No. FY16-RES-THEME-005).

### REFERENCES

[1] H. Mi, H. Wang, Y. Zhou, R. Lyu, and H. Cai, “Toward fine-grained, unsupervised, scalable performance diagnosis for production cloud computing systems,” IEEE Transactions on Parallel and Distributed Systems, vol. 24, pp. 1245–1255, 2013.

[2] W. Xu, L. Huang, A. Fox, D. Patterson, and M. Jordon, “Detecting large-scale system problems by mining console logs,” in SOSP’09: Proc. of the ACM Symposium on Operating Systems Principles, 2009.

[3] Q. Fu, J. Lou, Y. Wang, and J. Li, “Execution anomaly detection in distributed systems through unstructured log analysis,” in ICDM’09: Proc. of International Conference on Data Mining, 2009.

[4] A. Makanju, A. Zincir-Heywood, and E. Milios, “Fast entropy based alert detection in super computer logs,” in DSN-W’10: Proc. of International Conference on Dependable Systems and Networks Workshops, 2010, pp. 52–58.

[5] I. Beschastnikh, Y. Brun, S. Schneider, M. Sloan, and M. Ernst, “Leveraging existing instrumentation to automatically infer invariant-constrained models,” in ESEC/FSE’11: Proc. of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering, 2011.

[6] W. Shang, Z. Jiang, H. Hemmati, B. Adams, A. Hassan, and P. Martin, “Assisting developers of big data analytics applications when deploying on Hadoop clouds,” in ICSE’13: Proc. of the 35th International Conference on Software Engineering, 2013, pp. 402–411.

[7] D. Yuan, S. Park, P. Huang, Y. Liu, M. Lee, X. Tang, Y. Zhou, and S. Savage, “Be conservative: enhancing failure diagnosis with proactive logging,” in OSDI’12: Proc. of the 10th USENIX Conference on Operating Systems Design and Implementation, 2012, pp. 293–306.

[8] K. Nagaraj, C. Killian, and J. Neville, “Structured comparative analysis of systems logs to diagnose performance problems,” in NSDI’12: Proc. of the 9th USENIX conference on Networked Systems Design and Implementation, 2012.

[9] A. Oprea, Z. Li, T. Yen, S. Chin, and S. Alrwais, “Detection of early-stage enterprise infection by mining large-scale log data,” in DSN’15, 2015.

[10] Z. Gu, K. Pei, Q. Wang, L. Si, X. Zhang, and D. Xu, “LEAPS: Detecting camouflaged attacks with statistical learning guided by program analysis,” in DSN’15, 2015.

[11] D. Lang, “Using SEC,” USENIX ;login: Magazine, vol. 38, no. 6, pp. 38–43, 2013.

[12] W. Xu, “System problem detection by mining console logs,” Ph.D. dissertation, University of California, Berkeley, 2010.

[13] R. Vaarandi, “A data clustering algorithm for mining patterns from event logs,” in IPOM’03: Proc. of the 3rd Workshop on IP Operations and Management, 2003.

[14] A. Makanju, A. Zincir-Heywood, and E. Milios, “Clustering event logs using iterative partitioning,” in KDD’09: Proc. of International Conference on Knowledge Discovery and Data Mining, 2009.

[15] L. Tang, T. Li, and C. Perng, “LogSig: Generating system events from raw textual logs,” in CIKM’11: Proc. of ACM International Conference on Information and Knowledge Management, 2011, pp. 785–794.

[16] Splunk. [Online]. Available: http://www.splunk.com

[17] Logstash. [Online]. Available: http://logstash.net

[18] S. Banerjee, H. Srikanth, and B. Cukic, “Log-based reliability analysis of software as a service (SaaS),” in ISSRE’10: Proc. of the 21st International Symposium on Software Reliability Engineering, 2010.

[19] R. Vaarandi, “Mining event logs with SLCT and LogHound,” in NOMS’08: Proc. of the IEEE/IFIP Network Operations and Management Symposium, 2008.

[20] L. Huang, X. Ke, K. Wong, and S. Mankovskii, “Symptom-based problem determination using log data abstraction,” in CASCON’10: Proc. of the Conference of the Center for Advanced Studies on Collaborative Research, 2010, pp. 313–326.

[21] R. Vaarandi and K. Podis, “Network IDS alert classification with frequent itemset mining and data clustering,” in CNSM’10: Proc. of the Conference on Network and Service Management, 2010.

[22] A. Makanju, A. Zincir-Heywood, and E. Milios, “A lightweight algorithm for message type extraction in system application logs,” IEEE Transactions on Knowledge and Data Engineering, vol. 24, pp. 1921–1936, 2012.

[23] ——, “Investigating event log analysis with minimum apriori information,” in IM’13: Proc. of International Symposium on Integrated Network Management, 2013, pp. 962–968.

[24] Y. Jiang, C. Perng, and T. Li, “META: Multi-resolution framework for event summarization,” in SDM’14: Proc. of the SIAM International Conference on Data Mining, 2014, pp. 605–613.

[25] J. Lou, Q. Fu, S. Yang, Y. Xu, and J. Li, “Mining invariants from console logs for system problem detection,” in ATC’10: Proc. of the USENIX Annual Technical Conference, 2010.

[26] L. Tang, T. Li, L. Shang, F. Pinel, and G. Grabarnik, “An integrated framework for optimizing automatic monitoring systems in large IT infrastructures,” in KDD’13: Proc. of International Conference on Knowledge Discovery and Data Mining, 2013, pp. 1249–1257.

[27] G. Salton and C. Buckley, “Term weighting approaches in automatic text retrieval,” Cornell, Tech. Rep., 1987.

[28] A. Oliner and J. Stearley, “What supercomputers say: A study of five system logs,” in DSN’07, 2007.

[29] L. A. N. S. LLC. Operational data to support and enable computer science research. [Online]. Available: http://institutes.lanl.gov/data/fdata

[30] C. Manning, P. Raghavan, and H. Schütze, Introduction to Information Retrieval. Cambridge University Press, 2008.

[31] Evaluation of clustering. [Online]. Available: http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html

[32] A. Lakhina, M. Crovella, and C. Diot, “Diagnosing network-wide traffic anomalies,” in SIGCOMM’04: Proc. of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications, 2004, pp. 219–230.

[33] H. Ringberg, A. Soule, J. Rexford, and C. Diot, “Sensitivity of PCA for traffic anomaly detection,” in SIGMETRICS’07: Proc. of International Conference on Measurement and Modeling of Computer Systems, 2007.

[34] F. Salfner, S. Tschirpke, and M. Malek, “Comprehensive logfiles for autonomic systems,” in IPDPS’04: Proc. of Parallel and Distributed Processing Symposium, 2004.

[35] C. Di Martino, M. Cinque, and D. Cotroneo, “Assessing time coalescence techniques for the analysis of supercomputer logs,” in DSN’12, 2012.

[36] [Online]. Available: http://stackoverflow.com/questions/154982/what-is-the-best-log-analysis-tool-that-you-used

[37] [Online]. Available: http://stackoverflow.com/questions/2590251/is-there-a-log-file-analyzer-for-log4j-files

[38] D. Yuan, S. Park, and Y. Zhou, “Characterizing logging practices in open-source software,” in ICSE’12: Proc. of the 34th International Conference on Software Engineering, 2012, pp. 102–112.

[39] Q. Fu, J. Zhu, W. Hu, J. Lou, R. Ding, Q. Lin, D. Zhang, and T. Xie, “Where do developers log? An empirical study on logging practices in industry,” in ICSE’14: Companion Proc. of the 36th International Conference on Software Engineering, 2014, pp. 24–33.

[40] J. Zhu, P. He, Q. Fu, H. Zhang, R. Lyu, and D. Zhang, “Learning to log: Helping developers make informed logging decisions,” in ICSE’15: Proc. of the 37th International Conference on Software Engineering, 2015.

[41] A. Pecchia, M. Cinque, G. Carrozza, and D. Cotroneo, “Industry practices and event logging: assessment of a critical software development process,” in ICSE’15: Proc. of the 37th International Conference on Software Engineering, 2015, pp. 169–178.