### Cohort Selection and Privacy Implications

Cohorts used in the collection design must be carefully selected and periodically updated to avoid privacy risks. If not, cohorts may become too small, facilitating client tracking, or clients may report as part of different cohorts over time, which can reduce their privacy. RAPPOR responses can also affect client anonymity, especially when collected on immutable client values that are consistent across all clients. If the responses contain too many bits (e.g., if the Bloom filters are too large), this can facilitate client tracking because the bits in the Permanent randomized responses are correlated. While some of these concerns may not apply in practice (e.g., tracking responses may be infeasible due to encryption), they must all be considered in RAPPOR collection design.

### Longitudinal Privacy Protection

Longitudinal privacy protection, guaranteed by the Permanent randomized response, assumes that the client's value does not change over time. This assumption is only slightly violated if the value changes very slowly. For a rapidly changing, correlated stream of values from a single user, additional measures must be taken to ensure longitudinal privacy. One practical approach is to budget ε over time, spending a small portion on each report. In the RAPPOR algorithm, this would be equivalent to letting q get closer and closer to p with each collection event.

### Differential Privacy and Bloom Filters

Differential privacy deals with the worst-case scenario, so the uncertainty introduced by the Bloom filter does not play a role in the calculation of its bounds. Depending on the random draw, there may or may not be multiple candidate strings mapping to the same h bits in the Bloom filter. However, for average-case privacy analysis, the Bloom filter does provide additional privacy protection (a flavor of k-anonymity) because it is difficult to reliably infer a client's value v from its Bloom filter representation B [4].

### Related Work

Data collection from clients while preserving their privacy and enabling meaningful aggregate inferences is an active area of research in both academia and industry. Our work fits into a category of recently-explored problems where an untrusted aggregator aims to learn the "heavy hitters" in the clients' data or run certain types of learning algorithms on the aggregated data while guaranteeing the privacy of each contributing client. In some cases, this also involves restricting the amount of client communication with the untrusted aggregator [7, 16, 18, 20]. Our contribution is to suggest an alternative that is intuitive, easy-to-implement, and potentially more suitable for certain learning problems. We also provide a detailed statistical decoding methodology for our approach and experimental data on its performance. Additionally, we make explicit algorithmic steps towards protection against linkability across reports from the same user.

### Why Randomized Response?

It is natural to ask why we built our mechanisms upon randomized response rather than the Laplace and Exponential mechanisms, which are commonly used to achieve differential privacy. The Laplace mechanism is not suitable because the client's reported values may be categorical rather than numeric, making direct noise addition semantically meaningless. The Exponential mechanism is not applicable in a local model where privacy is ensured by each client individually without a trusted third party. In such a case, the client does not have sufficient information about the data space to perform the necessary biased sampling required by the Exponential mechanism. Finally, randomized response has the additional benefit of being relatively easy to explain to the end user, making the reasoning about the algorithm used to ensure privacy more accessible.

### Dimensionality Reduction and Bloom Filters

The use of various dimensionality reduction techniques to improve the privacy properties of algorithms while retaining utility is common [1, 17, 20, 22]. Our reliance on Bloom filters is driven by the desire to obtain a compact representation of the data to lower each client's potential transmission costs and to use technologies already widely adopted in practice [6]. Although related work in this space suggests that careful selection of hash functions or other Bloom filter parameters may further enhance privacy, we have not explored this direction in detail.

### Similar Work

The work most similar to ours is by Mishra and Sandler [24]. Our main additional contributions include a more extensive decoding step, providing both experimental and statistical analyses of collected data for queries that are more complex than those considered in their work. We also use a second randomization step, the Instantaneous randomized response, to make linking reports from a single user more difficult, along with more detailed models of attackers' capabilities.

### Distributed Solutions

The challenge of eliminating the need for a trusted aggregator has also been addressed with distributed solutions that place trust in other clients [11]. Differentially private protocols can be implemented over distributed user data by relying on honest-but-curious proxies or aggregators bound by certain commitments [2, 8].

### Longitudinal Data Collection

Several lines of work aim to address the question of longitudinal data collection with privacy. Some recent work considers scenarios where many predicate queries are asked against the same dataset, using an approach that reconstructs the answer to some queries based on previously given answers [25]. While the high-level idea of RAPPOR bears some resemblance to this technique—reusing the result of the Permanent randomized response step—the overall goal is different. RAPPOR collects reports to the same query over data that may be changing over time. Although it does not operate under the same local model as RAPPOR, recent work on pan-private streaming and privacy under continual observation introduces additional ideas relevant for longitudinal data collection with privacy [13, 14].

### Summary

RAPPOR is a flexible, mathematically rigorous, and practical platform for anonymous data collection, designed for privacy-preserving crowdsourcing of population statistics on client-side data. It gracefully handles multiple data collections from the same client by providing well-defined longitudinal differential privacy guarantees. Highly tunable parameters allow balancing risk versus utility over time, depending on one’s needs and assessment of different attack models. RAPPOR is purely a client-based privacy solution, eliminating the need for a trusted third-party server and putting control over client data back into their own hands.

### Acknowledgements

The authors would like to thank our many colleagues at Google and its Chrome team who have helped with this work, with special thanks to Steve Holte and Moti Yung. Thanks also to the CCS reviewers and many others who provided insightful feedback on the ideas and this paper, particularly Frank McSherry, Arvind Narayanan, Elaine Shi, and Adam D. Smith.

### References

[1] C. C. Aggarwal and P. S. Yu. On privacy-preservation of text and sparse binary data with sketches. In Proceedings of the 2007 SIAM International Conference on Data Mining (SDM), pages 57–67, 2007.

[2] I. E. Akkus, R. Chen, M. Hardt, P. Francis, and J. Gehrke. Non-tracking web analytics. In Proceedings of the 2012 ACM Conference on Computer and Communications Security (CCS), pages 687–698, 2012.

[3] Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society Series B (Methodological), 57(1):289–300, 1995.

[4] G. Bianchi, L. Bracciale, and P. Loreti. ‘Better Than Nothing’ privacy with Bloom filters: To what extent? In Proceedings of the 2012 International Conference on Privacy in Statistical Databases (PSD), pages 348–363, 2012.

[5] B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422–426, July 1970.

[6] A. Z. Broder and M. Mitzenmacher. Network applications of Bloom filters: A Survey. Internet Mathematics, 1(4):485–509, 2003.

[7] T.-H. H. Chan, M. Li, E. Shi, and W. Xu. Differentially private continual monitoring of heavy hitters from distributed streams. In Proceedings of the 12th International Conference on Privacy Enhancing Technologies (PETS), pages 140–159, 2012.

[8] R. Chen, A. Reznichenko, P. Francis, and J. Gehrke. Towards statistical queries over distributed private user data. In Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI), pages 169–182, 2012.

[10] C. Dwork. A firm foundation for private data analysis. Commun. ACM, 54(1):86–95, Jan. 2011.

[11] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our data, ourselves: Privacy via distributed noise generation. In Proceedings of 25th Annual International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), pages 486–503, 2006.

[12] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the 3rd Theory of Cryptography Conference (TCC), pages 265–284, 2006.

[13] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum. Differential privacy under continual observation. In Proceedings of the 42nd ACM Symposium on Theory of Computing (STOC), pages 715–724, 2010.

[14] C. Dwork, M. Naor, T. Pitassi, G. N. Rothblum, and S. Yekhanin. Pan-private streaming algorithms. In Proceedings of The 1st Symposium on Innovations in Computer Science (ICS), pages 66–80, 2010.

[15] J. Hsu, M. Gaboardi, A. Haeberlen, S. Khanna, A. Narayan, B. C. Pierce, and A. Roth. Differential privacy: An economic method for choosing epsilon. In Proceedings of 27th IEEE Computer Security Foundations Symposium (CSF), 2014.

[16] J. Hsu, S. Khanna, and A. Roth. Distributed private heavy hitters. In Proceedings of the 39th International Colloquium Conference on Automata, Languages, and Programming (ICALP) - Volume Part I, pages 461–472, 2012.

[17] K. Kenthapadi, A. Korolova, I. Mironov, and N. Mishra. Privacy via the Johnson-Lindenstrauss transform. Journal of Privacy and Confidentiality, 5(1):39–71, 2013.

[18] D. Keren, G. Sagy, A. Abboud, D. Ben-David, A. Schuster, I. Sharfman, and A. Deligiannakis. Monitoring distributed, heterogeneous data streams: The emergence of safe zones. In Proceedings of the 1st International Conference on Applied Algorithms (ICAA), pages 17–28, 2014.

[19] D. Kifer and A. Machanavajjhala. No free lunch in data privacy. In Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD), pages 193–204, 2011.

[20] B. Liu, Y. Jiang, F. Sha, and R. Govindan. Cloud-enabled privacy-preserving collaborative learning for mobile sensing. In Proceedings of the 10th ACM Conference on Embedded Network Sensor Systems (SenSys), pages 57–70, 2012.

[9] Chromium.org. Design Documents: RAPPOR (Randomized Aggregatable Privacy Preserving Ordinal Responses). http://www.chromium.org/developers/design-documents/rappor.

[21] F. McSherry and K. Talwar. Mechanism design via differential privacy. In Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 94–103, 2007.

[22] D. J. Mir, S. Muthukrishnan, A. Nikolov, and R. N. Wright. Pan-private algorithms via statistics on sketches. In Proceedings of Symposium on Principles of Database Systems (PODS), pages 37–48, 2011.

[23] I. Mironov. On significance of the least significant bits for differential privacy. In Proceedings of ACM Conference on Computer and Communications Security (CCS), pages 650–661, 2012.

[24] N. Mishra and M. Sandler. Privacy via pseudorandom sketches. In Proceedings of Symposium on Principles of Database Systems (PODS), pages 143–152, 2006.

[25] A. Roth and T. Roughgarden. Interactive privacy via the median mechanism. In Proceedings of the 42nd ACM Symposium on Theory of Computing (STOC), pages 765–774, 2010.

[26] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58:267–288, 1994.

[27] S. L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. Journal of the American Statistical Association, 60(309):pp. 63–69, 1965.

[28] Wikipedia. Randomized response. http://en.wikipedia.org/wiki/Randomized_response.

### Appendix

#### Observation 1
For \(a, b \geq 0\) and \(c, d > 0\):
\[
\frac{a + b}{c + d} \leq \max\left(\frac{a}{c}, \frac{b}{d}\right)
\]

**Proof.** Assume without loss of generality that \(\frac{a}{c} \geq \frac{b}{d}\), and suppose the statement is false, i.e., \(\frac{a + b}{c + d} > \frac{a}{c}\). Then:
\[
\frac{a + b}{c + d} > \frac{a}{c} \implies a(c + d) > c(a + b) \implies ac + ad > ac + bc \implies ad > bc
\]
This contradicts the assumption that \(\frac{a}{c} \geq \frac{b}{d}\).

#### Deriving Limits on Learning

We consider the Basic One-time RAPPOR algorithm to establish theoretical limits on what can be learned using a particular parameter configuration and a number of collected reports \(N\). Since the Basic One-time RAPPOR is more efficient (lossless) than the original RAPPOR, the following provides a strict upper bound for all RAPPOR modifications.

**Decoding for the Basic RAPPOR** is quite simple. Here, we assume that \(f = 0\). The expected number that bit \(i\) is set in a set of reports, \(C_i\), is given by:
\[
E(C_i) = qT_i + p(N - T_i)
\]
where \(T_i\) is the number of times bit \(i\) was truly set (was the signal bit). This immediately provides the estimator:
\[
\hat{T}_i = \frac{C_i - pN}{q - p}
\]

It can be shown that the variance of our estimator under the assumption that \(T_i = 0\) is given by:
\[
\text{Var}(\hat{T}_i) = \frac{p(1 - p)N}{(q - p)^2}
\]

Determining whether \(T_i\) is larger than 0 comes down to statistical hypothesis testing with \(H_0: T_i = 0\) vs \(H_1: T_i > 0\). Under the null hypothesis \(H_0\) and letting \(p = 0.5\), the standard deviation of \(\hat{T}_i\) equals:
\[
\text{sd}(\hat{T}_i) = \sqrt{\frac{N}{2q - 1}}
\]

We reject \(H_0\) when:
\[
\hat{T}_i > Q \times \text{sd}(\hat{T}_i) \implies \hat{T}_i > Q \sqrt{\frac{N}{2q - 1}}
\]
where \(Q\) is the critical value from the standard normal distribution \(Q = \Phi^{-1}(1 - \frac{0.05}{M})\) (\(\Phi^{-1}\) is the inverse of the standard Normal cdf). Here, \(M\) is the number of tests; in this case, it is equal to \(k\), the length of the bit array. Dividing by \(M\), the Bonferroni correction, is necessary to adjust for multiple testing to avoid a large number of false positive findings.

Let \(x\) be the largest number of bits for which this condition is true (i.e., rejecting the null hypothesis). \(x\) is maximized when \(x\) out of \(M\) items have a uniform distribution and a combined probability mass of almost 1. The other \(M - x\) bits have essentially 0 probability. In this case, each non-zero bit will have frequency \(\frac{1}{x}\) and its expected count will be \(E(\hat{T}_i) = \frac{N}{x} \forall i\).

Thus, we require:
\[
\sqrt{\frac{N}{2q - 1}} > \frac{N}{x} \implies x \leq (2q - 1) \sqrt{\frac{N}{Q^2}}
\]