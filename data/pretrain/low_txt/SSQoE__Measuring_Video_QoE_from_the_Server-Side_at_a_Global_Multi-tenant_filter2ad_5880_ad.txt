### RTT Measurement (Left)
**Figure 12.** RTT vs. QoE for U.S. Wireless Carriers.

### 7. Discussion
This paper does not cover certain aspects of video content delivery in detail, which must be considered when deploying the proposed method.

#### Variable Segment Size and Quality
A common practice for delivering large files is to chunk them into smaller, similarly sized segments (typically in the order of MBs). Each chunk is treated as a separate content asset. In such cases, a single video segment request from the client can map to multiple chunks. It is essential to account for the flow completion times of each chunk and add only the corresponding video segment duration to the buffer estimation algorithm (described in Section 4) that the chunk represents. Note that subsequent requests from the client may be of different quality, leading to variations in the video duration of the chunks.

#### Estimation During Fewer Video Sessions
While the proposed method detects rebuffering on a per-session basis, relying on this signal with too few sessions can introduce noise. We aggregate the estimated rebuffering for all sessions within the current time bin and compute an average value. Therefore, it is crucial to have enough data points to ensure that the average is a reliable representative value. Too few data points can result in averages that are sensitive to outliers.

Our analysis uses a minimum of 50 sessions per minute, a threshold that has shown empirical evidence in tracking meaningful rebuffering ratios. SSQoE is designed with CDN delivery and performance-based decisions in mind. We do not focus on all client behavior metrics that video providers might want to track, such as video startup failures, ad engagement metrics, or protocol performance level A/B testing (e.g., QUIC vs. TCP clients). These analyses are currently not supported by our system.

#### Impacts of User Interactions with the Video Stream
User actions, such as pausing or switching from WiFi to LTE, can impact buffer occupancy and lead to estimation inaccuracies. The SSQoE score is an aggregate metric that combines the performance (rebuffer estimations, time taken to download segments, etc.) of all sessions in a time window. It is highly unlikely that a large fraction of viewers will pause the video or introduce similar user behavior simultaneously. Thus, on a large scale, individual session inaccuracies become negligible. The aggregate signal represents the average performance for many users, such as those in the same ASN or connected to the same PoP. Future work could quantify and account for this small fraction of error margin in SSQoE.

#### Change Management
The QoE score's ability to track user-level impacts due to degraded server-side performance, such as CPU bottlenecks (Figure 8) or origin server issues (Figure 10), makes it a good candidate for change/configuration management. During a recent update to the cache management software at the CDN, a bug caused degraded I/O performance. The impact of this bug on live video streams was captured by our methodology, where the QoE score spiked up by three times its baseline value. Thanks to our automated detection, our site-reliability team proactively worked on rolling back the change. We are working towards integrating the QoE score into the CDN’s ML-based monitoring service.

#### Other Considerations
We do not make any protocol assumptions while estimating the QoE score. We have successfully tested the feasibility of SSQoE for HLS and DASH. However, future protocols that support low-latency streaming, such as WebRTC, might need to be evaluated to determine the efficacy of SSQoE. Ethical considerations are kept in mind while designing the proposed method. We do not track or expose anything more than what is already captured in the CDN access logs. We only track the performance of the video stream by looking at the meta information, and the content segments remain encrypted (HTTPS).

### 8. Related Work
Measuring video performance and evaluating QoE has been studied from various angles. To the best of our knowledge, our work is the first to propose a pure server-side client player buffer estimation and show its feasibility at a large multi-tenant scale. However, several methods are relevant to our work, which we classify into three categories: a) video performance monitoring and characterization of traffic, b) steering traffic at large content providers, and c) evaluating changes in network configurations.

#### Video Performance Monitoring
Poor video performance leads to less user engagement [15, 23]. In [7], authors quantify the impact of QoE metrics on user engagement during a large live OTT streaming event. Specifically, this work highlights that bitrate and rebuffering have the most significant impact on how users engage. The authors propose using PCA and Hampel filter for live detection of QoE impairments. We take insights from this work and build a more scalable method that estimates client player behavior to detect rebuffering, rather than performing resource-consuming PCA. In [18], using client-side metrics, authors identify that video quality is determined by a subset of critical features. Their methodology provides a QoE prediction scheme that can be deployed on beacon data at scale, tangential to our server-side scoring mechanism.

Authors in [11, 14, 20] address the challenge of detecting QoE in encrypted traffic. As video streaming platforms provide end-to-end encryption, it has become challenging for middle-mile network providers to perform video-specific optimizations. Our work differs from these papers as SSQoE is designed for the video provider infrastructure (i.e., CDN) where TLS termination occurs. The CDN can identify unique video traffic, video segments, etc., to perform the proposed QoE estimation. In [22], similar to our work's motivation, authors emphasize that network metrics alone may miss QoE degradation events. They propose using user behavior information, such as pausing or reducing viewing area, as indicators to predict QoE degradation. This differs from our server-side methodology, where we do not rely on user metrics, which are often tracked by the video player and may not be available for 3rd party CDNs.

In [21], authors use automated tests on the client side to interact with several online services and throttle throughput to monitor video performance. This work focuses on understanding the diversity in how different streaming services operate. Different platforms might optimize their player's ABR behavior for different guarantees of QoE, such as utilizing only limited available bandwidth or compromising bitrate to keep up with live streams. The methods used in this paper revolve around a one-time study to understand the landscape of video streaming. It is not designed to be an operational component of a multi-tenant CDN provider. Given the diversity in the streaming landscape, this work motivates the need to perform measurements from the server-side and use a method like ours that is completely player-agnostic. YouTube’s video traffic has been studied by many researchers [9, 13]. These works differ from ours as they either aim to analyze YouTube’s traffic behavior or try to understand the ISP and CDN caching implications on QoE using client-side data.

#### Traffic Management Systems
Recent performance measurement and traffic management systems developed by Google [28], Facebook [26, 27], and Microsoft [12, 19] use several measurement schemes to evaluate performance and use that information to localize faults or pick an alternate route for egress traffic that will lead to better QoE. EdgeFabric [27] and Espresso [28] focus on egress steering methodologies and leverage data from client apps to gain performance insights. Odin [12] uses several data sources from both client and server-side along with active measurements for CDN performance evaluation. Although these systems are relevant for measuring performance, they do not explicitly track video performance. A more general approach proposed by Facebook is by tracking HDratio [26], which indeed focuses on video performance. The authors propose using the achievement of 2.5 Mbps throughput, i.e., enough to serve HD content, as an indicator to measure performance. Similar to our work, this method also relies on only server-side measurements and can be applied in a multi-tenant environment. However, relying on such hard thresholds in a multi-tenant mixed-workload streaming landscape does not scale well. It is operationally hard to perform different analyses for HD, Ultra-HD, 4K, and multiple bitrate qualities using multiple thresholds. Moreover, tracking throughput gives a red flag on network performance degradation but does not guarantee that the client player actually suffered rebuffering. Our buffer estimation algorithm tracks the actual client player buffer, so every time an event is detected (i.e., estimated buffer at the client is zero), we know with high accuracy that the video has rebuffered. Using throughput-based metrics also do not work well for server-side ad insertion in the video stream, as ads in the middle of the video could be encoded in a lower or higher bitrate, leading to expected changes in throughput that may not necessarily indicate performance degradation. As shown in Section 6, our proposed methodology does not suffer from such unintended impacts.

#### Configuration Evaluation
In [16], authors measure QoE to evaluate network buffer changes. They attempt to measure the impact on QoE while tuning network buffers in a testbed. However, they do not measure important user engagement-impacting metrics such as rebuffering [7]. We agree with the motivation of this work, that QoE impacts of network configurations are largely unknown. We have proposed an easily scalable server-side methodology that we hope will be used by future research on network parameter tunings and evaluate impacts on client player buffer. The proposed methodology is part of our change/configuration management strategy at the CDN, including for a recent change we made to our network buffer sizes. While the evaluation of that change is out of scope of this work, we note that server-side QoE was able to accurately track the progress of this network change, and we encourage other large content providers to include such metrics in their change management process.

### 9. Conclusion
With the rise of video streaming applications on the Internet, their ingest, distribution, and performance monitoring have become increasingly complex. Current state-of-the-art monitoring solutions rely on client-side beacons, which require considerable instrumentation. Moreover, this beacon data is not easily exposed to multi-tenant third-party CDN providers. This results in cases where CDNs deliver a bulk of the video traffic without proper visibility into client-perceived QoE and performance.

In this paper, we analyzed the video processing pipeline, characterized the video streaming workload on a large-scale CDN, and derived key features that can be tracked from the server-side to understand client-perceived QoE. We then presented and validated SSQoE, a method for estimating client rebuffering using passive measurements, by analyzing a sequence of requests from CDN access logs to derive a QoE score that represents the health of the video stream. Traditional client-side metrics can only reveal device or last-mile problems. Mapping them to the CDN infrastructure is generally not easy, making client-side beacons less viable for large-scale CDN operations. Server-side QoE estimation has been in operation globally on our CDN for the past year. To the best of our knowledge, this is the largest deployment of server-side video monitoring at a commercial CDN. It is currently used for monitoring some of the biggest live news, sports events, conferences, and movie releases that millions of users engage with, and it has helped identify issues using the QoE score in near-real-time and correlate performance degradation with other CDN insights during large-scale events.

We have explored the possibilities of server-side QoE analytics and invite the industry and academia to collaborate, contribute, and explore more use cases in this direction.

### References
1. Adaptive bitrate (ABR). https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming
2. Conviva platform. https://www.conviva.com/about/
3. FFmpeg utility. https://ffmpeg.org
4. HLS.js player. https://github.com/video-dev/hls.js
5. Linux Traffic Control (TC) utility. https://man7.org/linux/man-pages/man8/tc.8.html
6. NGNIX RTMP. https://github.com/arut/nginx-rtmp-module

7. Ahmed, A., Shaﬁq, Z., Bedi, H., Khakpour, A.: Suffering from buffering? Detecting QoE impairments in live video streams. In: 2017 IEEE 25th International Conference on Network Protocols (ICNP), pp. 1–10 (2017)
8. Ahmed, A., Shaﬁq, Z., Khakpour, A.: QoE analysis of a large-scale live video streaming event. In: Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science, SIGMETRICS 2016, pp. 395–396. Association for Computing Machinery, New York (2016)
9. A˜norga, J., Arrizabalaga, S., Sedano, B., Goya, J., Alonso-Arce, M., Mendizabal, J.: Analysis of YouTube’s traffic adaptation to dynamic environments. Multimedia Tools Appl. 77, 7977–8000 (2017). https://doi.org/10.1007/s11042-017-4695-9
10. B¨ottger, T., Cuadrado, F., Uhlig, S.: Looking for hypergiants in PeeringDB. SIGCOMM Comput. Commun. Rev. 48(3), 13–19 (2018)
11. Bronzino, F., Schmitt, P., Ayoubi, S., Martins, G., Teixeira, R., Feamster, N.: Inferring streaming video quality from encrypted traffic: practical models and deployment experience. Proc. ACM Meas. Anal. Comput. Syst. 3(3), 1–25 (2019)
12. Calder, M., et al.: Odin: Microsoft’s scalable fault-tolerant CDN measurement system. In: USENIX NSDI, April 2018
13. D’Alconzo, A., Casas, P., Fiadino, P., Bar, A., Finamore, A.: Who to blame when YouTube is not working? Detecting anomalies in CDN-provisioned services. In: 2014 International Wireless Communications and Mobile Computing Conference (IWCMC), pp. 435–440 (2014)
14. Dimopoulos, G., Leontiadis, I., Barlet-Ros, P., Papagiannaki, K.: Measuring video QoE from encrypted traffic. In: Proceedings of the 2016 Internet Measurement Conference, IMC 2016, pp. 513–526. Association for Computing Machinery, New York (2016)
15. Dobrian, F., et al.: Understanding the impact of video quality on user engagement. Commun. ACM 56(3), 91–99 (2013)
16. Hohlfeld, O., Pujol, E., Ciucu, F., Feldmann, A., Barford, P.: A QoE perspective on sizing network buffers. In: Proceedings of the 2014 Conference on Internet Measurement Conference, IMC 2014, pp. 333–346. Association for Computing Machinery, New York (2014)
17. Hyndman, R.J., Athanasopoulos, G.: Classical decomposition of time-series data. https://otexts.com/fpp2/classical-decomposition.html
18. Jiang, J., Sekar, V., Milner, H., Shepherd, D., Stoica, I., Zhang, H.: CFA: A practical prediction system for video QoE optimization. In: Proceedings of the 13th USENIX Conference on Networked Systems Design and Implementation, NSDI 2016, pp. 137–150. USENIX Association, USA (2016)
19. Jin, Y., et al.: Zooming in on wide-area latencies to a global cloud provider. In: ACM SIGCOMM, August 2019
20. Khokhar, M.J., Ehlinger, T., Barakat, C.: From network traffic measurements to QoE for internet video. In: 2019 IFIP Networking Conference (IFIP Networking), pp. 1–9 (2019)
21. Licciardello, M., Gr¨uner, M., Singla, A.: Understanding video streaming algorithms in the wild. In: Sperotto, A., Dainotti, A., Stiller, B. (eds.) Passive and Active Measurement. pp, pp. 298–313. Springer International Publishing, Cham (2020). https://doi.org/10.1007/978-3-030-44081-7_18
22. Mok, R.K., Chan, E.W., Luo, X., Chang, R.K.: Inferring the QoE of HTTP video streaming from user-viewing activities. In: Proceedings of the First ACM SIGCOMM Workshop on Measurements up the Stack, W-MUST 2011, pp. 31–36. Association for Computing Machinery, New York (2011)
23. Nam, H., Kim, K., Schulzrinne, H.: QoE matters more than QoS: Why people stop watching cat videos. In: IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications, pp. 1–9 (2016)
24. Richter, P., et al.: A multi-perspective analysis of carrier-grade NAT deployment. In: Proceedings of the 2016 Internet Measurement Conference, IMC 2016, pp. 215–229. Association for Computing Machinery, New York (2016)
25. R¨uth, J., Wolsing, K., Wehrle, K., Hohlfeld, O.: Perceiving QUIC: Do users notice or even care? arXiv:1910.07729 (2019)
26. Schlinker, B., Cunha, I., Chiu, Y.-C., Sundaresan, S., Katz-Bassett, E.: Internet performance from Facebook’s edge. In: Proceedings of the Internet Measurement Conference, IMC 2019, pp. 179–194. Association for Computing Machinery, New York (2019)
27. Schlinker, B., et al.: Engineering egress with Edge Fabric: Steering oceans of content to the world. In: Proceedings of the Conference of the ACM Special Interest Group on Data Communication, SIGCOMM 2017, pp. 418–431. Association for Computing Machinery, New York (2017)
28. Yap, K., et al.: Taking the edge off with Espresso: Scale, reliability, and programmability for global internet peering (2017)