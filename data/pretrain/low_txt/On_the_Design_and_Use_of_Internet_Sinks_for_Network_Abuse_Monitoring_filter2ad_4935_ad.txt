# Sampled Hash Table Size and Error Rates

The following data represents the error rates for different hash table sizes, using the Sample and Hold method with rates of 1/100 (left) and 1/300 (right). The x-axis is on a logarithmic scale.

- **Sampled Hash Table Sizes:**
  - 500
  - 2000
  - 8000
  - 32000

- **Error Rates:**
  - 40%
  - 35%
  - 30%
  - 25%
  - 20%
  - 15%
  - 10%
  - 5%
  - 0%

- **Time Intervals:**
  - Hour 1
  - Hour 2
  - Hour 3
  - Hour 4

**Figure 11.** Error rates for different hash table sizes (x-axis is log scale) using Sample and Hold method with rates of 1/100 (left) and 1/300 (right).

## Rationale for Random Sampling in Subnet Selection

Our analysis in this paper is based on the use of random sampling as a means for subnet selection. This approach is justified by the observation that overall traffic volumes across the service-provider class A address space we monitor are quite uniform. The key strengths of this method include:
- **Simplicity:** It provides a straightforward method for subnet selection.
- **Unbiased Estimates:** It ensures unbiased estimates.
- **Analytical Tractability:** It lends itself directly to analysis.

However, a potential drawback is that more sophisticated sampling designs, such as clustered or adaptive sampling, could provide more accurate population estimates. We leave the exploration of these and other sampling methods to future work.

## Detectability and Estimation Accuracy

After selecting the sampling design, our analysis focused on the problem of detectability. Specifically, we aimed to understand the accuracy of estimates of total probe populations from randomly selected subsets. If \(\hat{\tau}\) is an unbiased estimator of a population total \(\tau\), then the estimated variance of \(\hat{\tau}\) is given by:

\[
\text{var}(\hat{\tau}) = N^2 \left( \frac{\sigma^2}{n} + \frac{1 - p}{p} \cdot \frac{\mu}{n} \right) \left( \frac{N - n}{N} \right)
\]

where:
- \(N\) is the total number of units (subnets),
- \(n\) is the sampled number of units,
- \(\mu\) is the population mean (mean number of occurrences of a specific type of probe),
- \(\sigma^2\) is the population variance,
- \(p\) is the probability of detection for a particular type of probe.

In Section 6.1, we evaluate the error in population estimates over a range of detection probabilities for different sample sizes. The samples consider dividing the class A address space into its component class B's. The probabilities relate directly to the detection of the worst offenders (top sources of unsolicited traffic) as in the prior sampling analysis. The results provide a means for judging population estimation error rates as a function of network bandwidth consumption.

## 6.1 Sampling Evaluation

### Memory Constrained Evaluation

Our evaluation of the impact of sampling in an iSink was an offline analysis using traces gathered during one day selected at random from the service-provider iSink. Our objective was to empirically assess the accuracy of sampling under both memory-constrained and bandwidth-constrained conditions.

In the memory-constrained evaluation, we compare the ability to accurately generate the top 100 heavy hitter source list over four consecutive 1-hour periods using different hash table sizes and different sampling rates. For each hour in the dataset, we compare the percentage difference in the number of scans generated by the "true" top 100 blacklist and the sampled top 100 blacklist sources.

### Bandwidth Constrained Evaluation

In the bandwidth-constrained evaluation, we consider accuracy along three dimensions:
1. Estimating the worst offender population with partial visibility.
2. Estimating blacklists of different lengths.
3. Estimating backscatter population.

### Results

- **Hash Table Sizes:** Varying from 500 to 64K entries, where each entry consists of a source IP and an access attempt count.
- **Sampling Rates:** Two different arbitrarily chosen rates – 1 in 100 and 1 in 300 with uniform probability.
- **Eviction Policy:** If tables become full during a given hour, entries with the lowest counts are evicted to make room for new entries.

The results, shown in Figure 11, indicate that even coarse sampling rates (1/300) and relatively small hash tables enable fairly accurate blacklists (between 5%–10% error). The factor of improvement between sampling at 1/100 and 1/300 is about 1.5, and there is little benefit to increasing the hash table size from 5,000 to 20,000. Thus, from the perspective of heavy hitter analysis in a memory-constrained system, sampling can be effectively employed in iSinks.

### Bandwidth Constrained Analysis

As discussed in the prior section, in our bandwidth-constrained evaluation, we consider the error introduced in population estimates when using simple random sampling over a portion of the available IP address space. We argue that simple random sampling is appropriate for some analysis given the uniform distribution of traffic over our class A monitor. The cumulative distribution of traffic over a one-hour period for half of the /16 subnets in our class A monitor is shown in Figure 13 (right). This figure shows that while traffic across all subnets is relatively uniform (at a rate of about 320 packets per minute per /16), specific traffic subpopulations—such as TCP backscatter—can show significant non-uniformity, which can have a significant impact on sampling.

We use the mean normalized standard deviation (\(\sigma/\mu\)) as an estimate of error in our analysis. In each case, using the data collected in a typical hour on the /8, we empirically assess the estimated error as a function of a randomly selected sample of /16 subnets. The results of this approach are shown in Figure 12. The graph on the left shows the ability to accurately estimate the number of probes from the single worst offending IP source over a range of detection probabilities. This graph indicates that worst offenders are detectable even with a small sample size and error-prone or incomplete measurements. The graph on the right shows the ability to accurately estimate blacklists from a selected sample of /16’s. This graph indicates that it is easier to estimate larger rather than smaller blacklists when sampling. We attribute this to the variability in blacklist ordering across the /16’s. Finally, Figure 13 (left) shows the ability to accurately estimate TCP backscatter traffic over a range of detection probabilities. The graph suggests that while backscatter estimates are robust in the face of error-prone or incomplete measurements, the estimated error of total backscatter is quite high even with a reasonably large number of /16’s. This can be attributed to the non-uniformity of backscatter traffic across the class A monitor shown in Figure 13 (right) and suggests that alternative sampling methods for backscatter traffic should be explored. On a broader scale, this indicates that traditional backscatter methodologies that assume uniformity could be error-prone.

## 7. Summary and Future Work

In this paper, we describe the architecture and implementation of an Internet Sink (iSink): a useful tool in a general network security architecture. iSinks have several general design objectives, including scalability, the ability to passively monitor network traffic on unused IP addresses, and to actively respond to incoming connection requests. These features enable large-scale monitoring of scanning activity as well as attack payload monitoring. The implementation of our iSink is based on a novel application of the Click modular router, NAT Filter, and the Argus flow monitor. This platform provides an extensible, scalable foundation for our system and enables its deployment on commodity hardware. Our initial implementation includes basic monitoring and active response capability, which we test in both laboratory and live environments.

We report results from our iSink’s deployment in a live environment comprising four class B networks and one entire class A network. The objectives of these case studies were to evaluate iSink’s design choices, to demonstrate the breadth of information available from an iSink, and to assess the differences in perspective based on iSink location in IP address space. We show that the amount of traffic delivered to these iSinks can be large and quite variable. We see clear evidence of well-documented worm traffic as well as other easily explained traffic, the aggregate of which can be considered Internet background noise. While we expected overall volumes of traffic in the class B monitors and class A monitor to differ, we also found that the overall characteristics of scans in these networks were quite different. We also demonstrate the capability of iSinks to provide insights on interesting network phenomena like periodic probing and SMTP hot-spots, and their ability to gather information on sources of abuse through sampling techniques. A discussion of operational issues, security, and passive fingerprinting techniques is provided in [32].

The evaluation of our iSink implementation demonstrates both its performance capabilities and expectations for live deployment. From laboratory tests, we show that iSinks based on commodity PC hardware have the ability to monitor and respond to over 20,000 connection requests per second, which is approximately the peak traffic volume we observed on our class A monitor. This also exceeds the current version of LaBrea’s performance by over 100%. Furthermore, we show that sampling techniques can be used effectively in an iSink to reduce system overhead while still providing accurate data on scanning activity.

We intend to pursue future work in several directions. First, we plan to expand the amount of IP address space we monitor by deploying iSinks in other networks. Next, we intend to supplement iSink by developing tools for data mining and automatic signature generation.

### Acknowledgements

The authors would like to thank Jeff Bartig, Geoff Horne, Bill Jensen, and Jim Martin for all of their help. Exploration of the filtering techniques grew out of fruitful discussions during Vinod’s internship with Vern Paxson. We would also like to acknowledge contributions of Ruoming Pang in the development of the DCERPC responder. Finally, we would like to thank the anonymous reviewers for their insightful comments and Diego Zamboni for his excellent shepherding.

### References

1. R. Anderson and A. Khattak. The Use of Information Retrieval Techniques for Intrusion Detection. In Proceedings of RAID, September 1998.
2. Network Associates. LovGate Virus Summary. http://vil.nai.com/vil/content/Print100183.htm, 2002.
3. C. Bullard. Argus Open Project. http://www.qosient.com/argus/.
4. C. Cranor, Y. Gao, T. Johnson, V. Shkapenyuk, and O. Spatscheck. Gigascope: High Performance Network Monitoring with an SQL Interface.
5. E-eye. Analysis: Sasser Worm. http://www.eeye.com/html/Research/Advisories/AD20040501.html.
6. C. Estan and G. Varghese. New Directions in Traffic Measurement and Accounting. In Proceedings of ACM SIGCOMM ’02, Pittsburgh, PA, August 2002.
7. A. Feldmann, A. Greenberg, C. Lund, N. Reingold, and J. Rexford. NetScope: Traffic Engineering for IP Networks. IEEE Network Magazine, Special Issue on Internet Traffic Engineering, 2000.
8. B. Greene. BGPv4 Security Risk Assessment, June 2002.
9. B. Greene. Remote Triggering Black Hole Filtering, August 2002.
10. Honeyd: Network Rhapsody for You. http://www.citi.umich.edu/u/provos/honeyd.
11. G. Iannaccone, C. Diot, I. Graham, and N. McKeown. Monitoring very high speed links. In SIGCOMM Internet Measurement Workshop, November 2001.
12. E. Kohler, R. Morris, B. Chen, J. Jannotti, and F. Kaashoek. The Click Modular Router. ACM Transactions on Computer Systems, August 2000.
13. W. Lee, S.J. Stolfo, and K.W. Mok. A Data Mining Framework for Building Intrusion Detection Models. In IEEE Symposium on Security and Privacy, 1999.
14. T. Liston. The Labrea Tarpit Homepage. http://www.hackbusters.net/LaBrea/.
15. D. Moore. Network Telescopes. http://www.caida.org/outreach/presentations/2003/dimacs0309/.
16. D. Moore, V. Paxson, S. Savage, C. Shannon, S. Staniford, and N. Weaver. The Spread of the Sapphire/Slammer Worm. Technical report, CAIDA, 2003.
17. D. Moore, C. Shannon, and K. Claffy. Code Red: A Case Study on the Spread and Victims of an Internet Worm. In Proceedings of ACM SIGCOMM Internet Measurement Workshop, Marseilles, France, November 2002.
18. D. Moore, C. Shannon, G. Voelker, and S. Savage. Internet Quarantine: Requirements for Containing Self-Propagating Code. In Proceedings of IEEE INFOCOM, April 2003.
19. D. Moore, G. Voelker, and S. Savage. Inferring Internet Denial of Service Activity. In Proceedings of the 2001 USENIX Security Symposium, Washington D.C., August 2001.
20. T. Oetiker. The Multi Router Traffic Grapher. In Proceedings of the USENIX Twelfth System Administration Conference LISA XII, December 1998.
21. V. Paxson. Bro: A System for Detecting Network Intruders in Real Time. In Proceedings of the 7th USENIX Security Symposium, 1998.
22. D. Plonka. Flawed Routers Flood University of Wisconsin Internet Time Server. http://www.cs.wisc.edu/plonka/netgear-sntp.
23. D. Plonka. Flowscan: A Network Traffic Flow Reporting and Visualization Tool. In Proceedings of the USENIX Fourteenth System Administration Conference LISA XIV, December 2000.
24. Y. Rekhter. RFC 1817: CIDR and Classful Routing, August 1995.
25. M. Roesch. The Snort Network Intrusion Detection System. http://www.snort.org.
26. S. Staniford, J. Hoagland, and J. McAlerney. Practical Automated Detection of Stealthy Portscans. In Proceedings of the ACM CCS IDS Workshop, November 2000.
27. S. Staniford, V. Paxson, and N. Weaver. How to Own the Internet in Your Spare Time. In Proceedings of the 11th USENIX Security Symposium, San Francisco, CA, August 2002.
28. H.S. Teng, K. Chen, and S. C-Y Lu. Adaptive Real-Time Anomaly Detection Using Inductively Generated Sequential Patterns. In IEEE Symposium on Security and Privacy, 1999.
29. The Honeynet Project. http://project.honeynet.org.
30. Trend Micro. WORM RBOT.CC. http://uk.trendmicro-europe.com/enterprise/security-info/-ve-detail.php?Vname=WORM-RBOT.CC.
31. V. Yegneswaran, P. Barford, and S. Jha. Global Intrusion Detection in the DOMINO Overlay System. In Proceedings of NDSS, San Diego, CA, 2004.
32. V. Yegneswaran, P. Barford, and D. Plonka. On the Design and Use of Internet Sinks for Network Abuse Monitoring. University of Wisconsin Technical Report #1497, 2004.
33. V. Yegneswaran, P. Barford, and J. Ullrich. Internet Intrusions: Global Characteristics and Prevalence. In Proceedings of ACM SIGMETRICS, San Diego, CA, June 2003.