### Derivation of the Flow Completion Time (FCT) Function

The flow completion time \( F_{CTn}(t) \) as a function of the threshold \( t \) for this model is given by:

\[
F_{CTn}(t) = F_S(t) + \frac{1}{1 - \rho_{BS}(t)} (1 - F_S(t)) + \frac{\lambda}{2(1 - \rho_{BS}(t))} \int_t^\infty x^2 f_S(x) \, dx + \frac{\lambda (1 - \rho)}{2(1 - \rho_{BS}(t))} \int_0^\infty \frac{f_S(y)}{y} \, dy
\]

This derivation is based on the well-known Pollaczek-Khintchine formula [13], which is used to compute the average waiting time for a flow in each priority queue (assuming M/G/1 queues). The formula can be generalized for any number of priority queues. It is important to note that \( F_{CTn}(t) \) depends on the flow size distribution and the overall load \( \rho \).

### Numerical Results and Optimal Thresholds

Figure 16 shows the \( F_{CTn}(t) \) for the web search flow size distribution at loads ranging from 10% to 80%. The red circles indicate the optimal threshold at each load. The threshold varies between approximately 880-1740 KB as the load increases from 10% to 80%. The figure also suggests that performance can be quite sensitive to the chosen threshold, especially at high loads. We evaluate this sensitivity using simulations in the next section.

### Simulations with Priority Queues

We compare the performance of using a few priority queues in existing switches with pFabric. Our results confirm that while this mechanism provides good performance with a sufficient number of priority queues (around 8), it still falls short of pFabric's performance. The performance is also sensitive to the values of the thresholds used and how the switch buffer is shared among the priority queues.

#### Simulation Setup

We simulate the web search workload (§5.1) for three scenarios with 2, 4, and 8 priority queues per fabric port. The queues at a port share a buffer pool of size 225 KB (150 packets). We reserve 15 KB (10 packets) of buffer per queue, and the rest is shared dynamically on a first-come-first-serve basis. In each scenario, we use the optimal flow size thresholds for each priority queue as derived in §6.1.

#### Results

- **Average Overall FCT**: As expected, the average overall FCT (Figure 17(a)) improves as we increase the number of priority queues and approaches pFabric's performance with 8 priority queues.
- **99th Percentile FCT for Small Flows**: Figure 17(b) shows a significant increase in the 99th percentile FCT for small flows at high loads in the 8-queue case. This is because, with 8 queues, 80 out of the total 150 packets are reserved, leaving only 70 packets to be shared among the queues. At high loads, during some bursts, the high-priority queue runs out of buffers and drops packets, increasing tail latency. This demonstrates the need for careful tuning of buffer allocations for each priority queue.

### Sensitivity to Thresholds

Finally, we explore the sensitivity of the performance with a few priority queues to using the "right" thresholds for splitting traffic. Figure 17(c) compares the 4-queue system with optimal thresholds to a reasonable heuristic that splits flows equally across the 4 queues. The plot shows the average FCT across all flows. We find a substantial improvement with the optimized thresholds. At 80% load, the average FCT is reduced by more than 30%, with even more significant performance gaps for the tail latencies of short flows. This confirms that the thresholds for splitting traffic across limited priority queues need to be chosen carefully. By allowing an essentially unlimited number of priorities, pFabric does not require such tuning and is not sensitive to parameters such as thresholds, minimum reserved buffer per priority queue, or overall buffer size.

### Discussion

pFabric advocates a different design philosophy for datacenter networks, informed by the fact that the datacenter network is more of an inter-connect for distributed computing workloads rather than a bit-pipe. Therefore, it is more important to orchestrate network resource allocation to meet overall computing objectives rather than traditional communication metrics such as throughput and fairness, which TCP optimizes for. This leads to a design ethos where flows (which are proxies for the data needed for compute tasks) become first-class citizens, and the network fabric is designed to schedule them in a lightweight fashion to maximize application-layer objectives.

#### Starvation & Gaming

A potential concern with strictly prioritizing small flows is that it may starve large flows. Additionally, a malicious user may game the system by splitting up their large flows to gain an advantage. However, under realistic heavy-tailed traffic distributions, SRPT actually improves the majority of flows (even large flows) compared to TCP's fair sharing. If desired, an operator can put in explicit safeguards against starvation, such as capping the priority numbers so that beyond a certain size, all flows get the same base priority.

#### Setting Packet Priorities

In many datacenter applications, flow sizes or deadlines are known at initiation time and can be conveyed to the network stack (e.g., through a socket API) to set priorities. Even with imprecise but reasonable estimates of flow sizes, pFabric can achieve good performance. With realistic distributions, most of the benefit can be achieved by classifying flows into a few (4-8) priority levels based on size.

#### Supporting Multiple Priority Schemes

Datacenter fabrics are typically shared by a variety of applications with different requirements. A single priority scheme may not always be appropriate. This can be handled by operating pFabric's priority scheduling and dropping mechanisms within individual "higher-level" traffic classes in a hierarchical fashion. Traditional QoS mechanisms such as WRR can be used to divide bandwidth between these high-level classes based on user-defined policy, while pFabric provides near-optimal scheduling of individual flows in each class according to the class’s priority scheme.

#### Other Datacenter Topologies

While we have focused on Fat-tree/Clos topologies, our results should carry over to any reasonable datacenter topology that provides uniform high throughput between ingress and egress ports.

#### Stability

The theoretical literature has shown scenarios where size-based traffic prioritization may reduce the stability region of the network. However, this problem is mostly for "linear" topologies with flows traversing different numbers of hops. We have not seen this issue in our study and do not expect it to be a major concern in real datacenter environments due to the uniform number of hops and the small overall load contributed by small (high-priority) flows.

### Conclusion

This paper decouples the key aspects of datacenter packet transport—flow scheduling and rate control—and shows that by designing simple mechanisms for these goals separately, we can realize a minimalistic datacenter fabric design that achieves near-ideal performance. Large buffers or complex rate control are largely unnecessary in datacenters. The next step is to integrate a prototype implementation of pFabric with a latency-sensitive application to evaluate the impact on application-layer performance. Further work on designing incrementally deployable solutions based on pFabric could be fruitful, paving the way for widespread use of these ideas in practice.

### Acknowledgments

We thank our shepherd, Jon Crowcroft, and the anonymous SIGCOMM reviewers for their valuable feedback. Mohammad Alizadeh thanks Tom Edsall for useful discussions regarding the practical aspects of this work.

### References

[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center network architecture. In Proc. of SIGCOMM, 2008.

[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: dynamic flow scheduling for data center networks. In Proc. of NSDI, 2010.

[3] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data center TCP (DCTCP). In Proc. of SIGCOMM, 2010.

[4] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat, and M. Yasuda. Less is more: trading a little bandwidth for ultra-low latency in the data center. In Proc. of NSDI, 2012.

[5] M. Alizadeh, S. Yang, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker. Deconstructing datacenter packet transport. In Proc. of HotNets, 2012.

[6] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker. pFabric: Minimal Near-Optimal Datacenter Transport. http://simula.stanford.edu/~alizade/pfabric-techreport.pdf.

[7] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny. Workload analysis of a large-scale key-value store. In Proc. of SIGMETRICS, 2012.

[8] N. Bansal and M. Harchol-Balter. Analysis of SRPT scheduling: investigating unfairness. In Proc. of SIGMETRICS, 2001.

[9] A. Bar-Noy, M. M. Halldórsson, G. Kortsarz, R. Salman, and H. Shachnai. Sum multicoloring of graphs. J. Algorithms, 2000.

[10] T. Bonald and L. Massoulié. Impact of fairness on Internet performance. In Proc. of SIGMETRICS, 2001.

[11] A. Dixit, P. Prakash, Y. C. Hu, and R. R. Kompella. On the Impact of Packet Spraying in Data Center Networks. In Proc. of INFOCOM, 2013.

[12] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: a scalable and flexible data center network. In Proc. of SIGCOMM, 2009.

[13] D. Gross, J. F. Shortle, J. M. Thompson, and C. M. Harris. Fundamentals of Queueing Theory. Wiley-Interscience, New York, NY, USA, 4th edition, 2008.

[14] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing Flows Quickly with Preemptive Scheduling. In Proc. of SIGCOMM, 2012.

[15] The Network Simulator NS-2. http://www.isi.edu/nsnam/ns/.

[16] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis, J. Leverich, D. Mazières, S. Mitra, A. Narayanan, D. Ongaro, G. Parulkar, M. Rosenblum, S. M. Rumble, E. Stratmann, and R. Stutsman. The case for RAMCloud. Commun. ACM, 2011.

[17] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and M. Handley. Improving datacenter performance and robustness with multipath TCP. In Proc. of the SIGCOMM, 2011.

[18] B. Vamanan, J. Hasan, and T. N. Vijaykumar. Deadline-Aware Datacenter TCP (D2TCP). In Proc. of SIGCOMM, 2012.

[19] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G. Andersen, G. R. Ganger, G. A. Gibson, and B. Mueller. Safe and effective fine-grained TCP retransmissions for datacenter communication. In Proc. of SIGCOMM, 2009.

[20] M. Verloop, S. Borst, and R. Núñez Queija. Stability of size-based scheduling disciplines in resource-sharing networks. Perform. Eval., 62(1-4), 2005.

[21] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowtron. Better never than late: meeting deadlines in datacenter networks. In Proc. of SIGCOMM, 2011.

[22] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. H. Katz. DeTail: Reducing the Flow Completion Time Tail in Datacenter Networks. In Proc. of SIGCOMM, 2012.