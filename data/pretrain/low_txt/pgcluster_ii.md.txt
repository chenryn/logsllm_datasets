# PGCluster-II: A Clustering System for PostgreSQL Using Shared Data

**Presented by:** Atsushi MITANI - PI:EMAIL  
**Event:** First Italian PostgreSQL Day (PGDay 2007)  
**Date:** July 6-7, 2007  
**Location:** Prato, Italy

## Agenda
1. Requirements
2. PGCluster
3. New Requirements
4. PGCluster-II
5. Structure and Process Sequence
6. Pros & Cons

## Requirements
### Original Requirement
- **Target Application:** Web applications with heavy session loads.
- **High Availability (HA):** No downtime using ordinary servers.
- **Performance:** High performance for data read queries (over 90% of sessions).
- **No Single Point of Failure:**
- **Automatic Takeover:**
- **Manual Restore:**
- **Dynamic Scalability:** Add cluster DBs and replication servers on the fly.

### New Requirements
- **Target Application:** Web and OLTP applications.
- **HA and High Performance (HP):** Required even for data write operations.
- **Service Continuity:** Service stop is not allowed.
- **Coexistence of HA and HP:** Balancing redundancy (for HA) and quick response (for HP).

## PGCluster (2002-)
- **Synchronous & Multi-Master Replication System:**
  - Query-based replication.
  - Independent data replication (e.g., `now()`, `random()`).
  - No single point of failure.
  - Multiplex load balancer, replication server, and cluster DBs.
  - Automatic takeover.
  - Manual restore required.
  - Dynamic scalability for adding cluster DBs and replication servers.
  - Version upgrade support.

### Structure of PGCluster
- **Client**
- **Load Balancer**
- **Session Manager**
- **Replicator**
- **Cluster Databases**

### Pros & Cons of PGCluster
- **Pros:**
  - High availability.
  - Good performance for data read loads.
  - Ordinary PC servers.
  - BSD license software.
- **Cons:**
  - Poor performance for data write loads.
  - Maintenance issues.
  - Cost concerns.
  - Documentation issues.

## PGCluster-II
- **Data Shared Clustering System:**
  - Storage shared by shared disk (NFS, GFS, GPFS, SAN/NAS).
  - Cache and lock status shared by Virtual IPC.

### Concept of Shared Data
- **Virtual Shared IPC:**
  - Shares semaphores and shared memory across DB nodes.
  - Writes to remote nodes through cluster processes.
  - Reads from local node directories.
  - Signal and message queue out of scope.

### Structure of PGCluster-II
- **DB Nodes:**
  - Read/Write requests.
  - Shared disk for storage.
  - Semaphore for lock control.
  - Shared memory for communication between backend processes.

### Semaphores
- **Usage:**
  - Depends on the "max-connections" setting.
  - Default: 7 x 16 semaphores.
  - Mapping table required for semaphore IDs.

### Shared Memory
- **Communication:**
  - Stores logs, caches, buffers, etc.
  - Single shared memory allocation, divided into multiple segments.
  - Over 100 entry pointers exist.

### Issues of Shared Memory
- **Activity Issue:**
  - Small size but high update frequency.
- **Contents Issue:**
  - Includes memory/function addresses.
  - Copying shared memory to other servers can cause crashes (OS-dependent).

### Solution
- **Mask Table & Localization Table:**
  - Data changed to offset from address.
  - Improved performance over previous methods.

### Benchmark
- **pgbench 1 vs. pgbench 2:**
  - Performance comparison graph (details provided in the presentation).

### Pros & Cons of PGCluster-II
- **Pros:**
  - Easy to add a node for redundancy or replacement.
  - Significant improvement in data read performance and many connection loads.
- **Cons:**
  - Requires large RAM.
  - Writing performance still needs improvement.
  - Shared disk system is expensive.

### Future Work
- **Performance Improvement:**
  - Further enhance writing performance.
- **Optimization:**
  - Narrow down target shared memory data.
  - Send multiple memory data at once.
- **Release:**
  - Release source code as soon as possible.
- **Documentation:**
  - Provide comprehensive documentation.

## Conclusion
Thank you for your attention. For more information:
- **PGCluster:** PI:EMAIL
- **PGCluster-II:** PI:EMAIL

Feel free to ask any questions!