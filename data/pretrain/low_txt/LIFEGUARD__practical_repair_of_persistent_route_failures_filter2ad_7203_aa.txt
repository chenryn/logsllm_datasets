# LIFEGUARD: Practical Repair of Persistent Route Failures

**Authors:**
- Ethan Katz-Bassett (University of Southern California, University of Washington)
- Colin Scott (UC Berkeley)
- David R. Choffnes (University of Washington)
- Ítalo Cunha (UFMG, Brazil)
- Vytautas Valancius (Georgia Tech)
- Nick Feamster (Georgia Tech)
- Harsha V. Madhyastha (UC Riverside)
- Thomas E. Anderson (University of Washington)
- Arvind Krishnamurthy (University of Washington)

## Abstract
The Internet is designed to always find a route if a policy-compliant path exists. However, connectivity disruptions often occur despite the presence of valid paths. While the research community has focused on short-term outages during route convergence, there has been less progress in addressing long-lasting, avoidable outages. Our measurements show that these long-lasting events significantly contribute to overall unavailability.

To address this issue, we developed LIFEGUARD, a system for automatic failure localization and remediation. LIFEGUARD uses active measurements and a historical path atlas to locate faults, even in the presence of asymmetric paths and failures. We argue that the Internet protocols should allow edge ISPs to steer traffic around failures without requiring involvement from the network causing the failure. Although the Internet does not explicitly support this functionality, we demonstrate how to approximate it using carefully crafted BGP messages. LIFEGUARD employs techniques to reroute around failures with minimal impact on working routes. Deploying LIFEGUARD on the Internet, we found that it can effectively route traffic around an AS without causing widespread disruption.

**Categories and Subject Descriptors:**
- C.2.2 [Communication Networks]: Network protocols

**Keywords:**
- Availability, BGP, Measurement, Outages, Repair

## 1. Introduction
With the proliferation of interactive web applications, always-connected mobile devices, and cloud data storage, we expect the Internet to be available anytime, from anywhere. However, even well-provisioned cloud data centers experience frequent routing problems. Existing research provides promising approaches to dealing with transient unavailability during routing protocol convergence, but we focus on long-lasting outages that are less likely to be convergence-related.

Monitoring paths from Amazon’s EC2 cloud service, we found that 84% of unavailability lasting at least 90 seconds was due to outages that lasted over ten minutes. We focus on disruptions where a working, policy-compliant path exists, but networks instead route along a different, failing path. In theory, this should never happen, as the Internet protocols are designed to find working paths. In practice, routers can fail to detect or reroute around a failed link, causing silent failures.

When an outage occurs, affected networks struggle to restore connectivity because the failure may be caused by a problem outside their control. Traditional route control techniques give operators direct influence only over routes between them and their immediate neighbors, which may not be sufficient to avoid a problem in a distant transit network. Even with multiple providers, operators have little control over the routes other ASes select.

To improve Internet availability, we need a way to combat long-lived failures. We propose that data centers and well-provisioned edge networks should be able to repair persistent routing problems, regardless of which network along the path is responsible. If an alternate, working, policy-compliant path exists, the edge network should be able to use it.

We achieve this goal by enabling an edge network to disable routes that traverse a misbehaving network, triggering route exploration to find new paths. We present the design and implementation of LIFEGUARD, a system that enables rerouting around many long-lasting failures while being deployable on today's Internet. LIFEGUARD aims to automatically repair partial outages in minutes, replacing the manual process that can take hours.

## 2. Background and Motivation

### 2.1 Quantifying Unreachability from EC2
To test the prevalence of outages, we conducted a measurement study using Amazon EC2, a major cloud provider. We rented EC2 instances in four AWS regions from July 20, 2010, to August 29, 2010. Each vantage point issued a pair of pings every 30 seconds to 250 targets—five routers each from the 50 highest-degree ASes. We defined an outage as four or more consecutive dropped pairs of pings from a single vantage point to a destination, with a minimum duration of 90 seconds.

Our study found that 84% of the unavailability came from outages lasting over ten minutes. Many long-lasting outages occur with few or no accompanying routing updates, indicating that routing protocols often fail to react. Such problems can arise when a router fails to detect an internal fault or when cross-layer interactions cause an MPLS tunnel to fail, even though the underlying IP network is operational.

During partial outages, some hosts cannot find working routes to the destination due to physical partitions, routing policies, or routers announcing non-working routes. We ruled out physical partitions as the cause in our EC2 study, as all EC2 instances maintained connectivity with a controller at our institution throughout the study. This suggests that either routing policies are eliminating all working paths, or routers are advertising non-working paths.

### 2.2 Assessing Policy-Compliant Alternate Paths
Previous systems demonstrated that overlays can route around many failures, but overlay paths tend to violate BGP export policies. We build on this work by showing that alternate, policy-compliant paths often exist during many failures. Generally, the longer a problem lasts, the more likely it is that alternative routes exist.

We issued traceroutes between all PlanetLab sites every ten minutes for a week starting September 5, 2011. This setting allowed us to issue traceroutes from both ends of every path and provided a rich view of other paths that might combine to form alternate routes. We considered an outage as any instance where a pair of hosts, previously able to send traceroutes between each other, failed to reach the destination AS for at least three consecutive rounds before working again. This yielded nearly 15,000 outages.

We checked if the traceroutes included working, policy-compliant routes around the failures. For each round of a failure, we tried to find a path from the source that intersected (at the IP-level) a path to the destination, such that the spliced path did not traverse the AS in which the failed traceroute terminated. We only considered a spliced path as valid if it would be available under observable export policies. Our methodology may fail to identify some valid paths that exist, but it provides a robust approach to assessing policy-compliant alternate paths.