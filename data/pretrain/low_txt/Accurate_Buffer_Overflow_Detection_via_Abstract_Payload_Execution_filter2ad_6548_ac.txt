### Intrusion Detection and Threshold Selection

This observation supports the assumptions presented in Section 4. For the actual detection of intrusions, a threshold must be set to distinguish between malicious and normal traffic, triggering an alert when the threshold is exceeded.

For our initial prototype, we selected a "reasonable" threshold value between the maximum value observed in normal requests (16) and the minimum value among the evaluated exploits (216). Given that an attacker might attempt to evade detection by using a shorter sledge, the threshold should be closer to the maximum value of normal requests. We decided on a threshold of 30 for the deployment of the probe. This setting allows sufficient room for regular requests, keeping the false positive rate low, while forcing attackers to reduce the executable parts of their exploit to a length below this limit to remain undetected. Such a short sledge significantly reduces the attacker's chances of guessing an address close enough to succeed.

### Limitations of the Approach

A notable limitation of this approach is its inability to detect exploits that avoid using executable sledges. Vulnerable services with debug routines that output information can be exploited by hackers. If an attacker triggers the service to execute the debug output and calculates the exact stack address (an infoleak attack), they can create buffer overflows without including executable sledges.

### Performance Evaluation

To evaluate the performance impact of our module on the web server, we used the WebSTONE benchmark provided by Mindcraft. WebSTONE simulates an arbitrary number of clients requesting pages of different sizes from the web server, providing a realistic load. It measures several key properties, including:

- Average and maximum connection time of requests
- Average and maximum response time of requests
- Data throughput rate

**Connection Time**: The interval between the client opening a TCP connection and the server accepting it.
**Response Time**: The interval between the client establishing the connection and requesting data, and the point when the first result is received.
**Data Throughput Rate**: The amount of data the web server can deliver to all clients.

These metrics are crucial for understanding the user experience, as they indicate the wait time after sending a request and the number of requests the web server can handle under specific loads. The data throughput rate determines how quickly data can be sent from the web server to the client, which affects the overall request completion time.

### Experimental Setup

Our experimental setup involved one machine simulating the clients performing HTTP requests (Athlon, 1 GHz, 256 MB RAM, Linux 2.4) and another host running the Apache server (Pentium III, 550 MHz, 512 MB RAM, Linux 2.4). Both machines were connected via a 100 Mb Fast Ethernet. WebSTONE was configured to launch 10 to 100 clients in steps of 10, each running for 2 minutes. Only static page tests were conducted, excluding dynamic content generation.

We measured the connection rate, average client response time, and average client throughput for each test run, both with and without our installed module. The results are shown in Figures 7, 8, and 9. The dotted line represents the statistics gathered from the unmodified Apache, while the solid line shows the results with our activated module.

**Connection Rate**: The connection rate slightly decreased when our sensor was activated. The most significant difference was observed with 50 active clients, where the rate dropped from 500.7 to 494.2 connections per second, a decrease of about 1.4%.

**Response Time**: There was no significant decrease in the average response time. Both lines were nearly congruent within the measurement precision.

**Client Throughput**: The client throughput decreased most with 10 active clients, dropping from 75.90 Mbit/s to 73.70 Mbit/s, a decrease of about 2.9%. On average, the client throughput decreased by only 0.8 Mbit/s, or about 1.05%.

The trie consumed approximately 16 MB of memory during the tests. While this may seem substantial, it is reasonable considering the typical main memory configuration of web servers, where 1 GB of RAM is common. Additionally, this data structure enables very fast tests, representing a classic trade-off in favor of speed.

### Conclusion and Future Work

This paper presents an accurate method for detecting buffer overflow exploit code in Internet service requests. We explain the structure and constraints of these attacks and discuss methods used by intruders to evade common detection techniques.

Our approach is based on the abstract execution of the packet payload to detect the sledge of an exploit. A valid instruction chain is defined as a sequence of consecutive bytes in a request that represent executable processor instructions. Our hypothesis, supported by theoretical analysis and empirical results, shows that requests containing buffer overflow code have noticeably longer chains than regular requests.

The system has the advantage of analyzing and denying requests a-priori before the service process is affected by a buffer overflow. It is also resistant to the evasion techniques discussed in Section 2. The performance impact of the probe was evaluated by integrating it into the Apache web server.

Future work will focus on emulating additional instructions (e.g., SIMD and MMX operations) and investigating the benefits of full emulation of instruction effects to detect self-modifying sledges. We also plan to collect experimental data for other protocols like FTP and NFS to validate the applicability of our approach in those contexts.

### References

1. AlephOne. Smashing the stack for fun and profit. Phrack Magazine, 49(14), 1996.
2. Debra Anderson, Thane Frivold, Ann Tamaru, and Alfonso Valdes. Next Generation Intrusion Detection Expert System (NIDES). SRI International, 1994.
3. The Apache Software Foundation. http://www.apache.org.
4. M. Bykova, S. Ostermann, and B. Tjaden. Detecting network intrusions via a statistical analysis of network packet characteristics. In Proceedings of the 33rd Southeastern Symposium on System Theory, 2001.
5. Crispin Cowan, Calton Pu, David Maier, Heather Hinton, Peat Bakke, Steve Beattie, Aaron Grier, Perry Wagle, and Qian Zhang. Automatic detection and prevention of buffer-overflow attacks. In 7th USENIX Security Symposium, January 1998.
6. Dorothy Denning. An intrusion-detection model. In IEEE Symposium on Security and Privacy, pages 118–131, Oakland, USA, 1986.
7. Laurent Eschenauer. Imsafe. http://imsafe.sourceforge.net, 2001.
8. Stephanie Forrest, Steven A. Hofmeyr, Anil Somayaji, and Thomas A. Longstaff. A sense of self for Unix processes. In Proceedings of the 1996 IEEE Symposium on Research in Security and Privacy, pages 120–128. IEEE Computer Society Press, 1996.
9. The GNU Compiler Collection. http://gcc.gnu.org.
10. A. Ghosh and A. Schwartzbard. A study in using neural networks for anomaly and misuse detection. In USENIX Security Symposium, 1999.
11. Judith Hochberg, Kathleen Jackson, Cathy Stallins, J. F. McClary, David DuBois, and Josephine Ford. NADIR: An automated system for detecting network intrusion and misuse. Computer and Security, 12(3):235–248, May 1993.
12. Intel. IA-32 Intel Architecture Software Developer’s Manual Volume 1-3, 2002. http://developer.intel.com/design/Pentium4/manuals/.
13. Home of K2. http://www.ktwo.ca.
14. Christopher Kruegel, Thomas Toth, and Clemens Kerer. Service Specific Anomaly Detection for Network Intrusion Detection. In Symposium on Applied Computing (SAC). ACM Scientific Press, March 2002.
15. Mudge. Compromised: Buffer-Overflows, from Intel to SPARC Version 8. http://www.l0pht.com, 1996.
16. Peter G. Neumann and Phillip A. Porras. Experience with EMERALD to date. In 1st USENIX Workshop on Intrusion Detection and Network Monitoring, pages 73–80, Santa Clara, California, USA, April 1999.
17. Phillip A. Porras and Peter G. Neumann. EMERALD: Event Monitoring Enabling Responses to Anomalous Live Disturbances. In Proceedings of the 20th NIS Security Conference, October 1997.
18. Martin Roesch. Snort - Lightweight Intrusion Detection for Networks. In USENIX Lisa 99, 1999.
19. SecurityFocus Corporate Site. http://www.securityfocus.com.
20. Jude Shavlik, Mark Shavlik, and Michael Fahland. Evaluating software sensors for actively profiling Windows 2000 computer users. In Recent Advances in Intrusion Detection (RAID), 2001.
21. E. Spafford. The Internet Worm Program: Analysis. Computer Communication Review, January 1989.
22. Stuart Staniford, James A. Hoagland, and Joseph M. McAlerney. Practical Automated Detection of Stealthy Portscans. In Proceedings of the IDS Workshop of the 7th Computer and Communications Security Conference, Athens, 2000.
23. Giovanni Vigna and Richard A. Kemmerer. NetSTAT: A Network-based Intrusion Detection System. In 14th Annual Computer Security Applications Conference, December 1998.
24. Giovanni Vigna and Richard A. Kemmerer. NetSTAT: A Network-based Intrusion Detection System. Journal of Computer Security, 7(1):37–71, 1999.
25. WebSTONE - Mindcraft Corporate Site. http://www.mindcraft.com.