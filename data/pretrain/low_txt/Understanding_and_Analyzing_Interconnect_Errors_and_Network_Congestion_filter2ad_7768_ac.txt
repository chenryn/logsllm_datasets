### Repeated Scheduling on the Same Nodes

When tasks are repeatedly scheduled on the same nodes, it can lead to an uneven spatial distribution of workloads. Applying a 1-hour filtering window shows that the spatial distribution becomes more even compared to the initial state (Fig. 11). However, the distribution still remains skewed towards the left part of the supercomputer, indicating that communication-intensive applications are not evenly distributed across the cabinet. This skew can result in performance degradation for applications running on the left part due to network congestion.

### Correlation Analysis

We calculated the Spearman correlation coefficient between the spatial distribution of congested nodes and lane degrades/failures. The coefficient was close to zero (0.01), suggesting that nodes with high ejection bandwidth are not strongly correlated with interconnect errors. This finding aligns with our previous observation that the correlation between throttle events and interconnect errors is low. Surprisingly, there is also a low correlation (Spearman correlation 0.01) between the heatmap of congested nodes and throttled blades. This discrepancy may be because the congested node information is collected after the throttle command has been issued, potentially missing the nodes that initially caused the congestion. This indicates that the aggregate network traffic at the blade level can differ from individual node-level traffic. Future network performance tools should focus on developing more accurate and fine-grained methods to detect the root cause in real-time.

### Application Characteristics on Congested Nodes

Next, we analyzed the characteristics of applications running on these congested nodes. We plotted the frequency of unique applications that were running on congested nodes when throttling events occurred (Fig. 13). Only a few applications dominate, which we refer to as congestion-causing applications. For example, five applications alone appear in more than 70% of congested node reporting events (Fig. 14), while over 250 unique applications are logged in total. When a 1-hour filtering is applied, the number of unique applications decreases significantly, with the top 5 most frequently occurring applications appearing in approximately 50% of the congested node reporting events (Fig. 14). The total number of unique applications reduces from 250 to 90, indicating that multiple throttle events within a short time period are not caused by the same application. In fact, multiple unique applications can cause nodes to be highly congested within a 1-hour window. Similar results are observed on a per-user basis, confirming that applications and users can serve as proxies for each other.

### Job Size Distribution

We then examined the job size distribution of the top 10 most frequent congestion-causing applications (Fig. 15). Most of these applications tend to run on the same number of nodes each time they appear in the congested node reporting events. For instance, applications A, B, and C run on 400, 2, and 45 nodes, respectively, for more than 90% of the time. Counterintuitively, the job sizes of these applications are relatively small. Seven out of the 10 applications have a job size of less than 512 nodes, and five applications have the most frequent job size of less than 128 nodes. This suggests that a many-to-few communication pattern can be responsible for node congestion, making the focus on large-scale jobs ineffective for identifying culprit applications. Our results show that node congestion is often caused by small-scale applications in real-world scenarios.

### Bandwidth-Heavy Applications

To further understand the job size and communication patterns of bandwidth-heavy applications, we collected data on the top 10 applications sorted by their network bandwidth consumption (total flits/s aggregated over all nodes) during each throttle event. These applications are different from those running on the top 10 heavily congested nodes.

Fig. 16 shows that a few applications are heavy hitters. For example, five applications alone appear in approximately 57% of the top bandwidth application reporting events (Fig. 17), while over 200 unique applications show up in total. When 1-hour filtering is applied, the number of unique applications decreases significantly, but the top 5 most frequently occurring applications still constitute 50% of the top bandwidth application reporting events (Fig. 17). The total number of unique applications reduces to 60, indicating that focusing on the top 5-10 applications can cover 50% of the communication-intensive applications space. Similar results are observed on a per-user basis, confirming that applications and users can serve as proxies for each other.

### Comparison of Bandwidth-Heavy and Congestion-Causing Applications

We then addressed two questions: (1) Are the top bandwidth applications the same as the top congestion-causing applications running on congested nodes? (2) Is the job size distribution of the top bandwidth applications different from that of the top congestion-causing applications?

Fig. 18 shows the job size distribution and anonymized names of the top 10 bandwidth-heavy applications. Most of these applications tend to run on the same number of nodes each time they appear in the top bandwidth application reporting events. However, these applications are not the same as the top congestion-causing applications. Only three applications are common between the two sets (Fig. 18 vs. Fig. 15), located at positions 1, 4, and 7. This indicates that bandwidth-heavy applications do not necessarily cause congestion or run on congested nodes. These applications produce significant traffic and are likely spread over a large number of nodes or have a many-to-many communication pattern. Only three applications have a job size larger than 4000 nodes, indicating that even bandwidth-heavy applications are not necessarily large in size. The communication pattern plays a critical role. For example, App K, which runs mostly on 32 and 128 nodes, appears second in the bandwidth-heavy applications list but does not appear in the congestion-heavy applications list. This could be because this application does not exhibit a many-to-one communication pattern. Many-to-few and many-to-one communication patterns can result in high congestion due to the concentration of messages over a few nodes. In summary, the job sizes of bandwidth-heavy applications are similar to those of congestion-causing applications, but there is no significant overlap between the two sets, and they may differ in their communication patterns.

### Related Work

Various HPC interconnect networks, such as QsNET [3], SeaStar [4], Tofu [5], Blue Gene/Q [6], Aries [7], TH Express-2 [8], and others, have been proposed to improve HPC system performance. These networks use different topologies like k-Ary n-Cube, fat-tree/Clos, and dragonfly. Interconnect networks have been a vital part of computer systems, and network resources have been a major performance bottleneck in HPC systems. Several studies have been conducted to understand [9], [10], [11], [12], [13], [14], [15], [16], [17] and improve [18], [19], [20], [21], [22], [23], [24] interconnect failures in HPC systems.

Titan, the successor of the Cray X-series, uses the XK7 system and 3D Gemini interconnect. The Gemini system interconnect architecture is explained in [1] and evaluated in [25], [2] using micro-benchmarks. Cray’s latest XC series is implemented using the Aries interconnect, which supports better bandwidth, latency, message rate, and scalability [26]. Our work differs from these studies and evaluations as none of them evaluate how different interconnect errors and congestion events occur on a large-scale HPC system. Our field data and analysis provide unique and useful insights that can be used by users, system architects, and operators to improve the overall efficiency of HPC systems.

### Conclusion

In conclusion, we have derived several interesting insights from our analysis. Interconnect faults like lane degrades are continuous and related to heterogeneous load imbalance among lanes. Link inactive errors do not have a temporal or spatial correlation with lane degrades, while interconnect errors have a high correlation with link inactive/failed errors. These characteristics can be exploited for various purposes. We also demonstrated that multiple applications can cause multiple congestion events within a short period. Moreover, these applications can be surprisingly small in size, not scheduled evenly across the cabinet, and have a many-to-few communication pattern. Our analysis can help identify such applications and users to minimize the performance impact on other applications.

### Acknowledgment

We thank the reviewers and Elmootazbellah Elnozahy for their constructive feedback. This work was supported in part through NSF Grants (#1563728, #1561216, and #1563750), Northeastern University, and the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, program manager Lucy Nowell. This work also used in part the resources of the Oak Ridge Leadership Computing Facility, located in the National Center for Computational Sciences at ORNL, which is managed by UT Battelle, LLC for the U.S. DOE under contract number DE-AC05-00OR22725.

### References

[1] R. Alverson, D. Roweth, and L. Kaplan, “The gemini system interconnect,” in High Performance Interconnects (HOTI), 2010 IEEE 18th Annual Symposium on.
IEEE, 2010, pp. 83–87.

[2] M. Ezell, “Understanding the impact of interconnect failures on system operation,” in Proceedings of Cray User Group Conference (CUG 2013), 2013.

[3] F. Petrini, E. Frachtenberg, A. Hoisie, and S. Coll, “Performance evaluation of the quadrics interconnection network,” Cluster Computing, vol. 6, no. 2, pp. 125–142, 2003.

[4] R. Brightwell, K. T. Pedretti, K. D. Underwood, and T. Hudson, “Seastar interconnect: Balanced bandwidth for scalable performance,” IEEE Micro, vol. 26, no. 3, pp. 41–57, 2006.

[5] Y. Ajima, S. Sumimoto, and T. Shimizu, “Tofu: A 6d mesh/torus interconnect for exascale computers,” Computer, vol. 42, no. 11, pp. 0036–41, 2009.

[6] D. Chen, N. A. Eisley, P. Heidelberger, R. M. Senger, Y. Sugawara, S. Kumar, V. Salapura, D. L. Satterfield, B. Steinmacher-Burow, and J. J. Parker, “The ibm blue gene/q interconnection network and message unit,” in High Performance Computing, Networking, Storage and Analysis (SC), 2011 International Conference for.
IEEE, 2011, pp. 1–10.

[7] G. Faanes, A. Bataineh, D. Roweth, E. Froese, B. Alverson, T. Johnson, J. Kopnick, M. Higgins, J. Reinhard et al., “Cray cascade: a scalable hpc system based on a dragonfly network,” in Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis.
IEEE Computer Society Press, 2012, p. 103.

[8] Z. Pang, M. Xie, J. Zhang, Y. Zheng, G. Wang, D. Dong, and G. Suo, “The th express high performance interconnect networks,” Frontiers of Computer Science, vol. 8, no. 3, pp. 357–366, 2014.

[9] S. L. Scott et al., “The cray t3e network: adaptive routing in a high performance 3d torus,” 1996.

[10] M. Blumrich, D. Chen, P. Coteus, A. Gara, M. Giampapa, P. Heidelberger, S. Singh, B. Steinmacher-Burow, T. Takken, and P. Vranas, “Design and analysis of the bluegene/l torus interconnection network,” IBM Research Report RC23025 (W0312-022), Tech. Rep., 2003.

[11] W. J. Dally and B. P. Towles, Principles and practices of interconnection networks. Elsevier, 2004.

[12] N. R. Adiga, M. A. Blumrich, D. Chen, P. Coteus, A. Gara, M. E. Giampapa, P. Heidelberger, S. Singh, B. D. Steinmacher-Burow, T. Takken et al., “Blue gene/l torus interconnection network,” IBM Journal of Research and Development, vol. 49, no. 2.3, pp. 265–276, 2005.

[13] J. Duato, S. Yalamanchili, and L. M. Ni, Interconnection networks: an engineering approach. Morgan Kaufmann, 2003.

[14] P. Gill, N. Jain, and N. Nagappan, “Understanding network failures in data centers: measurement, analysis, and implications,” in ACM SIGCOMM Computer Communication Review, vol. 41, no. 4. ACM, 2011, pp. 350–361.

[15] D. Abts and B. Felderman, “A guided tour of data-center networking,” Communications of the ACM, vol. 55, no. 6, pp. 44–51, 2012.

[16] C. Di Martino, W. Kramer, Z. Kalbarczyk, and R. Iyer, “Measuring and understanding extreme-scale application resilience: A field study of 5,000,000 HPC application runs,” in Dependable Systems and Networks (DSN), 2015 45th Annual IEEE/IFIP International Conference on.
IEEE, 2015, pp. 25–36.

[17] S. Jha, V. Formicola, Z. Kalbarczyk, C. Di Martino, W. T. Kramer, and R. K. Iyer, “Analysis of gemini interconnect recovery mechanisms: Methods and observations,” Cray User Group, pp. 8–12, 2016.

[18] C. E. Leiserson, “Fat-trees: universal networks for hardware-efficient supercomputing,” IEEE transactions on Computers, vol. 100, no. 10, pp. 892–901, 1985.

[19] W. J. Dally, “Performance analysis of k-ary n-cube interconnection networks,” IEEE transactions on Computers, vol. 39, no. 6, pp. 775–785, 1990.

[20] ——, “Express cubes: Improving the performance of k-ary n-cube interconnection networks,” IEEE Transactions on Computers, vol. 40, no. 9, pp. 1016–1023, 1991.

[21] D. W. Mackenthun, “Method and apparatus for automatically routing around faults within an interconnect system,” Sep. 12 1995, uS Patent 5,450,578.

[22] Y. Inoguchi and S. Horiguchi, “Shifted recursive torus interconnection for high performance computing,” in High Performance Computing on the Information Superhighway, 1997. HPC Asia’97.
IEEE, 1997, pp. 61–66.

[23] V. Puente, R. Beivide, J. A. Gregorio, J. Prellezo, J. Duato, and C. Izu, “Adaptive bubble router: a design to improve performance in torus networks,” in Parallel Processing, 1999. Proceedings. 1999 International Conference on.
IEEE, 1999, pp. 58–67.

[24] J. Domke, T. Hoeﬂer, and S. Matsuoka, “Fail-in-place network design: interaction between topology, routing algorithm and failures,” in High Performance Computing, Networking, Storage and Analysis, SC14: International Conference for.
IEEE, 2014, pp. 597–608.

[25] A. Vishnu, M. ten Bruggencate, and R. Olson, “Evaluating the potential of cray gemini interconnect for pgas communication runtime systems,” in High Performance Interconnects (HOTI), 2011 IEEE 19th Annual Symposium on.
IEEE, 2011, pp. 70–77.

[26] B. Alverson, E. Froese, L. Kaplan, and D. Roweth, “Cray xc series network,” Cray Inc., White Paper WP-Aries01-1112, 2012.