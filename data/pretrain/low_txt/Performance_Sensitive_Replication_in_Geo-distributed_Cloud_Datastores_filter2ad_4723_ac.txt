### Mathematical Notation
- \( Q_l \in \mathbb{Z} \)
- \( q_{ij}^l, x_j, Y^l \)
- \( l \in \{r, w\} \)
- \( i_k \in \{0, 1\}, \forall i, j, k; l \in \{r, w\} \)

### Model Enhancements
We discuss enhancements to the N-1 Contingency model:

1. **Cost-Sensitive Replication:**
   When datastores are deployed on public clouds, it is important to consider dollar costs in addition to latency and availability. We focus on wide-area communication costs because:
   - This is a dominant component of costs in geo-replicated settings [34].
   - Best practices involve storing data in local instance storage with periodic backups to persistent storage [25]. The costs of such backups are independent of our replication policy decision.
   - Instance costs are comparable to a single DC deployment with the same number of replicas.

   Most cloud providers charge for outbound bandwidth transfers at a flat rate per byte (inbound transfers are typically free), though the rate depends on the location of the DC. Let \( C_i \) be the cost per byte of outbound bandwidth transfer from DC \( i \).

   For an operation that originates in DC \( i \) and involves writing a data item of size \( S \) bytes, the total cost associated with all write operations is \( \sum_j X_j \).

   However, read operations in Cassandra retrieve the full data item only from its nearest neighbor but receive a digest from all other nodes. Let \( n_{ij} \) denote an indicator variable, which is 1 if the full data item is fetched from DC \( j \). The size of the digest is assumed to be negligibly small. The total cost associated with all read operations is \( \sum_i n_{ij} S C_j \).

   It is now straightforward to modify the N-1 Contingency model (N-1C) to optimize costs subject to a delay constraint. This can be done by making the threshold \( T \) a fixed parameter rather than a variable of optimization and adding additional constraints on \( n_{ij} \).

2. **Jointly Considering Normal Operation and Failures:**
   The N-1 Contingency model finds replication strategies that reduce latency under failure. In practice, a designer prefers strategies that work well in both normal conditions and under failure. This is achieved by combining the constraints in the Latency (LAT) and N-1C models, with an objective function that is a weighted sum of latency under normal conditions \( T \) and under failures \( T_f \). The weights are chosen to capture the desired preferences.

3. **Failures of Multiple DCs:**
   While simultaneous failures of multiple DCs are relatively uncommon, it is easy to extend our formulations to consider such scenarios. Let \( K \) be a set whose elements are sets of indices of DCs that may fail simultaneously, and we are interested in guarding performance against such failures. We then employ the N-1C model but with \( k \) iterating over elements of \( K \) instead of the set of DCs. A naive approach may exhaustively enumerate all possible combinations of DC failures, which could be computationally expensive and may result in schemes optimized for unlikely events at the expense of more typical occurrences. A more practical approach would involve explicit operator specifications of correlated failure scenarios of interest. For example, DCs that share the same network Point of Presence (PoP) are more likely to fail together, and thus are of practical interest to operators.

4. **Network Partitions:**
   In general, it is impossible to guarantee availability with network partition tolerance given the strict quorum requirement [31]. For more common network outages that partition one DC from others, our N-1C model ensures that requests from all other DCs can still be served with low latency. To handle more complex network partitions, an interesting future direction is to consider weaker quorum requirements subject to bounds on data staleness [14].

### Evaluation Methodology
We evaluate our replication strategies—Latency Only (LAT), Basic Availability (BA), and N-1 Contingency (N-1C)—to explore several aspects such as:
- Accuracy of our model in predicting performance
- Limits on latency achievable given consistency constraints
- Benefits and costs of optimizing latency under failures
- Importance of employing heterogeneous configurations for different groups of data items within an application
- Robustness to variations in network delays and workloads

We explore these questions using experiments on a real wide-area Cassandra cluster deployed across all 8 regions (and 21 availability zones) of Amazon EC2 and using trace-driven simulations from three real-world applications: Twitter, Wikipedia, and Gowalla. Our EC2 experiments enable us to validate our models and evaluate the benefits of our approach in practice. Simulation studies allow us to evaluate our strategies on a larger scale (hundreds of thousands of data items) and to explore the impact of workload characteristics and model parameters on performance. We use GAMS [18] (a modeling system for optimization problems) and solve the models using the CPLEX optimizer.

### Application Workloads
The applications we choose are widely used, have geographically dispersed users who edit and read data, and fit naturally into a key-value model. Both Twitter and Gowalla are known to use Cassandra [10]. We discuss details of the traces below (see Table III for a summary):

- **Twitter:**
  We obtained Twitter traces [39] which included a user friendship graph, a list of user locations, and public tweets sent by users (along with timestamps) over a 5-year period. We analyzed Twissandra, an open-source Twitter-like application, and found three types of data items: users, tweets, and timelines. We focus our evaluations on timeline objects, which are pre-materialized views that map each user to a list of tweets sent by the user and her friends. Writes to a timeline occur when the associated user or her friends post a tweet, and can be obtained directly from the trace. Since the traces do not include reads, we model reads by assuming each user reads her own timeline periodically (every 10 minutes) and reads her friend’s timeline with some probability (0.1) each time the friend posts a tweet.

- **Wikipedia:**
  We obtained statistics regarding Wikipedia usage from [11], which lists the total as well as the breakdown of the number of views and edits by geographic region for each language and collaborative project. The data spans a 3-year period with trends shown on a quarterly basis. Our model for the Wikipedia application consists of article objects with the document ID as a key and the content along with its metadata (timestamps, version information, etc.). Article page views are modeled as reads, while page edits are modeled as writes. Since per-article access data is not available, we model all articles of the same language and project as seeing similar access patterns, as access patterns are likely dominated by the location of native speakers of the language.

- **Gowalla:**
  Gowalla is a (now disabled) geo-social networking application where users "check-in" at various locations they visit and friends receive their check-in messages. The traces [9] contained user friendship relationships and a list of all check-ins sent over a two-year period. Since the application workflows are similar, we model Gowalla in a similar fashion to Twitter. Check-ins represent writes to user timelines from the location of the check-in, and reads to timelines were modeled like with Twitter.

### Experimental Validation
In this section, we present results from our experiments using Cassandra deployed on Amazon EC2.

#### Implementation
Cassandra employs a random partitioner that implements consistent hashing to distribute load across multiple storage nodes in the cluster. The output range of a hash function is treated as a fixed circular space, and each data item is assigned to a node by hashing its key to yield its position on the ring. Nodes assume responsibility for the region in the ring between itself and its predecessor, with immediately adjacent nodes in the ring hosting replicas of the data item. Cassandra allows applications to express replication policies at the granularity of keyspaces (partitions of data). We modified the applications to treat groups of data items as separate keyspaces and configure distinct replication policies for each keyspace. Keyspace creation is a one-time process and does not affect the application performance. The mapping from data object to the keyspace is maintained in a separate directory service. We implemented the directory service as an independent Cassandra cluster deployed in each of the DCs and configured its replication such that lookups (reads) are served locally within a DC (e.g., R = 1, W = N).

#### Experimental Platform on EC2
We performed our experiments and model validations using Cassandra deployed on medium-size instances on Amazon EC2. Our datastore cluster comprises nodes deployed in each of the 21 distinct availability zones (AZs) across all 8 regions of EC2 (9 in the US, 3 in Europe, 5 in Asia, 2 in South America, and 2 in Australia). We treat availability zones (AZs) as distinct DCs in all our experiments. The inter-DC delays (21 * 21 pairs) were simultaneously measured for a period of 24 hours using medium instances deployed on all 21 AZs, and the median delay values (MED) were used as input to our models. We mapped users from their locations to the nearest DC. Since the locations are free-text fields in our traces, we make use of geocoding services [3] to obtain the user’s geographical coordinates.

#### Accuracy and Model Validation
We validate the accuracy of our models with experiments on our EC2 Cassandra cluster described above. We use the example from our Twitter trace (Figure 2) for this experiment. Replica configurations were generated with the MED delay values measured earlier, and read/write requests to the Cassandra cluster were generated from application servers deployed at the corresponding DCs as per the trace data. The duration of the entire experiment was about 6 hours.

Figure 5 shows the cumulative distribution functions (CDFs) of the observed and predicted latencies for read and write requests for the BA configuration. The CDFs almost overlap for write requests, while we observe a delay of approximately 9 milliseconds evenly for all read requests. This constant delay difference in the reads can be attributed to the processing overhead of read requests in Cassandra, which includes reconciling the response of multiple replicas to ensure consistency of the read data. Overall, our results validate the accuracy of our models. They also show that our solutions are fairly robust to the natural delay variations present in real cloud platforms.

#### Benefits of Performance-Sensitive Replication
We first evaluate the benefits of a flexible replication policy over a fixed replication policy on the EC2 Cassandra cluster described above. For this experiment, we use a month-long trace from Twitter consisting of 524,759 objects corresponding to user timelines in Twitter. The replica configurations...