### References

[19] A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes, “ML-Leaks: Model and Data-Independent Membership Inference Attacks and Defenses on Machine Learning Models,” in *Network and Distributed System Security Symposium (NDSS)*, 2019.

[20] M. Nasr, R. Shokri, and A. Houmansadr, “Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-Box Inference Attacks against Centralized and Federated Learning,” in *IEEE Symposium on Security and Privacy (SP)*, 2019.

[21] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language Models are Unsupervised Multitask Learners,” 2019.

[22] Z. Wang, M. Song, Z. Zhang, Y. Song, Q. Wang, and H. Qi, “Beyond Inferring Class Representatives: User-Level Privacy Leakage from Federated Learning,” in *IEEE Conference on Computer Communications (INFOCOM)*, 2019.

[23] J. Geiping, H. Bauermeister, H. Dröge, and M. Möller, “Inverting Gradients - How Easy is it to Break Privacy in Federated Learning?” in *Conference on Neural Information Processing Systems (NeurIPS)*, 2020.

[24] A. Wainakh, F. Ventola, T. Müßig, J. Keim, C. G. Cordero, E. Zimmer, T. Grube, K. Kersting, and M. Mühlhäuser, “User Label Leakage from Gradients in Federated Learning,” arXiv:2105.09369, 2021.

[25] S. Z. Béguelin, L. Wutschitz, S. Tople, V. Rühle, O. Ohrimenko, M. Brockschmidt, and B. Köpf, “Analyzing Information Leakage of Updates to Natural Language Models,” in *ACM Conference on Computer and Communications Security (CCS)*, 2020.

[26] A. Salem, A. Bhattacharya, M. Backes, M. Fritz, and Y. Zhang, “Updates-Leak: Dataset Inference and Reconstruction Attacks in Online Learning,” in *USENIX Security Symposium*, 2020.

[27] P. McCullagh and J. A. Nelder, *Generalized Linear Models*. Routledge, 2019.

[28] R. W. Wedderburn, “On the Existence and Uniqueness of the Maximum Likelihood Estimates for Certain Generalized Linear Models,” *Biometrika*, 1976.

[29] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,” in *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018.

[30] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-Image Translation with Conditional Adversarial Networks,” in *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2017.

[31] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley, “Least Squares Generative Adversarial Networks,” in *IEEE International Conference on Computer Vision (ICCV)*, 2017.

[32] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-Based Learning Applied to Document Recognition,” *Proceedings of the IEEE*, 1998.

[33] S. Zagoruyko and N. Komodakis, “Wide Residual Networks,” in *British Machine Vision Conference (BMVC)*, 2016.

[34] S. Fort, J. Ren, and B. Lakshminarayanan, “Exploring the Limits of Out-of-Distribution Detection,” arXiv:2106.03004, 2021.

[35] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, “Visualizing the Loss Landscape of Neural Nets,” in *Conference on Neural Information Processing Systems (NeurIPS)*, 2018.

[36] T. Hennigan, T. Cai, T. Norman, and I. Babuschkin, “Haiku: Sonnet for JAX,” 2020. [Online]. Available: <http://github.com/deepmind/dm-haiku>

[37] M. Jagielski, J. Ullman, and A. Oprea, “Auditing Differentially Private Machine Learning: How Private is Private SGD?” *Advances in Neural Information Processing Systems*, 2020.

[38] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor, “Our Data, Ourselves: Privacy via Distributed Noise Generation,” in *International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT)*, 2006.

[39] I. Mironov, “Rényi Differential Privacy,” in *IEEE Computer Security Foundations Symposium (CSF)*, 2017.

[40] M. Bun and T. Steinke, “Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds,” in *Theory of Cryptography Conference (TCC)*, 2016.

[41] A. Bhowmick, J. C. Duchi, J. Freudiger, G. Kapoor, and R. Rogers, “Protection Against Reconstruction in Private Federated Learning and Its Applications,” arXiv:1812.00984, 2018.

[42] N. Papernot, S. Song, I. Mironov, A. Raghunathan, K. Talwar, and Ú. Erlingsson, “Scalable Private Learning with PATE,” in *International Conference on Learning Representations (ICLR)*, 2018.

[43] I. Mironov, K. Talwar, and L. Zhang, “Rényi Differential Privacy of the Sampled Gaussian Mechanism,” arXiv:1908.10530, 2019.

[44] A. Blum, C. Dwork, F. McSherry, and K. Nissim, “Practical Privacy: The SuLQ Framework,” in *ACM Symposium on Principles of Database Systems (PODS)*, 2005.

[45] F. McSherry, “I suspect the 'Discovery' had a different feel for the various involved people. I personally spent a lot of time trying to remove explicit references to adversaries and assumptions about them.” Jan 2021. [Online]. Available: <https://twitter.com/frankmcsherry/status/1354789417727234049>

[46] D. Kifer and A. Machanavajjhala, “Pufferfish: A Framework for Mathematical Privacy Definitions,” *ACM Trans. Database Syst.*, 2014.

[47] A. Ghosh and R. Kleinberg, “Inferential Privacy Guarantees for Differentially Private Mechanisms,” in *Innovations in Theoretical Computer Science Conference (ITCS)*, 2017.

[48] S. P. Kasiviswanathan and A. D. Smith, “On the ‘Semantics’ of Differential Privacy: A Bayesian Formulation,” *J. Priv. Confidentiality*, 2014.

[49] Y. Duan, “Privacy without Noise,” in *ACM Conference on Information and Knowledge Management (CIKM)*, 2009.

[50] R. Bhaskar, A. Bhowmick, V. Goyal, S. Laxman, and A. Thakurta, “Noiseless Database Privacy,” in *International Conference on the Theory and Application of Cryptology and Information Security (ASIACRYPT)*, 2011.

[51] R. Bassily, A. Groce, J. Katz, and A. D. Smith, “Coupled-Worlds Privacy: Exploiting Adversarial Uncertainty in Statistical Data Privacy,” in *IEEE Symposium on Foundations of Computer Science (FOCS)*, 2013.

[52] D. Desfontaines, E. Mohammadi, E. Krahmer, and D. Basin, “Differential Privacy with Partial Knowledge,” arXiv:1905.00650, 2019.

[53] R. Shokri, J. Freudiger, M. Jadliwala, and J. Hubaux, “A Distortion-Based Metric for Location Privacy,” in *ACM Workshop on Privacy in the Electronic Society (WPES)*, 2009.

[54] R. Shokri, G. Theodorakopoulos, J. L. Boudec, and J. Hubaux, “Quantifying Location Privacy,” in *IEEE Symposium on Security and Privacy (SP)*, 2011.

[55] L. Wasserman and S. Zhou, “A Statistical Framework for Differential Privacy,” *Journal of the American Statistical Association*, 2010.

[56] P. Kairouz, S. Oh, and P. Viswanath, “The Composition Theorem for Differential Privacy,” in *International Conference on Machine Learning (ICML)*, 2015.

[57] B. Balle, G. Barthe, M. Gaboardi, J. Hsu, and T. Sato, “Hypothesis Testing Interpretations and Rényi Differential Privacy,” in *International Conference on Artificial Intelligence and Statistics (AISTATS)*, 2020.

[58] I. Dinur and K. Nissim, “Revealing Information while Preserving Privacy,” in *ACM Symposium on Principles of Database Systems (PODS)*, 2003.

[59] C. Dwork, F. McSherry, and K. Talwar, “The Price of Privacy and the Limits of LP Decoding,” in *ACM Symposium on Theory of Computing (STOC)*, 2007.

[60] C. Dwork, A. Smith, T. Steinke, and J. Ullman, “Exposed! A Survey of Attacks on Private Data,” *Annual Review of Statistics and Its Application*, 2017.

[61] A. Cohen and K. Nissim, “Linear Program Reconstruction in Practice,” *J. Priv. Confidentiality*, 2020.

[62] A. Cohen, S. Nikolov, Z. Schutzman, and J. Ullman, “Reconstruction Attacks in Practice,” 2020. [Online]. Available: <https://differentialprivacy.org/diffix-attack/>

[63] G. Smith, “On the Foundations of Quantitative Information Flow,” in *International Conference on Foundations of Software Science and Computational Structures (FoSSaCS)*, 2009.

[64] M. S. Alvim, K. Chatzikokolakis, A. McIver, C. Morgan, C. Palamidessi, and G. Smith, *The Science of Quantitative Information Flow*. Springer, 2020.

[65] E. ElSalamouny, K. Chatzikokolakis, and C. Palamidessi, “Generalized Differential Privacy: Regions of Priors that Admit Robust Optimal Mechanisms,” in *Horizons of the Mind. A Tribute to Prakash Panangaden - Essays Dedicated to Prakash Panangaden on the Occasion of His 60th Birthday*, 2014.

[66] B. Balle, G. Cherubin, and J. Hayes, “Reconstructing Training Data with Informed Adversaries,” arXiv:2201.04845, 2022.

[67] M. Shaked and J. G. Shanthikumar, *Stochastic Orders*. Springer Science & Business Media, 2007.

[68] S. Dasgupta and A. Gupta, “An Elementary Proof of a Theorem of Johnson and Lindenstrauss,” *Random Struct. Algorithms*, 2003.

[69] M. Johnson, “Add GPU Determinism Note,” Nov 2020. [Online]. Available: <https://github.com/google/jax/pull/4824>

[70] “JAX Activations,” <https://jax.readthedocs.io/en/latest/jax.nn.html>, accessed: 2022-03-25.

### Appendix

#### Proofs

We provide proof sketches for the main theoretical results of the paper. Full proofs can be found in the arXiv version of the paper [66].

**Proof Sketch of Theorem 1.** For a GLM model \(\theta\) trained to convergence, the update rule takes the form:
\[ x(g^{-1}(\langle x, \theta \rangle) - y) = - \bar{X}^\top (g^{-1}(\bar{X} \theta) - \bar{Y}) - \lambda \theta. \]
When the model contains an intercept parameter, this yields \(d\) equations with \(d\) unknowns \((x_2, \ldots, x_d, y)\) because \(x_1 = 1\). From the equation corresponding to this coordinate, we obtain:
\[ g^{-1}(\langle x, \theta \rangle) - y = \bar{X}_1^\top B + \lambda \theta_1, \]
which we can plug into the rest of the equations to obtain the desired expression for \(x\). Once we have \(x\), we plug it back into the first equation to recover \(y\).

**Proof Sketch of Theorem 2.** Fix \(R: \Theta \to Z\) and \(D^- \in Z^{n-1}\), and let \(Z \sim \pi\), \(D_Z = D^- \cup \{Z\}\), and \(\theta \sim M(D_Z)\). We write \(p_M(\theta|z) = P[M(D_z) = \theta]\) for the output density of \(M\) on input \(D_z\). First, we take an arbitrary \(z_0 \in Z\) and show that the probability \(P[\ell(Z, R(\theta)) \leq \eta]\) equals:
\[ \int p_M(\theta|z) \mathbb{1}_{\{\ell(z, R(\theta)) \leq \eta\}} p_M(d\theta|z_0) \pi(dz). \]
Next, we take \(\alpha' = \frac{\alpha}{\alpha-1}\) and use Hölder's inequality to bound the inner integral:
\[ \left( \int \left( \frac{p_M(\theta|z)}{p_M(\theta|z_0)} \right)^\alpha \pi(dz) \right)^{1/\alpha} \leq \kappa^{1/\alpha'} \left( \int \left( \frac{p_M(\theta|z)}{p_M(\theta|z_0)} \right) \pi(dz) \right)^{1/\alpha}. \]
Plugging this bound into the expression for \(P[\ell(Z, R(\theta)) \leq \eta]\) and re-arranging terms, we use Jensen's inequality and the RDP assumption on \(M\) to obtain:
\[ \left( P[\ell(Z, R(\theta)) \leq \eta] \right)^\alpha \leq \kappa \pi(\eta)^{1/\alpha'} e^{(\alpha-1)\epsilon}. \]

**Proof Sketch of Theorem 5.** Fix arbitrary \(D^- \in Z^{n-1}\), \(z, z' \in Z\), \(z \neq z'\), and \(E \subseteq \Theta\), and let \(\pi = \pi_p, z, z'\). Define the reconstruction mapping \(R_E\) mapping \(\theta\) to \(z\) if \(\theta \in E\) and to \(z'\) otherwise. By the ReRo assumptions on \(M\), we have:
\[ P_{Z \sim \pi, \theta \sim M(D_Z)}[R_E(\theta) = Z] \leq \gamma. \]
On the other hand, by definition of \(\pi\) and \(R_E\), \(P_{Z \sim \pi, \theta \sim M(D_Z)}[R_E(\theta) = Z]\) equals:
\[ P[M(D_z) \in E] - e^\epsilon P[M(D_{z'}) \in E] + \frac{e^\epsilon}{e^\epsilon + 1}. \]
Upper bounding by \(\gamma\) and re-arranging completes the proof.

**Proof of Proposition 6.** Let \(\pi = U(B_d^1(0))\) and write \(\text{Vol}(A)\) to denote the Euclidean volume of a set \(A \subset \mathbb{R}^d\). By definition of the baseline error, for \(\eta \in (0, 1)\) we have:
\[ \kappa_{\pi, \ell_2}(\eta) = \sup_{z_0} \frac{\text{Vol}(B_d^1(0) \cap B_d^\eta(z_0))}{\text{Vol}(B_d^1(0))} = \eta^d = e^{-\Omega(d)}. \]
Plugging this expression into Corollary 3 shows that any \(\epsilon\)-DP mechanism with \(\epsilon = o(d)\) provides \((\eta, \gamma)\)-ReRo with respect to \(\pi\) and \(\ell\) with \(\gamma = e^{-\Omega(d)}\). A similar claim follows from Corollary 4 applied to \(\rho\)-zCDP mechanisms with \(\rho = o(d)\).

**Proof Sketch of Proposition 7.** Let \(Z \sim N(0, I)\) and \(F_\eta(z_0) = P[\|Z + z_0\|_2 \leq \eta^2]\). First, we show that \(\arg\max_{z_0} F_\eta(z_0) = 0\). The proof of this intuitive fact relies on extending a 1-dimensional stochastic domination property of Gaussian random variables [67, Example 1.A.27] to \(d\) dimensions using an orthogonal decomposition of \(Z\) along the space spanned by \(z_0\) and its orthogonal complement. Then we show that for \(\nu = N(w, \sigma^2 I)\), this claim implies \(\kappa_{\nu, \ell_2}(\eta) = F_{\eta/\sigma}(0)\). Next, we use a tail lower bound for chi-squared random variables [68, Lemma 2.2] to get:
\[ \kappa_{\nu, \ell_2}(\eta) \leq e^{-\frac{d}{2} \left( \frac{\eta^2}{\sigma^2} - 1 - \log \frac{\eta^2}{\sigma^2 d} \right)}. \]
In particular, for \(\sigma \geq 2 \eta \sqrt{d}\), we get \(\kappa_{\nu, \ell_2}(\eta) \leq e^{-\Omega(d)}\). The remaining part of the proof follows the same argument as in Proposition 6.

### Additional Experimental Results

We provide additional experimental results here. The interested reader can find a more expansive set of findings in [66].