### Evaluation Methods for Quantifying User-Centric Web Search Query Obfuscation Mechanisms

In [9], the authors conduct a qualitative analysis by examining the specifics of the mentioned obfuscation solutions and propose at least one countermeasure to defeat each obfuscation mechanism. This study helps in understanding the limitations of the proposed obfuscation mechanisms but does not allow for a quantitative comparison of their effectiveness. In [8], the authors analyze TrackMeNot (TMN) dummy queries by clustering them and labeling them based on their similarity to the set of recently issued user queries. However, the clustering algorithm computes the similarity between queries without considering the obfuscation mechanism, which can result in fake queries that are similar to user queries being grouped together. Additionally, [12] presents two features that can help differentiate TMN from real user queries. Another study [28] shows that simple supervised learning classifiers with a few features can reliably identify TMN queries when given access to recent user search history. This work also proposes a clustering attack, but it cannot distinguish between user and TMN queries because it focuses on query similarity rather than linkability.

Our proposed linkage function considers all differentiating features and automatically learns their importance. For example, the time interval between queries influences our decision-making process by evaluating the values of other features, rather than ignoring the queries. Our generic framework complements existing work by providing a systematic and quantitative approach that is not limited to any specific obfuscation mechanism and can evaluate user privacy even if the adversary lacks a specific model of the target user.

### Related Research: Protection of Web Query Logs

Another related area of research focuses on protecting the privacy of web query logs. A survey of various obfuscation techniques for search query logs is presented in [13]. In [23], the authors propose using differential privacy to address the problem of releasing web query logs.

### Conclusions

A systematic methodology for quantitatively assessing user privacy is essential for designing effective obfuscation mechanisms. In the context of web-search privacy, despite numerous contributions to protect users' web-search privacy and specific attacks on particular obfuscation mechanisms, there is a lack of a generic formal framework for specifying protection mechanisms and evaluating privacy. In this paper, we have raised the questions of "What is web-search privacy?" and "How can it be quantified, given an adversary model and a protection mechanism?" We have proposed a quantitative framework to answer these questions. In this framework, we have modeled various types of adversary knowledge and user privacy sensitivities, leading to the definition of privacy metrics. To model obfuscation mechanisms and the adversary's knowledge about the user, we designed a function called the linkage function. This function is the main building block of our quantification framework and helps us reason like an adversary, distinguishing pairs of queries from a user from pairs of queries containing fake information. The function is constructed in a way that does not require, but can incorporate, knowledge about the web-search behavior of the target user. We used this to assess how much information (at the query or semantic level) about the user is still leaked through the obfuscation process. We applied our methodology to real datasets and compared two example obfuscation mechanisms. As a follow-up to this work, we aim to design web-search obfuscation mechanisms that anticipate the possibility of linkage attacks, leading to more robust protection mechanisms.

### References

[1] D. C. Howe, H. Nissenbaum, and V. Toubiana, TrackMeNot - available from http://cs.nyu.edu/trackmenot.
[2] Open Directory Project (ODP) ontology, available from http://www.dmoz.org/.
[3] Lancaster Stemming Algorithm, available from http://www.comp.lancs.ac.uk/computing/research/stemming.
[4] Maxmind Free World Cities Database, Available from http://www.maxmind.com/en/worldcities.
[5] Jaccard, P. (1901) Distribution de la flore alpine dans le bassin des Dranses et dans quelques regions voisines. Bulletin de la Societe Vaudoise des Sciences Naturelles 37, 241-272.
[6] Google Books - Ngram datasets, Available from http://storage.googleapis.com/books/ngrams/books/datasetsv2.html.
[7] CLUTO - Software for Clustering High-Dimensional Datasets, available from http://glaros.dtc.umn.edu/gkhome/views/cluto.
[8] R. Al-Rfou, W. Jannen, and N. Patwardhan. TrackMeNot-so-good-after-all. arXiv preprint arXiv:1211.0320, 2012.
[9] E. Balsa, C. Troncoso, and C. Diaz. Ob-PWS: Obfuscation-based private web search. In Security and Privacy (SP), 2012 IEEE Symposium on, pages 491–505. IEEE, 2012.
[10] P. N. Bennett, K. Svore, and S. T. Dumais. Classification-enhanced ranking. In Proc. International World Wide Web Conference (WWW), pages 111–120, 2010.
[11] J. Castellí-Roca, A. Viejo, and J. Herrera-Joancomartí. Preserving user’s privacy in web search engines. Comput. Commun., 32(13-14):1541–1551, Aug. 2009.
[12] R. Chow and P. Golle. Faking contextual data for fun, profit, and privacy. In Proceedings of the 8th ACM workshop on Privacy in the electronic society, pages 105–108. ACM, 2009.
[13] A. Cooper. A survey of query log privacy-enhancing techniques from a policy perspective. ACM Transactions on the Web (TWEB), 2(4):19, 2008.
[14] R. Dingledine, N. Mathewson, and P. Syverson. Tor: The second-generation onion router. In Proceedings of the 13th Conference on USENIX Security Symposium - Volume 13, SSYM’04, pages 21–21, Berkeley, CA, USA, 2004. USENIX Association.
[15] J. Domingo-Ferrer, A. Solanas, and J. Castellá-Roca. h(k)-private information retrieval from privacy-uncooperative queryable databases. Online Information Review, 33(4):720–744, 2009.
[16] P. Eckersley. How unique is your web browser? In Proceedings of the 10th International Conference on Privacy Enhancing Technologies, PETS’10, pages 1–18, Berlin, Heidelberg, 2010. Springer-Verlag.
[17] Y. Elovici, B. Shapira, and A. Maschiach. A new privacy model for hiding group interests while accessing the web. In Proceedings of the 2002 ACM workshop on Privacy in the Electronic Society, pages 63–70. ACM, 2002.
[18] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28:2000, 1998.
[19] J. H. Friedman. Stochastic gradient boosting. Computational Statistics and Data Analysis, 38:367–378, 1999.
[20] I. Goldberg. Improving the robustness of private information retrieval. In Security and Privacy, 2007. SP ’07. IEEE Symposium on, pages 131–148, May 2007.
[21] D. C. Howe and H. Nissenbaum. TrackMeNot: Resisting surveillance in web search. Lessons from the Identity Trail: Anonymity, Privacy, and Identity in a Networked Society, 23:417–436, 2009.
[22] R. Jones, R. Kumar, B. Pang, and A. Tomkins. “I know what you did last summer”: Query logs and user privacy. In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM ’07, pages 909–914, New York, NY, USA, 2007. ACM.
[23] A. Korolova, K. Kenthapadi, N. Mishra, and A. Ntoulas. Releasing search queries and clicks privately. In Proceedings of the 18th international conference on World wide web, pages 171–180. ACM, 2009.
[24] E. Kushilevitz and R. Ostrovsky. Replication is not needed: Single database, computationally-private information retrieval. In Proceedings of the 38th Annual Symposium on Foundations of Computer Science, FOCS ’97, pages 364–, Washington, DC, USA, 1997. IEEE Computer Society.
[25] V. I. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710, February 1966.
[26] M. Murugesan and C. Clifton. Providing privacy through plausibly deniable search. In SDM, pages 768–779. SIAM, 2009.
[27] G. Pass, A. Chowdhury, and C. Torgeson. A picture of search. In Proceedings of the 1st International Conference on Scalable Information Systems, InfoScale ’06, New York, NY, USA, 2006. ACM.
[28] S. T. Peddinti and N. Saxena. On the privacy of web search based on query obfuscation: a case study of TrackMeNot. In Privacy Enhancing Technologies, pages 19–37. Springer, 2010.
[29] A. Rajaraman and J. D. Ullman. Mining of massive datasets. Cambridge University Press, 2012.
[30] D. Rebollo-Monedero and J. Forné. Optimized query forgery for private information retrieval. Information Theory, IEEE Transactions on, 56(9):4631–4642, 2010.
[31] M. K. Reiter and A. D. Rubin. Anonymous web transactions with crowds. Commun. ACM, 42(2):32–48, Feb. 1999.
[32] B. Shapira, Y. Elovici, A. Meshiach, and T. Kuflik. PRAWA - privacy model for the web. Journal of the American Society for Information Science and Technology, 56(2):159–172, 2005.
[33] R. Shokri, G. Theodorakopoulos, J.-Y. Le Boudec, and J.-P. Hubaux. Quantifying location privacy. In IEEE Symposium on Security and Privacy, Oakland, CA, USA, 2011.
[34] A. Singla, R. W. White, A. Hassan, and E. Horvitz. Enhancing personalization via search activity attribution. In Proc. Special Interest Group On Information Retrieval (SIGIR), 2014.
[35] B. Tancer. Click: What Millions of People Are Doing Online and Why it Matters. Hyperion, 2008.
[36] R. W. White, A. Hassan, A. Singla, and E. Horvitz. From devices to people: Attribution of search activity in multi-user settings. In Proc. International World Wide Web Conference (WWW), 2014.
[37] S. Ye, F. Wu, R. Pandey, and H. Chen. Noise injection for search privacy protection. In Computational Science and Engineering, 2009. CSE’09. International Conference on, volume 3, pages 1–8. IEEE, 2009.

### Appendix

The number of users used to learn the linkage function is a parameter in our evaluation. Here, we study how the privacy gain from using an obfuscation mechanism is affected by varying the number of users. We focus on the relative privacy metric, as it removes the effect of the target queries on the quantified privacy and only reflects the privacy gain from obfuscation. We construct the linkage function from different sets of users, ranging in size from 20 to 100, and quantify the relative privacy of TMN and USR. Each set of users includes the users in the smaller sets. For each of these cases, we plot the empirical CDF for the users' privacy (as in Figure 2). To compare these plots, we use statistics (5, 50, and 95 percentiles) of these distributions. Figure 4 shows these statistics about the privacy of users versus the number of users used for learning the linkage function. As the plot illustrates, the privacy values do not fluctuate significantly as we change the set of users. Additionally, we observe that increasing the number of users beyond 100 does not improve the accuracy of the privacy metric. Therefore, a set size of 100 is reasonable for learning the linkage function in our evaluation.