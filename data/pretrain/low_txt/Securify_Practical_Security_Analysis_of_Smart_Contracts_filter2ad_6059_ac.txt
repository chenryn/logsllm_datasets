# Program Structure and Vulnerability Detection

Our results indicate that the features we extracted can detect vulnerabilities with reasonable accuracy. Consequently, we assume that both code structure patterns and complexity have an impact on the existence of vulnerabilities. Another advantage of using a machine learning (ML) based security vulnerability analysis is the support for traceability from binary labels to one or a subset of static code analyzers. This feature allows developers to focus on specific analyzers for further investigation, thereby saving time and resources. For example, in our system, integer overflow is associated with Mythril. If the vulnerability is present, developers can focus solely on Mythril and rule out Slither for further investigation.

## Performance of ML Algorithms

We found that certain ML algorithms outperform others for specific vulnerabilities. For instance, as shown in Table III, Support Vector Machines (SVM) performed better for integer underflow, while Neural Networks (NN) achieved higher accuracy for re-entrancy vulnerabilities with balance changes. Therefore, there is no single machine learning algorithm that is suitable for detecting all types of vulnerabilities.

Among the 36 vulnerabilities described in Table II, 16 were identified with high accuracy. This is a significant limitation of our study, which may be attributed to several factors:
1. **Dataset Size**: Our dataset was limited to 1,013 verified smart contracts, which did not cover a wide range of vulnerabilities. Expanding the dataset with more diverse occurrences of vulnerabilities could improve the ML model's ability to detect other vulnerabilities.
2. **Number of Analyzers**: We used only two static code analyzers in this model. Increasing the number of analyzers would improve the detection of intersected vulnerabilities and increase the coverage of different types of vulnerabilities.
3. **Feature Extraction**: The 17 extracted features were not sufficient to capture the patterns of code for some vulnerabilities. Related to the dataset size, the number of features can be expanded by extracting more features from the bytecodes, as proposed in [8]. Additionally, certain vulnerabilities may not be associated with the features extracted from code complexity and structure.

## Efficiency of ML Models

Applying our ML model to predict security vulnerabilities in smart contracts achieved significant improvements in terms of efficiency compared to static code analysis. As illustrated in Figure 2, the elapsed execution time for two static code analyzers (Mythril and Slither) to find vulnerabilities in approximately 1,000 smart contracts was around 7.31e3 seconds. In contrast, the execution time for running 184 ML models to identify 16 vulnerabilities from the same dataset was only 3.20e−1 seconds. This indicates that the ML model-based approach is about 2.28e4 times faster than static code analyzers. While this efficiency comes at the cost of accuracy and the number of detectable vulnerabilities, it is particularly beneficial in the rapid development process of smart contracts, where developers often need to release code quickly. Our ML model provides the possibility of finding vulnerabilities with substantially less time while maintaining reasonable accuracy.

## Probabilistic Nature of Static Analyzers

Another reason for using the ML approach is that, although static code analyzers are deterministic in principle, their reported results carry a notion of probability. As described in Section III-C, for a semantically consistent vulnerability, two analyzers may report conflicting results. Therefore, incorporating more analyzers into the ML model can significantly increase the probability of finding intersected vulnerabilities.

## Related Work

We have examined literature on three related areas: formal verification, static code analysis, and machine learning-based code analysis. Bhargavan et al. [4] proposed a framework that verifies security vulnerabilities using formal verification methods. This proposal translates smart contract source code or bytecode into F* and checks both functional correctness and runtime safety. Park et al. [3] adopted a full formal semantics of EVM to verify smart contracts. However, these proposals require deep knowledge of developing complex abstractions, making them less practical for widespread use in smart contract development.

Several studies addressed software vulnerabilities with static code analysis. Oyente [1] and Mythril [6] are two static analyzers that detect bugs through symbolic execution with EVM bytecode. Tikhomirov et al. [2] introduced a static analysis tool for Solidity that detects vulnerabilities through pattern matching using XPath. These tools can only verify predefined defects and often require manual auditing to address high false-positive rates.

Other studies have utilized machine learning algorithms to reveal potential security threats. Pang et al. [7] presented a vulnerability code pattern matching method that combined text mining with statistical feature selection to identify vulnerable code patterns in Java. Harer et al. [8] proposed a data-driven approach to reveal vulnerabilities in C and C++ code. Existing ML-based approaches are typically applicable to specific programming languages. To the best of our knowledge, there is no machine learning-based proposal to detect security vulnerabilities for Solidity.

## Conclusion

In this research, we proposed a machine learning-based model to detect security vulnerabilities in smart contracts on the Ethereum platform. We used static code analysis as the underlying technology and trained an array of machine learning models for different security vulnerabilities. Our model was able to find 16 different vulnerabilities with an average accuracy of 95%. Our approach significantly improved computational time and resource usage compared to directly using static code analyzer tools. Checking a large number of smart contracts using different static code analyzers is a significant burden on developers, who also need to learn how each analyzer works and combine the results for a full evaluation. Furthermore, our model can be used to identify security vulnerabilities in parallel with the development process, reducing the cost of development by preventing vulnerabilities from being introduced in early stages. Our proposed model is also applicable to other languages and platforms, as there are no language or platform dependencies. By training the model with different datasets and choosing the corresponding static code analyzers and AST builders, new machine learning code analyzers can be generated following the steps described in Section III.

## Acknowledgements

We acknowledge the support from NSERC, MITACS, and the Vector Institute for Artificial Intelligence.

## References

[1] L. Luu, D.-H. Chu, H. Olickel, P. Saxena, and A. Hobor, “Making smart contracts smarter,” in Proc 23rd ACM CCS, 2016, pp. 254–269.

[2] S. Tikhomirov, E. Voskresenskaya, I. Ivanitskiy, R. Takhaviev, E. Marchenko, and Y. Alexandrov, “Smartcheck: Static analysis of ethereum smart contracts,” in Proc 1st WETSEB of 40th ICSE. IEEE/ACM, 2018, pp. 9–16.

[3] D. Park, Y. Zhang, M. Saxena, P. Daian, and G. Ros¸u, “A formal verification tool for ethereum VM bytecode,” in Proc. 26th ACM Joint Meeting Eur. Soft. Eng. Conf. Symp. Oxford, U.K, 2018, pp. 912–915.

[4] K. Bhargavan, A. Delignat-Lavaud, C. Fournet, A. Gollamudi, G. Gonthier, N. Kobeissi, A. Rastogi, T. Sibut-Pinote, N. Swamy, and S. Zanella-Beguelin, “Formal verification of smart contracts,” in Proc. ACM Workshop Programming Lang. Analysis Sec., 2016, pp. 91–96.

[5] Crytic, “Slither, the solidity source analyzer,” 2019. [Online]. Available: https://github.com/crytic/slither

[6] B. Mueller, “Smashing ethereum smart contracts for fun and real profit,” in Proc 9th Annual HITB Sec. Conf., 2018.

[7] Y. Pang, X. Xue, and A. S. Namin, “Predicting vulnerable software components through n-gram analysis and statistical feature selection,” in Proc. 14th Int. Conf. Machine Learning and Applications (ICMLA). IEEE, 2015, pp. 543–548.

[8] J. A. Harer, L. Y. Kim, R. L. Russell, O. Ozdemir, L. R. Kosta, A. Rangamani, L. H. Hamilton, G. I. Centeno, J. R. Key, P. M. Ellingwood et al., “Automated software vulnerability detection with machine learning,” arXiv preprint arXiv:1803.04497, 2018.

[9] N. Atzei, M. Bartoletti, and T. Cimoli, “A survey of attacks on ethereum smart contracts (sok),” in PST. Springer, 2017, pp. 164–186.

[10] “What is ethereum,” 2016. [Online]. Available: http://www.ethdocs.org/en/latest/introduction/what-is-ethereum.html

[11] L. Hollander, “The ethereum virtual machine: How does it work?” 2019. [Online]. Available: https://medium.com/mycrypto/the-ethereum-virtual-machine-how-does-it-work-9abac2b7c9e.

[12] V. Buterin, “Vyper,” 2017. [Online]. Available: https://vyper.readthedocs.io/en/latest/index.html

[13] K. Delmolino, M. Arnett, A. Kosba, A. Miller, and E. Shi, “A programmer’s guide to ethereum and serpent,” 2015. [Online]. Available: URL:https://mc2-umd.github.io/ethereumlab/docs/serpent tutorial.

[14] C. Chinchilla, “Solidity v0.5.6,” 2019. [Online]. Available: https://solidity.readthedocs.io/en/v0.5.6/

[15] M. Grincalaitis, “Don’t code another smart contract without understanding the 4 languages in 6 minutes first,” 2018. [Online]. Available: https://medium.com/@merunasgrincalaitis/dont-code-another-smart-contract-without-understanding-the-4-languages-in-10-minutes-first-1c2dea165fcf.

[16] E. J. Schwartz, T. Avgerinos, and D. Brumley, “All you ever wanted to know about dynamic taint analysis and forward symbolic execution (but might have been afraid to ask),” in Proc. Sympos. Sec. Privacy. IEEE, 2010, pp. 317–331.

[17] Etherscan, “Verified smart contracts,” 2018. [Online]. Available: https://etherscan.io/contractsVerified

[18] Ethereum, “Solidity version 0.4.0,” 2016. [Online]. Available: https://github.com/ethereum/solidity/releases?after=untagged-3024eaee36d028412763

[19] ConsenSys, “Ethereum smart contract best practices,” 2019. [Online]. Available: https://consensys.github.io/smart-contract-best-practices/security-tools/

[20] ——, “Mythril detection capabilities,” 2018. [Online]. Available: https://github.com/ConsenSys/mythril-classic/wiki/Mythril-Detection-Capabilities

[21] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.

[22] M. B. Fraj, “In depth: Parameter tuning for SVC,” 2018. [Online]. Available: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769

[23] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectifier nonlinearities improve neural network acoustic models,” in Proc. ICML, vol. 30, no. 1, 2013, p. 3.

[24] M. B. Fraj, “In depth: Parameter tuning for decision tree,” 2017. [Online]. Available: https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3.

[25] ——, “In depth: Parameter tuning for random forest,” 2017. [Online]. Available: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d

[26] D. M. Powers, “Evaluation: From precision, recall and F-measure to ROC, informedness, markedness, and correlation,” 2011.