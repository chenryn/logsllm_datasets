# Performance Evaluation and Results

## 7.2 Experimental Results

Our experimental results demonstrate how Iris performs under various workloads on the full end-to-end system described in Section 6. We used a setup with seven hard drives for storage and three 1 Gbps network links between the Portal and Cloud. Even under these conditions, the Portal was never the bottleneck. The limiting factor was either the network or the hard drives, depending on the workload.

### Varying the Merkle Tree Cache Size

The parallel Merkle Tree Cache is crucial for the performance of our system. It allows the Portal to perform file operations without reading and writing entire Merkle tree paths from the server for each operation. The asynchronous cache also enables pausing operations that are waiting to retrieve Merkle tree nodes while other operations actively use the cache. Multiple paths can be loaded into the Merkle Tree Cache simultaneously while maintaining consistency.

To demonstrate the usefulness of the cache, we varied its size (i.e., the number of nodes it can hold at once) and timed each of the workloads under different cache sizes. The results are shown in Figure 4.

#### Interpretation

As demonstrated in Figure 4, the Tar, Untar, and IOZone workloads significantly benefit from a Merkle tree cache of 5 to 10 MB (about 10,000 to 20,000 nodes). In contrast, the sequential and random read/write workloads are mostly unaffected by the cache size.

The reason is straightforward: The Tar, Untar, and IOZone benchmarks frequently revisit the same part of the Merkle tree. For example, the Tar/Untar workloads often read/write multiple files within the same directory, sharing many Merkle tree nodes. Similarly, the random write portion of the IOZone benchmark creates a file with a large uncompactable Merkle tree, which is then read sequentially, resulting in an in-order traversal of the Merkle tree that is significantly sped up by the cache.

On the other hand, the sequential read and write workloads generate version tree nodes that are quickly compacted, so the Merkle Tree Cache only needs to hold a few dozen nodes at a time. The random read/write workloads are extremely intensive on the Cloud's disks, causing almost every operation to trigger a seek. This makes the Cloud's disks the bottleneck. Because the random read/write operations are executed very slowly by the Cloud's disks and the Portal parallelizes requests for the Merkle tree nodes, there is ample time for the Merkle tree nodes to be fetched without delaying the workload.

### Scalability

In Iris, all operations are handled by the same thread pool, and each file has its own queue of pending/active operations. From the Portal's perspective, there is little difference between each operation being issued by a different client and all operations being issued by the same client. Most of the overhead of having multiple clients comes from managing multiple TCP sockets and their associated buffers.

To show that Iris can easily scale to 100 clients accessing it simultaneously, we generated a sequential access pattern for each client to maximize both strain on the Portal's CPU and the number of cryptographic operations performed. (With more seek-intensive access, the bottleneck would be disk seeks on the Cloud.) We averaged the sequential read and sequential write speeds for 10 to 100 clients. The results are shown in Figure 5. As can be seen, Iris consistently reads/writes at 250 MB/s to 280 MB/s. The slight performance degradation for 100 clients is due to the fact that many files are accessed at once, causing a larger portion of disk seeks.

### Latency

Figure 7 shows the latency for several basic operations in Iris. The latency is measured under two scenarios: when the portal cache is hot and cold. A hot cache means that the cache already contains all the necessary data (Merkle tree nodes and blocks) to perform the operation on the portal alone. A cold cache means that all the data has been evicted from the portal's cache.

The bulk of the latency (over 84%) comes from the portal-cloud and client-portal network latencies. Our results show that the latency introduced by the portal for integrity checking and cache management (denoted as portal processing time) is much smaller in comparison: less than 14% for a cold cache and less than 29% for a hot cache.

The 1 MB read operation takes about half the time of the 1 MB write operation because the portal notifies the client that the write operation has completed while uploading the file to the cloud in the background. For the read operation, the portal must first read the file from the cloud.

The high cold cache latency for high-depth operations (e.g., create depth 3 and list directory) is due to the fact that each file is represented as a separate node in the Merkle tree, and tree paths are fetched one node at a time. This latency can be significantly reduced by having the portal fetch all nodes in a path in parallel or grouping multiple files into a single file node.

### PoR Encoding Rate

Finally, we measure the rate at which the Portal can perform erasure-encoding for file system recovery if auditing detects corruption. Figure 6 shows encoding speeds for data blocks of different sizes. As can be seen, the PoR encoding rate is sufficiently fast to sustain a throughput of 500 MB/s for 4 KB blocks.

## 8. Conclusions

We have presented Iris, an authenticated file system designed to outsource enterprise-class file systems to the cloud. Iris goes beyond basic data-integrity verification to achieve two stronger properties: file freshness and retrievability. Using a lightweight, tenant-side portal as a point of aggregation, Iris efficiently processes asynchronous requests from multiple clients transparently, i.e., with no underlying file system interface changes.

Iris achieves a degree of end-to-end optimization possible only through a carefully crafted, holistic architecture, one of the system's major contributions. Iris's architecture also relies on several technical novelties: the authenticating data-structure design and management, caching techniques, sequential-file-access optimizations, and a new erasure code enabling the first efficient dynamic PoR protocol.

## Acknowledgements

We extend our thanks to Roxana Geambasu for her insightful comments on a previous draft of the paper and the anonymous reviewers for all their feedback and suggestions.

## 9. References

[1] Full version, http://eprint.iacr.org/2011/585.pdf.
[2] IOzone filesystem benchmark. www.iozone.org. 2011.
[3] www.memcached.org.
[4] A. Adya, W. J. Bolosky, M. Castro, G. Cermak, R. Chaiken, J. R. Douceur, J. Howell, J. R. Lorch, M. Theimer, and R. P. Wattenhofer. FARSITE: Federated, available, and reliable storage for an incompletely trusted environment. Usenix, 2002.
[5] G. Ateniese, R. Burns, R. Curtmola, J. Herring, L. Kissner, Z. Peterson, and D. Song. Provable data possession at untrusted stores. In 14th ACM CCS, pages 598–609, 2007.
[6] M. Blaze. A cryptographic filesystem for Unix. In Proc. First ACM Conference on Computer and Communication Security (CCS 1993), pages 9–16, 1993.
[7] K. Bowers, A. Juels, and A. Oprea. Proofs of retrievability: Theory and implementation. In Proc. ACM Cloud Computing Security Workshop (CCSW 2009), 2009.
[8] G. Cattaneo, L. Catuogno, A. D. Sorbo, and P. Persiano. The design and implementation of a transparent cryptographic filesystem for Unix. pages 199–212, 2001.
[9] Y. Chen and R. Sion. To cloud or not to cloud? musings on costs and viability. In ACM Symposium on Cloud Computing (SOCC), 2011.
[10] Y. Dodis, S. Vadhan, and D. Wichs. Proofs of retrievability via hardness amplification. In Proc. 6th IACR TCC, volume 5444 of LNCS, pages 109–127, 2009.
[11] C. Erway, A. Kupcu, C. Papamanthou, and R. Tamassia. Dynamic provable data possession. In Proc. ACM Conference on Computer and Communications Security (CCS 2009), 2009.
[12] A. J. Feldman, W. P. Zeller, M. J. Freedman, and E. W. Felten. Sporc: Group collaboration using untrusted cloud resources. In Proc. OSDI, 2010.
[13] K. Fu. Group sharing and random access in cryptographic storage filesystems. Master’s thesis, Massachusetts Institute of Technology, 1999.
[14] K. Fu, F. Kaashoek, and D. Mazieres. Fast and secure distributed read-only filesystem. ACM Transactions on Computer Systems, 20:1–24, 2002.
[15] R. Geambasu, J. P. John, S. D. Gribble, T. Kohno, and H. M. Levy. Keypad: An auditing filesystem for theft-prone devices. In Proc. European Conference on Computer Systems (EuroSys), 2011.
[16] E. Goh, H. Shacham, N. Modadugu, and D. Boneh. SiRiUS: Securing remote untrusted storage. In Proc. Network and Distributed Systems Security Symposium (NDSS 2003), pages 131–145, 2003.
[17] M. T. Goodrich, C. Papamanthou, R. Tamassia, and N. Triandopoulos. Athos: Efficient authentication of outsourced filesystems. In Proc. Information Security Conference 2008, 2008.
[18] A. Juels and B. Kaliski. PORs: Proofs of retrievability for large files. In Proc. ACM Conference on Computer and Communications Security (CCS 2007), pages 584–597, 2007.
[19] M. Kallahalla, E. Riedel, R. Swaminathan, Q. Wang, and K. Fu. Plutus: Scalable secure file sharing on untrusted storage. In Proc. 2nd USENIX Conference on File and Storage Technologies (FAST), 2003.
[20] S. Kamara, C. Papamanthou, and T. Roeder. CS2: A searchable cryptographic cloud storage system. Technical Report MSR-TR-2011-58, Microsoft, 2011.
[21] J. Li, M. Krohn, D. Mazieres, and D. Shasha. Secure untrusted data repository. In Proc. 6th Symposium on Operating System Design and Implementation (OSDI), pages 121–136. Usenix, 2004.
[22] P. Mahajan, S. Setty, S. Lee, A. Clement, L. Alvisi, M. Dahlin, and M. Walfish. Depot: Cloud storage with minimal trust. In Proc. OSDI, 2010.
[23] E. Miller, D. Long, W. Freeman, and B. Reed. Strong security for distributed filesystems. In Proc. 1st USENIX Conference on File and Storage Technologies (FAST), 2002.
[24] A. Oprea and M. K. Reiter. Integrity checking in cryptographic filesystems with constant trusted storage. In Proc. Usenix Security Symposium 2007, 2007.
[25] R. Pletka and C. Cachin. Cryptographic security for a high-performance distributed filesystem. In Proc. 24th IEEE Conf. on Mass Storage Systems and Technologies (MSST 2007), 2007.
[26] R. A. Popa, J. Lorch, D. Molnar, H. J. Wang, and L. Zhuang. Enabling security in cloud storage SLAs with CloudProof. In Proc. 2011 USENIX Annual Technical Conference (USENIX), 2011.
[27] H. Shacham and B. Waters. Compact proofs of retrievability. In Proc. ASIACRYPT, volume 5350 of LNCS, pages 90–107, 2008.
[28] A. Shraer, C. Cachin, A. Cidon, I. Keidar, Y. Michalevsky, and D. Shaket. Venus: Verification for untrusted cloud storage. In Proc. Workshop on Cloud Computing Security, 2010.
[29] C. A. Stein, J. H. Howard, and M. Selzer. Unifying filesystem protection. In Proc. USENIX Annual Technical Conference, 2001.
[30] Q. Wang, C. Wang, J. Li, K. Ren, and W. Lou. Enabling public verifiability and data dynamics for storage security in cloud computing. In Proc. 14th European Symposium on Research in Computer Security (ESORICS 2009), 2009.
[31] Q. Zheng and S. Xu. Fair and dynamic proofs of retrievability. In Proc. 1st ACMM Conference on Data and Application Security and Privacy (CODASPY), 2011.