### Packet Loss and Bitrate Analysis

Figure 4 presents a bitrate sample of a connection between `iperf` running on NewtOS and Linux. We used `tcpdump` to capture the network trace and `Wireshark` for analysis. By using a single connection, we were able to capture all packets, enabling us to observe all lost segments and retransmissions. The trace shows a gap 4 seconds after the start of the connection, which was caused by injecting a fault in the IP server. During this period, no lost segments were observed, and only one retransmission occurred from the sender due to a missing ACK and a subsequent timeout. This retransmitted packet had already been received by the receiver. The connection quickly recovered to its original bitrate.

As previously mentioned, due to hardware limitations, we had to reset the network card when the IP server crashed. This reset caused a gap in the trace as it took time for the link to re-establish and for the driver to come back online. Therefore, all traces inspected after a driver crash exhibit similar patterns.

### Packet Filter (PF) Crash Analysis

A similar sample trace, shown in Figure 5, demonstrates that a PF crash is almost imperceptible. Our design ensures that no packets are lost because IP must receive a reply from PF; otherwise, it knows the packet was not processed. 

### Related Work

Our work builds upon previous research in operating systems, blending ideas from various projects into a new approach. Although the concept of microkernel-based multiserver systems is not new, historically, they could not match the performance of monolithic systems due to their inefficient use of scarce resources. However, modern multicore hardware has revived the multiserver system idea. Similarly, virtual machines, first invented in the 1960s, have seen a resurgence due to advancements in hardware.

Monolithic systems are increasingly adopting some of the multiserver design principles. For example, drivers and components like file systems can now run mostly in user space with kernel support for privileged execution. Additionally, kernel threads, which are similar to independent servers, can be scheduled both in time and space. These threads have independent execution contexts in the privileged mode and share the same address space, simplifying data sharing, although they require locks for synchronization. Parts of the networking stack run synchronously during system calls and asynchronously in kernel threads, which may execute on different cores than the application using them, depending on the number and usage of the cores. Coarse-grained locking has significant overhead, while fine-grained locking is challenging to implement correctly.

To use hardware more efficiently, kernel threads are becoming more distinct from the core kernel code. They run on dedicated cores to avoid contention with user applications and each other. An example is FlexSC's [39], [40] modification of Linux, which splits available cores into those dedicated to the kernel and those for applications. In such a setup, multithreaded applications can pass requests to the kernel asynchronously and without exceptions, reducing contention on scarce resources.

Another step towards a true multiserver system is IsoStack [37], a modification of AIX. Instances of the entire networking stack run isolated on dedicated cores, showing that monolithic systems can achieve a performance boost by dedicating cores to heavy tasks. This approach is also beneficial for multiserver systems, which can pin components to cores without fundamental design changes. Our primary motivation is dependability and reliability, while the techniques presented in this paper also enable competitive performance.

We are not the first to partition the OS into smaller components. Variants less extreme than multiserver systems isolate a smaller set of OS components in user-level processes, typically drivers [15], [25]. Barrelfish [5] is a so-called multikernel, designed for scalability and diversity, which can serve as a solid platform for a multiserver system. We plan to port our network stack on top of it.

Hypervisors, essentially microkernels, host multiple isolated systems. Colp et al. [8] show how to adopt the multiserver design for enhanced security in Xen's Dom0. Periodic microreboots and component isolation reduce the attack surface.

All commercial systems targeting safety and security-critical embedded systems, such as QNX, Integrity, or PikeOS, are microkernel/multiserver real-time operating systems. However, these are closed-source proprietary platforms, and we do not compare to them. Unlike NewtOS, they target very constrained embedded environments, whereas we show that the same basic design is applicable to areas where commodity systems like Windows or Linux dominate.

### Conclusion and Future Work

In this paper, we present our vision for future dependable operating systems. Our design excludes the kernel from inter-process communication (IPC) and promotes asynchronous user-space communication channels. We argue that multiserver systems must distribute the operating system across many cores to eliminate overheads. Only such asynchronous multiserver systems, where each component can run whenever needed, will perform well while preserving fault resilience and live-updatability.

We describe the challenges of designing the system and present an implementation of a networking stack built on these principles. The high networking load handled by our stack suggests that our design is applicable to other parts of the system. We acknowledge that dedicating large cores of current mainstream processors to system components results in resource loss, which must be addressed in future work. We need to investigate how to efficiently adapt the system to its current workload, for instance, by coalescing lightly utilized components on a single core and dedicating cores to heavily used ones. Equally important, we are interested in how future chips can be designed to best meet our needs. For example, can some of the large cores be replaced by many simpler ones to run the system?

### Acknowledgments

This work has been supported by the European Research Council Advanced Grant 227874. We would like to thank Arun Thomas for his invaluable comments on early versions of this paper.

### References

[1] Minix 3, Official Website and Download. http://www.minix3.org.
[2] big.LITTLE Processing. http://www.arm.com/products/processors/technologies/biglittleprocessing.php, 2011.
[3] Variable SMP - A Multi-Core CPU Architecture for Low Power and High Performance. Whitepaper - http://www.nvidia.com/, 2011.
[4] Vulnerability in TCP/IP Could Allow Remote Code Execution. http://technet.microsoft.com/en-us/security/bulletin/ms11-083, Nov. 2011.
[5] A. Baumann, P. Barham, P.-E. Dagand, T. Harris, R. Isaacs, S. Peter, T. Roscoe, A. Schupbach, and A. Singhania. The Multikernel: A New OS Architecture for Scalable Multicore Systems. Proc. of Symp. on Oper. Sys. Principles, 2009.
[6] H. Bos, W. de Bruijn, M. Cristea, T. Nguyen, and G. Portokalidis. Ffpf: Fairly fast packet filters. In Proc. of Symp. on Oper. Sys. Des. and Impl., 2004.
[7] S. Boyd-Wickizer, A. T. Clements, Y. Mao, A. Pesterev, M. F. Kaashoek, R. Morris, and N. Zeldovich. An Analysis of Linux Scalability to Many Cores. In Proc. of Symp. on Oper. Sys. Des. and Impl., 2010.
[8] P. Colp, M. Nanavati, J. Zhu, W. Aiello, G. Coker, T. Deegan, P. Loscocco, and A. Warfield. Breaking up is hard to do: security and functionality in a commodity hypervisor. In Proc. of Symp. on Oper. Sys. Principles, 2011.
[9] F. M. David, E. M. Chan, J. C. Carlyle, and R. H. Campbell. CuriOS: improving reliability through operating system structure. In Proc. of Symp. on Oper. Sys. Des. and Impl., 2008.
[10] W. de Bruijn, H. Bos, and H. Bal. Application-Tailored I/O with Streamline. ACM Transactions on Computer Systems, 29, May 2011.
[11] L. Deri. Improving Passive Packet Capture: Beyond Device Polling. In Proc. of Sys. Admin. and Net. Engin. Conf., 2004.
[12] P. Druschel and L. L. Peterson. Fbufs: A High-bandwidth Cross-domain Transfer Facility. In Proc. of Symp. on Oper. Sys. Principles, 1993.
[13] A. Dunkels. Full TCP/IP for 8-bit architectures. In International Conference on Mobile Systems, Applications, and Services, 2003.
[14] J. Erickson. Hacking: The Art of Exploitation. No Starch Press, 2003.
[15] V. Ganapathy, A. Balakrishnan, M. M. Swift, and S. Jha. Microdrivers: A New Architecture for Device Drivers. In Workshop on Hot Top. in Oper. Sys., 2007.
[16] A. Gefflaut, T. Jaeger, Y. Park, J. Liedtke, K. J. Elphinstone, V. Uhlig, J. E. Tidswell, L. Deller, and L. Reuther. The SawMill Multiserver Approach. In Proc. of workshop on Beyond the PC: new challenges for the oper. sys., 2000.
[17] J. Giacomoni, T. Moseley, and M. Vachharajani. FastForward for Efficient Pipeline Parallelism: A Cache-optimized Concurrent Lock-free Queue. In PPoPP, 2008.
[18] C. Giuffrida, L. Cavallaro, and A. S. Tanenbaum. We Crashed, Now What? In HotDep, 2010.
[19] L. Hatton. Reexamining the Fault Density-Component Size Connection. IEEE Softw., 14, March 1997.
[20] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum. Failure Resilience for Device Drivers. In Proc. of Int. Conf. on Depend. Sys. and Net., 2007.
[21] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum. Countering IPC Threats in Multiserver Operating Systems (A Fundamental Requirement for Dependability). In Pacific Rim Int. Symp. on Dep. Comp., 2008.
[22] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum. Fault Isolation for Device Drivers. In Proc. of Int. Conf. on Depend. Sys. and Net., 2009.
[23] Intel. Single-Chip Cloud Computer. http://techresearch.intel.com/ProjectDetails.aspx?Id=1.
[24] N. Jalbert, C. Pereira, G. Pokam, and K. Sen. RADBench: A Concurrency Bug Benchmark Suite. In HotParâ€™11, May 2011.
[25] B. Leslie, P. Chubb, N. Fitzroy-dale, S. Gtz, C. Gray, L. Macpherson, D. Potts, Y. Shen, K. Elphinstone, and G. Heiser. User-level Device Drivers: Achieved Performance. Computer Science and Technology, 20, 2005.
[26] J. Liedtke, K. Elphinstone, S. SchÃ¶nberg, H. Hrtig, G. Heiser, N. Islam, and T. Jaeger. Achieved IPC Performance (Still the Foundation for Extensibility), 1997.
[27] J. LÃ¶ser, H. HÃ¤rtig, and L. Reuther. A Streaming Interface for Real-Time Interprocess Communication. In Workshop on Hot Top. in Oper. Sys., 2001.
[28] S. Lu, S. Park, C. Hu, X. Ma, W. Jiang, Z. Li, R. A. Popa, and Y. Zhou. Muvi: Automatically Inferring Multi-Variable Access Correlations and Detecting Related Semantic and Concurrency Bugs. SIGOPS Oper. Syst. Rev., 41:103â€“116, October 2007.
[29] T. Mattson. Intel: 1,000-core Processor Possible. http://www.pcworld.com/article/211238/intel-1000core-processor-possible.html, Nov. 2010.
[30] W. T. Ng and P. M. Chen. The Systematic Improvement of Fault Tolerance in the Rio File Cache. In Proceedings of the Twenty-Ninth Annual International Symposium on Fault-Tolerant Computing, 1999.
[31] E. B. Nightingale, O. Hodson, R. McIlroy, C. Hawblitzel, and G. Hunt. Helios: Heterogeneous Multiprocessing with Satellite Kernels. In Proc. of Symp. on Oper. Sys. Principles, 2009.
[32] M. Peloquin, L. Olson, and A. Coonce. Simultaneity Safari: A Study of Concurrency Bugs in Device Drivers. University of Wisconsin-Madison Report, pages.cs.wisc.edu/~markus/736/concurrency.pdf, 2009.
[33] D. C. Sastry and M. Demirci. The QNX Operating System. Computer, 28, November 1995.
[34] M. Scondo. Concurrency and Race Conditions in Kernel Space (Linux 2.6). LinuxSupport.com (extract from "Linux Device Drivers"), December 2009.
[35] L. Seiler, D. Carmean, E. Sprangle, T. Forsyth, M. Abrash, P. Dubey, S. Junkins, A. Lake, J. Sugerman, R. Cavin, R. Espasa, E. Grochowski, T. Juan, and P. Hanrahan. Larrabee: A Many-core x86 Architecture for Visual Computing. ACM Trans. Graph., 27, August 2008.
[36] M. Shah, J. Barren, J. Brooks, R. Golla, G. Grohoski, N. Gura, R. Hetherington, P. Jordan, M. Luttrell, C. Olson, B. Sana, D. Sheahan, L. Spracklen, and A. Wynn. UltraSPARC T2: A Highly-Threaded, Power-Efficient, SPARC SOC. In ASSCCâ€™07.
[37] L. Shalev, J. Satran, E. Borovik, and M. Ben-Yehuda. IsoStack: Highly Efficient Network Processing on Dedicated Cores. In Proc. of USENIX Annual Tech. Conf., 2010.
[38] J. S. Shapiro. Vulnerabilities in Synchronous IPC Designs. In Proc. of IEEE Symp. on Sec. and Priv. IEEE Computer Society, 2003.
[39] L. Soares and M. Stumm. FlexSC: Flexible System Call Scheduling with Exception-Less System Calls. In Proc. of Symp. on Oper. Sys. Des. and Impl., 2010.
[40] L. Soares and M. Stumm. Exception-less System Calls for Event-Driven Servers. In Proc. of USENIX Annual Tech. Conf., 2011.
[41] R. Strong, J. Mudigonda, J. C. Mogul, N. Binkert, and D. Tullsen. Fast Switching of Threads Between Cores. SIGOPS Oper. Syst. Rev., 43, April 2009.
[42] M. M. Swift, B. N. Bershad, and H. M. Levy. Improving the Reliability of Commodity Operating Systems. In Proc. of Symp. on Oper. Sys. Principles, pages 207â€“222, 2003.
[43] D. Wentzlaff, C. Gruenwald, III, N. Beckmann, K. Modzelewski, A. Belay, L. Youseff, J. Miller, and A. Agarwal. An Operating System for Multicore and Clouds: Mechanisms and Implementation. In Proc. of Symp. on Cloud Computing, 2010.

---

**Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021 at 07:17:40 UTC from IEEE Xplore. Restrictions apply.**