# Figure 13: Additional TCP Flow on the Slow Link
Time (s)  
80  
100  
120  
140  

## 5. Related Work

Several single-rate multicast congestion control schemes have been proposed to date. A notable recent example is PGMCC [17]. This protocol selects the receiver with the worst network conditions, referred to as the "acker," to represent the group. The selection of the acker is crucial for ensuring the fairness of the protocol and is based on a simplified version of the TCP throughput model given in Equation (4). Similar to TFMCC, each receiver tracks the Round-Trip Time (RTT) and the smoothed loss rate, feeding these values into the model. The results are communicated to the sender using randomized feedback timers to prevent an implosion. If available, PGMCC also leverages network elements to aggregate feedback.

Once an acker is selected, a TCP-style window-based congestion control algorithm is run between the sender and the acker. Minor modifications compared to TCP include the separation of congestion control and reliability, allowing PGMCC to be used for both reliable and unreliable data transport. Additionally, it handles out-of-order packets and RTT changes when a different receiver is selected as the acker.

Simulations in [17] demonstrate that PGMCC competes fairly with TCP under various network conditions. The basic congestion control mechanism is simple and well-understood from the analysis of TCP congestion control. By closely mimicking TCP's window behavior, PGMCC produces rate variations similar to TCP's sawtooth-like rate, making it suitable for applications that can tolerate larger variations in the sending rate. In contrast, TFMCC generally produces a smoother and more predictable rate, making it better suited for applications with stricter requirements on rate changes. Given the critical role of the acker selection process in PGMCC, it could benefit from a feedback mechanism similar to that of TFMCC, which uses biased exponentially weighted timers. Both PGMCC and TFMCC present viable solutions for single-rate multicast congestion control, each targeting somewhat different application domains.

While PGMCC relies on a congestion window, TCP-Emulation at Receivers (TEAR) [16] combines window- and rate-based congestion control. TEAR features a TCP-like window emulation algorithm at the receivers, but the window is not used to directly control transmission. Instead, the average window size is calculated and transformed into a smoothed sending rate, which the sender uses to space out data packets. Although only a unicast version of TEAR exists, it can be made multicast-capable by implementing a TFMCC-like scalable feedback suppression scheme to communicate the calculated rate to the sender, along with scalable RTT measurements. The advantage of TEAR is that it does not require a detailed TCP model with all the necessary assumptions to compute a rate. However, for low levels of statistical multiplexing, TEAR's emulation assumptions about the independence of loss timing from transmit rate and of timeout emulation mean that it shares many of the limitations of the TCP models we use. Thus, we do not expect a multicast variant of TEAR to perform significantly better or worse than TFMCC.

## 6. Conclusions

We have described TFMCC, a single-rate multicast congestion control mechanism designed to scale to groups of several thousand receivers. Performing multicast congestion control while remaining TCP-friendly is challenging, particularly because TCP's transmission rate depends on the RTT, and measuring RTT in a scalable manner is difficult. Given the limitations of end-to-end protocols, we believe that TFMCC represents a significant improvement over previous work in this area.

We have extensively evaluated TFMCC through analysis and simulation, and we believe we have a good understanding of its behavior under a wide range of network conditions. Under typical real-world conditions, TFMCC is expected to perform well. However, we have also examined certain pathological cases where TFMCC may achieve a slower than desired transmission rate. Given that all protocols have bounds to their good behavior, this failure mode ensures the safety of the Internet.

An important part of any research is to identify the limitations of a new design. TFMCC's main weakness is in the startup phase, where it can take a long time for sufficient receivers to measure their RTT (assuming we cannot use NTP to provide approximate default values). Additionally, with large receiver sets, TCP-style slowstart is not an appropriate mechanism, and a linear increase can take some time to reach the correct operating point. These weaknesses are not specific to TFMCC; any safe single-rate multicast congestion control mechanism that is TCP-compatible will face these same limitations. Therefore, single-rate multicast congestion control mechanisms like TFMCC are best suited for relatively long-lived data streams. Fortunately, most current multicast applications, such as stock-price tickers or video streaming, involve such long-lived data streams.

### 6.1 Future Work

We plan to further pursue this work in several areas. While large-scale multicast experiments are challenging to perform in the real world, we intend to deploy TFMCC in a multicast file system synchronization application (e.g., rdist) to gain small-scale experience with a real application.

Some reliable multicast protocols build an application-level tree for acknowledgment aggregation. We have devised a hybrid rate/window-based variant of TFMCC that uses implicit RTT measurement combined with suppression within the aggregation nodes. This variant does not need to perform explicit RTT measurements or end-to-end feedback suppression. While this initially seems to be a significant improvement, it moves the complex initialization problem from RTT measurement to scalable ack-tree construction, which shares many of the problems posed by RTT measurement. Nonetheless, this appears to be a promising additional line of research.

Finally, the basic equation-based rate controller in TFMCC seems suitable for use in receiver-driven layered multicast, especially if combined with dynamic layering [4] to eliminate problems with unpredictable multicast leave latency.

## 7. Acknowledgements

We would like to thank Sally Floyd, Luigi Rizzo, and all reviewers for their invaluable comments. We also acknowledge feedback and suggestions received from RMRG members on earlier versions of TFMCC.

## 8. References

[1] M. Allman. A web server’s view of the transport layer. ACM Computer Communication Review, 30(5), Oct. 2000.
[2] S. Bajaj, L. Breslau, D. Estrin, K. Fall, S. Floyd, P. Haldar, M. Handley, A. Helmy, J. Heidemann, P. Huang, S. Kumar, S. McCanne, R. Rejaie, P. Sharma, K. Varadhan, Y. Xu, H. Yu, and D. Zappala. Improving simulation for network research. Technical Report 99-702b, University of Southern California, March 1999. Revised September 1999, to appear in IEEE Computer.
[3] S. Bhattacharyya, D. Towsley, and J. Kurose. The loss path multiplicity problem in multicast congestion control. In Proc. of IEEE Infocom, volume 2, pages 856 – 863, New York, USA, March 1999.
[4] J. Byers, M. Frumin, G. Horn, M. Luby, M. Mitzenmacher, A. Roetter, and W. Shaver. FLID-DL: Congestion control for layered multicast. In Proc. Second Int’l Workshop on Networked Group Communication (NGC 2000), Palo Alto, CA, USA, Nov. 2000.
[5] S. Floyd, M. Handley, J. Padhye, and J. Widmer. Equation-based congestion control for unicast applications. In Proc. ACM SIGCOMM, pages 43 – 56, Stockholm, Sweden, Aug. 2000.
[6] S. Floyd, V. Jacobson, C. Liu, S. McCanne, and L. Zhang. A reliable multicast framework for light-weight sessions and application level framing. IEEE/ACM Transactions on Networking, 5(6):784 – 803, Dec. 1997.
[7] T. Fuhrmann and J. Widmer. On the scaling of feedback algorithms for very large multicast groups. Computer Communications, 24(5-6):539 – 547, Mar. 2001.
[8] S. S. Gupta. Order statistics from the gamma distribution. Technometrics, 2:243 – 262, 1960.
[9] M. Handley. Session directories and scalable Internet multicast address allocation. In Proc. ACM Sigcomm, pages 105 – 116, Vancouver, B.C., Canada, Sept. 1998.
[10] A. Mankin, A. Romanow, S. Bradner, and V. Paxson. RFC 2357: IETF criteria for evaluating reliable multicast transport and application protocols, June 1998. Obsoletes RFC1650. Status: INFORMATIONAL.
[11] M. Mathis, J. Semke, J. Mahdavi, and T. Ott. The macroscopic behavior of the congestion avoidance algorithm. Computer Communications Review, 1997.
[12] S. McCanne, V. Jacobson, and M. Vetterli. Receiver-driven layered multicast. In Proc. of ACM SIGCOMM, pages 117 – 130, Palo Alto, CA, USA, Aug. 1996.
[13] D. L. Mills, A. Thyagarajan, and B. C. Huffman. Internet timekeeping around the globe. Proc. Precision Time and Time Interval (PTTI) Applications and Planning Meeting, pages 365 – 371, Dec. 1997.
[14] J. Nonnenmacher and E. W. Biersack. Scalable feedback for large groups. IEEE/ACM Transactions on Networking, 7(3):375 – 386, June 1999.
[15] J. Padhye, V. Firoiu, D. F. Towsley, and J. F. Kurose. Modeling TCP Reno performance: a simple model and its empirical validation. IEEE/ACM Transactions on Networking, 8(2):133–145, April 2000.
[16] I. Rhee, V. Ozdemir, and Y. Yi. TEAR: TCP emulation at receivers - flow control for multimedia streaming. Technical report, Department of Computer Science, NCSU, Apr. 2000.
[17] L. Rizzo. pgmcc: A TCP-friendly single-rate multicast congestion control scheme. In Proc. ACM SIGCOMM, pages 17 – 28, Stockholm, Sweden, August 2000.
[18] L. Vicisano, J. Crowcroft, and L. Rizzo. TCP-like congestion control for layered multicast data transfer. In Proc. of IEEE INFOCOM, volume 3, pages 996 – 1003, March 1998.
[19] J. Widmer and T. Fuhrmann. Extremum feedback for very large multicast groups. Technical Report TR 12-2001, Praktische Informatik IV, University of Mannheim, Germany, May 2001.
[20] J. Widmer and M. Handley. Extending equation-based congestion control to multicast applications. Technical Report TR 13-2001, Praktische Informatik IV, University of Mannheim, Germany, May 2001.

## Appendix

### A. Using the Initial RTT for the Aggregation of Loss Events

Using the initial RTT for the rate computation before a valid RTT measurement is obtained is safe since it leads to a lower calculated rate. In contrast, using the initial RTT for the aggregation of lost packets to loss events results in more aggressive protocol behavior. In this section, we argue that these two effects cancel each other out in most cases, and the initial RTT can be used for both purposes.

The initial RTT only has an impact on the loss event rate when separate loss intervals are merged into a single loss interval (i.e., more than one packet is lost per RTT). From Equation (1), the number of loss events per RTT is:

\[ \frac{1}{\text{RTT}} = \frac{1}{3c} \left( \frac{12}{\gamma^3} + \frac{2\gamma^2}{8} \right) \]

The corresponding curve is plotted in Figure 14. The maximum value is approximately 0.13 loss events per RTT. Thus, when multiple losses are aggregated to form a loss event and a loss event occurs during each RTT, the condition is unstable. TFMCC will reduce the sending rate due to the high loss event rate until the number of loss events per RTT is smaller than 0.13.

**Figure 14: Loss Events per RTT**

Even during the transition time, a TFMCC flow with an RTT estimate that is too high will behave more conservatively than a similar flow with a correct RTT estimate. The size of the loss intervals can only increase in proportion to the ratio of the initial RTT to the true RTT. Using Equation (4), an initial RTT that is too high by a factor of \( c \) will allow for a loss rate that is too low by a factor of \( c^2 \), resulting in the same throughput. The rate calculated at the receiver will therefore still be conservative. Numerical analysis indicates that this also holds for the complex TCP model (1) when loss event rates are less than approximately 10%.

If there are many receivers with a high loss rate, then throughput will be very low (see Section 3). If there are few such receivers, they can measure their RTT soon after startup. For these reasons, it is safe to use a high initial RTT to both aggregate losses to loss events and to compute the rate.

The loss history must be remodeled after the first valid RTT measurement is obtained; otherwise, the rate calculated by the receiver will be too high. When the lost packets and their timestamps are known, the correct loss intervals can easily be determined based on the measured RTT rather than the initial RTT. This process can be optimized by storing information about some of the more recently lost packets and approximating the correct distribution of loss intervals.