### Optimized Text

**Preventing User-Visible Impact and Reinforcing Integration Testing:**
The control-plane outage highlighted the necessity of including all control and management plane components in integration testing to prevent any user-visible impact. This incident reinforced the importance of comprehensive testing in maintaining system reliability.

**Localized Control Plane for Fast Response:**
During the initial deployment of Espresso, we observed that new routing prefixes were not being utilized quickly after peering was established, leading to underutilization of available capacity. The root cause was the slow propagation of new prefixes to the Global Controller (GC) and their subsequent incorporation into the forwarding rules. This feature in GC is designed to reduce churn in the global map computed by GC.

To address this, we augmented the local control plane to compute a set of default forwarding rules based on locally collected routing prefixes within an edge metro. Espresso uses this default set for traffic that is not explicitly steered by GC, allowing new prefixes to be utilized quickly. This default also provides an in-metro fallback in case of a GC failure, enhancing system reliability.

**Performance Metrics:**
Figure 9 illustrates the performance of the packet processor over a three-month period from a live production host, showing:
- (a) Routing update arrival rate.
- (b) Route count (IPv4 and IPv6).
- (c) Memory usage of LPM and control plane structures.

**Packet Processing Optimization:**
We first install programming updates in a shadow Longest Prefix Match (LPM) structure, updating all packet processing path pointers to point to the new LPM afterward. We use a compressed multibit trie for IPv4 LPM and a binary trie for IPv6 LPM. For a detailed survey of possible LPM implementations, see [31].

**Global Drain of PF Traffic:**
To support incremental deployment and emergency operational procedures, GC can disable or enable the use of a set of Espresso Peer Forwarding (PF) devices as egress options. An operator pushed an erroneous GC configuration that excluded all Espresso PF devices from the viable egress options. Fortunately, sufficient spare capacity was available, and GC successfully shifted traffic to other peering devices, minimizing user-visible impact. This incident underscored the need for additional safety checks and a robust simulation environment.

**Ingress Traffic Blackholing:**
Espresso's most visible outage resulted from our phased development model. Before implementing ACL-based filtering in end hosts, we restricted the announcement of prefixes to Espresso peers to those we could support with limited PF hardware TCAMs. An inadvertent policy change on one of our backbone routers caused external announcement of most Google prefixes via the PFs. Some of the attracted traffic, such as VPN, did not find an "allow" match in the PF and was blackholed. We subsequently fixed our BGP policies to prevent accidental route leaking, evaluated the limited ACLs on PF to allow additional protocols, and expedited the work to deploy complete Internet ACL filtering in end-hosts. This incident highlights the risks associated with introducing new network functionality and the potential for subtle interactions with existing infrastructure to affect availability.

**Related Work:**
Recent large-scale SDN deployments [11, 21, 29] have been discussed in the literature. These deployments typically interoperate with a controlled number of vendors and/or software. SDN peering [17, 18] provides finer-grained policy but does not allow application-specific traffic engineering. These works are limited to the Point of Presence (PoP) scale, while Espresso targets a globally available peering surface. None of these deployments aim for the high-level availability required for a global-scale public peering. Several efforts [4, 28] target more cost-effective hardware for peering, but we demonstrate our approach with a large-scale deployment evaluation.

**Centralized Traffic Engineering:**
Espresso employs centralized traffic engineering to select egress based on application requirements and fine-grained communication patterns. While centralized traffic engineering [7, 8, 20–22] is not novel, doing so at Internet scale and considering end-user metrics has not been previously demonstrated. We achieve substantial flexibility by leveraging host-based encapsulation to implement centralized traffic engineering policies, programming Internet-sized FIBs in packet processors rather than in the network, extending earlier ideas [9, 25].

**Comparison with Edge Fabric:**
Edge Fabric [27] shares similar traffic engineering goals with Espresso. However, Edge Fabric primarily aims to relieve congested peering links in a metro, while Espresso focuses on fine-grained global optimization of traffic. Consequently, Edge Fabric has independent local and global optimization, which may not always yield globally optimal traffic placement, and relies on BGP to steer traffic. In contrast, Espresso integrates egress selection with our global TE system, allowing us to move traffic to different serving locations and perform fine-grained traffic engineering at the hosts. Espresso also considers downstream congestion, which Edge Fabric is exploring as future work. Another key difference is Espresso’s use of commodity MPLS switches, whereas Edge Fabric relies on peering routers with Internet-scale forwarding tables, which can be costly.

**Declarative Network Management:**
Espresso’s declarative network management is similar to Robotron [30]. We further build a fully automated configuration pipeline where a change in intent is automatically and safely staged to all devices.

**Conclusions:**
Two principal criticisms of SDN are that it is best suited for walled gardens that do not require interoperability at Internet scale and that SDN mainly targets cost reductions. Through a large-scale deployment of Espresso, a new Internet peering architecture, we address these concerns in two ways. First, we demonstrate that it is possible to incrementally evolve a traditional peering architecture based on vendor gear while maintaining full interoperability with peers, their myriad policies, and varied hardware/protocol deployments at the scale of one of the Internet’s largest content provider networks. Second, we show that the value of SDN comes from its capability and software feature velocity, with any cost savings being secondary.

Espresso decouples complex routing and packet-processing functions from the routing hardware. A hierarchical control-plane design and close attention to fault containment for loosely-coupled components underlie a system that is highly responsive, reliable, and supports global/centralized traffic optimization. After more than a year of incremental rollout, Espresso supports six times the feature velocity, a 75% cost reduction, many novel features, and exponential capacity growth relative to traditional architectures. It carries more than 22% of all of Google’s Internet traffic, with this fraction rapidly increasing.

**Acknowledgments:**
Many teams contributed to the success of Espresso. We would like to acknowledge the contributions of G-Scale Network Engineering, Network Edge (NetEdge), Network Infrastructure (NetInfra), Network Software (NetSoft), Platforms Infrastructure Engineering (PIE), Site Reliability Engineering SRE, and many individuals, including Yuri Bank, Matt Beaumont-Gay, Bernhard Beck, Matthew Blecker, Luca Bigliardi, Kevin Brintnall, Carlo Contavalli, Kevin Fan, Mario Fanelli, Jeremy Fincher, Wenjian He, Benjamin Helsley, Pierre Imai, Chip Killian, Vinoj Kumar, Max Kuzmin, Perry Lorier, Piotr Marecki, Waqar Mohsin, Michael Rubin, Erik Rubow, Murali Suriar, Srinath Venkatesan, Lorenzo Vicisano, Carmen Villalobos, Jim Wanderer, and Zhehua Wu, among others. We also thank our reviewers, shepherd Kun Tan, Jeff Mogul, Dina Papagiannaki, and Anees Shaikh for their valuable feedback.

**References:**
[1] GNU Quagga Project. www.nongnu.org/quagga/. (2010).
[2] Best Practices in Core Network Capacity Planning. White Paper. (2013).
[3] Prometheus - Monitoring system & time series database. https://prometheus.io/. (2017).
[4] Joo Taveira Arajo. 2016. Building and scaling the Fastly network, part 1: Fighting the FIB. https://www.fastly.com/blog/building-and-scaling-fastly-network-part-1-fighting-fib. (2016).
[5] Ajay Kumar Bangla, Alireza Ghaffarkhah, Ben Preskill, Bikash Koley, Christoph Albrecht, Emilie Danna, Joe Jiang, and Xiaoxue Zhao. 2015. Capacity planning for the Google backbone network. In ISMP 2015 (International Symposium on Mathematical Programming).
[6] Mike Burrows. 2006. The Chubby lock service for loosely-coupled distributed systems. In Proceedings of the 7th symposium on Operating systems design and implementation. USENIX Association, 335–350.
[7] Matthew Caesar, Donald Caldwell, Nick Feamster, Jennifer Rexford, Aman Shaikh, and Jacobus van der Merwe. 2005. Design and Implementation of a Routing Control Platform. In Proceedings of the 2Nd Conference on Symposium on Networked Systems Design & Implementation - Volume 2 (NSDI’05). USENIX Association, Berkeley, CA, USA, 15–28.
[8] Martin Casado, Michael J. Freedman, Justin Pettit, Jianying Luo, Nick McKeown, and Scott Shenker. 2007. Ethane: Taking Control of the Enterprise. SIGCOMM Comput. Commun. Rev. 37, 4 (Aug. 2007), 1–12.
[9] Martin Casado, Teemu Koponen, Scott Shenker, and Amin Tootoonchian. 2012. Fabric: A Retrospective on Evolving SDN. In Proceedings of the First Workshop on Hot Topics in Software Defined Networks (HotSDN ’12). ACM, New York, NY, USA, 85–90.
[10] Florin Dobrian, Vyas Sekar, Asad Awan, Ion Stoica, Dilip Joseph, Aditya Ganjam, Jibin Zhan, and Hui Zhang. 2011. Understanding the Impact of Video Quality on User Engagement. In Proceedings of the ACM SIGCOMM 2011 Conference (SIGCOMM ’11). ACM, New York, NY, USA, 362–373.
[11] Sarah Edwards, Xuan Liu, and Niky Riga. 2015. Creating Repeatable Computer Science and Networking Experiments on Shared, Public Testbeds. SIGOPS Oper. Syst. Rev. 49, 1 (Jan. 2015), 90–99.
[12] Nick Feamster. 2016. Revealing Utilization at Internet Interconnection Points. CoRR abs/1603.03656 (2016).
[13] Nick Feamster, Jay Borkenhagen, and Jennifer Rexford. 2003. Guidelines for interdomain traffic engineering. ACM SIGCOMM Computer Communication Review 33, 5 (2003), 19–30.
[14] O. Filip. 2013. BIRD internet routing daemon. http://bird.network.cz/. (May 2013).
[15] Tobias Flach, Nandita Dukkipati, Andreas Terzis, Barath Raghavan, Neal Cardwell, Yuchung Cheng, Ankur Jain, Shuai Hao, Ethan Katz-Bassett, and Ramesh Govindan. 2013. Reducing Web Latency: the Virtue of Gentle Aggression. In Proceedings of the ACM Conference of the Special Interest Group on Data Communication (SIGCOMM ’13).
[16] Ramesh Govindan, Ina Minei, Mahesh Kallahalla, Bikash Koley, and Amin Vahdat. 2016. Evolve or Die: High-Availability Design Principles Drawn from Google's Network Infrastructure. In Proceedings of the 2016 Conference on ACM SIGCOMM 2016 Conference (SIGCOMM ’16). ACM, New York, NY, USA, 58–72.
[17] Arpit Gupta, Robert MacDavid, Rüdiger Birkner, Marco Canini, Nick Feamster, Jennifer Rexford, and Laurent Vanbever. 2016. An Industrial-scale Software Defined Internet Exchange Point. In Proceedings of the 13th Usenix Conference on Networked Systems Design and Implementation (NSDI’16). USENIX Association, Berkeley, CA, USA, 1–14.
[18] Arpit Gupta, Laurent Vanbever, Muhammad Shahbaz, Sean Patrick Donovan, Brandon Schlinker, Nick Feamster, Jennifer Rexford, Scott Shenker, Russ Clark, and Ethan Katz-Bassett. 2014. SDX: A Software Defined Internet Exchange. SIGCOMM Comput. Commun. Rev. 44, 4 (Aug. 2014), 579–580.
[19] Mark Handley, Orion Hodson, and Eddie Kohler. 2003. XORP: An Open Platform for Network Research. SIGCOMM Comput. Commun. Rev. 33, 1 (Jan. 2003), 53–57.
[20] Chi-Yao Hong, Srikanth Kandula, Ratul Mahajan, Ming Zhang, Vijay Gill, Mohan Nanduri, and Roger Wattenhofer. 2013. Achieving high utilization with software-driven WAN. In ACM SIGCOMM Computer Communication Review, Vol. 43. ACM, 15–26.
[21] Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, et al. 2013. B4: Experience with a globally-deployed software defined WAN. ACM SIGCOMM 43, 4, 3–14.
[22] Matthew K Mukerjee, David Naylor, Junchen Jiang, Dongsu Han, Srinivasan Seshan, and Hui Zhang. 2015. Practical, real-time centralized control for CDN-based live video delivery. ACM SIGCOMM Computer Communication Review 45, 4 (2015), 311–324.
[23] Abhinav Pathak, Y Angela Wang, Cheng Huang, Albert Greenberg, Y Charlie Hu, Randy Kern, Jin Li, and Keith W Ross. 2010. Measuring and evaluating TCP splitting for cloud services. In International Conference on Passive and Active Network Measurement. Springer Berlin Heidelberg, 41–50.
[24] Rachel Potvin and Josh Levenberg. 2016. Why Google Stores Billions of Lines of Code in a Single Repository. Commun. ACM 59, 7 (June 2016), 78–87.
[25] Barath Raghavan, Martín Casado, Teemu Koponen, Sylvia Ratnasamy, Ali Ghodsi, and Scott Shenker. 2012. Software-defined Internet Architecture: Decoupling Architecture from Infrastructure. In Proceedings of the 11th ACM Workshop on Hot Topics in Networks (HotNets-XI). ACM, New York, NY, USA, 43–48.
[26] S. Sangli, E. Chen, R. Fernando, J. Scudder, and Y. Rekhter. 2007. Graceful Restart Mechanism for BGP. RFC 4724 (Proposed Standard). (Jan. 2007).
[27] Brandon Schlinker, Hyojeong Kim, Timothy Chiu, Ethan Katz-Bassett, Harsha Madhyastha, Italo Cunha, James Quinn, Saif Hasan, Petr Lapukhov, and Hongyi Zeng. 2017. Engineering Egress with Edge Fabric. In Proceedings of the ACM SIGCOMM 2017 Conference (SIGCOMM ’17). ACM, New York, NY, USA.
[28] Tom Scholl. 2013. Building A Cheaper Peering Router. NANOG50. (2013). nLayer Communications, Inc.
[29] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand Kanagala, Hong Liu, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer, Urs Hölzle, Stephen Stuart, and Amin Vahdat. 2016. Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network. Commun. ACM 59, 9 (Aug. 2016), 88–97.
[30] Yu-Wei Eric Sung, Xiaozheng Tie, Starsky H.Y. Wong, and Hongyi Zeng. 2016. Robotron: Top-down Network Management at Facebook Scale. In Proceedings of the 2016 Conference on ACM SIGCOMM 2016 Conference (SIGCOMM ’16). ACM, New York, NY, USA, 426–439.
[31] David E Taylor. 2005. Survey and taxonomy of packet classification techniques. ACM Computing Surveys (CSUR) 37, 3 (2005), 238–275.