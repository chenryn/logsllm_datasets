# Node Failures and Joins: Impact on Lookups

## 6.4 Lookup Failures as a Function of Node Fail/Join Rate

Figure 12 illustrates the fraction of lookups that fail as a function of the rate at which nodes fail and join, measured in nodes per second. The graph only includes failures caused by Chord state inconsistency, not those due to lost keys.

### Analysis
- **Node Failure and Join Rate**: The x-axis represents the node fail/join rate, ranging from 0.02 to 0.1 nodes per second.
- **Expected Behavior**: In an ideal scenario, one might expect half of the requests to fail if the querier and target are in different partitions half the time. However, our results do not show this, indicating that Chord is robust even in the face of multiple simultaneous node failures.

## 6.5 Lookups During Stabilization

A lookup issued after some failures but before stabilization has completed may fail for two reasons:
1. The node responsible for the key may have failed.
2. Some nodes' finger tables and predecessor pointers may be inconsistent due to concurrent joins and node failures.

### Methodology
- **Success Criteria**: A lookup is considered successful if it reaches the current successor of the desired key.
- **Optimistic Assumption**: This method is slightly optimistic, as in a real system, there might be periods when the real successor of a key has not yet acquired the data associated with the key from the previous successor.
- **Simulation Details**: The simulator does not retry queries; if a query is forwarded to a node that is down, the query simply fails. Thus, the results represent the worst-case scenario for query failures induced by state inconsistency.

### Experiment Setup
- **Key Lookups**: Generated according to a Poisson process at a rate of one per second.
- **Joins and Failures**: Modeled by a Poisson process with a mean arrival rate \( R \).
- **Stabilization Routine**: Each node runs the stabilization routines at randomized intervals averaging 30 seconds. The simulator updates all finger table entries on every invocation.
- **Initial Network Size**: Starts with 500 nodes.

### Results
- **Failure Rate**: For a node failure rate of 0.01 (one node joining and leaving every 100 seconds on average), the graph shows the average failure rates and confidence intervals.
- **Comparison**: The x-axis ranges from a rate of 1 failure per 3 stabilization steps to a rate of 3 failures per one stabilization step.
- **Confidence Intervals**: Computed over 10 independent runs, averaged over approximately two hours of simulated time.

### Explanation
- **Path Lengths**: With 500 nodes, the average lookup path length is around 5.
- **Failure Probability**: If \( k \) nodes fail, the probability that one of them is on the finger path is roughly \( \frac{5k}{500} = \frac{k}{100} \). This suggests a failure rate of about 3% if there are 3 failures between stabilizations. The graph shows results in this range, but slightly worse, as it might take more than one stabilization to completely clear out a failed node.

## 6.6 Experimental Results

This section presents latency measurements obtained from a prototype implementation of Chord deployed on the Internet. The Chord nodes are located at ten sites across the United States, including California, Colorado, Massachusetts, New York, North Carolina, and Pennsylvania. The Chord software runs on UNIX, uses 160-bit keys obtained from the SHA-1 cryptographic hash function, and communicates using TCP. Chord operates in an iterative style and is part of an experimental distributed file system.

### Latency Measurements

Figure 13 shows the measured latency of Chord lookups as a function of the total number of nodes. Experiments with more than ten nodes are conducted by running multiple independent copies of the Chord software at each site.

- **Experiment Setup**: Each physical site issues 16 Chord lookups for randomly chosen keys one-by-one.
- **Graph Plots**: Median, 5th, and 95th percentile of lookup latency.
- **Median Latency**: Ranges from 180 to 285 ms, depending on the number of nodes.
- **Example Calculation**: For 180 nodes, a typical lookup involves five two-way message exchanges, with typical round-trip delays of 60 milliseconds. The expected lookup time is about 300 milliseconds, close to the measured median of 285 ms.
- **Latency Variations**: Low 5th percentile latencies are caused by lookups for keys close to the querying node or by hops that remain local. High 95th percentiles are caused by lookups following high-delay paths.

### Conclusion
The results confirm that lookup latency grows slowly with the total number of nodes, validating the simulation results that demonstrate Chord's scalability.

## 7. Future Work

Based on our experience with the prototype, we aim to improve the Chord design in several areas:

- **Partition Healing**: Chord currently lacks a specific mechanism to heal partitioned rings. One approach is to periodically check global consistency by having each node ask other nodes to perform a Chord lookup for itself.
- **Malicious Nodes**: A malicious set of Chord participants could present an incorrect view of the Chord ring. Requiring and checking that nodes use IDs derived from the SHA-1 hash of their IP addresses can mitigate this.
- **Message Efficiency**: Reducing the number of messages per lookup by changing the placement of finger table entries.
- **Server Selection**: Using server selection to forward lookups to the node with the lowest delay.

## 8. Conclusion

Chord is a powerful primitive for decentralized peer-to-peer applications, efficiently determining the node responsible for storing a key's value. It maintains routing information for only about \( \log N \) other nodes in an \( N \)-node network and resolves lookups via \( \log N \) messages. Chord is simple, provably correct, and scalable, even in the face of concurrent node arrivals and departures. Our theoretical analysis, simulations, and experimental results confirm its robustness and performance.

## Acknowledgments

We thank Frank Dabek for the measurements of the Chord prototype and David Andersen for setting up the testbed used in those measurements.

## References

[1] ANDERSEN, D. Resilient overlay networks. Master’s thesis, Department of EECS, MIT, May 2001. http://nms.lcs.mit.edu/projects/ron/.

[2] BAKKER, A., AMADE, E., BALLINTIJN, G., KUZ, I., VERKAIK, P., VAN DER WIJK, I., VAN STEEN, M., AND TANENBAUM., A. The Globe distribution network. In Proc. 2000 USENIX Annual Conf. (FREENIX Track) (San Diego, CA, June 2000), pp. 141–152.

[3] CHEN, Y., EDLER, J., GOLDBERG, A., GOTTLIEB, A., SOBTI, S., AND YIANILOS, P. A prototype implementation of archival intermemory. In Proceedings of the 4th ACM Conference on Digital libraries (Berkeley, CA, Aug. 1999), pp. 28–37.

[4] CLARKE, I. A distributed decentralised information storage and retrieval system. Master’s thesis, University of Edinburgh, 1999.

[5] CLARKE, I., SANDBERG, O., WILEY, B., AND HONG, T. W. Freenet: A distributed anonymous information storage and retrieval system. In Proceedings of the ICSI Workshop on Design Issues in Anonymity and Unobservability (Berkeley, California, June 2000). http://freenet.sourceforge.net.

[6] DABEK, F., BRUNSKILL, E., KAASHOEK, M. F., KARGER, D., MORRIS, R., STOICA, I., AND BALAKRISHNAN, H. Building peer-to-peer systems with Chord, a distributed location service. In Proceedings of the 8th IEEE Workshop on Hot Topics in Operating Systems (HotOS-VIII) (Elmau/Oberbayern, Germany, May 2001), pp. 71–76.

[7] DABEK, F., KAASHOEK, M. F., KARGER, D., MORRIS, R., AND STOICA, I. Wide-area cooperative storage with CFS. In Proceedings of the 18th ACM Symposium on Operating Systems Principles (SOSP ’01) (To appear; Banff, Canada, Oct. 2001).

[8] DRUSCHEL, P., AND ROWSTRON, A. Past: Persistent and anonymous storage in a peer-to-peer networking environment. In Proceedings of the 8th IEEE Workshop on Hot Topics in Operating Systems (HotOS 2001) (Elmau/Oberbayern, Germany, May 2001), pp. 65–70.

[9] FIPS 180-1. Secure Hash Standard. U.S. Department of Commerce/NIST, National Technical Information Service, Springfield, VA, Apr. 1995.

[10] Gnutella. http://gnutella.wego.com/.

[11] KARGER, D., LEHMAN, E., LEIGHTON, F., LEVINE, M., LEWIN, D., AND PANIGRAHY, R. Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web. In Proceedings of the 29th Annual ACM Symposium on Theory of Computing (El Paso, TX, May 1997), pp. 654–663.

[12] KUBIATOWICZ, J., BINDEL, D., CHEN, Y., CZERWINSKI, S., EATON, P., GEELS, D., GUMMADI, R., RHEA, S., WEATHERSPOON, H., WEIMER, W., WELLS, C., AND ZHAO, B. OceanStore: An architecture for global-scale persistent storage. In Proceeedings of the Ninth international Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2000) (Boston, MA, November 2000), pp. 190–201.

[13] LEWIN, D. Consistent hashing and random trees: Algorithms for caching in distributed networks. Master’s thesis, Department of EECS, MIT, 1998. Available at the MIT Library, http://thesis.mit.edu/.

[14] LI, J., JANNOTTI, J., DE COUTO, D., KARGER, D., AND MORRIS, R. A scalable location service for geographic ad hoc routing. In Proceedings of the 6th ACM International Conference on Mobile Computing and Networking (Boston, Massachusetts, August 2000), pp. 120–130.

[15] MOCKAPETRIS, P., AND DUNLAP, K. J. Development of the Domain Name System. In Proc. ACM SIGCOMM (Stanford, CA, 1988), pp. 123–133.

[16] MOTWANI, R., AND RAGHAVAN, P. Randomized Algorithms. Cambridge University Press, New York, NY, 1995.

[17] Napster. http://www.napster.com/.

[18] Ohaha, Smart decentralized peer-to-peer sharing. http://www.ohaha.com/design.html.

[19] PLAXTON, C., RAJARAMAN, R., AND RICHA, A. Accessing nearby copies of replicated objects in a distributed environment. In Proceedings of the ACM SPAA (Newport, Rhode Island, June 1997), pp. 311–320.

[20] RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP, R., AND SHENKER, S. A scalable content-addressable network. In Proc. ACM SIGCOMM (San Diego, CA, August 2001).

[21] STOICA, I., MORRIS, R., KARGER, D., KAASHOEK, M. F., AND BALAKRISHNAN, H. Chord: A scalable peer-to-peer lookup service for internet applications. Tech. Rep. TR-819, MIT LCS, March 2001. http://www.pdos.lcs.mit.edu/chord/papers/.

[22] VAN STEEN, M., HAUCK, F., BALLINTIJN, G., AND TANENBAUM, A. Algorithmic design of the Globe wide-area location service. The Computer Journal 41, 5 (1998), 297–310.