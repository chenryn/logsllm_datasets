### Impact of Region Selection on Web Service Performance in Modern Public IaaS Clouds

While the impact of Content Delivery Networks (CDNs) has been studied in various contexts [27], there is a lack of research assessing its effect on the diverse range of modern public Infrastructure-as-a-Service (IaaS) clouds. To address this, we conducted measurements to answer two key questions:
1. How does the choice of region affect the performance experienced by clients of a web service?
2. To what extent can the use of multiple regions (or zones) improve client-perceived performance?

#### Latency and Throughput Measurements

**Latency Measurements:**
To study per-region latency performance, we set up 40 m1.medium instances, with 2 instances in each of the 20 available EC2 availability zones. We selected 80 geographically distributed PlanetLab nodes to represent real clients. Using the `hping3` utility, we conducted 5 TCP pings to each of the 40 instances, excluding timed-out pings from the average Round-Trip Time (RTT) calculations. Probing was performed every 15 minutes for three consecutive days.

**Throughput Measurements:**
We used the same 40 m1.medium EC2 instances and 80 PlanetLab nodes to measure throughput. The PlanetLab nodes were divided into two groups of 40. Each node in each group performed an HTTP GET request for a 2 MB file from one of the 40 EC2 instances running an Apache web server. Only one HTTP connection was established with each EC2 instance at any given time to avoid contention. Each client in each group performed an HTTP GET operation every 11.25 seconds, canceling the download if it took more than 10 seconds. Each client accessed all 40 servers in each round, taking 450 seconds to complete. Thus, it took 15 minutes for 80 clients to perform one round of throughput measurements. The final throughput was calculated as `file_size / download_time`. We ran the measurements for three consecutive days, resulting in 288 data points per client. The throughput measurements were interspersed with the latency measurements.

#### Performance Across Different Regions

Figures 9 and 10 illustrate the latency and throughput measurements for 15 representative PlanetLab locations and the three US EC2 regions. Key observations include:
1. **Single-Region Deployments:** Careful selection of the region is crucial. For example, ec2.us-west-1 offers better average latency (130 ms) and throughput (1143 KB/s) compared to ec2.us-west-2 (145 ms and 895 KB/s), averaged across all client locations.
2. **Impact of Region Choice:** The chosen region significantly affects performance. For a client in Seattle, using ec2.us-west-2 can reduce latency by almost a factor of 6 and improve throughput by nearly a factor of 5 compared to using ec2.us-east-1.
3. **Dynamic Region Selection:** The optimal region may vary based on the client's location. For a client in Boulder, the best region may change over time (Figure 11).

#### Multi-Region Deployment Benefits

To determine the upper bound on performance from a k-region deployment, we identified the best k regions out of the 8, for 1 ≤ k ≤ 8. We used our measurement data to calculate the overall performance that clients would achieve with a routing algorithm that selects the optimal region from the k regions for each client at each 15-minute interval. Specifically, for each value of k, we:
1. Enumerated all size-k subsets of regions.
2. Computed the average performance across all clients, assuming each client uses the lowest latency or highest throughput region from the k at each time round.
3. Chose the size-k subset with the lowest latency or highest throughput.

Figure 12 shows the results. Adding more regions to a deployment can significantly increase average performance, but there are diminishing returns after k = 3. For example, latency decreases by 33% when k = 3 compared to k = 1, but only by 39% when k = 4.

The best regions by throughput and latency are:
- **Throughput:**
  - k = 1: ec2.us-east-1
  - k = 2: ec2.us-east-1, ec2.eu-west-1
  - k = 3: ec2.us-east-1, ec2.eu-west-1, ec2.us-west-1
  - k = 4: ec2.us-east-1, ec2.eu-west-1, ec2.us-west-1, ec2.ap-southeast-1

- **Latency:**
  - k = 1: ec2.us-east-1
  - k = 2: ec2.us-east-1, ec2.ap-northeast-1
  - k = 3: ec2.us-east-1, ec2.ap-northeast-1, ec2.us-west-1
  - k = 4: ec2.us-east-1, ec2.ap-northeast-1, ec2.us-west-1, ec2.ap-southeast-1

#### Performance Across Different Zones

We also investigated the performance differences between different zones within the same region. We found that the zone has little impact on latency, with almost equivalent average RTTs for all clients across the two days. For throughput, the variation is somewhat higher but not as significant as that seen across regions. This variation is likely due to local effects such as contention on shared instances or network switches, as suggested by other recent measurements of EC2 performance variability [25].

#### Summary and Implications

Using multiple regions can significantly improve latency and throughput. However, leveraging multiple regions may be challenging, as the optimal region for some clients may need to adapt dynamically. This can be achieved via global request scheduling (effective but complex) or requesting from multiple regions in parallel (simple but increases server load). While a multi-region deployment enhances web service performance and resilience, cloud tenants must consider factors such as inter-region network traffic costs and the design of cloud features, which may restrict data sharing across regions.

### ISP Diversity

To investigate tolerance to wide-area faults, we focused on the diversity of immediate downstream ISPs at each EC2 zone. Greater diversity and an even spread of routes across downstream ISPs generally indicate greater fault tolerance.

We set up three m1.medium instances in each available EC2 availability zone and ran traceroute 50 times from each instance to 200 geographically diverse PlanetLab nodes. We used the UNIX `whois` utility to determine the Autonomous System (AS) number associated with the first non-EC2 hop and counted that AS as an immediate downstream ISP for the zone hosting the instance. Table 16 provides the number of distinct ASs seen for each zone and region. We observed that:
1. Different zones in a region have almost the same number of downstream ISPs.
2. The extent of diversity varies across regions, with some connected to more than 30 downstream ISPs and others to just 4. Most regions, except South America and Asia Pacific Sydney, are well-multihomed.

We also studied the spread of routes across downstream ISPs and found it to be uneven, even in well-connected regions like ec2.us-west-1 and ec2.eu-west-1, where up to 31% and 33% of routes, respectively, use the same downstream ISP.

### Summary and Implications

Although individual regions are multihomed, the uneven spread of routes implies that local failures in downstream ISPs can cause availability problems for large fractions of clients of cloud-using web services. This can be mitigated by using multiple regions simultaneously or by leveraging dynamic route control solutions [20].

### Related Work

Several studies have examined the network characteristics of data centers (DCs). Kandula et al. [26] focused on measuring flow-level characteristics, while Benson et al. [21] characterized internal network traffic in different types of DCs. Mishra et al. [32] analyzed workloads inside Google compute clusters. Li et al. [29] benchmarked and compared computing and storage performance across different public cloud providers. Chen et al. [22] studied wide-area traffic patterns between Yahoo! DCs. Our work extends these studies by focusing on the deployment patterns of the front ends of cloud-using web services, the wide-area traffic they impose, and their implications on client-perceived performance and availability.

### Conclusion

In this study, we conducted extensive measurements of modern IaaS cloud usage, particularly Amazon EC2 and Windows Azure. Our findings confirm the widespread use of these cloud systems and highlight the precarious nature of many deployments, with most websites using only one cloud region. We show that multi-region deployments can significantly enhance latency and throughput performance in EC2. Beyond providing insights into current cloud usage, our work aims to spur further research on tracking cloud usage and developing region-aware routing and deployment mechanisms. All datasets used in this paper are publicly available [10], except for the packet capture.

### Acknowledgments

We thank Dale Carder, Michael Hare, and Mark Tinberg from the Department of Information Technology at the University of Wisconsin-Madison for helping capture packet traces. We also thank our shepherd and anonymous reviewers for their valuable feedback. This work is supported by NSF awards 1040757 and 1065134.

### References

[1] Alexa. http://alexa.com/topsites.
[2] Alexa Web Information Service. http://aws.amazon.com/awis/.
[3] Amazon CloudFront. http://aws.amazon.com/cloudfront.
[4] Amazon ELB Service Event in the US-East Region. http://aws.amazon.com/message/680587/.
[5] AWS Elastic Beanstalk. http://aws.amazon.com/elasticbeanstalk.
[6] AWS Service Event in the US-East Region. https://aws.amazon.com/message/680342/.
[7] Azure CDN. http://msdn.microsoft.com/en-us/library/windowsazure/ee795176.aspx.
[8] Azure Datacenter IP Ranges. http://microsoft.com/en-us/download/details.aspx?id=29840.
[9] Azure TM. http://msdn.microsoft.com/en-us/library/windowsazure/hh744833.aspx.
[10] Cloud Measurement Data. http://pages.cs.wisc.edu/~keqhe/cloudmeasure_datasets.html.
[11] Cloudflare CDN. http://cloudflare.com.
[12] EC2 public IP ranges. https://forums.aws.amazon.com/ann.jspa?annID=1528.
[13] ELB. http://aws.amazon.com/elasticloadbalancing.
[14] Heroku. http://heroku.com.
[15] Knock. http://code.google.com/p/knock.
[16] Passive DNS Network Mapper. http://code.google.com/p/dnsmap/.
[17] PlanetLab. http://planet-lab.org/.
[18] B. Agarwal, A. Akella, A. Anand, A. Balachandran, P. Chitnis, C. Muthukrishnan, R. Ramjee, and G. Varghese. EndRE: An End-System Redundancy Elimination Service for Enterprises. In NSDI, pages 419–432, 2010.
[19] A. Akella, S. Seshan, and A. Shaikh. An Empirical Evaluation of Wide-area Internet Bottlenecks. In Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement, pages 101–114. ACM, 2003.
[20] A. Akella, S. Seshan, and A. Shaikh. Multihoming Performance Benefits: An Experimental Evaluation of Practical Enterprise Strategies. In USENIX Annual Technical Conference, General Track, pages 113–126, 2004.
[21] T. Benson, A. Akella, and D. A. Maltz. Network Traffic Characteristics of Data Centers in the Wild. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pages 267–280. ACM, 2010.
[22] Y. Chen, S. Jain, V. K. Adhikari, Z.-L. Zhang, and K. Xu. A First Look at Inter-Data Center Traffic Characteristics via Yahoo! Datasets. In INFOCOM, 2011 Proceedings IEEE, pages 1620–1628. IEEE, 2011.
[23] I. Drago, M. Mellia, M. M Munafo, A. Sperotto, R. Sadre, and A. Pras. Inside Dropbox: Understanding Personal Cloud Storage Services. In Proceedings of the 2012 ACM conference on Internet measurement conference, pages 481–494. ACM, 2012.
[24] J. Edberg. Post-Mortem of October 22, 2012 AWS Degradation. http://techblog.netflix.com/2012/10/post-mortem-of-october-222012-aws.html.
[25] B. Farley, A. Juels, V. Varadarajan, T. Ristenpart, K. D. Bowers, and M. M. Swift. More for Your Money: Exploiting Performance Heterogeneity in Public Clouds. In Proceedings of the Third ACM Symposium on Cloud Computing, page 20. ACM, 2012.
[26] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and R. Chaiken. The Nature of Data Center Traffic: Measurements & Analysis. In Proceedings of the 9th ACM SIGCOMM conference on Internet measurement conference, pages 202–208. ACM, 2009.
[27] R. Krishnan, H. V. Madhyastha, S. Srinivasan, S. Jain, A. Krishnamurthy, T. Anderson, and J. Gao. Moving beyond End-to-End Path Information to Optimize CDN Performance. In Proceedings of the 9th ACM SIGCOMM conference on Internet measurement conference, pages 190–201. ACM, 2009.
[28] C. Labovitz. How Big is Amazon’s Cloud? http://deepfield.net/blog, April 2012.
[29] A. Li, X. Yang, S. Kandula, and M. Zhang. CloudCmp: Comparing Public Cloud Providers. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pages 1–14. ACM, 2010.
[30] A. Li, X. Zong, S. Kandula, X. Yang, and M. Zhang. CloudProphet: Towards Application Performance Prediction in Cloud. In ACM SIGCOMM Computer Communication Review, volume 41, pages 426–427. ACM, 2011.
[31] R. McMillan. Amazon’s Secretive Cloud Carries 1 Percent of the Internet. http://www.wired.com/wiredenterprise/2012/04/amazon-cloud/, 2012.
[32] A. K. Mishra, J. L. Hellerstein, W. Cirne, and C. R. Das. Towards Characterizing Cloud Backend Workloads: Insights from Google Compute Clusters. ACM SIGMETRICS Performance Evaluation Review, 37(4):34–41, 2010.
[33] V. Paxson. Bro: A System for Detecting Network Intruders in Real-Time. In USENIX Security Symposium (SSYM), 1998.
[34] T. Ristenpart, E. Tromer, H. Shacham, and S. Savage. Hey, You, Get Off of My Cloud: Exploring Information Leakage in Third-party Compute Clouds. In Proceedings of the 16th ACM conference on Computer and communications security, pages 199–212. ACM, 2009.
[35] J. Schectman. Netflix Amazon Outage Shows Any Company Can Fail. http://blogs.wsj.com/cio/2012/12/27/netflix-amazon-outage-shows-any-company-can-fail/, 2012.
[36] R. Teixeira, A. Shaikh, T. Griffin, and J. Rexford. Dynamics of Hot-potato Routing in IP Networks. In ACM SIGMETRICS Performance Evaluation Review, volume 32, pages 307–319. ACM, 2004.