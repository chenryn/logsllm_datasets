### 3. Security and Privacy Aspects

Across the 38 papers surveyed, various security-related issues were identified, including phishing attacks [81, 82], session safety during redirections [83], and domain squatting [58]. Nine additional papers focused on privacy and censorship, such as the Tor overlay network [61] and user tracking [35]. Network and application performance is another popular area, with ten papers examining topics like HTTP/2 server push [72], mobile web performance [71], and Internet latency [26]. Some studies also explored economic aspects, such as hosting providers.

### 4. Network Layers Measured

We reviewed the network layers measured in each study. Many papers focused on web infrastructure, with 22 papers addressing content, 8 focusing on HTTP(S) protocols, and 7 on applications (e.g., browsers [39, 40]). Studies related to core network protocols are common, including DNS [32, 36, 51, 52, 61], TCP [19, 31], IP [14, 15, 18, 30, 64, 69], and TLS/HTTPS [21, 37, 38, 50, 57, 76, 83]. We identified 12 studies that measure more than one specific layer, such as a full connection establishment from the initial DNS query to the HTTP request. This indicates that top lists are frequently used to measure DNS, IP, and TLS/HTTPS characteristics, which we will investigate further in §8.

### 3.4. Dependence on Top Lists

In this section, we discuss the dependence of study results on top lists. The "dependent" columns in Table 1 are filled as follows:

- **Dependent (Y):** We identified 45 studies where the results may be affected by the list chosen. These studies typically measure a characteristic over a set of domains from a specific day's list and draw conclusions based on those measurements. Different sets of domains could yield different results.
- **Verification (V):** Seventeen studies use a list only to verify their results. For example, an algorithm might be developed to find domains with a certain property, and a top list is used to check if these domains are popular. In such cases, the algorithm is independent of the list's content.
- **Independent (N):** Eight studies cite and use a list, but their results are not necessarily reliant on it. These papers often use a top list as one of many sources, so changes in the top list would likely not affect the overall results.

### 3.5. Replicability of Studies

Repeatability, replicability, and reproducibility are ongoing concerns in Computer Networks [85, 86] and Internet Measurement [87]. While specifying the date when a top list was downloaded and when measurements were conducted is not sufficient for reproduction, it is an important first step. Table 1 includes two "date" columns indicating whether the list download date or the measurement dates were provided. Out of 69 papers using top lists, only 7 stated the list retrieval date, and 9 stated the measurement date. Only 2 papers provided both, fulfilling basic criteria for reproducibility. Recent investigations suggest that expecting reproducibility in networking may be unrealistic [87, 88]. Two papers explicitly discussed the instability and bias of top lists and used aggregation or enrichment to stabilize results [45, 67].

### 3.6. Summary

Although our survey has a certain level of subjectivity, we consider its broad findings meaningful:
- Top lists are frequently used.
- Many papers' results depend on the list content.
- Few papers indicate precise list download or measurement dates.
- Top lists are commonly used to measure network and security characteristics (DNS, IP, HTTPS/TLS).

We will further investigate how the use of top lists impacts result quality and stability in studies measuring these layers in §8.

### 4. Top Lists Dataset

For the three lists we focus on, we sourced daily snapshots as far back as possible. Many snapshots come from our own archives, and others were shared by the research community [89–91]. Table 2 provides an overview of our datasets and some metrics discussed in §5. For the Alexa list, we have daily snapshots from January 2009 to March 2012 (AL0912) and from April 2013 to April 2018 (AL1318). A partial dataset (AL18) was created after a significant change in January 2018. For the Umbrella list, we have data from 2016 to 2018 (UM1618), and for the Majestic Million list, we cover June 2017 to April 2018.

To facilitate comparative analyses, we created a JOINT dataset spanning June 2017 to the end of April 2018. We also sourced individual daily snapshots from the community and the Internet Archive [92], but only used periods with continuous daily data for our study.

### 5. Structure of Top Lists

In this section, we analyze the structure and nature of the three top lists in our study, including top-level domain (TLD) coverage, subdomain depth, and list intersection.

#### 5.1. Domain Name Depth and Breadth

Understanding the scope of top lists involves assessing TLD coverage and subdomain depth. Per IANA [94, 95], there were 1,543 TLDs as of May 20, 2018. Based on this, we counted valid and invalid TLDs per list. The average coverage of valid TLDs in the JOINT period is approximately 700, covering about 50% of active TLDs. This implies that measurements based on top lists may miss up to 50% of TLDs in the Internet.

At the Top 1k level, we observed different behavior: 105 valid TLDs for Alexa, 50 for Majestic, and only 13 for Umbrella (com/net/org and a few other TLDs). This suggests that DNS administrators of highly queried names prefer well-established TLDs over new gTLDs [96–98].

Invalid TLDs were not found in any Top 1k domains or Alexa Top 1M domains, but a minor count (7 invalid TLDs, resulting in 35 domain names) was found in the Majestic Top 1M. In contrast, the Umbrella Top 1M contained 1,347 invalid TLDs (2.3% of the list), indicating that misconfigured hosts or outdated software can easily include invalid domain names.

Comparing valid and invalid TLDs reveals a structural change in the Alexa list on July 20, 2014. Before that date, Alexa had 206 invalid and 248 valid TLDs. Afterward, invalid TLDs were reduced to nearly zero, and valid TLDs grew from 248 to approximately 800. This confirms that top lists can undergo rapid and unannounced changes, which may significantly influence measurement results.

Subdomain depth is another important property. Base domains offer more breadth and variety, while subdomains provide interesting targets beyond a domain's main web presence. The ratio of base to subdomains represents a breadth/depth trade-off. Table 2 shows the average number of base domains (µBD) per top list. Alexa and Majestic contain almost exclusively base domains, while 28% of the names in the Umbrella list are base domains, emphasizing depth. Table 2 also details the subdomain depth for a single-day snapshot (April 30, 2018) of all lists. The Umbrella list, based on DNS lookups, includes subdomains up to level 33.

We also note that the base domain is usually part of the list when its subdomains are listed. On average, each list contains only a few hundred subdomains whose base domain is not part of the list. Domain aliases (domains with the same second-level domain but different top-level domains, e.g., google.com and google.de) are also analyzed, with a moderate level of ≈5% within various top lists, and only 1.5% for Majestic.

#### 5.2. Intersection between Lists

We next study the intersection between lists, which promises a view of the most popular domains (or websites) in the Internet. Figure 1a shows the intersection between top lists over time during the JOINT period. The intersection is quite small: for the Top 1M domains, Alexa and Majestic share an average of 285k domains during the JOINT duration.