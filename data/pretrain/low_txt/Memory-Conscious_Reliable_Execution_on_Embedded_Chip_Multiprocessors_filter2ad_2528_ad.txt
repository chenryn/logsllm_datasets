### Sensitivity Analysis and Experimental Setup

To conduct a sensitivity analysis, we will use the default values provided. Table 2 lists the application codes used in our experimental evaluation. Our applications are divided into two groups. The first group includes C benchmarks from the SPECFP2000 suite [25] and two FORTRAN benchmarks, for which we manually generated C versions. The second group consists of representative applications from the domain of embedded computing, collected from various sources.

For the SPEC benchmarks, we fast-forwarded the first 1 billion instructions and then simulated the next 400 million instructions. The embedded applications were run to completion. In Table 2, the second column provides a brief description of each benchmark, and the third column indicates the input file or data size used.

### Results

#### Memory Space Requirements

Our first set of results, presented in Figure 13, captures the increase in memory space requirements due to data duplication. We evaluated three different schemes: FULL, NRWD, and CDDR. CDDR is the memory space-conscious approach proposed in this paper. 

The FULL scheme, though not detailed here, increased the original memory space (i.e., without duplication) by nearly 45% on average. Since the data memory requirements of the different applications vary, the results in Figure 13 are normalized with respect to those obtained with the FULL scheme. Our compiler-directed approach (CDDR) saves, on average, about 31.2% of memory space compared to the FULL scheme and 19.7% compared to the NRWD scheme. These results highlight the importance of using compiler analysis to reduce the extra memory space demand caused by data duplication.

#### Execution Cycles

In the next set of experiments, we evaluated the impact of our approach on execution cycles. Figure 14 presents the execution cycles, normalized with respect to the total execution cycles when no duplication is used. It can be seen that all three duplication-based schemes lead to an increase in execution cycles, primarily due to the extra synchronization needed among processors and the degradation in cache behavior caused by additional data elements. Specifically, the increase in execution cycles for the FULL, NRWD, and CDDR schemes are 13.9%, 9.2%, and 3.9%, respectively. These results indicate that minimizing extra memory space requirements through compiler analysis can also benefit performance, although its primary objective is to minimize the extra memory requirements arising from data duplication.

#### Parallelization Strategy

Recall from Section 3.3 that our approach can reuse memory locations from arrays referenced in the current nest (intra-nest optimization) and the previous nest (inter-nest optimization). Figure 15 shows the individual contributions of intra-nest and inter-nest optimizations to the memory reductions achieved by our approach (CDDR) over the FULL scheme. On average, the intra-nest optimization is more effective than the inter-nest optimization (57.3% vs. 42.7%), but both are necessary for achieving the best results.

We then modified the default values of two simulation parameters to perform a sensitivity analysis. First, we varied the number of processors, as shown in Figure 16. The results indicate that the relative improvements achieved by our compiler approach over the FULL and NRWD schemes are consistent across different processor counts. Next, we changed the parallelization scheme. The default strategy was outer-most loop parallelism, where the outermost loop that can run in parallel is parallelized. Figure 17 shows the results when the underlying parallelization strategy does not parallelize any loop in a nest where the outermost loop cannot be run in parallel. This new scheme leads to higher memory space savings, as it is more conservative in parallelization, increasing the opportunities for duplication.

### Related Work

We discuss related work in three categories: reliability efforts on CMP architectures, memory reuse optimizations, and efforts targeting the minimization of transient and permanent errors.

There have been various prior efforts [6, 13, 14, 19, 20, 28, 32] on chip multiprocessors, improving their behavior in terms of memory performance, communication, and reliability. While chip multiprocessors pose new challenges for compiler researchers, they also provide new opportunities compared to traditional parallel architectures. Most prior reliability-oriented work on CMPs has been in the architecture domain [6, 13, 28], whereas our work is compiler-oriented.

Array contraction, proposed by Wolfe [31], optimizes programs in vector architectures. Memory reuse optimization for loop-based codes has been studied in [12, 24, 26, 27]. Wilde and Rajopadhye [29] proposed using a polyhedral model for studying memory reuse. All these prior studies and our approach exploit variable lifetime information extracted by the compiler. The main difference is that we use this information to reduce the additional memory space demand due to enhanced reliability against transient errors, rather than reducing the original memory demand of the application.

Software techniques for fault detection and recovery have been studied by prior research [7, 10, 21, 16, 22]. Rebaudengo et al. [17] and Nicolescu et al. [15] proposed systematic approaches for introducing redundancy into programs to detect errors in both data and code. Their approach demonstrated good error detection capabilities but introduced considerable memory overheads due to full duplication for all variables. In contrast, our approach tries to minimize memory overhead while retaining the same degree of reliability that would be provided by full duplication.

Audet et al. [2] presented an approach for reducing a program's sensitivity to transient errors by modifying the program structure without introducing redundancy. Although this approach introduces almost no extra memory overhead, it cannot provide the same degree of reliability as full duplication. Benso et al. [3] presented similar work that improves the reliability of C code by code reordering. Reis et al. [18] presented a compiler-assisted fault-tolerant approach for VLIW architecture. These approaches focus on performance issues and do not consider memory consumption. In [5], a compiler-based strategy was proposed to reuse memory space requirements due to data duplication, but it targets single-processor machines, whereas our approach targets CMP-based architectures.

### Conclusions

An important problem in embedded chip multiprocessors is the tradeoff between memory space requirements and reliability. While code duplication for improving reliability also requires data duplication, little work has been done to reduce these memory space overheads. This work proposes a compiler-directed, memory-conscious computation duplication scheme for chip multiprocessors. The proposed approach uses dead memory locations to store the extra data elements required for duplicating computations, thereby keeping the extra memory space requirements at a minimum. Our experimental results show that the proposed approach significantly improves over a straightforward data duplication scheme (31.2% on average) and reduces the performance overhead incurred by such schemes.

### Acknowledgments

This work is supported in part by NSF Career Award #0093082 and grants from GSRC and SRC.

### References

[1] R. Allen and K. Kennedy. Optimizing compilers for modern architectures: a dependence-based approach. Morgan Kaufmann Publishers Inc., San Francisco, CA, 2001.
[2] D. Audet, S. Masson, and Y. Savaria. Reducing fault sensitivity of microprocessor-based systems by modifying workload structure. In Proc. IEEE International Symposium in Defect and Fault Tolerant in VLSI Systems, 1998.
[3] A. Benso, S. Chiusano, P. Prinetto, and L. Tagliaferri. A C/C++ source-to-source compiler for dependable applications. In Proc. International Conference on Dependable Systems and Networks, pp. 71-78, June 2000.
[4] F. Catthoor, K. Danckaert, C. Kulkarni, E. Brockmeyer, P. G. Kjeldsberg, T. V. Achteren, and T. Omnes. Data Access and Storage Management for Embedded Programmable Processors. Kluwer Academic Publishers, 2002.
[5] G. Chen, M. Kandemir, and M. Karakoy. Memory space conscious loop iteration duplication for reliable execution. In Proc. the 12th International Static Analysis Symposium, September 2005.
[6] M. Gomaa, C. Scarbrough, T. N. Vijaykumar, and I. Pomeranz. Transient-fault recovery for chip multiprocessors. In Proc. International Symposium on Computer Architecture, 2003.
[7] C. Gong, R. Melhem, and R. Gupta. Loop transformations for fault detection in regular loops on massively parallel systems. IEEE Transaction on Parallel and Distributed Systems, 7(12):1238-1249, December 1996.
[8] M. Gschwind, P. Hofstee, B. Flachs, M. Hopkins, Y. Watanabe, and T. Yamazaki. A novel SIMD architecture for the Cell heterogeneous chip-multiprocessor. Hot Chips 17, August 2005.
[9] R. Hetherington. The UltraSPARC T1 Processor - Power Efficient Throughput Computing. Sun White Paper, December 2005.
[10] K. H. Huang and J. A. Abraham. Algorithm-based fault tolerance for matrix operations. IEEE Transactions on Computers, vol. C-33, pp. 518-528, June 1984.
[11] I. Kadayif, M. Kandemir, and M. Karakoy. An energy-saving strategy based on adaptive loop parallelization. In Proceedings of Design Automation Conference, June 2002.
[12] V. Lefebvre and P. Feautrier. Automatic storage management for parallel programs. Research Report PRiSM 97/8, France, 1997.
[13] S. S. Mukherjee, M. Kontz, and S. K. Reinhardt. Detailed design and evaluation of redundant multi-threading alternatives. In Proceedings of International Symposium on Microarchitecture, 2002.
[14] B. A. Nayfeh and K. Olukotun. Exploring the design space for a shared-cache multiprocessor. In Proc. International Symposium on Computer Architecture, 1994.
[15] B. Nicolescu and Raoul Velazco. Detecting soft errors by a purely software approach: method, tools, and experimental results. In Proc. Design, Automation and Test in Europe Conference and Exhibition, March 2003.
[16] N. Oh and E. J. McCluskey. Error detection by selective procedure call duplication for low energy consumption. IEEE Transactions on Reliability, 51(4):392-402, December 2002.
[17] M. Rebaudengo, M. Sonza Reorda, M. Violante, P. Cheynet, B. Nicolescu, and R. Velazco. System safety through automatic high-level code transformations: an experimental evaluation. In Proc. IEEE Design Automation and Testing in Europe, Munich, Germany, March 13-16, 2001.
[18] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. I. August. SWIFT: Software implemented fault tolerance. In Proc. International Symposium on Code Generation and Optimization, 2005.
[19] J. Renau, K. Strauss, L. Ceze, W. Liu, S. Sarangi, J. Tuck, and J. Torrellas. Energy-efficient thread-level speculation on a CMP. IEEE Micro Special Issue: Micro’s Top Picks from Computer Architecture Conferences, January-February 2006.
[20] S. Richardson. MPOC: A chip multiprocessor for embedded systems. Technical Report HPL-2002-186, HP Labs, 2002.
[21] Amber Roy-Chowdhury. Manual and compiler-assisted methods for generating error-detecting parallel programs. Ph.D. thesis, Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, 1996.
[22] P. P. Shirvani, N. Saxena, and E. J. McCluskey. Software-implemented EDAC protection against SEUs. IEEE Transaction on Reliability, 49(3):273-284, September 2000.
[23] Simics. http://www.simics.com/.
[24] Y. Song, R. Xu, C. Wang, and Z. Li. Data locality enhancement by memory reduction. In Proc. the 15th ACM International Conference on Supercomputing, June 2001.
[25] http://www.spec.org/osg/cpu2000/CFP2000/.
[26] M. Strout, L. Carter, J. Ferrante, and B. Simon. Schedule-independent storage mapping in loops. In Proc. ACM International Conference on Architectural Support for Programming Languages and Operating Systems, October 1998.
[27] P. Unnikrishnan, G. Chen, M. Kandemir, M. Karakoy, and I. Kolcu. Loop transformations for reducing data space requirements of resource-constrained applications. In Proc. International Static Analysis Symposium, June 11-13, 2003.
[28] C. Weaver and T. Austin. A fault-tolerant approach to microprocessor design. In Proc. IEEE International Conference on Dependable Systems and Networks, June 2001.
[29] D. Wilde and S. Rajopadhye. Memory reuse analysis in the polyhedral model. Parallel Processing Letters, 1997.
[30] R. Wilson et al. SUIF: An infrastructure for research on parallelizing and optimizing compilers. SIGPLAN Notices, 29(12):31-37, December 1994.
[31] M. J. Wolfe. High Performance Compilers for Parallel Computing, Addison-Wesley Publishing Company, 1996.
[32] W. Wolf. The future of multiprocessor systems-on-chips. In Proc. Design Automation Conference, 2004.

Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06)  
0-7695-2607-1/06 $20.00 © 2006 IEEE  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021, at 12:27:26 UTC from IEEE Xplore. Restrictions apply.