### Reconstruction Attack Results from Mechanical Turk Surveys

- **Identified**
- **Excluded**
- **Overall**

#### Evaluation Metrics
- (a) Average over all responses.
- (b) Correct by majority vote.
- (c) Accuracy with skilled workers.

**Note:** "Skilled workers" are defined as those who completed at least five MTurk tasks, achieving at least 75% accuracy.

### Figure 9: Reconstruction Attack Results from Mechanical Turk Surveys
- This figure presents the results of reconstruction attacks based on data collected from Mechanical Turk surveys. The performance is evaluated using the metrics mentioned above.

### Figure 10: Reconstruction of an Individual
- This figure illustrates the reconstruction of an individual's image using different models: Softmax, MLP, and DAE.

### Figure 11: White-Box Model Inversion (MI) vs. Classification Accuracy
- This figure shows the white-box model inversion (MI) accuracy compared to classification accuracy on decision trees trained on FiveThirtyEight data, with the sensitive feature at each priority level (l).
- **Key Findings:**
  - The optimal placement of the sensitive feature is at the first level, achieving the best classification accuracy while maintaining MI accuracy only 1% greater than the baseline.
  - The effectiveness of the attack is influenced by the depth at which the sensitive feature appears in the tree. When the feature is near the top or bottom, the attack is less successful.
  - Prioritizing the placement of the sensitive feature impacts model accuracy, but there is an optimal placement that maximizes classification accuracy while keeping inversion accuracy close to the baseline.

### Figure 12: Black-Box Face Reconstruction Attack with Rounding Level r
- This figure demonstrates the black-box face reconstruction attack with varying rounding levels (r). The attack fails to produce a non-empty image at r = 0.1, indicating that rounding can be an effective countermeasure.
- **Key Observations:**
  - Even at r = 0.05, the attack fails to produce a recognizable image, suggesting that black-box facial recognition models can provide useful confidence scores while remaining resistant to reconstruction attacks.

### Understanding Attack Performance
- To understand why attack performance is not monotonic with (l), we counted the number of times each tree used the sensitive feature as a split.
- **Findings:**
  - The frequency of splits using the sensitive feature increases until it reaches its maximum at l = 8, then decreases steadily until l = 12.
  - The difference in split frequency between l = 8 and l = 12 is approximately 6×.
  - Once most features have been used, the training algorithm deems further splitting unnecessary, omitting the sensitive feature from many subtrees.
  - The inversion algorithm cannot do better than baseline guessing for individuals matching paths through these subtrees, making the attack less effective.

### Facial Recognition Attacks
- Our attacks on facial recognition models are based on gradient descent.
- **Possible Defense:**
  - Degrading the quality or precision of the gradient information retrievable from the model.
  - In the black-box setting, this can be achieved by reducing the precision at which confidence scores are reported.
  - **Results:**
    - Rounding the score produced by the softmax model and running the black-box reconstruction attack showed that even at r = 0.05, the attack fails to produce a recognizable image.
    - This suggests that black-box facial recognition models can produce confidence scores that are useful for many purposes while remaining resistant to reconstruction attacks.

### Related Work
- **Machine Learning Applications:**
  - Machine learning techniques are used in various applications, such as intrusion detection, spam filtering, and virus detection.
  - Adversaries can subvert these systems through evasion or mimicry attacks, or by creating false alarms to overwhelm the system.
- **Privacy Implications:**
  - Previous work has explored linear reconstruction attacks, where given some released information, the attacker constructs a system of linear inequalities to infer sensitive features.
  - We extend these attacks to non-linear models and investigate them in realistic settings.
- **Disclosure Attacks:**
  - Sweeney [38] demonstrated re-identification of individuals from anonymized hospital visit data.
  - Narayanan [32] showed that an adversary with prior knowledge can identify records in the anonymized Netflix prize dataset.
  - Wang et al. [39], Sankararaman et al. [34], and Homer et al. [18] considered disclosure attacks on genetic datasets.
- **Partial Disclosure:**
  - Komarova et al. [25] studied partial disclosure, where an adversary infers sensitive features from fixed statistical estimates.
  - Our setting assumes the adversary has access to a statistical estimator as a function, allowing direct predictions about individuals.

### Conclusion
- We demonstrated how confidence information returned by machine learning classifiers enables new model inversion attacks, leading to privacy issues.
- Our algorithms were evaluated over decision trees published on a ML-as-a-service marketplace and showed they can infer sensitive responses without false positives.
- Using a large-scale study on Mechanical Turk, we showed that these algorithms can extract images from facial recognition models that skilled humans can consistently re-identify.
- We explored simple countermeasures and initiated an experimental evaluation of defensive strategies, illustrating trends for future work towards more complete algorithms.

### References
- [1] DeepFace: Closing the Gap to Human-Level Performance in Face Verification. In Conference on Computer Vision and Pattern Recognition (CVPR).
- [2] AT&T Laboratories Cambridge. The ORL database of faces. http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html.
- [3] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can machine learning be secure? In Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pages 16–25. ACM, 2006.
- [4] BigML. https://www.bigml.com/.
- [5] G. Bradski. The OpenCV library. Dr. Dobb’s Journal of Software Tools, Jan. 2000.
- [6] C.-L. Chi, W. Nick Street, J. G. Robinson, and M. A. Crawford. Individualized patient-centered lifestyle recommendations: An expert system for communicating patient-specific cardiovascular risk information and prioritizing lifestyle options. J. of Biomedical Informatics, 45(6):1164–1174, Dec. 2012.
- [7] G. Cormode. Personal privacy vs population privacy: learning to attack anonymization. In KDD, 2011.
- [8] M. Dabbah, W. Woo, and S. Dlay. Secure authentication for face recognition. In IEEE Symposium on Computational Intelligence in Image and Signal Processing, pages 121–126, April 2007.
- [9] C. Dillow. Augmented identity app helps you identify strangers on the street. Popular Science, Feb. 23 2010.
- [10] I. Dinur and K. Nissim. Revealing information while preserving privacy. In PODS, 2003.
- [11] C. Dwork. Differential privacy. In ICALP. Springer, 2006.
- [12] C. Dwork, F. McSherry, and K. Talwar. The price of privacy and the limits of lp decoding. In STOC, 2007.
- [13] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In USENIX Security Symposium, pages 17–32, 2014.
- [14] Fredrikson, M. and Jha, S. and Ristenpart, T. Model inversion attacks and basic countermeasures (Technical Report). Technical report, 2015.
- [15] I. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra, F. Bastien, and Y. Bengio. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013.
- [16] Google. Prediction API. https://cloud.google.com/prediction/.
- [17] W. Hickey. FiveThirtyEight.com DataLab: How Americans like their steak. http://fivethirtyeight.com/datalab/how-americans-like-their-steak/, May 2014.
- [18] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays. PLOS Genetics, 2008.
- [19] G. Huang, H. Lee, and E. Learned-Miller. Learning hierarchical representations for face verification with convolutional deep belief networks. In Computer Vision and Pattern Recognition (CVPR), June 2012.
- [20] International Warfarin Pharmacogenetic Consortium. Estimation of the warfarin dose with clinical and pharmacogenetic data. New England Journal of Medicine, 360(8):753–764, 2009.
- [21] Kairos AR, Inc. Facial recognition API. https://developer.kairos.com/docs.
- [25] T. Komarova, D. Nekipelov, and E. Yakovlev. Estimation of Treatment Effects from Combined Data: Identification versus Data Security. NBER volume Economics of Digitization: An Agenda, To appear.
- [26] Lambda Labs. Facial recognition API. https://lambdal.com/face-recognition-api.
- [27] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 609–616, New York, NY, USA, 2009. ACM.
- [28] N. Li, W. Qardaji, D. Su, Y. Wu, and W. Yang. Membership privacy: A unifying framework for privacy definitions. In Proceedings of ACM CCS, 2013.
- [29] G. Loukides, J. C. Denny, and B. Malin. The disclosure of diagnosis codes can breach research participants’ privacy. Journal of the American Medical Informatics Association, 17(3):322–327, 2010.
- [30] D. Lowd and C. Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641–647. ACM, 2005.
- [31] Microsoft. Microsoft Azure Machine Learning.
- [32] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. In IEEE Symposium on Security and Privacy, pages 111–125, 2008.
- [33] J. Prince. Social science research on pornography. http://byuresearch.org/ssrp/downloads/GSShappiness.pdf.
- [34] S. Sankararaman, G. Obozinski, M. I. Jordan, and E. Halperin. Genomic privacy and limits of individual detection in a pool. Nature Genetics, 41(9):965–967, 2009.
- [35] C. Savage. Facial scanning is making gains in surveillance. The New York Times, Aug. 21 2013.
- [36] SkyBiometry. Facial recognition API. https://www.skybiometry.com/Documentation#faces/recognize.
- [37] T. W. Smith, P. Marsden, M. Hout, and J. Kim. General social surveys, 1972-2012. National Opinion Research Center [producer]; The Roper Center for Public Opinion Research, University of Connecticut [distributor], 2103.
- [38] L. Sweeney. Simple demographics often identify people uniquely. 2000.
- [39] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou. Learning your identity and disease from research papers: information leaks in genome wide association studies. In CCS, 2009.
- [22] S. P. Kasiviswanathan, M. Rudelson, and A. Smith. The power of linear reconstruction attacks. In SODA, 2013.
- [23] S. P. Kasiviswanathan, M. Rudelson, A. Smith, and J. Ullman. The price of privately releasing contingency tables and the spectra of random matrices with correlated rows. In STOC, 2010.
- [24] J. Klontz, B. Klare, S. Klum, A. Jain, and M. Burge. Open source biometric recognition. In IEEE International Conference on Biometrics: Theory, Applications and Systems, pages 1–8, Sept 2013.