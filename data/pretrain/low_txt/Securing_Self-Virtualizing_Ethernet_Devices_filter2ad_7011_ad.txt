### Flow Control Mechanism and Pause Frame Handling

The flow control mechanism of the device receives pause frames when flow control is enabled; otherwise, the device silently drops them. In our setup, we disable the flow control feature of Intel NICs installed in the bridge machine and configure them to forward pause frames up to the OS, where they should be processed by the bridge and ebtables. This is achieved by enabling the Pass MAC Control Frames (PMCF) bit of the MAC Flow Control (MFLCN) register, as described in section 3.7.7.2 of the Intel 82599 data-sheet [42].

### Ring Buffer Exhaustion and Pause Frame Generation

Some SR-IOV devices can monitor a VF’s ring buffer and automatically generate pause frames when it is exhausted. In such scenarios, the generated pause frames will have the source MAC address of the PF and will not be recognized by the VANFC. We argue that such automatic pause frame generation should be disabled in any SR-IOV-based setup, regardless of whether the VMs are trusted.

Since the VM fully controls the VF’s ring buffer, a malicious VM can modify its software stack (e.g., the VF device driver) to manipulate the ring buffer, causing the SR-IOV device to generate pause frames on the VM’s behalf. These pause frames will reach the external switch, which will stop its transmissions to the host and other VMs, leading to the same attack vector.

Even if all VMs are trusted, automatic generation of pause frames on VF ring buffer exhaustion can be problematic. For example, a VM that does not have enough CPU resources to process all incoming traffic may exhaust the VF’s ring buffer. Sending pause frames to the switch may help this VM process the buffer but will halt traffic to other VMs. Therefore, to keep the SR-IOV device secure, an SR-IOV NIC should not automatically send pause frames when the VF’s ring buffer is exhausted, regardless of whether the VM is trusted.

### Monitoring VF Ring Buffers

Monitoring VF ring buffers can be useful for keeping the Ethernet network lossless and avoiding dropped frames. We propose that the SR-IOV device monitor ring buffers but, instead of automatically generating pause frames on ring buffer exhaustion, it should notify the hypervisor. The hypervisor, unlike the device, could then carefully consider whether the VM is malicious or simply slow. If the VM is simply slow, the hypervisor could give it a scheduling boost or assign more CPU resources, thereby giving it a chance to process its ring buffer before it fills up. We plan to explore this approach in future work.

### Evaluating VANFC

We evaluate VANFC in several scenarios:

1. **Baseline Scenario**: An unprotected system with no attack during the test. We measure the system’s baseline throughput and latency.
2. **Baseline System Under Attack**: The same unprotected system, but here VM2 runs the attack during the test, sending pause frames at a constant rate of 150 frames/sec. We measure the effectiveness of the attack on an unprotected system.
3. **Protected System Scenario**: VANFC replaces the unprotected system, and VM2 does not perform any attack during the test. We use this scenario to measure the performance overhead introduced by VANFC compared to the baseline.
4. **Protected System Under Attack**: We also use VANFC, but here the attacker VM2 sends pause frames at a constant rate of 150 frames/sec. We verify that VANFC indeed overcomes the attack.

All tests are performed on a 10GbE network with the same environment, equipment, and methodology as described in Section 4.1.

As explained in Section 6.2, to filter malicious pause frames, our solution uses a software-based filtering device, which adds a constant latency of 55µs. A production solution would filter these frames in hardware, eliminating this latency overhead. Thus, in latency-oriented performance tests of the VANFC, we reduce 55µs from the results.

### Evaluation Tests

To evaluate the performance of the described scenarios, we test throughput and latency using iperf and netperf. Additionally, we configure the Apache 2.4.6 web server on VM1 to serve two files: one sized 1KB and one sized 1MB. We use the `ab` benchmark tool from the client to test the performance of the web server on VM1.

VM1 also runs the memcached server version 1.4.14, installed from the Ubuntu repository with the default configuration. On the client, we run the `memslap` benchmark tool, part of the libmemcached client library, to measure the performance of the memcached server on VM1.

Figure 9 displays normalized results of the performed tests, grouped into two categories: throughput-oriented and latency-oriented. Throughput-oriented tests include iperf running pure TCP stream and Apache serving a 1MB file. These tests are limited by the 10GbE link bandwidth. During the tests, the client and server CPUs are almost idle.

From Figure 9, we conclude that VANFC completely blocks VM2’s attack and introduces no performance penalty.

### Necessity of Flow Control

One might argue that flow control is not required for proper functionality of high-level protocols such as TCP. However, many studies have shown that relying solely on TCP for flow control leads to increased CPU utilization, which is undesirable in public cloud environments where users pay for computational resources. Higher CPU utilization results in higher charges.

Certain traffic patterns in high-bandwidth, low-latency data center environments may suffer from catastrophic TCP throughput collapse, known as the incast problem. Ethernet flow control functionality, together with congestion control protocols, can mitigate the incast problem, improving TCP performance.

As part of recent efforts to converge current network infrastructures, many existing protocols have been implemented over Ethernet, such as Remote DMA over Converged Ethernet (RoCE). RoCE significantly reduces CPU utilization compared to TCP. Studies show that RoCE cannot function properly without flow control. Disabling flow control would cause less effective resource utilization and lead to higher costs for cloud customers and organizations deploying SR-IOV. Conversely, securing SR-IOV against flow control attacks would make it possible for SR-IOV and flow control to coexist, providing the performance benefits of both without compromising security.

### Discussion

**Notes on Implementation**: VANFC can be implemented as part of an SR-IOV device equipped with an embedded Ethernet switch or in the edge Ethernet switch, by programming the edge switch to filter flow control frames from VFs’ MAC addresses. Adding VANFC functionality to the NIC requires less manufacturing effort and is more convenient and cheaper than replacing an edge switch. However, in large-scale virtualization deployments, upgrading multiple SR-IOV devices connected to a single switch requires considerable resources.

We argue that the proper implementation of the solution to the described problem is in the SR-IOV NIC, not in the edge Ethernet switch. The problem is strictly related to the virtualization platform and caused by a design flaw in the SR-IOV NIC’s internal switching implementation. Mitigating the problem in the edge switch, an external device, would force the edge switch to learn about each VF’s MAC address and distinguish PFs from VFs, coupling the edge switch too closely with the NICs.

**VEB and VEPA**: Another important security aspect of SR-IOV is VM-to-VM traffic. In SR-IOV devices with an embedded VEB switch, VM-to-VM traffic does not leave the host network device and is not visible to the external edge switch, which enforces the security policy on the edge of the network. To make all VM traffic visible to the external switch, the VEB switch should act as a VEPA and send all VM traffic to the adjacent switch.

A properly configured Ethernet switch and the use of a VEPA device can enforce a security policy (ACL, port security) on malicious VM traffic and prevent most L2 attacks. However, while VEPA solves many manageability and security issues, it does not address the flow control attack we presented earlier. This is because VEPA still shares the same single link between multiple untrusted guests and the host and does not manage flow control per VF. Thus, a VEPA extension should not be considered for the solution, and the problem should be solved in the SR-IOV NIC.

### Related Work

Several recent works have discussed the security of self-virtualizing devices. P´ek et al. [61] described a wide range of attacks on host and tenant VMs using directly assigned devices. They performed successful attacks on PCI/PCIe configuration space, memory-mapped I/O, and by injecting interrupts. Most of these attacks can be blocked by a fix in the hypervisor or proper hardware configuration.

Richter et al. [68] showed how a malicious VM with a directly attached VF can perform DoS attacks on other VMs sharing the same PCIe link by overloading its own Memory Mapped I/O (MMIO) resources and flooding the PCIe link with write request packets. As the authors mention, this attack can be mitigated by using the QoS mechanisms defined by the PCIe standard [59].