### Productionization Challenges and Comparisons

We observe significant alignment between the productionization challenges we faced and those discussed by Facebook [10]. The differences in our approaches primarily stem from the underlying technologies employed.

### Performance Metrics and Latency Analysis

The following timestamps and latency metrics provide a detailed view of the system's performance:

- **RMA Command Executor Timestamps:**
  - 18:00, 18:10, 18:20, 18:30, 18:40, 18:50
  - Normalized Latency: 5% GETs, 50% GETs, 95% GETs
  - Latency (us): 0, 150, 300, 450, 600
  - GET-50p, GET-99p, SET-50p, SET-99p

- **CliqueMap + 1RMA GET Latencies:**
  - 5% GETs, 50% GETs, 95% GETs
  - CPU*s/s: 0, 2K, 4K, 6K, 8K
  - Data Sizes: 32B, 256B, 2KB, 16KB
  - Latency (us): 0, 500
  - GET-50p, GET-99p, SET-50p, SET-99p

### Design Insights and Comparisons

From systems like Pilaf and others [36, 39], we adopted the key insight of combining RMA and RPC, along with specific techniques such as self-validation. Our design also shares similarities with RPC-based systems. For example, the combination of Pony Express and CliqueMap (SCAR, §6.3) resembles message-oriented rather than strictly RMA-oriented lookup strategies, akin to those in HERD [23]. Similarly, SCAR itself is similar to highly-specialized RPCs [8, 22].

### Availability and Replication Strategies

In evaluating solutions for availability and replication, we chose quoruming over the primary/backup architecture used by HydraDB [40] and FaRM [18]. This choice allowed us to leverage preferred backends (§5). To minimize overheads, we opted for client-side quoruming instead of server-side indirection, avoiding serialization points (e.g., ZAB [21], CR [38]) and inter-replica communication (e.g., ABD [9, 30], Paxos [28] for reads; Hermes [26], CR [37, 38] for writes).

### User Privileges and Transport Mechanisms

To ensure that no user required special privileges to operate CliqueMap, we avoided Unreliable Connected/Unreliable Datagram transports, as used by HERD [23] and FaSST [24]. Instead, we relied on indirection through Pony Express to avoid binding to high-privilege network APIs. Low default privilege levels also made it challenging to achieve predictable CPU and NIC siloing, despite the performance benefits demonstrated by MICA [29].

### Experience and Conclusions

CliqueMap underscores the importance of a comprehensive system design that balances performance, robustness, and efficiency. By deploying high-performance, low-programmability RMA primitives on critical performance paths and agile but less-efficient RPCs for other functionalities, we meet the demands of serving workloads with stringent performance requirements while maintaining the feature-richness expected in RPC-based systems.

### Key Takeaways

1. **Leverage RPC in Composition with RMA:**
   - Maintain post-deployment agility by using RPC in conjunction with RMA. CliqueMap’s lookup path is the only heavily tailored RMA path, making the system adaptable to changes.
   - Embrace RPCs for control, management actions, and dataplane operations, allowing for design refinement and support for new features like planned maintenance, diverse eviction algorithms, compression, and new mutation types.

2. **Enable Multi-Language Software Ecosystems:**
   - Focus on supporting multiple programming languages to foster growth and adoption. Initially, CliqueMap was limited to C++, which hindered its adoption in multilingual environments. Our current approach, using named pipes to subprocesses, meets performance requirements but is not optimal. Future innovations in this area are needed to explore trade-offs between maintenance, complexity, efficiency, and performance.

3. **Do Not Compromise Memory Efficiency:**
   - Early versions of CliqueMap were designed to provision for peak DRAM usage, leading to a trade-off between faster lookups and higher DRAM usage. By investing in memory efficiency while preserving lookup performance, CliqueMap became more appealing.

4. **Simplify Design with Self-Validating Server Responses and Client Retries:**
   - Combining self-validating responses with client-side retries simplifies the design, making clients resilient to various hazards across all layers of the stack. This approach can handle RMA operation failures, data races, backend configuration changes, and wire protocol format changes. However, GET forward progress is not guaranteed, but this can be managed through tuning.

5. **Programmable NICs Offer Advantages Through Specialization:**
   - Software NICs offer continuous innovation and post-deployment customization, enabling optimizations like Scan-and-Read (§6.3), which saves an entire RTT. The expressivity and reprogrammability of software NICs give them an edge over hardware-only designs and help bridge gaps caused by heterogeneous hardware deployments.

### Recommendations for Future Infrastructure

Designers of future infrastructure should:
- Maintain agility.
- Sacrifice neither common-case performance nor DRAM efficiency.
- Enable customers' practical needs.
- Adapt to the underlying technology landscape.

These principles underlie CliqueMap’s design, execution, and evolution, resulting in a production-friendly and practically useful system.

### Ethical Considerations

This work raises no ethical concerns.

### Acknowledgments

We thank early reviewers Jeff Mogul, Jason Hsueh, Jeff Hightower, and Philip Wells, as well as the anonymous SIGCOMM reviewers and our shepherd, Nathan Bronson, for their valuable feedback. We also thank the production, serving, and support teams at Google, including the Pony Express, 1RMA, Ads, Geo, and Travel teams, for their partnership and contributions.

### References

[1] Chelsio Terminator 6 NICs. https://www.chelsio.com/terminator-6-asic/
[2] Google’s Application Layer Transport Security. https://cloud.google.com/security/encryption-in-transit/application-layer-transport-security
[3] Marvell FastLinQ 41000 Series Ethernet NICs. https://www.marvell.com/products/ethernet-adapters-and-controllers/41000-ethernet-adapters.html
[4] Memcached. http://memcached.org/
[5] Nvidia Mellanox Connect-X NICs. https://www.nvidia.com/en-us/networking/ethernet-adapters/
[6] RDMA Core Userspace Libraries (libibverbs). https://github.com/linux-rdma/rdma-core
[7] Marcos K Aguilera, Kimberly Keeton, Stanko Novakovic, and Sharad Singhal. 2019. Designing far memory data structures: Think outside the box. In Proceedings of the Workshop on Hot Topics in Operating Systems (HotOS’19). 120–126.
[8] Emmanuel Amaro, Zhihong Luo, Amy Ousterhout, Arvind Krishnamurthy, Aurojit Panda, Sylvia Ratnasamy, and Scott Shenker. 2020. Remote Memory Calls. In Proceedings of the 19th ACM Workshop on Hot Topics in Networks (HotNets’20). 38–44.
[9] Hagit Attiya, Amotz Bar-Noy, and Danny Dolev. 1995. Sharing memory robustly in message-passing systems. Journal of the ACM (JACM) 42, 1 (1995), 124–142.
[10] Benjamin Berg, Daniel S. Berger, Sara McAllister, Isaac Grosof, Sathya Gunasekar, Jimmy Lu, Michael Uhlar, Jim Carrig, Nathan Beckmann, Mor Harchol-Balter, and Gregory R. Ganger. 2020. The CacheLib Caching Engine: Design and Experiences at Scale. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI’20). 753–768.
[11] Jeff Bonwick. 1994. The Slab Allocator: An Object-Caching Kernel. In USENIX Summer 1994 Technical Conference (USTC’94).
[12] Eric Brewer. 2017. Spanner, TrueTime and the CAP Theorem. Technical Report. https://research.google/pubs/pub45855/
[13] Mike Burrows. 2006. The Chubby lock service for loosely-coupled distributed systems. In Proceedings of the 7th symposium on Operating systems design and implementation (OSDI’06). 335–350.
[14] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, and Robert E Gruber. 2008. Bigtable: A distributed storage system for structured data. ACM Transactions on Computer Systems (TOCS) 26, 2 (2008), 1–26.
[15] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, J. J. Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang, and Dale Woodford. 2013. Spanner: Google’s Globally Distributed Database. ACM Transactions on Computer Systems (TOCS) 31, 3 (2013), 1–22.
[16] Jeffrey Dean. 2010. Evolution and future directions of large-scale storage and computation systems at Google. (2010). https://research.google/pubs/pub44877/
[17] Aleksandar Dragojević, Dushyanth Narayanan, Orion Hodson, and Miguel Castro. 2014. FaRM: Fast Remote Memory. In Proceedings of the Eleventh USENIX Symposium on Networked Systems Design and Implementation (NSDI’14). 401–414.
[18] Aleksandar Dragojević, Dushyanth Narayanan, Edmund B Nightingale, Matthew Renzelmann, Alex Shamis, Anirudh Badam, and Miguel Castro. 2015. No compromises: distributed transactions with consistency, availability, and performance. In Proceedings of the 25th Symposium on Operating Systems Principles (SOSP’15). 54–70.
[19] David K Gifford. 1979. Weighted voting for replicated data. In Proceedings of the seventh ACM Symposium on Operating Systems Principles (SOSP’79). 150–162.
[20] Maurice Herlihy, Victor Luchangco, and Mark Moir. 2003. Obstruction-free synchronization: Double-ended queues as an example. In 23rd International Conference on Distributed Computing Systems, 2003. Proceedings. 522–529.
[21] Flavio P Junqueira, Benjamin C Reed, and Marco Serafini. 2011. Zab: High-performance broadcast for primary-backup systems. In 41st International Conference on Dependable Systems & Networks (DSN’11). 245–256.
[22] Anuj Kalia, Michael Kaminsky, and David Andersen. 2019. Datacenter RPCs can be General and Fast. In Proceeding of Sixteenth USENIX Symposium on Networked Systems Design and Implementation. 1–16.
[23] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2014. Using RDMA efficiently for key-value services. In Proceedings of the 2014 Conference of ACM SIGCOMM. 295–306.
[24] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2016. FaSST: Fast, Scalable and Simple Distributed Transactions with Two-Sided RDMA Datagram RPCs. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI’16). 185–201.
[25] Svilen Kanev, Juan Pablo Darago, Kim Hazelwood, Parthasarathy Ranganathan, Tipp Moseley, Gu-Yeon Wei, and David Brooks. 2015. Profiling a warehouse-scale computer. In Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA’15). 158–169.
[26] Antonios Katsarakis, Vasilis Gavrielatos, MR Siavash Katebzadeh, Arpit Joshi, Aleksandar Dragojevic, Boris Grot, and Vijay Nagarajan. 2020. Hermes: a Fast, Fault-Tolerant and Linearizable Replication Protocol. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS’20). 201–217.
[27] Leslie Lamport. 1994. The temporal logic of actions. ACM Transactions on Programming Languages and Systems (TOPLAS) 16, 3 (1994), 872–923.
[28] Leslie Lamport. 1998. The Part-Time Parliament. ACM Transactions on Computer Systems (TOCS) 16, 2 (1998), 133–169.
[29] Hyeontaek Lim, Dongsu Han, David G Andersen, and Michael Kaminsky. 2014. MICA: A holistic approach to fast in-memory key-value storage. In 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI’14). 429–444.
[30] Nancy A Lynch and Alexander A Shvartsman. 1997. Robust emulation of shared memory using dynamic quorum-acknowledged broadcasts. In Proceedings of IEEE 27th International Symposium on Fault Tolerant Computing. 272–281.
[31] Michael Marty, Marc de Kruijf, Jacob Adriaens, Christopher Alfeld, Sean Bauer, Carlo Contavalli, Michael Dalton, Nandita Dukkipati, William C. Evans, Steve Gribble, Nicholas Kidd, Roman Kononov, Gautam Kumar, Carl Mauer, Emily Musick, Lena Olson, Erik Rubow, Michael Ryan, Kevin Springborn, Paul Turner, Valas Valancius, Xi Wang, and Amin Vahdat. 2019. Snap: A Microkernel Approach to Host Networking. In Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP’19). 399–413.
[32] Nimrod Megiddo and Dharmendra S. Modha. 2003. ARC: A Self-Tuning, Low Overhead Replacement Cache. In Proceedings of the 2nd USENIX Conference on File and Storage Technologies (FAST ’03). 115–130.
[33] Christopher Mitchell, Yifeng Geng, and Jinyang Li. 2013. Using One-Sided RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store. In 2013 USENIX Annual Technical Conference (ATC’13). 103–114.
[34] Arjun Singhvi, Aditya Akella, Dan Gibson, Thomas F. Wenisch, Monica Wong-Chan, Sean Clark, Milo M. K. Martin, Moray McLaren, Prashant Chandra, Rob Cauble, Hassan M. G. Wassel, Behnam Montazeri, Simon L. Sabato, Joel Scherpelz, and Amin Vahdat. 2020. 1RMA: Re-Envisioning Remote Memory Access for Multi-Tenant Datacenters. In Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM ’20). 708–721.
[35] Patrick Stuedi, Animesh Trivedi, and Bernard Metzler. 2012. Wimpy nodes with 10GbE: leveraging one-sided operations in soft-RDMA to boost memcached. In 2012 USENIX Annual Technical Conference (ATC’12). 347–353.
[36] Maomeng Su, Mingxing Zhang, Kang Chen, Zhenyu Guo, and Yongwei Wu. 2017. RFP: When RPC is Faster than Server-Bypass with RDMA. In Proceedings of the Twelfth European Conference on Computer Systems (EuroSys ’17). 1–15.
[37] Jeff Terrace and Michael J Freedman. 2009. Object Storage on CRAQ: High-Throughput Chain Replication for Read-Mostly Workloads. In 2009 USENIX Annual Technical Conference. 1–16.
[38] Robbert van Renesse and Fred B. Schneider. 2004. Chain Replication for Supporting High Throughput and Availability. In Proceedings of the 6th Conference on Symposium on Operating Systems Design and Implementation (OSDI’04). 7.
[39] Yandong Wang, Xiaoqiao Meng, Li Zhang, and Jian Tan. 2014. C-hint: An effective and reliable cache management for RDMA-accelerated key-value stores. In Proceedings of the ACM Symposium on Cloud Computing (SoCC’14). 1–13.
[40] Yandong Wang, Li Zhang, Jian Tan, Min Li, Yuqing Gao, Xavier Guerin, Xiaoqiao Meng, and Shicong Meng. 2015. HydraDB: a resilient RDMA-driven key-value middleware for in-memory cluster computing. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC’15). 1–11.
[41] Juncheng Yang, Yao Yue, and K. V. Rashmi. 2020. A large scale analysis of hundreds of in-memory cache clusters at Twitter. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20). 191–208.