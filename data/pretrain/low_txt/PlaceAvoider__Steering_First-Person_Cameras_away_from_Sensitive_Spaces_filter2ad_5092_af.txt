### Opposite Scenario and Intention-Based Access Control

In the opposite scenario, where photos are taken in the background, intention-based access control does not provide a suitable defense. This is because such controls are typically designed to manage intentional actions, rather than passive or unintentional data collection.

### Imaging Defenses

There have been few systems similar to PlaceAvoider that aim to control the collection of imagery. Truong et al. [53] describe a third-party system that can detect and disable offending CCD or CMOS cameras in a space using directed pulsing light. While this system provides an interesting and useful way to prevent camera use, it requires specialized and dedicated infrastructure to be installed in each sensitive area. In contrast, PlaceAvoider integrates similar functionality directly within the camera, making it more practical for widespread use.

The DARKLY system [26] presents a novel approach to add a privacy-protection layer to systems where untrusted applications have access to camera resources. By integrating OpenCV within device middleware, DARKLY controls the type and amount of image content available to applications, applying the principle of least privilege to image information. For example, a policy may permit an application to access only the number of faces detected in any image, regardless of context. When invoking the camera with DARKLY, the application would receive only this parameterized image information. PlaceAvoider, on the other hand, enforces policies based on the context derived from image content. Although they solve different problems, DARKLY and PlaceAvoider could potentially be combined, with analysis by PlaceAvoider informing transformations applied by DARKLY.

### Inferring Location from Images

Inferring location or user activity from smartphone sensors is an active research area. CenceMe [39] uses ambient audio and movement information to infer activity and conversation type but relies on GPS for location, without using recorded images for classification. CrowdSense@Place [9] employs computer vision techniques (alongside audio processing) to classify locations into general categories (e.g., home, workplace, shops). While this system was not evaluated for specific scene recognition as PlaceAvoider does, it could be useful for identifying types of locations with high privacy risks.

Much of this work is in the computer vision domain, particularly for robotics applications. Robot topological localization techniques often require specialized cameras that are incompatible with the form factors used by phones and lifelogging devices. Se et al. [49], [50] use a Triclops stereo vision camera for robot localization, while Ulrich and Nourbakhsh [54] use a specialized 360-degree panoramic camera. Even without such specialized cameras, localization techniques for robots often leverage conditions that are unrealistic for mobile devices. Ledwich and Williams [34] impose strict constraints on training images that are impractical for our proposed applications. Kosecka and Li [31] and Jensfelt et al. [27] also require precise odometry or dead-reckoning sensors, which are not standard in mobile devices. Thus, these robotics solutions are not directly applicable to the dynamic movement of mobile devices.

Recent work has studied geo-location in consumer images, often limited to highly photographed outdoor landmarks with large datasets [37], [51], [20], [36]. An abstraction of absolute camera location seeks to classify images based on the type of scene (e.g., indoors vs. outdoors). Oliva and Torralba [41] label scenes according to the 'gist' of the image by analyzing the distribution of spatial image frequencies. Subsequent work aims for finer granularity by classifying the type of scene at a high level (e.g., living room vs. bedroom) [56], [45]. Most of this work has focused on well-composed, deliberately-taken images, although recent papers have considered first-person video. This includes selecting important moments from raw first-person video [35], jointly recognizing and modeling common objects [18], inferring the camera owner’s actions from object interaction [43], and even using first-person video to collect psychological data about people’s visual systems in naturalistic environments [3]. None of this work considers privacy issues as we do here, though we plan to leverage some of these approaches to assign semantic labels with privacy meanings.

### Indoor Localization and Positioning

The computational expense of inferring camera location using computer vision applied to images may be mitigated through localization and positioning methods to reduce search spaces. Hightower [23] provides a comprehensive survey of localization and positioning approaches. Most of these systems require external infrastructure (e.g., audio or electromagnetic beacons) or a dense constellation of cooperating devices [47], and often need a priori knowledge of the environment (e.g., maps). Some approaches rely less on infrastructure and operate in a peer-to-peer ad hoc manner. Kourogi [32] developed a system that requires no infrastructure but uses sensors more sophisticated than those in consumer mobile devices. Woodman et al. [55] developed a system that performs effective localization but requires a sensor array attached to an individual’s foot. As discussed in Section I, camera location and the location of image content are not necessarily the same; the PlaceAvoider classifier is necessary to enforce privacy policies based on image content.

### Conclusion

As cameras become more pervasive and the background collection of imagery becomes more popular, people’s privacy is increasingly at risk. We have presented an approach for detecting potentially sensitive images taken from first-person cameras, even in the face of motion, blur, and occlusion, by recognizing physical areas where sensitive images are likely to be captured. Camera owners can review images from these sensitive regions to avoid privacy leaks. We believe this is an important first step in this increasingly important area of privacy research.

Our results are promising and may be sufficient for some applications, but our classifier accuracies are likely insufficient for others. The problem of highly accurate indoor visual place classification from first-person imagery remains open. We plan to continue investigating computer vision techniques that estimate the meanings of images to better identify potentially sensitive photo content and situations. We also plan to investigate privacy concerns of bystanders, as devices like Google Glass become more common in society, and bystanders need ways to actively protect their own privacy.

### Acknowledgment

This material is based upon work supported by the National Science Foundation under grants CNS-1016603, CNS-1252697, and IIS-1253549. This work was also partially funded by the Office of the Vice Provost of Research at Indiana University Bloomington through the Faculty Research Support Program. We thank the anonymous reviewers for their valuable comments and John McCurley for his editorial help.

### References

[1] A. Allen, “Dredging up the past: Lifelogging, memory, and surveillance,” The University of Chicago Law Review, pp. 47–74, 2008.
[2] S. Arya and D. Mount, “Approximate nearest neighbor queries in fixed dimensions,” in ACM Symposium on Discrete Algorithms, 1993.
[3] S. Bambach, D. Crandall, and C. Yu, “Understanding embodied visual attention in child-parent interaction,” in Joint IEEE International Conference on Development and Learning and and on Epigenetic Robots, 2013.
[4] J. Brassil, “Technical challenges in location-aware video surveillance privacy,” in Protecting Privacy in Video Surveillance. Springer, 2009, pp. 91–113.
[5] S. Bugiel, L. Davi, A. Dmitrienko, T. Fischer, and A.-R. Sadeghi, “XManDroid: A new Android evolution to mitigate privilege escalation attacks,” Technische Universit¨at Darmstadt, Technical Report TR-2011-04, Apr. 2011.
[6] S. Bugiel, L. Davi, A. Dmitrienko, T. Fischer, A.-R. Sadeghi, and B. Shastry, “Towards taming privilege-escalation attacks on Android,” in 19th Annual Network & Distributed System Security Symposium (NDSS), Feb. 2012.
[7] J. Chaudhari, S. Cheung, and M. Venkatesh, “Privacy protection for life-log video,” in IEEE Workshop on Signal Processing Applications for Public Security and Forensics, 2007, pp. 1–5.
[8] W. Cheng, L. Golubchik, and D. Kay, “Total recall: are privacy changes inevitable?” in ACM Workshop on Continuous Archival and Retrieval of Personal Experiences, 2004, pp. 86–92.
[9] Y. Chon, N. Lane, F. Li, H. Cha, and F. Zhao, “Automatically characterizing places with opportunistic crowdsensing using smartphones,” in ACM Conference on Ubiquitous Computing, 2012, pp. 481–490.
[10] M. Conti, V. T. N. Nguyen, and B. Crispo, “Crepe: context-related policy enforcement for Android,” in International Conference on Information Security, 2011, pp. 331–345.
[11] N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2005, pp. 886–893.
[12] M. Dietz, S. Shekhar, Y. Pisetsky, A. Shu, and D. S. Wallach, “Quire: Lightweight provenance for smart phone operating systems,” CoRR, vol. abs/1102.2445, 2011.
[13] M. Douze, H. Jegou, H. Sandhawalia, L. Amsaleg, and C. Schmid, “Evaluation of gist descriptors for web-scale image search,” in ACM International Conference on Image and Video Retrieval, 2009.
[14] K. Duan, D. Batra, and D. Crandall, “A Multi-layer Composite Model for Human Pose Estimation,” in British Machine Vision Conference, 2012.
[15] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung, P. McDaniel, and A. N. Sheth, “TaintDroid: An information-flow tracking system for realtime privacy monitoring on smartphones,” in USENIX Conference on Operating Systems Design and Implementation, 2010, pp. 1–6.
[16] W. Enck, M. Ongtang, and P. McDaniel, “Mitigating Android software misuse before it happens,” Pennsylvania State University, Tech. Rep. NAS-TR-0094-2008, 2008.
[17] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin, “Liblinear: A library for large linear classification,” The Journal of Machine Learning Research, vol. 9, pp. 1871–1874, 2008.
[18] A. Fathi, X. Ren, and J. Rehg, “Learning to recognize objects in egocentric activities,” in IEEE Conference on Computer Vision and Pattern Recognition, 2011.
[19] A. P. Felt, H. J. Wang, A. Moshchuk, S. Hanna, and E. Chin, “Permission re-delegation: attacks and defenses,” in Proceedings of the USENIX Conference on Security, 2011, pp. 22–22.
[20] J.-M. Frahm, P. Georgel, D. Gallup, T. Johnson, R. Raguram, C. Wu, Y.-H. Jen, E. Dunn, B. Clipp, and S. Lazebnik, “Building Rome on a Cloudless Day,” in European Conference on Computer Vision, 2010.
[21] A. Gionis, P. Indyk, R. Motwani et al., “Similarity search in high dimensions via hashing,” in IEEE International Conference on Very Large Data Bases, vol. 99, 1999, pp. 518–529.
[22] J. Halderman, B. Waters, and E. Felten, “Privacy management for portable recording devices,” in ACM Workshop on Privacy in the Electronic Society, 2004, pp. 16–24.
[23] J. Hightower and G. Borriello, “Location systems for ubiquitous computing,” Computer, vol. 34, no. 8, pp. 57–66, Aug. 2001.
[24] S. Hodges, L. Williams, E. Berry, S. Izadi, J. Srinivasan, A. Butler, G. Smyth, N. Kapur, and K. Wood, “Sensecam: a retrospective memory aid,” in ACM Conference on Ubiquitous Computing, 2006.
[25] C. Hsu, C. Lu, and S. Pei, “Homomorphic encryption-based secure sift for privacy-preserving feature extraction,” in IS&T/SPIE Electronic Imaging, 2011.
[26] S. Jana, A. Narayanan, and V. Shmatikov, “A Scanner Darkly: Protecting user privacy from perceptual applications,” in 34th IEEE Symposium on Security and Privacy, 2013.
[27] P. Jensfelt, D. Kragic, J. Folkesson, and M. Bjorkman, “A framework for vision based bearing only 3D SLAM,” in Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International Conference on. IEEE, 2006, pp. 1944–1950.
[28] T. Karkkainen, T. Vaittinen, and K. Vaananen-Vainio-Mattila, “I don’t mind being logged, but want to remain in control: a field study of mobile activity and context logging,” in SIGCHI Conference on Human Factors in Computing Systems, 2010, pp. 163–172.
[29] D. Koller and N. Friedman, Probabilistic Graphical Models: Principles and Techniques. The MIT Press, 2009.
[30] M. Korayem, A. Mohamed, D. Crandall, and R. Yampolskiy, “Solving avatar captchas automatically,” in Advanced Machine Learning Technologies and Applications, 2012.
[31] J. Kosecka and F. Li, “Vision based topological markov localization,” in Robotics and Automation, 2004. Proceedings. ICRA’04. 2004 IEEE International Conference on, vol. 2. IEEE, 2004, pp. 1481–1486.
[32] M. Kourogi and T. Kurata, “Personal positioning based on walking locomotion analysis with self-contained sensors and a wearable camera,” in IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003, pp. 103–112.
[33] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories,” in IEEE Conference on Computer Vision and Pattern Recognition, 2006.
[34] L. Ledwich and S. Williams, “Reduced SIFT features for image retrieval and indoor localisation,” in Australian Conference on Robotics and Automation, 2004.
[35] Y. J. Lee, J. Ghosh, and K. Grauman, “Discovering important people and objects for egocentric video summarization,” in IEEE Conference on Computer Vision and Pattern Recognition, 2012.
[36] X. Li, C. Wu, C. Zach, S. Lazebnik, and J.-M. Frahm, “Modeling and Recognition of Landmark Image Collections Using Iconic Scene Graphs,” in European Conference on Computer Vision, 2008, pp. 427–440.
[37] Y. Li, D. Crandall, and D. P. Huttenlocher, “Landmark Classification in Large-scale Image Collections,” in IEEE International Conference on Computer Vision, 2009.
[38] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,” International Journal of Computer Vision, vol. 60, no. 2, pp. 91–110, Nov. 2004.
[39] E. Miluzzo, N. Lane, K. Fodor, R. Peterson, H. Lu, M. Musolesi, S. Eisenman, X. Zheng, and A. Campbell, “Sensing meets mobile social networks: the design, implementation and evaluation of the cenceme application,” in ACM Conference on Embedded Network Sensor Systems, 2008, pp. 337–350.
[40] M. Nauman, S. Khan, and X. Zhang, “Apex: Extending Android permission model and enforcement with user-defined runtime constraints,” in ACM Symposium on Information, Computer and Communications Security, 2010, pp. 328–332.
[41] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic representation of the spatial envelope,” International Journal of Computer Vision, vol. 42, no. 3, pp. 145–175, 2001.
[42] M. Ongtang, K. Butler, and P. McDaniel, “Porscha: Policy oriented secure content handling in Android,” in Annual Computer Security Applications Conference, 2010, pp. 221–230.
[43] H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in first-person camera views,” in IEEE Conference on Computer Vision and Pattern Recognition, 2012.
[44] G. Portokalidis, P. Homburg, K. Anagnostakis, and H. Bos, “Paranoid Android: Versatile protection for smartphones,” in Annual Computer Security Applications Conference, 2010, pp. 347–356.
[45] A. Quattoni and A. Torralba, “Recognizing indoor scenes,” in IEEE Conference on Computer Vision and Pattern Recognition, 2009.
[46] F. Roesner, T. Kohno, A. Moshchuk, B. Parno, H. J. Wang, and C. Cowan, “User-driven access control: Rethinking permission granting in modern operating systems,” in IEEE Symposium on Security and Privacy, 2012, pp. 224–238.
[47] A. Savvides, C. Han, and M. Strivastava, “Dynamic fine-grained localization in ad-hoc networks of sensors,” in International Conference on Mobile Computing and Networking, 2001, pp. 166–179.
[48] J. Schiff, M. Meingast, D. Mulligan, S. Sastry, and K. Goldberg, “Respectful cameras: Detecting visual markers in real-time to address privacy concerns,” in Protecting Privacy in Video Surveillance. Springer, 2009, pp. 65–89.
[49] S. Se, D. Lowe, and J. Little, “Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks,” International Journal of Robotics Research, vol. 21, no. 8, pp. 735–758, 2002.
[50] ——, “Vision-based global localization and mapping for mobile robots,” IEEE Transactions on Robotics, vol. 21, no. 3, pp. 364–375, 2005.
[51] N. Snavely, S. Seitz, and R. Szeliski, “Modeling the World from Internet Photo Collections,” International Journal of Computer Vision, vol. 80, pp. 189–210, 2008.
[52] R. Templeman, Z. Rahman, D. Crandall, and A. Kapadia, “PlaceRaider: Virtual theft in physical spaces with smartphones,” in Network and Distributed System Security Symposium, 2013.
[53] K. Truong, S. Patel, J. Summet, and G. Abowd, “Preventing camera recording by designing a capture-resistant environment,” in International Conference on Ubiquitous Computing, 2005, pp. 73–86.
[54] I. Ulrich and I. Nourbakhsh, “Appearance-based place recognition for topological localization,” in IEEE International Conference on Robotics and Automation, 2000, pp. 1023–1029.
[55] O. Woodman and R. Harle, “Pedestrian localisation for indoor environments,” in International Conference on Ubiquitous Computing, 2008, pp. 114–123.
[56] J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba, “SUN database: Large-scale scene recognition from abbey to zoo,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010, pp. 3485–3492.