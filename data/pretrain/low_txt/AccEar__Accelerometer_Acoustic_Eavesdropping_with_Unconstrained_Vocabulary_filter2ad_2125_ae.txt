### VI. Limitations and Future Work

While various GAN variants (e.g., WGAN [41], CycleGAN [42], StyleGAN [43]) have been proposed, they either do not achieve a one-to-one mapping between inputs and outputs or are not suitable for our specific task. To the best of our knowledge, cGAN is the only variant that meets our requirements.

However, our cGAN-based approach has some limitations. Specifically, the inputs and outputs of our cGAN are image-like two-dimensional data. Therefore, we must transform the accelerometer data into a spectrogram using SFTF and then convert the output Mel spectrogram back into an audio waveform using the Griffin-Lim algorithm. This transformation can lead to information loss in the signal phase, which may distort the reconstructed audio. Additionally, cGANs are known for their training difficulties, including model tuning and computational overhead.

In future work, we plan to explore neural network-based approaches that can directly process time series data to avoid the information loss caused by spectrogram conversion.

### VII. Conclusion

In this paper, we introduce AccEar, an accelerometer eavesdropping system that reconstructs audio played by a built-in speaker from accelerometer data. With AccEar, an adversary can reconstruct unconstrained words from accelerometer data, making it applicable in various scenarios such as voice and video calls, voice navigation, and voice assistants. We implemented and extensively evaluated AccEar on different smartphones and users, achieving high accuracy across various settings and scenarios.

### Acknowledgements

We would like to thank the anonymous reviewers and the shepherd for their insightful comments. This work is supported by the National Key Research and Development Program of China (Grant No. 2021YFB3100400) and the National Natural Science Foundation of China (Grant No. 61832012).

Authorized licensed use limited to: Tsinghua University. Downloaded on August 07, 2022 at 12:36:50 UTC from IEEE Xplore. Restrictions apply.

### References

[1] Y. Michalevsky, D. Boneh, and G. Nakibly, “Gyrophone: Recognizing speech from gyroscope signals,” in 23rd USENIX Security Symposium (USENIX Security 14), 2014, pp. 1053–1067.

[2] L. Zhang, P. H. Pathak, M. Wu, Y. Zhao, and P. Mohapatra, “Accelword: Energy-efficient hotword detection through accelerometer,” in Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services, 2015, pp. 301–315.

[3] S. A. Anand and N. Saxena, “Speechless: Analyzing the threat to speech privacy from smartphone motion sensors,” in 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 2018, pp. 1000–1017.

[4] Z. Ba, T. Zheng, X. Zhang, Z. Qin, B. Li, X. Liu, and K. Ren, “Learning-based practical smartphone eavesdropping with built-in accelerometer.” in NDSS, 2020.

[5] J. Han, A. J. Chung, and P. Tague, “Pitchln: Eavesdropping via intelligible speech reconstruction using non-acoustic sensor fusion,” in Proceedings of the 16th ACM/IEEE International Conference on Information Processing in Sensor Networks, 2017, pp. 181–192.

[6] M. Mirza and S. Osindero, “Conditional generative adversarial nets,” Computer Science, pp. 2672–2680, 2014.

[7] D. Griffin and J. Lim, “Signal estimation from modified short-time Fourier transform,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 2, pp. 236–243, 1984.

[8] S. A. Anand, C. Wang, J. Liu, N. Saxena, and Y. Chen, “Spearphone: A lightweight speech privacy exploit via accelerometer-sensed reverberations from smartphone loudspeakers,” in Proceedings of the 14th ACM Conference on Security and Privacy in Wireless and Mobile Networks, 2021, pp. 288–299.

[9] A. Davis, M. Rubinstein, N. Wadhwa, G. J. Mysore, F. Durand, and W. T. Freeman, “The visual microphone: Passive recovery of sound from video,” ACM Transactions on Graphics (TOG), vol. 33, no. 4, pp. 1–10, 2014.

[10] A. Kwong, W. Xu, and K. Fu, “Hard drive of hearing: Disks that eavesdrop with a synthesized microphone,” in 2019 IEEE Symposium on Security and Privacy (SP).

[11] M. Guri, Y. Solewicz, A. Daidakulov, and Y. Elovici, “Speake(a)r: Turn speakers to microphones for fun and profit,” in 11th USENIX Workshop on Offensive Technologies (WOOT 17), 2017. IEEE, 2019, pp. 905–919.

[12] N. Roy and R. Roy Choudhury, “Listening through a vibration motor,” in Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services, 2016, pp. 57–69.

[13] G. Wang, Y. Zou, Z. Zhou, K. Wu, and L. M. Ni, “We can hear you with Wi-Fi!” IEEE Transactions on Mobile Computing, vol. 15, no. 11, pp. 2907–2920, 2016.

[14] T. Wei, S. Wang, A. Zhou, and X. Zhang, “Acoustic eavesdropping through wireless vibrometry,” in Proceedings of the 21st Annual International Conference on Mobile Computing and Networking, 2015, pp. 130–141.

[15] R. P. Muscatell, “Laser microphone,” The Journal of the Acoustical Society of America, vol. 76, no. 4, pp. 1284–1284, 1984.

[16] B. Nassi, Y. Pirutin, A. Shamir, Y. Elovici, and B. Zadov, “Lamphone: Real-time passive sound recovery from light bulb vibrations.” IACR Cryptol. ePrint Arch., vol. 2020, p. 708, 2020.

[17] W. Sousa Lima, E. Souto, K. El-Khatib, R. Jalali, and J. Gama, “Human activity recognition using inertial sensors in a smartphone: An overview,” Sensors, vol. 19, no. 14, p. 3213, 2019.

[18] S. Shen, M. Gowda, and R. Roy Choudhury, “Closing the gaps in inertial motion tracking,” in Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, 2018, pp. 429–444.

[19] J. L. Hicks, T. Althoff, P. Kuhar, B. Bostjancic, A. C. King, J. Leskovec, S. L. Delp et al., “Best practices for analyzing large-scale health data from wearables and smartphone apps,” NPJ Digital Medicine, vol. 2, no. 1, pp. 1–12, 2019.

[20] “Android API reference.” [Online]. Available: https://developer.android.com/reference/

[21] J. J. Ohala, “Phonetic explanations for sound patterns,” A Figure of Speech: A Festschrift for John Laver, vol. 23, 2005.

[22] “English phonology.” [Online]. Available: https://en.wikipedia.org/wiki/English_phonology

[23] R. J. Baken and R. F. Orlikoff, Clinical Measurement of Speech and Voice. Cengage Learning, 2000.

[24] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” Advances in Neural Information Processing Systems, vol. 27, 2014.

[25] S. S. Stevens, J. Volkmann, and E. B. Newman, “A scale for the measurement of the psychological magnitude pitch,” The Journal of the Acoustical Society of America, vol. 8, no. 3, pp. 185–190, 1937.

[26] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh, J. Sotelo, A. de Brébisson, Y. Bengio, and A. Courville, “MelGAN: Generative adversarial networks for conditional waveform synthesis,” arXiv preprint arXiv:1910.06711, 2019.

[27] I. Ananthabhotla, S. Ewert, and J. A. Paradiso, “Towards a perceptual loss: Using a neural network codec approximation as a loss for generative audio models,” in Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 1518–1525.

[28] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1125–1134.

[29] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks for biomedical image segmentation,” in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2015, pp. 234–241.

[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” 2017.

[31] “Mobile operating systems’ market share worldwide from January 2012 to June 2021.” [Online]. Available: https://www.statista.com/statistics/272698/global-market-share-held-by-mobile-operating-systems-since-2009/

[32] “Motion sensors are rate-limited.” [Online]. Available: https://developer.android.com/about/versions/12/behavior-changes-12#motion-sensor-rate-limiting

[33] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda, “Speaker-dependent WaveNet vocoder.” in Interspeech, vol. 2017, 2017, pp. 1118–1122.

[34] C. Yan, G. Zhang, X. Ji, T. Zhang, T. Zhang, and W. Xu, “The feasibility of injecting inaudible voice commands to voice assistants,” IEEE Transactions on Dependable and Secure Computing, 2019.

[35] ITU-T Recommendation P.862, “Perceptual evaluation of speech quality (PESQ): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs,” 2001.

[36] “Mobile vendor market share worldwide.” [Online]. Available: https://gs.statcounter.com/vendor-market-share/mobile/worldwide

[37] X. Qi, M. Keally, G. Zhou, Y. Li, and Z. Ren, “AdaSense: Adapting sampling rates for activity recognition in body sensor networks,” in 2013 IEEE 19th Real-Time and Embedded Technology and Applications Symposium (RTAS). IEEE, 2013, pp. 163–172.

[38] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional generative adversarial networks,” arXiv preprint arXiv:1511.06434, 2015.

[39] J. Zhao, M. Mathieu, and Y. LeCun, “Energy-based generative adversarial network,” arXiv preprint arXiv:1609.03126, 2016.

[40] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley, “Least squares generative adversarial networks,” in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2794–2802.

[41] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” 2017.

[42] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2223–2232.

[43] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4401–4410.

[44] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018.

### Appendix

#### A. Detailed Parameters of Mobile Devices

Table III lists the detailed information on the different models of the mobile devices used in our experiments. We observe that while the maximum accelerometer sampling rate is within the range of 416–500 Hz on smartphones, it is significantly lower on tablets (i.e., 200–250 Hz). Despite this difference, our attack scheme shows consistency across multiple devices.

| Type   | System Version | Model                | Screen Size | Acceler. MSR |
|--------|----------------|----------------------|-------------|--------------|
| Phone  | HarmonyOS 2.0  | Huawei Mate40 Pro    | 6.76 in.    | 500 Hz       |
| Phone  | HarmonyOS 2.0  | Huawei Mate30 Pro    | 6.53 in.    | 500 Hz       |
| Phone  | Android 11     | OPPO Reno6 Pro       | 6.55 in.    | 420 Hz       |
| Phone  | Android 11     | Samsung S21+         | 6.70 in.    | 416 Hz       |
| Phone  | Android 11     | Xiaomi RedMi 10X Pro | 6.57 in.    | 418 Hz       |
| Phone  | Android 11     | OPPO Find X3         | 6.70 in.    | 425 Hz       |
| Tablet | Android 11     | Huawei MatePad Pro   | 10.80 in.   | 250 Hz       |
| Tablet | Android 11     | Samsung Galaxy Tab S6 Lite | 10.40 in. | 200 Hz       |

**Note:** Acceler. MSR stands for Accelerometer Maximum Sampling Rate.

#### B. Relationship Between MCD and Reconstruction Performance at Word Level

We further explore the relationship between MCD and reconstruction performance at the word level and present some sample results in Table IV. Even if the model cannot reconstruct all the words in a sentence, we can infer the missing words from the context. Additionally, recent Natural Language Processing (NLP) techniques, such as BERT [44], can be used to infer the semantics of sentences even with missing words.

| MCD Range | Original Audio                                                                                   | Reconstructed Audio                                                                                 |
|-----------|--------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|
| 2～3      | Zheng ji bi sai, jiang jin shi wan, mei                                                         | Zheng ji bi sai, jiang jin shi wan, mei                                                             |
| 3～4      | You might be all over the world so good afternoon                                               | * are be all over the world so good afternoon                                                       |
| 4～5      | We had a barrel like this down in our basement, filled with cans of food and water               | We had a barrel like this * in our basement, filled with cans of food and *                         |
| 5～6      | This is our product line. We have a very clean product line, we think we have the best notebooks | This is our product line. * most * clean product line, we think we have the best * in the business  |
| 6～7      | Talking about here at Ted is that ther’re right in the middle of rainforest was of some solar panels | Talking about here at Ted is * * * in * * the middle of rainforest was of some solar panels the     |
| 7～8      | Community could have light for I think it was about half an hour each evening and there is the chief in all his | Community could have light *. * think * * * half * hour each evening and there is the chief in all his |

**Note:** * represents the word we cannot recognize.

#### C. The Effect of Dataset Diversity on Model Performance

The diversity of a person's speech mainly lies in two aspects: speed and frequency. People's speech speed can change depending on the speaker's mood and context. Training the model with normal speech speed and testing it with much faster or slower speeds can lead to unsatisfactory results. Similarly, the pronunciation frequency can vary based on the speaker's emotional state. For example, the frequency can be lower when the mood is low and higher when the mood is excited. Therefore, both speed and frequency variations are considered in our dataset.

We prepared the following datasets:
1. **Speed:**
   - ×0.75
   - ×1.0
   - ×1.25
   - Mixed dataset including ×0.75, ×1.0, and ×1.25

2. **Frequency:**
   - ×0.8
   - ×1.0
   - ×1.2
   - Mixed dataset including ×0.8, ×1.0, and ×1.2

3. **Mixed dataset including 1.d and 2.d**

As shown in Fig. 21, the model with more diversity generally achieves better performance. In terms of speed, the model (1.d) based on mixed datasets achieves a 2.9% improvement over single datasets, as shown in Fig. 21(a). In terms of frequency, the model (2.d) based on mixed datasets achieves a 4.9% improvement, as shown in Fig. 21(b). The model (3) with the most diversity achieves the largest improvement of 6.0%, as shown in Fig. 21(c).

#### D. Transferability Between Different Users

We test the transferability between all users, as shown in Fig. 22. The results indicate that our model generalizes well under cross-user training.

### Figures

**Fig. 21: Audio reconstruction performance with speed and frequency diversity.**
- (a) Speed
- (b) Frequency
- (c) All

**Fig. 22: Performance of model generalization with cross-user training.**

Authorized licensed use limited to: Tsinghua University. Downloaded on August 07, 2022 at 12:36:50 UTC from IEEE Xplore. Restrictions apply.