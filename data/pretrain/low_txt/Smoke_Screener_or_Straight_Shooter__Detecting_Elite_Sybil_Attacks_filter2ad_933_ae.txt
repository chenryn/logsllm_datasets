### Filtered Reviews Analysis

The percentage of filtered reviews for regular Sybil users significantly outnumbers that of benign users, as illustrated in Figure 8(c). Specifically, 80% of Sybil users have more than 90% of their reviews filtered, while 80% of benign users have less than 20% of their reviews filtered. This user-level observation is consistent with the community-level results shown in Figure 7. Additionally, elite Sybil users have fewer reviews filtered by Dianping, primarily because a large portion of their reviews are not assigned to any specific task.

### Community Structure

Understanding the behavior of elite Sybil users is crucial for revealing the characteristics of the (quasi) permanent workforce of Sybil organizations on Dianping. At the macro level, communities of elite Sybil users form large-scale, sparsely knit networks with much lower graph density. Figure 10 provides an example of the induced network structure of elite Sybil users. In this figure, a dot represents an elite Sybil user, and a square represents a Sybil community. An edge between a dot and a square indicates that the elite Sybil user belongs to the community. Red dots represent elite Sybil users who belong to a single community, while blue dots represent those who belong to multiple communities. The data shows that many elite Sybil users are connected to a single community, forming a large-scale, sparsely knit network. Some elite Sybil users appear in multiple communities. Ranked by Sybilness, the top 1,000 elite Sybil users out of 12,292 in our dataset were analyzed. Of these, 824 participated in a single community, 160 in two communities, and 16 in at least three communities. This confirms that elite Sybil users are sparsely connected, with a much lower graph density compared to regular Sybil users.

### Review Manipulation for Chain Stores

Recent research from Harvard [27] suggests that chain stores are less likely to hire Sybil accounts to generate favorable reviews, as they rely heavily on various forms of promotion and branding to establish their reputation. However, our findings contradict this statement. We discovered that a significant number of chain stores leverage Sybil organizations to post fake reviews to manipulate their online ratings. Specifically, 12.37% of the 566 Sybil communities in our dataset posted fake reviews for chain stores listed on Dianping, with the number of involved chain stores ranging from 2 to 11. One possible explanation is that these chain stores hired the same Sybil agent, who recruited the same Sybil community for multiple campaigns. Figure 11 illustrates the main part of the entire network structure of Sybil communities and overhyped stores, pruned by a small portion of tiny networks. In the figure, a yellow square represents a Sybil community, a red dot represents an overhyped store, and an edge between a yellow and a red dot indicates a connection between a Sybil community and an overhyped store. Almost all Sybil communities act as central nodes, indicating that they provide services to a large number of overhyped stores. Some overhyped stores connect to multiple communities, suggesting repeated use of Sybil services.

### Early Alerts for Sybil Campaigns

It is feasible to uncover Sybil campaigns by monitoring detected elite Sybil users. By continually monitoring the collusive behaviors of elite Sybil users, social network operators can determine whether a Sybil campaign has been launched at the earliest stage, serving as an early alert. Our goal is to detect the presence of a Sybil campaign at the early stage by identifying elite Sybil users via continuous monitoring. We apply 7-day sliding windows along the timeline to each store to detect campaigns. A Sybil campaign is determined if more than a predetermined threshold number (e.g., 7 in our experiment) of reviews are posted by elite Sybil users at the same store within a 7-day window. During non-campaign periods, elite Sybil users post reviews at different stores in a manner similar to innocent users. However, during campaign periods, they collusively post reviews at the same stores to fulfill the Sybil campaign tasks. Evaluation results show that approximately 90.40% of campaigns can be determined by scanning the activities of elite Sybil users. To determine campaigns at an early stage, we ran the campaign window determination algorithm using the first 1/4, 1/3, and 1/2 of the entire campaign period. The results showed that 56.77%, 63.08%, and 75.14% of campaigns could be successfully detected, respectively. Given the average Sybil campaign period of 68 days, more than 50% of campaigns can be determined within the first two weeks by observing the activities of elite Sybil users.

### Temporal Dynamics

We demonstrate two temporal dynamics: user posting period and Sybil campaign duration. Figure 12 shows that elite Sybil users in Community 4559 repeatedly post fake reviews in Store 4112200. The x-axis shows the time when an account posts a review, and the y-axis is the account’s ID. A dot (x, y) in the figure represents that an account with ID y posts a review at time x. Staggered colors encode reviews posted by different users. Within a two-month period, 33 users in Community 4559 posted 127 reviews, which is much denser than the posting pattern of benign users. These elite Sybil users also deliberately manipulate the posting time of reviews, sometimes posting periodically (every week/month). This manipulation is key to orchestrating evasive strategies. By applying the campaign window detection algorithm, we identified 4,162 Sybil campaigns. Figure 13 shows the distribution of the number of campaigns across campaign duration. The distribution is unimodal with a spike at 7 days, echoing our 7-day sliding windows. There are 466 one-day ephemeral Sybil campaigns, where Sybil communities complete tasks fleetingly.

### Case Study: Sybil Communities and Campaigns

Figure 11 shows that some stores employ several Sybil communities to increase their star ratings. We zoom in on a case study involving a hotel that employed three different Sybil communities to post fake reviews. Figure 14 illustrates the variation in the star rating and the number of reviews over time. Orange represents aggregate reviews of the hotel; blue, purple, and green represent reviews from three respective Sybil communities. The red line denotes the star rating of the hotel, and the blue line denotes the star rating without detected fake reviews. Many spikes in the number of reviews, generated by the three Sybil communities, correspond to sudden increases in the star rating. Before the first spike, the red and blue lines overlap, but afterward, the red line increases sharply while the blue line maintains moderate growth. This indicates that fake reviews posted by Sybil communities significantly impact the online rating. Community 7677 committed the largest-scale fake reviews, contributing most to the star rating increase. Community 7668, despite a long-term presence, had a moderate impact, possibly due to the hotel's previously accumulated reviews or Dianping's ranking algorithm, which considers factors like the number of reviews and page views.

### Evading Dianping’s Sybil Detection System

We present three examples of elite Sybil users in the same community to illustrate their evasive strategies. Figure 15 shows the reviews posted by each elite Sybil user, with each dot representing a review according to the timeline. The upper (resp. lower) dotted line represents the posting timeline generated by ELSIEDET (resp. Dianping's filtering system). Blue (resp. red) dots represent real (resp. fake) reviews labeled by ELSIEDET, and blue (resp. red) triangles represent existing (resp. filtered) reviews according to Dianping. Elite Sybil users post massive reviews to camouflage fake ones, evading most aggregate behavioral-based clustering approaches. These users write fake reviews in similar stores and share similar behavioral patterns. However, Dianping's filtering system varies in effectiveness. For a given user, no reviews may be filtered (Figure 15(a)), partial reviews may be filtered (Figure 15(b)), or all reviews may be filtered (Figure 15(c)). This suggests that Dianping's filtering system is more effective against regular Sybils but has a high false alarm rate for elite Sybils, leading to the false filtering of real reviews.

### Discussion and Limitations

#### Application of ELSIEDET

ELSIEDET can be integrated into Dianping's Sybil system to enhance its tolerance of elite Sybil attacks. Mitigating Sybil attacks involves changing the weight of reviews based on Sybilness. By assigning a lower weight to highly suspicious users, it becomes more difficult for Sybil organizations to manipulate ratings. Monitoring the top elite Sybil users can help predict Sybil campaigns, saving resources for the social network operator.

#### Limitations

Our detection system, while focused on Dianping, is applicable to a wider range of user-generated content platforms. Examples include e-commerce sites (Amazon, eBay, BizRate), movie-rating platforms (IMDb, Netflix, Douban), travel services (TripAdvisor), and multi-agent systems (Advogato). However, if a Sybil community minimizes involvement in multiple campaigns, it may evade detection, though this contradicts the economic basis. Additionally, we did not study the relationships among reviewers on Dianping, such as friendships and friend lists.