### Example: Auditd and Restartable Images

The `auditd` daemon records system call names issued by specified processes, opening the log file without the `O_APPEND` flag. If this daemon is included in the list of restartable candidates, it cannot be used for a restartable image. To ensure that such a daemon runs consistently after a phase-based reboot, we must carefully avoid placing it in the list of restartable candidates. Instead, we need to configure the daemon to start after the phase-based reboot. If a user wants to skip the boot phase for such a daemon, they must redesign the daemon to be aware of phase-based reboots. In this case, the `O_APPEND` flag should be added to the open argument.

### C. Recovery from Kernel Failures

To confirm that the phase-based reboot can successfully recover from kernel failures, we synthetically injected faults into the running kernel. We measured the success rate of recovery from 200 induced kernel crashes. For fault injection, we used a mechanism originally developed at the University of Michigan, which has been employed in other studies [9], [10], [11]. Each fault alters a single integer value on the kernel stack of a random thread, or a single instruction, or instruction operand in the kernel code. This emulates common errors such as stack corruption, uninitialized variables, incorrect testing conditions, incorrect function parameters, and wild writes.

We allocated 1.7 GB of memory to the guest domain. The experimental results demonstrated that the phase-based reboot successfully recovered from all the injected kernel failures. Since the phase-based reboot completely destroys the crashed memory state and reconstructs a fresh state from the restartable image, the kernel failures do not affect the reboot procedure. For example, when the fault injection tool altered values in the stack memory region, leading to a kernel stop error, the phase-based reboot overwrote the memory image of the crashed VM with the restartable image, allowing the VM to continue providing services. In this experiment, the fault injection tool never introduced non-transient faults, such as those that write incorrect values to the disks. Although such kernel failures occur in the real world, they are outside the scope of the phase-based reboot, which targets transient failures that can be recovered by a normal OS reboot.

### VII. Related Work

Various approaches have been proposed to reduce downtime caused by whole program restarts. Microreboot [12] achieves fine-grained software reboots by dividing the target application into small, independent components, each of which can be individually rebooted. If a small component cannot recover from a failure, a larger component is rebooted. Microreboot focuses on application-level failures and does not shorten the reboot time for kernel failures. It is complementary to the phase-based reboot, which focuses on "phases" of software systems, particularly useful for rebooting larger components like OS kernels.

Kexec [13] and Fast Reboot [14] allow quick kernel startup without hardware reset. However, these mechanisms require kernel support and cannot be used if the kernel has stopped due to failures. The phase-based reboot, on the other hand, can work even when the kernel has crashed.

Otherworld [9] reboots the kernel while preserving the state of running applications. After a kernel crash and reboot, Otherworld restores the application memory spaces, open files, and other resources. However, the downtime for Otherworld is about one minute. For rapid service recovery, both Otherworld and the phase-based reboot can be used depending on the situation. Otherworld is suitable if the running states of applications are critical, while the phase-based reboot is more appropriate if the running states are not critical.

Akeso [15] is a kernel-level mechanism that handles recovery at the request level, such as system calls or interrupts. When a failure occurs, Akeso rolls back the kernel state to the beginning of the function and returns an error. However, it requires complex annotations in the kernel code, which is laborious and error-prone. The phase-based reboot does not require any annotations and is complementary to kernel-level mechanisms, quickly rejuvenating them for more reliable services.

Previous studies have focused on specific kernel components. Nooks [10], [16] isolates device drivers in lightweight protection domains and transparently recovers them upon failure. LeVasseur et al. [17] proposed isolating device drivers using dedicated VMs to limit their impact. Membrane [18] periodically saves checkpoints of file system states and restores them upon failure. These approaches focus on specific kernel components, whereas the phase-based reboot addresses failures in any kernel component.

Some studies have utilized virtualization to improve system reliability. Bressoud and Schneider [19] proposed a hypervisor-based approach for fault-tolerant systems, replicating the system state remotely. Remus [20] replicates snapshots of an entire running OS instance between physical machines. These failover approaches focus on hardware failures, while the phase-based reboot targets software failures in the kernel.

CuriOS [21] recovers failed services transparently to clients in a microkernel OS by storing client-specific states in inaccessible memory. Vino [22] encapsulates extensions in transactions to abort and clean up their states without rebooting the OS. These methods run on microkernels or special kernels, while the phase-based reboot is suited for commodity OSes like Linux.

Approaches to improving the reliability of applications and virtual machine monitors include checkpoint-restarting methods [23], protecting against code injection attacks [24], [25], diagnosing and patching failures online [26], [27], and changing application execution environments [28]. Roothammer [29] achieves fast VMM rejuvenation by preserving running VMs in memory during VMM reboot. These approaches complement the phase-based reboot, which targets kernel failures.

The phase-based reboot is also complementary to OS error detection sensors like SVA runtime mechanisms [30] and software guards in the XFI system [31], which can reduce reboot recovery latency.

### VIII. Conclusion

We proposed a "phase-based" reboot to shorten the downtime of reboot-based recovery. The key idea is to divide the boot sequence into phases, reusing the system state from the previous boot if the next boot reproduces the same state. This skips time-consuming phases that reproduce the same states. A prototype was implemented on Xen 3.4.1 with paravirtualized Linux 2.6.18. Experimental results showed that the prototype successfully recovered from kernel failures inserted by a fault injector, with downtime 34.3% to 93.6% shorter than normal reboot-based recovery.

### References

[1] N. Palix, G. Thomas, S. Saha, C. Calvés, J. Lawall, and G. Muller, “Faults in Linux: Ten Years Later,” in Proceedings of the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’11), Mar. 2011, pp. 305–318.

[2] R. C. Baumann, “Soft errors in commercial semiconductor technology: overview and scaling trends,” IEEE 2002 Reliability Physics Tutorial Notes, Reliability Fundamental, 2002.

[3] C. S. Advisory, “Cisco catalyst memory leak vulnerability. ID:13618,” 2001.

[4] V. Castelli, R. E. Harper, P. Heidelberger, S. W. Hunter, K. S. Trivedi, K. Vaidyanathan, and W. P. Zeggert, “Proactive Management of Software Aging,” IBM Journal of Research and Development, vol. 45, no. 2, pp. 311–332, 2001.

[5] C. A. Waldspurger, “Memory Resource Management in VMware ESX Server,” in Proceedings of the 5th USENIX Symposium on Operating System Design and Implementation (OSDI ’02), Dec. 2002, pp. 181–194.

[6] RUBiS: Rice University bidding system, http://rubis.objectweb.org/.

[7] eBay.com, http://www.ebay.com/.

[8] Amazon.com, Amazon Elastic Compute Cloud (Amazon EC2), http://aws.amazon.com/ec2/.

[9] A. Depoutovitch and M. Stumm, “Otherworld - Giving Applications a Chance to Survive OS Kernel Crashes,” in Proceedings of the 5th European Conference on Computer Systems (EuroSys ’10), Apr. 2010, pp. 181–194.

[10] M. M. Swift, B. N. Bershad, and H. M. Levy, “Improving the Reliability of Commodity Operating Systems,” in Proc. of the 19th ACM Simp. on Operating Systems Principles (SOSP ’03), Oct. 2003, pp. 207–222.

[11] W. T. Ng and P. M. Chen, “The Systematic Improvement of Fault Tolerance in the Rio File Cache,” in Proceedings of the 1999 Symposium on Fault-Tolerant Computing (FTCS ’99), Jun. 1999, pp. 76–83.

[12] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox, “Microreboot - a technique for cheap recovery,” in Proc. of the 6th USENIX Simp. on Operating Systems Design and Implementation (OSDI ’04), Dec. 2004, pp. 31–44.

[13] “Reboot linux faster using kexec,” http://www.ibm.com/developerworks/linux/library/l-kexec.html.

[14] Sun Microsystems, “Using Fast Boot on the x86 Platform,” 2008, http://dlc.sun.com/osol/docs/content/SYSADV1/ghsut.html/.

[15] A. Lenharth, V. Adve, and S. T. King, “Recovery domains: An organizing principle for recoverable operating systems,” in Proceedings of the 14th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’09), Mar. 2009, pp. 49–60.

[16] M. M. Swift, M. Annamalai, B. N. Bershad, and H. M. Levy, “Recovering device drivers,” in Proc. of the 6th USENIX Simp. on Operating Systems Design and Implementation (OSDI ’04), Dec. 2004, pp. 1–16.

[29] K. Kourai and S. Chiba, “A Fast Rejuvenation Technique for Server Consolidation with Virtual Machines,” in Proc. of the 37th Annual IEEE/IFIP International Conf. on Dependable Systems and Networks (DSN ’07), Jun. 2007, pp. 245–255.

[30] J. Criswell, A. Lenharth, D. Dhurjati, and V. Adve, “Secure virtual architecture: A safe execution environment for commodity operating systems,” in Proc. of the 21st ACM Simp. on Operating Systems Principles (SOSP ’07), 2007, pp. 31–44.

[31] Úlfar Erlingsson, Martín Abadi, Mihai Budiu, and George C. Necula, “XFI: Software Guards for System Address Spaces,” in Proc. of the 7th USENIX Simp. on Operating Systems Design and Implementation (OSDI ’06), Nov. 2006, pp. 75–88.

[17] J. LeVasseur, V. Uhlig, J. Stoess, and S. Götz, “Unmodified Device Driver Reuse and Improved System Dependability via Virtual Machines,” in Proc. of the 6th USENIX Simp. on Operating Systems Design and Implementation (OSDI ’04), Dec. 2004, pp. 17–30.

[18] S. Sundararaman, S. Subramanian, A. Rajimwale, A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, and M. M. Swift, “Membrane: Operating System Support for Restartable File Systems,” in Proceedings of the 8th USENIX Conference on File and Storage Technologies (FAST ’10), Feb. 2010, pp. 281–294.

[19] T. C. Bressoud and F. B. Schneider, “Hypervisor-based Fault-tolerance,” in Proc. of the 15th ACM Simp. on Operating Systems Principles (SOSP ’95), Dec. 1995, pp. 1–11.

[20] B. Cully, G. Lefebvre, D. Meyer, M. Feeley, N. Hutchinson, and A. Warfield, “Remus: High Availability via Asynchronous Virtual Machine Replication,” in Proceedings of the 5th USENIX Symposium on Networked Systems Design and Implementation (NSDI ’08), Apr. 2008, pp. 161–174.

[21] F. M. David, E. M. Chan, J. C. Carlyle, and R. H. Campbell, “CuriOS: Improving Reliability through Operating System Structure,” in Proceedings of the 8th USENIX Symposium on Operating Systems Design and Implementation (OSDI ’08), Dec. 2008, pp. 59–72.

[22] M. I. Seltzer, Y. Endo, C. Small, and K. A. Smith, “Dealing With Disaster: Surviving Misbehaved Kernel Extensions,” in Proceedings of the 2nd USENIX Symposium on Operating Systems Design and Implementation (OSDI ’96), Oct. 1996, pp. 213–227.

[23] E. N. M. Elnozahy, L. Alvisi, Y.-M. Wang, and D. B. Johnson, “A survey of rollback-recovery protocols in message-passing systems,” ACM Computer Surveys, vol. 34, no. 3, pp. 375–408, Sep. 2002.

[24] J. Etoh, “Gcc extension for protecting applications from stack-smashing attacks,” http://www.trl.ibm.com/projects/security/ssp.

[25] P. Team, “Address space layout randomization,” 2003, http://pax.grsecuirty.net/docs/aslr.txt.

[26] Q. Gao, W. Zhang, Y. Tang, and F. Qin, “First-aid: Surviving and preventing memory management bugs during production runs,” in Proc. of the 4th ACM European Conf. on Computer Systems (EuroSys ’09), Apr. 2009, pp. 159–172.

[27] S. Sidiroglou, O. Laadan, C. R. Perez, N. Viennot, J. Nieh, and A. D. Keromytis, “ASSURE: Automatic Software Self-healing Using REscue points,” in Proceedings of the 14th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’09), Mar. 2009, pp. 37–48.

[28] F. Qin, J. Tucek, J. Sundaresan, and Y. Zhou, “Rx: Treating Bugs As Allergies - A Safe Method to Survive Software Failures,” in Proc. of the 20th ACM Simp. on Operating Systems Principles (SOSP ’05), Oct. 2005, pp. 235–248.