### Spines Congestion Control and Reliability

**Loss Rate (%)**
- 0.6
- 0.8
- 1.0
- 1.2
- 1.4
- 1.6
- 1.8
- 2.0

**Figure 9. Spines Congestion Control (Emulab)**
- At least four acknowledgments per end-to-end window are used for end-to-end reliability and congestion control.
- When possible, acknowledgments are piggybacked with data packets.
- The control traffic is minimal, consisting primarily of hello packets (currently, two 28-byte packets per second on each link) and link state packets, which are sent only when network conditions change.
- A single link state packet can contain information about up to 90 links, depending on the network's dispersion.
- Due to this overhead, our experiments show that compared to a standard TCP connection running alone on a network link with a capacity ranging from 500 Kbps to 100 Mbps, the Spines link protocol achieves about 3.5% less data throughput.
- The end-to-end connection that uses both levels of reliability and congestion control (on the hop and end-to-end) shows an overhead of at most 5.7%.
- The best-effort, unreliable protocol in Spines has an overhead of about 2.3%.

### Experimental Results

In this section, we evaluate the hop-by-hop reliability behavior using the Spines overlay network deployed on the Emulab testbed. Emulab [9] is a network facility that allows real instantiation in a hardware network (composed of actual computers and network switches) of a given topology, simply by using an ns script in the configuration setup. Link latencies, loss rates, and bandwidths are emulated with additional nodes that delay packets or drop them according to specified link characteristics.

We instantiated on Emulab the network setup presented in Figure 10, which follows the topology used in our Section III simulations. In addition to the five links A-B, B-C, ..., E-F, we also connected the nodes through a fast, local area network (LAN) used to obtain accurate clock measurements between the overlay nodes.

**Figure 10. Emulab Network Setup**
- A: 10ms, 10Mbps
- B: 10ms, 10Mbps
- C: 10ms, 10Mbps
- D: 10ms, 10Mbps
- E: 10ms, 10Mbps
- F: 10ms, 10Mbps
- LAN: 0.1ms, 100Mbps

The routing was set up such that all experiment traffic went on the 10-millisecond links, while on the LAN, we continuously measured (every 100 milliseconds) the clock difference between the computers making the end nodes of a connection. The one-way delay of the data packets was calculated as the difference between the timestamp at the sender and the current time at the receiver, adjusted with the clock difference between the end nodes.

On the overlay network, the round-trip delay between nodes A and F, measured with ping under no traffic, was 99.96 milliseconds, and the throughput achieved by a TCP connection on each of the 10-millisecond links was about 9.59 Mbps. On the LAN, the round-trip delay between any two nodes was about 0.135 milliseconds, providing high accuracy in measuring the clock difference and one-way delay of the packets. For each experiment in this section, we sent 200,000 messages of 1,000 bytes each.

We compared the packet delay of a data stream using an end-to-end TCP connection between nodes A and F with that of a hop-by-hop connection using Spines on the overlay nodes, while varying the sending rate (at node A) and the loss rate on the intermediate link C-D. Note that the end-to-end TCP connection does not go through the Spines application-level routers but only through the overlay nodes A, B, ..., F—so it is not affected by the Spines overhead in user-level processing and added headers.

**Figures 11 and 12** show that the low latency effect of hop-by-hop reliability is significant in the experimental setting, overcoming the overhead of user-level processing at the level of the intermediate overlay network nodes. The latency of a real TCP connection is lower than the simulation result (presented in Figures 3 and 4), especially at high loss rates, indicating that the TCP model used in the simulation (TCP-Fack) does not exactly resemble the Linux kernel implementation. The latency achieved by Spines hop-by-hop reliability is slightly higher than the latency obtained in the simulator, mainly due to simplifying assumptions in the simulation.

**Figure 11. Average Delay for a 500 Kbps Stream (Emulab)**
- End-to-End
- Hop-by-Hop
- Loss rate (%): 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0

**Figure 12. Average Delay for a 1000 Kbps Stream (Emulab)**
- End-to-End
- Hop-by-Hop
- Loss rate (%): 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0

However, the hop-by-hop latency remains very low and increases much slower compared to the latency of the end-to-end TCP connection. Jitter follows a similar pattern, as seen in **Figure 13** (compared with Figure 5). Packets sent through the Spines overlay network arrive at the destination with jitter up to three to four times smaller than the jitter of an end-to-end connection.

**Figure 13. Average Jitter for a 500 Kbps Stream (Emulab)**
- End-to-End
- Hop-by-Hop
- Loss rate (%): 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0

**Figure 14. Packet Delay Distribution for a 500 Kbps Stream (Emulab)**
- End-to-End
- Hop-by-Hop
- Delay more than (ms): 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280

Although the delay distribution for the end-to-end TCP connection is almost identical to the simulation result (Figure 6), the overhead of the application-level routing is clearly visible in the hop-by-hop delay distribution. However, even with this overhead, the number of packets delayed by Spines is significantly (more than three times) lower than the number of packets delayed by the end-to-end connection.

### Related Work

The idea of using reliable intermediate links is not new. In 1976, the International Committee for Telegraph and Telephony (CCITT) recommended X.25 as a store-and-forward, connection-oriented protocol between end-nodes (DTE) and routers (DCE). In [10], the authors provide a detailed description of the X.25 protocol. However, since the Internet was developed as a connectionless, best-effort network (which allows better scalability and interoperability), it did not incorporate the X.25 specifications but relied on end-to-end protocols such as TCP/IP to provide reliable connections.

One of the early uses of overlay networks in the Internet was in a proposed overlay network called EON (Experimental OSI-based Network) [11] on top of the IP network, which would allow experimentation with the OSI network layer. The scheme was only experimental and did not specify hop-by-hop reliability. More recently, overlay networks have emerged mainly by providing new services to applications. The Mbone [12] is a routing mechanism that creates an overlay infrastructure over the global Internet and extends the use of IP multicast by creating virtual tunnels between the networks that support native IP multicast. The Mbone facilitates the use of multicast services on the global Internet but does not provide reliability by itself.

SRM [13] provides a form of localized recovery for reliable multicast by using randomized timeouts for sending retransmission requests and the retransmissions themselves. SRM does not guarantee recovery from the nearest node, as the closest one may set its timeout to be higher than that of an upstream node. Its probabilistic algorithm allows for double retransmission requests and recovery messages to be sent. The Spread system [14] uses a network of daemons to provide wide-area group communication, where missed messages are recovered from the nearest daemon on the path, localizing message recovery in a way similar to ours. The system is confined to group communication and does not provide a generic service such as ours.

Yoid [15] is a set of protocols that allows host-based content distribution using unicast tunnels and, where available, IP multicast. Yoid has the option of using TCP as the link protocol on the overlay network but does not guarantee either end-to-end congestion control or end-to-end reliability. In addition to these guarantees, our approach uses an out-of-order forwarding mechanism that provides less burstiness at the network level and lower packet latency and jitter.

The X-Bone [16] is a system that uses a graphical user interface for automatic configuration of IP-based overlay networks. RON [1] creates a fully connected graph between several nodes, monitors the connectivity between them, and, in case of Internet route failures, redirects packets through alternate overlay nodes. Both X-Bone and RON are implemented at the IP level, do not provide reliability other than the regular end-to-end offered by TCP/IP, and are complementary to our work.

### Conclusion

This paper presented a hop-by-hop reliability approach that considerably reduces the latency and jitter of reliable connections in overlay networks. We first quantified these effects in simulation. Overlay networks pay a performance price due to the need to process each message at the application level and to maintain the overlay. The paper presented experimental results with a new overlay network software we have built. These results resemble the simulation results and show that the overhead associated with overlay network processing does not play an important factor compared with the considerable gain of the approach. We also learned that having a small number of approximately equal hops (two to four) is sufficient to capture most of the performance benefit.

While network bandwidth increases exponentially over time, latency is very slow to improve. This work shows how coupling cheap processing and memory with the programmable platform provided by overlay networks and paying a small price in throughput overhead can considerably improve the latency characteristics of reliable connections.

### Acknowledgment

The authors would like to thank Mike Dahlin for insightful comments and discussions. This work was partially funded by DARPA grant F30602-00-2-0550 to Johns Hopkins University.

### References

[1] David G. Andersen, Hari Balakrishnan, and M. Frans Kaashoek and Robert Morris, “Resilient overlay networks,” in Operating Systems Review, December 2001, pp. 131–145.
[2] “The Spines overlay network,” http://www.spines.org/.
[3] V. Jacobson, “Congestion avoidance and control,” ACM Computer Communication Review; Proceedings of the Sigcomm '88 Symposium in Stanford, CA, August, 1988, vol. 18, 4, pp. 314–329, 1988.
[4] K. K. Ramakrishnan and Sally Floyd, “A proposal to add explicit congestion notification (ECN) to IP,” RFC 2481, January 1999.
[5] B. Braden, D. Clark, J. Crowcroft, B. Davie, S. Deering, D. Estrin, S. Floyd, V. Jacobson, G. Minshall, C. Partridge, L. Peterson, K. Ramakrishnan, S. Shenker, J. Wroclawski, and L. Zhang, “Recommendations on queue management and congestion avoidance in the internet,” RFC 2309, April 1998.
[6] Jitedra Padhye, Victor Firoiu, Don Towsley, and Jim Krusoe, “Modeling TCP throughput: A simple model and its empirical validation,” in ACM Computer Communications Review: Proceedings of SIGCOMM 1998, Vancouver, CA, 1998, pp. 303–314.
[7] Sally Floyd, Mark Handley, Jitendra Padhye, and Jorg Widmer, “Equation-based congestion control for unicast applications,” in ACM Computer Communications Review: Proceedings of SIGCOMM 2000, Stockholm, Sweden, August 2000, vol. 30, pp. 43–56.
[8] “ns2 network simulator,” Available at http://www.isi.edu/nsnam/ns/.
[9] “The Utah network emulation facility,” http://www.emulab.net/.
[10] R. Perlman, Interconnections: Bridges, Routers, Switches, and Internetworking Protocols, Addison-Wesley Professional Computing Series, second edition, 1999.
[11] R. Hagens, N. Hall, and M. Rose, “Use of the internet as a subnetwork for experimentation with the OSI network layer,” RFC 1070, February 1989.
[12] H. Eriksson, “Mbone: the multicast backbone,” in Communications of the ACM, August 1994, vol. 37, pp. 54–60.
[13] Sally Floyd, Van Jacobson, Ching-Gung Liu, Steven McCanne, and Lixia Zhang, “A reliable multicast framework for lightweight sessions and application-level framing,” IEEE/ACM Transactions on Networking, vol. 5, no. 6, pp. 784–803, Dec. 1997.
[14] Yair Amir, Claudiu Danilov, and Jonathan Stanton, “A low latency, loss-tolerant architecture and protocol for wide-area group communication,” in Proceedings of the International Conference on Dependable Systems and Networks. June 2000, pp. 327–336, IEEE Computer Society Press, Los Alamitos, CA.
[15] Paul Francis, “Yoid: Extending the internet multicast architecture,” http://www.icir.org/yoid/docs/yoidArch.ps, April 2000.
[16] J. Touch and S. Hotz, “X-bone: a system for automatic network overlay deployment,” Third Global Internet Mini Conference in conjunction with Globecom98, November 1998.