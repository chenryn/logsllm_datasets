# CloudPD: A Framework for Problem Determination and Diagnosis in Shared Dynamic Clouds

## Analysis Time of Each Stage of CloudPD

### Figures
- **Figure 11:** Analysis time of each stage of CloudPD.
  - (a) Hadoop analysis time
  - (b) Olio analysis time
  
  The analysis time grows sub-linearly with the number of VMs, with the most significant increase occurring during the Event Generation phase. Parallelizing event generation across different VMs can reduce this time. This experiment demonstrates the effectiveness of CloudPD's multi-layer approach, allowing it to scale to larger systems with more VMs and servers.

- **Figure 12:** Effect of VM scaling on the analysis time of CloudPD.
  - (a) Hadoop VM scaling
  - (b) Olio VM scaling

## Trace-Driven Fault Generation Results: Case Study

### Real Cloud Setting
- **Cloud Testbed:** Managed by a cloud management stack that can provision virtual machines and perform dynamic consolidation to reduce power.
- **Consolidation Manager:** pM apper [20], which has been studied extensively.
- **Clusters:**
  - Hadoop cluster: 10 VMs
  - Olio cluster: 6 VMs (4 VMs for the web server tier, 2 VMs for the database tier)

### Workload Traces
- **Traces:** Two real traces from a production data center of a Fortune 500 company running enterprise applications.
- **Duration:** 24 hours, with data points every 15 minutes.
- **Profiles:**
  - Hadoop: Relationship between workload size and CPU/memory utilization.
  - Olio: Relationship between the number of concurrent users and CPU/memory utilization of the Web and database tiers.

### Resource Profiles
- **Equations:**
  - \( \text{CPU (Olio - Web)} = \text{Users} \times 0.1 + 5 \)
  - \( \text{CPU (Olio - DB)} = \text{Users} \times 0.035 + 7.5 \)
  - \( \text{CPU (Hadoop)} = \text{DataSize} \times 2.83 + 12.9 \)

### Anomaly Detection
- **Monitoring:** CloudPD independently monitors the cluster, identifying cloud-related anomalies, workload intensity, and application anomalies.
- **Injected Anomalies:** Randomly injected in some intervals as listed in Table V.
- **Observed Anomalies:**
  - 4 intervals experienced invalid VM migrations.
  - 7 intervals experienced invalid VM sizing anomalies due to cloud reconfiguration.

### Performance Metrics
- **Table IX:** Comparing end-to-end diagnosis effectiveness for trace-driven case study.
  - **Metrics:** Recall, Precision, Accuracy, False Alarm Rate (FAR)

### Detailed Case Study
- **Figure 13:** Time-series showing the effectiveness of CloudPD and other base schemes in detecting faults in a 24-hour case study.
  - **Y1-axis:** Ground truth of intervals with anomalies and anomaly predictions.
  - **Y2-axis:** Normalized application latency.

### Undetected Anomalies
- **Table X:** Undetected anomalies for trace-driven case study.
  - **CloudPD:** Missed 2 disk hog anomalies, 2 invalid resizing events, and 1 invalid VM migration.
  - **Baselines (B1, B2, B3, B4):** Missed various types of anomalies.

## Diagnosis Overheads

### Resource Utilization
- **Table XI:** CPU usage (% CPU time), memory usage, and network bandwidth overhead of data collection using the `sar` tool.
  - **Minimal Overhead:** CloudPD introduces minimal overhead on the system.

## Conclusions

- **Proposed Framework:** CloudPD, a lightweight, automated, and accurate fault detection and diagnosis framework for shared utility clouds.
- **Key Features:**
  - Layered online learning approach.
  - Monitors a wide range of metrics across VMs and physical servers.
  - Uses pre-computed fault signatures for anomaly diagnosis.
  - Integrates remediation with a cloud management stack.
- **Evaluation:**
  - Extensive evaluation on Hadoop, Olio, and RUBiS workloads.
  - High accuracy with low false alarms in detecting and distinguishing cloud-related faults from application anomalies and workload changes.
  - First end-to-end fault management system for virtualized cloud-based anomalies.

## Acknowledgments

- **Contributors:** Anonymous reviewers, Adwait Jog, Mahshid Sedghi, Nachiappan Chidambaram, and Onur Kayiran.
- **Funding:** NSF grants and research grants from Google and Intel.

## References

- [1] K. Mahendra, G. Eisenhauer, et al., "Monalytics: Online Monitoring and Analytics for Managing Large Scale Data Centers," in ICAC, 2010.
- [2] P. Bodik, M. Goldszmidt, A. Fox, D. Woodard, H. Andersen, "Fingerprinting the Datacenter: Automated Classification of Performance Crises," in EuroSys, 2010.
- [3] T. Wood, E. Cecchet, et al., "Disaster Recovery as a Cloud Service: Economic Benefits and Deployment Challenges," in HotCloud, 2010.
- [4] M. Y. Chen, E. Kiciman, E. Fratkin, A. Fox, E. Brewer, "Pinpoint: Problem Determination in Large, Dynamic Internet Services," in DSN, 2002.
- [5] IT Downtime Financial Cost, http://tinyurl.com/itdowntime-cost.
- [6] Compuware Corporation, "Performance in the Clouds," White paper, 2011.
- [7] T. Benson, S. Sahu, A. Akella, A. Shaikh, "A First Look at Problems in the Cloud," in HotCloud, 2010.
- [8] M. Gallet, N. Yigitbasi, et al., "A Model for Space-correlated Failures in Large-scale Distributed Systems," in EuroPar, 2010.
- [9] Y. Tan, et al., "PREPARE: Predictive Performance Anomaly Prevention for Virtualized Cloud Systems," in ICDCS, 2012.
- [10] W. Chengwei, et al., "Online Detection of Utility Cloud Anomalies using Metric Distributions," in NOMS, 2010.
- [11] C. Wang, K. Viswanathan, L. Chodur, V. Talwar, W. Satterfield, K. Schwan, "Evaluation of Statistical Techniques for Online Anomaly Detection in Data Centers," in IEEE IM, 2011.
- [12] H. Kang, H. Chen, G. Jiang, "PeerWatch: A Fault Detection and Diagnosis Tool for Virtualized Consolidation Systems," in ICAC, 2010.
- [13] H. Kang, X. Zhu, J. Wong, "DAPA: Diagnosing Application Performance Anomalies for Virtualized Infrastructures," in Hot-ICE, 2012.
- [14] S. Agarwala, F. Alegre, K. Schwan, J. Mehalingham, "E2EProf: Automated End-to-End Performance Management for Enterprise Systems," in DSN, 2007.
- [15] L. Cherkasova, et al., "Anomaly? Application Change? or Workload Change? Towards Automated Detection of Application Performance Anomaly and Change," in DSN, 2008.
- [16] G. Bronevetsky, I. Laguna, B. Supinski, S. Bagchi, "Automatic Fault Characterization via Abnormality-enhanced Classification," in DSN, 2012.
- [17] G. Jing, J. Guofei, C. Haifeng, H. Jiawei, "Modeling Probabilistic Measurement Correlations for Problem Determination in Large-Scale Distributed Systems," in ICDCS, 2009.
- [18] S. Zhang, I. Cohen, J. Symons, A. Fox, "Ensembles of Models for Automated Diagnosis of System Performance Problems," in DSN, 2005.
- [19] D. Pelleg, M. Ben-Yehuda, R. Harper, L. Spainhower, T. Adeshiyan, "Vigilant: Out-of-band Detection of Failures in Virtual Machines," SIGOPS Oper. Syst. Rev., 2008.
- [20] A. Verma, P. Ahuja, A. Neogi, "pMapper: Power and Migration Cost Aware Application Placement in Virtualized Systems," in ACM/IFIP/USENIX Middleware, 2008.
- [21] A. Verma, G. Kumar, R. Koller, A. Sen, "CosMig: Modeling the Impact of Reconfiguration in a Cloud," in IEEE MASCOTS, 2011.
- [22] KNN Classifier, http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Classification/kNN.
- [23] Z. Li, X. Wang, Z. Liang, M. Reiter, "AGIS: Towards Automatic Generation of Infection Signatures," in DSN, 2008.
- [24] VMware, "Vmkperf for VMware ESX 5.0, 2011."
- [25] vSphere, "Powercli Cmdlets," http://tinyurl.com/powercli-cmd.
- [26] V. Chandola, A. Banerjee, V. Kumar, "Anomaly Detection: A Survey," ACM Computing Surveys, vol. 41, 2009.
- [27] B. Sharma, P. Jayachandran, A. Verma, C. Das, "CloudPD: A Framework for Problem Determination and Diagnosis in Shared Dynamic Clouds," Penn State, Tech. Rep. CSE #12-001, 2012.
- [28] Apache Hadoop, "Open Source BigData Framework," http://hadoop.apache.org.
- [29] W. Sobel, et al., "Cloudstone: Multi-platform, Multi-language Benchmark and Measurement Tools for Web 2.0," in CCA, 2008.
- [30] RUBiS, "E-commerce Application," http://rubis.ow2.org.

---

This optimized text provides a clear, professional, and structured overview of the CloudPD framework, its evaluation, and the results.