### Table 9: Comparison of Target Loss Value Based Search to Retroactive Optimal and Random Search

**Context**: The table below compares the target loss value-based search to retroactive optimal and random search (AutoZOOM baseline) for an untargeted attack on a robust CIFAR10 model and a targeted attack on a standard ImageNet model. The results are averaged over five runs. The "Top x%" columns indicate the total number of queries needed to find adversarial examples for x% of the total seeds.

| Model | Prioritization Method | Top 1% | Top 2% | Top 5% | Top 10% |
|-------|-----------------------|--------|--------|--------|---------|
| Robust CIFAR10 (1,000 Seeds) | Retroactive Optimal | 10.0 ± 0.0 | 20.0 ± 0.0 | 50.0 ± 0.0 | 107.8 ± 17.4 |
| | Two-Phase Strategy | 20.4 ± 2.1 | 54.2 ± 5.6 | 218.2 ± 28.2 | 826.2 ± 226.6 |
| | Random | 24,054 ± 132 | 49,372 ± 270 | 125,327 ± 686 | 251,917 ± 137 |
| Standard ImageNet (100 Seeds) | Retroactive Optimal | 1.0 ± 0.0 | 2.0 ± 0.0 | 3.0 ± 0.0 | 34.9 ± 3.7 |
| | Two-Phase Strategy | 28.0 ± 2.0 | 38.6 ± 7.5 | 3,992 ± 3,614 | 78,844 ± 11,837 |
| | Random | 15,046 ± 423 | 45,136 ± 1,270 | 135,406 ± 3,811 | 285,855 ± 8,045 |

### Results and Discussion

The results of the two black-box attacks on various datasets and different combinations of target and local models (specifically for the CIFAR10 dataset) show similar patterns. The seed prioritization results for baseline attacks are presented in Figure 5 and Table 9. For the robust CIFAR10 model, the target loss strategy significantly outperforms the random scheduling strategy. For example, to obtain 1% of the total 1,000 seeds, the target loss prioritization strategy requires an average of 1,070 queries, while the random strategy consumes 25,005 queries on average, resulting in a 96% query savings. The retroactive optimal strategy is highly effective, requiring only 34 queries.

In contrast, against the ImageNet model, the target loss-based strategy offers little improvement over random scheduling (Figure 5b). However, the two-phase strategy still performs significantly better than random ordering.

We hypothesize that the difference in performance between the target loss strategy (for baseline attacks) and the two-phase strategy (for hybrid attacks) on ImageNet is due to the baseline attack starting from natural images, which ImageNet models tend to overfit. This makes the target loss value less useful for predicting the actual attack cost, leading to poor prioritization. Conversely, the hybrid attack starts from local adversarial examples, which deviate from the natural distribution, making the target loss more correlated with the true attack cost and improving prioritization.

### Full Two-Phase Strategy Results

Figure 6 and Table 10 present the results for the full two-phase strategy. The seed-prioritized two-phase strategy approaches the performance of the (unrealizable) retroactive optimal strategy and substantially outperforms random scheduling. Table 10 shows the number of queries needed using each prioritization method to successfully attack 1%, 2%, 5%, and 10% of the total candidate seeds (1,000 images for CIFAR10 and 100 images for ImageNet).

For the robust CIFAR10 model, obtaining 10 new adversarial examples (1%) costs 20.4 queries on average using our two-phase strategy, compared to 24,054 queries with random ordering. For ImageNet, the cost of obtaining the first new adversarial example (1%) using our two-phase strategy is 28 queries, compared to over 15,000 with random prioritization.

### Conclusion

Our results enhance our understanding of black-box attacks against machine learning classifiers and demonstrate how efficiently an attacker can successfully attack even robust target models. We propose a hybrid attack strategy that combines recent transfer-based and optimization-based attacks. Across multiple datasets, our hybrid attack strategy dramatically improves state-of-the-art results in terms of average query cost, providing more accurate estimates of black-box adversary costs. We further consider a practical attack setting where the attacker has limited resources and aims to find many adversarial examples with a fixed number of queries. A simple seed prioritization strategy can significantly improve the overall efficiency of hybrid attacks.

### Availability

Implementations and data for reproducing our results are available at [GitHub Repository](https://github.com/suyeecav/Hybrid-Attack).

### Acknowledgements

This work was supported by grants from the National Science Foundation (#1619098, #1804603, and #1850479) and research awards from Baidu and Intel, as well as cloud computing grants from Amazon.

### References

[1] Abdullah Al-Dujaili and Una-May O’Reilly. There are no bit parts for sign bits in black-box attacks. arXiv:1902.06894, 2019.
[2] Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, and Mani Srivastava. GenAttack: Practical black-box attacks with gradient-free optimization. In The Genetic and Evolutionary Computation Conference, 2019.
[3] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2017.
[4] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Exploring the space of black-box attacks on deep neural networks. In European Conference on Computer Vision, 2019.
[5] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In International Conference on Learning Representations, 2018.
[6] Thomas Brunner, Frederik Diehl, Michael Truong Le, and Alois Knoll. Guessing smart: Biased sampling for efficient black-box adversarial attacks. arXiv:1812.09803, 2018.
[7] Nicholas Carlini, Ulfar Erlingsson, and Nicolas Papernot. Prototypical examples in deep learning: Metrics, characteristics, and utility. https://openreview.net/forum?id=r1xyx3R9tQ, 2018.
[8] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy, 2017.
[9] Jianbo Chen and Michael I Jordan. Boundary attack++: Query-efficient decision-based adversarial attack. arXiv:1904.02144, 2019.
[10] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In 10th ACM Workshop on Artificial Intelligence and Security, 2017.
[11] Steven Chen, Nicholas Carlini, and David Wagner. Stateful detection of black-box adversarial attacks. arXiv:1907.05587, 2019.
[12] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Query-efficient hard-label black-box attack: An optimization-based approach. In International Conference on Learning Representations, 2019.
[13] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior. arXiv:1906.06919, 2019.
[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In IEEE Conference on Computer Vision and Pattern Recognition, 2009.
[15] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.
[16] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examples by translation-invariant attacks. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.
[17] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.
[18] Chuan Guo, Jacob R Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q Weinberger. Simple black-box adversarial attacks. In International Conference on Machine Learning, 2019.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.
[20] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[21] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In International Conference on Machine Learning, July 2018.
[22] Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial attacks with bandits and priors. In International Conference on Learning Representations, 2019.
[23] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report, 2009.
[24] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In ICLR Workshop, 2016.
[25] Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.
[26] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haﬀner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[38] Fnu Suya, Yuan Tian, David Evans, and Paolo Papotti. Query-limited black-box attacks to classifiers. In NIPS Workshop in Machine Learning and Computer Security, 2017.
[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.
[40] Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita Vemuri. Targeted adversarial examples for black box audio systems. arXiv:1805.07820, 2018.
[41] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations, 2018.
[42] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations, 2019.
[43] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Hsieh Cho-Jui Yi, Jinfeng, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. In AAAI Conference on Artificial Intelligence, 2018.
[44] Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies. In IEEE Congress on Evolutionary Computation, 2008.
[45] Cihang Xie, Zhishuai Zhang, Jianyu Wang, Yuyin Zhou, Zhou Ren, and Alan Yuille. Improving transferability of adversarial examples with input diversity. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.
[27] Pengcheng Li, Jinfeng Yi, and Lijun Zhang. Query-efficient black-box attack by active learning. In IEEE International Conference on Data Mining, 2018.
[28] Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing Gong. Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks. In International Conference on Machine Learning, 2019.
[29] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In International Conference on Learning Representations, 2017.
[30] Aleksander Madry. CIFAR10 adversarial examples challenge. https://github.com/MadryLab/cifar10_challenge, July 2017.
[31] Aleksander Madry. MNIST adversarial examples challenge. https://github.com/MadryLab/mnist_challenge, June 2017.
[32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
[33] Seungyong Moon, Gaon An, and Hyun Oh Song. Parsimonious black-box adversarial attacks via efficient combinatorial optimization. In International Conference on Machine Learning, 2019.
[34] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations for deep networks. In CVPR Workshop, 2017.
[35] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv:1605.07277, 2016.
[36] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In ACM Asia Conference on Computer and Communications Security, 2017.
[37] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.