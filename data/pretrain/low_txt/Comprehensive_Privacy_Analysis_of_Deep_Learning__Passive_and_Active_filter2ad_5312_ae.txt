### The Impact of the Global Active Gradient Ascent Attack on the Target Model’s Training Process

**Figure 7:**
- **(a)**, **(b)**, and **(c)**: These figures illustrate the gradient norms of various instances from the Purchase100 dataset during the training phase, with target instances under attack.

### Dataset Sizes in Federated Learning Experiments

| Datasets | Training | Test |
|----------|----------|------|
| CIFAR100 | 30,000   | 10,000 |
| Texas100 | 8,000    | 70,000 |
| Purchase100 | 10,000 | 50,000 |

**Training Data for Members:**
- **CIFAR100**: 15,000
- **Texas100**: 4,000
- **Purchase100**: 5,000

**Inference Attack Model:**
- **Training (Members)**: 5,000
- **Test (Non-members)**: 5,000
- **Test (Members)**: 5,000
- **Test (Non-members)**: 5,000

### Accuracy of the Passive Global Attacker in the Federated Setting

| Observed Epochs | Attack Accuracy |
|-----------------|-----------------|
| 5, 10, 15, 20, 25 | 57.4%           |
| 10, 20, 30, 40, 50 | 76.5%           |
| 50, 100, 150, 200, 250 | 79.5%         |
| 100, 150, 200, 250, 300 | 85.1%         |

### Accuracy of the Passive Local Attacker for Different Numbers of Participants

| Number of Participants | Attack Accuracy |
|------------------------|-----------------|
| 2                      | 89.0%           |
| 3                      | 78.1%           |
| 4                      | 76.7%           |
| 5                      | 67.2%           |

### Analysis

- For the CIFAR100-Alexnet model, when the attacker has access to several training epochs, a high membership attack accuracy is achieved.
- In the Texas100 and Purchase100 datasets, the accuracy of the attack decreases compared to the stand-alone setting due to the averaging in federated learning scenarios, which reduces the impact of each individual party.
- A local attacker, who can only observe the aggregate model parameters, achieves lower accuracy compared to the global attack. This is because the aggregate model parameters limit the extent of membership leakage.
- The accuracy of the local attacker degrades as the number of participants increases, as shown in Table XIII for the CIFAR100 on Alexnet model.

### Federated Learning Settings: Active Inference Attacks

- **Gradient Ascent Attacker**: The attacker manipulates the learning process to improve membership inference accuracy by updating data features towards ascending the gradients of the global or local model.
- **Figure 7**:
  - **(a)**: When the attacker ascends on the gradients of the target instances, the gradient norm of the target members becomes similar to that of non-target member instances across various training epochs.
  - **(b)**: This is not true for non-member instances, as the model does not explicitly change their gradient.
  - **(c)**: The distinction between the gradient norm of member and non-member target instances is depicted, showing that the active gradient ascent attacker forces the target model to behave differently between target member and non-member instances, making the membership inference attack easier.

- **Isolating Attacker**: By isolating a target participant and segregating its learning process, the attacker can overcome the negative influence of parameter aggregation in federated learning, significantly increasing the attack accuracy.

### Related Work

- **Membership Inference Attacks**: Multiple research papers have studied membership inference attacks in black-box settings, including Homer et al. [4], Shokri et al. [6], Salem et al. [17], and Yeom et al. [7].
- **Other Inference Attacks**: Various types of inference attacks, such as input inference, attribute inference, parameter inference, and side-channel attacks, can be performed by an attacker with additional information about the training data distribution.

### Conclusions

- We designed and evaluated novel white-box membership inference attacks against neural network models, demonstrating their effectiveness in both stand-alone and federated settings.
- Even well-generalized models are significantly susceptible to such white-box membership inference attacks.
- Future research will focus on theoretical bounds on the privacy leakage of deep learning in the white-box setting.

### Acknowledgements

- This work was supported by the NSF grant CNS-1525642, the Singapore Ministry of Education Academic Research Fund Tier 1, and the donation of a Titan Xp GPU from NVIDIA Corporation.

### References

- [1] C. Dwork, A. Smith, T. Steinke, and J. Ullman, “Exposed! a survey of attacks on private data,” 2017.
- [2] I. Dinur and K. Nissim, “Revealing information while preserving privacy,” in Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. ACM, 2003, pp. 202–210.
- [3] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou, “Learning your identity and disease from research papers: information leaks in genome wide association study,” in Proceedings of the 16th ACM conference on Computer and communications security. ACM, 2009, pp. 534–544.
- [4] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig, “Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays,” PLoS genetics, vol. 4, no. 8, p. e1000167, 2008.
- [5] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan, “Robust traceability from trace amounts,” in Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on. IEEE, 2015, pp. 650–669.
- [6] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference attacks against machine learning models,” in Security and Privacy (SP), 2017 IEEE Symposium on, 2017.
- [7] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in machine learning: Analyzing the connection to overfitting,” in IEEE Computer Security Foundations Symposium, 2018.
- [8] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon, “Federated learning: Strategies for improving communication efficiency,” arXiv preprint arXiv:1610.05492, 2016.
- [9] A. Krizhevsky, “Learning multiple layers of features from tiny images,” Citeseer, Tech. Rep., 2009.
- [10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105.
- [11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
- [12] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks.” in CVPR, vol. 1, no. 2, 2017, p. 3.
- [13] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning, 2016, vol. 1.
- [14] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and computing, vol. 17, no. 4, pp. 395–416, 2007.
- [15] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding deep learning requires rethinking generalization,” arXiv preprint arXiv:1611.03530, 2016.
- [16] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A. Gunter, and K. Chen, “Understanding membership inferences on well-generalized learning models,” arXiv preprint arXiv:1802.04889, 2018.
- [17] A. Salem, Y. Zhang, M. Humbert, M. Fritz, and M. Backes, “ML-leaks: Model and data independent membership inference attacks and defenses on machine learning models,” arXiv preprint arXiv:1806.01246, 2018.
- [18] J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro, “Logan: evaluating privacy leakage of generative models using generative adversarial networks,” arXiv preprint arXiv:1705.07663, 2017.
- [19] L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting unintended feature leakage in collaborative learning,” arXiv preprint arXiv:1805.04049, 2018.
- [20] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. ACM, 2015, pp. 1310–1321.
- [21] H. B. McMahan, E. Moore, D. Ramage, S. Hampson et al., “Communication-efficient learning of deep networks from decentralized data,” arXiv preprint arXiv:1602.05629, 2016.
- [22] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to sensitivity in private data analysis,” in Theory of Cryptography Conference. Springer, 2006, pp. 265–284.
- [23] C. Dwork, A. Roth et al., “The algorithmic foundations of differential privacy,” Foundations and Trends® in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2014.
- [24] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differentially private empirical risk minimization,” Journal of Machine Learning Research, vol. 12, no. Mar, pp. 1069–1109, 2011.
- [25] R. Bassily, A. Smith, and A. Thakurta, “Private empirical risk minimization: Efficient algorithms and tight error bounds,” in Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on. IEEE, 2014, pp. 464–473.
- [26] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, “Deep learning with differential privacy,” in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 308–318.
- [27] N. Papernot, S. Song, I. Mironov, A. Raghunathan, K. Talwar, and Ú. Erlingsson, “Scalable private learning with PATE,” arXiv preprint arXiv:1802.08908, 2018.
- [28] M. Nasr, R. Shokri, and A. Houmansadr, “Machine learning with membership privacy using adversarial regularization,” in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2018, pp. 634–646.
- [29] J. Hamm, “Minimax filter: learning to preserve privacy from inference attacks,” The Journal of Machine Learning Research, vol. 18, no. 1, pp. 4704–4734, 2017.
- [30] C. Huang, P. Kairouz, X. Chen, L. Sankar, and R. Rajagopal, “Generative adversarial privacy,” arXiv preprint arXiv:1807.05306, 2018.
- [31] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that exploit confidence information and basic countermeasures,” in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. ACM, 2015, pp. 1322–1333.
- [32] N. Carlini, C. Liu, J. Kos, Ú. Erlingsson, and D. Song, “The secret sharer: Measuring unintended neural network memorization & extracting secrets,” arXiv preprint arXiv:1802.08232, 2018.
- [33] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing machine learning models via prediction APIs,” in USENIX Security, 2016.
- [34] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine learning,” arXiv preprint arXiv:1802.05351, 2018.
- [35] L. Wei, Y. Liu, B. Luo, Y. Li, and Q. Xu, “I know what you see: Power side-channel attack on convolutional neural network accelerators,” arXiv preprint arXiv:1803.05847, 2018.
- [36] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici, “Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers,” International Journal of Security and Networks, vol. 10, no. 3, pp. 137–150, 2015.
- [37] C. Robert, Machine learning, a probabilistic perspective. Taylor & Francis, 2014.

### Appendix A: Architecture of the Attack Model

| Name | Layers | Details |
|------|--------|---------|
| Output Component | 2 Fully Connected Layers | Sizes: 128, 64; Activation: ReLU; Dropout: 0.2 |
| Label Component | 2 Fully Connected Layers | Sizes: 128, 64; Activation: ReLU; Dropout: 0.2 |
| Loss Component | 2 Fully Connected Layers | Sizes: 128, 64; Activation: ReLU; Dropout: 0.2 |
| Gradient Component | Convolutional Layer | Kernels: 1000; Kernel size: 1× Next layer; Stride: 1; Dropout: 0.2 |
| Encoder Component | 4 Fully Connected Layers | Sizes: 256, 128, 64, 1; Activation: ReLU; Dropout: 0.2 |
| Decoder Component | 2 Fully Connected Layers | Sizes: 64, 4; Activation: ReLU; Dropout: 0.2 |

---

This optimized version provides a clear, coherent, and professional presentation of the content.