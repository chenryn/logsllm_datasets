### Overview of Existing Research

The vast majority of the reviewed papers (104 out of 115) only capture a single snapshot of scanning results, without considering the dynamic changes in labels. A small number of studies [18, 58, 73] acknowledge the potential for label changes and opt to wait for a period before using the labels. The waiting time varies significantly, ranging from ten days [58] to more than two years [18]. Other researchers submit files multiple times to observe any differences in the results [25, 41, 67, 79, 83].

### Our Goals

Our literature review has two key takeaways:
1. **Label Determination**: Most researchers use a simple threshold or a trusted set of vendors to determine if a file is malicious. These thresholds and trusted sets are often chosen without validating their rationality.
2. **Dynamic Changes**: Most researchers only take one snapshot of VirusTotal results, failing to consider possible changes over time.

In this paper, we aim to conduct empirical measurements on label dynamics to provide justifications for existing labeling methods. More importantly, we seek to identify potentially questionable approaches and propose better alternatives. Several works are related to ours, and we briefly discuss the differences below.

### Closely Related Works

- **Peng et al. [57]**: Examined URL scanning engines of VirusTotal for phishing URL detection over a month. Our work focuses on anti-malware engines (file scanning) over a longer period (a year). We show that anti-malware engines have different and sometimes contradictory characteristics compared to URL engines.
- **Kantchelian et al. [40]**: Proposed a machine learning model to aggregate VirusTotal labels, assuming independence among VirusTotal engines. Our study provides evidence contradicting this assumption. Additionally, [40] did not control the file re-scanning frequency, missing the opportunity to observe fine-grained label dynamics. They also assumed that VirusTotal labels become stable after four weeks, which our observations do not support. Our unique contribution is identifying previously unknown dynamic patterns (e.g., hazard label flips), measuring the "influence" among vendors, and providing simpler suggestions for researchers.

### Other Related Works

VirusTotal also provides malware family information for known malware samples. Researchers have found that different VirusTotal engines may attribute the same malware to different families [37, 55, 63]. One study [55] measured the correctness and inconsistency of malware family names based on a manually labeled dataset. Other works aim to assign a family name to a malware sample by aggregating the family names reported by different VirusTotal engines [37, 63]. These studies focus on a different aspect of VirusTotal engines and are based on a single snapshot of scan results, without considering the possible changes over time.

### Data Collection

To achieve our goals, we need to capture the dynamic label changes of different engines over a long period. Our measurement follows two main principles:
1. **Fresh Files**: We choose files submitted to VirusTotal for the first time to observe label dynamics from the beginning.
2. **Fine-Grained Changes**: We use the rescan API to trigger daily analysis of our files and the report API to query the latest scanning results. Table 2 summarizes the datasets we collected.

| Dataset | Main | Malware-I | Malware-II | Benign-I | Benign-II |
|---------|------|-----------|------------|----------|-----------|
| # Files | 14,423 | 60 | 60 | 80 | 156 |
| Type | U | M | M | B | B |
| Observation Period | 08/2018 – 09/2019 | 06/2019 – 09/2019 | 06/2019 – 09/2019 | 06/2019 – 09/2019 | 07/2019 – 09/2019 |
| # Days | 396 | 105 | 97 | 93 | 70 |

#### Main Dataset

To obtain a large set of fresh files, we used VirusTotal's distribute API, which returns information on the latest user submissions. We randomly sampled 14,423 PE files submitted to VirusTotal on August 31, 2018, for the first time. We focused on PE files as they are the most common type. We ensured a mix of malicious and benign files: about half (7,197) had at least one "malicious" label, and the other half (7,226) were labeled "benign" by all engines. From August 31, 2018, to September 30, 2019, we invoked the rescan API daily to monitor label changes. We successfully collected data on 378 out of 396 days (95.5%), with 18 days (4.5%) missed due to technical issues.

#### Ground-Truth Dataset

The main dataset lacks ground-truth labels, so we created a smaller, curated set of files to assess engine correctness. Creating ground-truth files was challenging because we could not use any engine's labels and needed fresh files. 

- **Ground-Truth Malware**: We obfuscated well-known ransomware to create new binaries, ensuring they had never been scanned by VirusTotal. We applied CodeVirtualizer and Themida to four ransomware samples, creating two sets of 60 new malware samples each. We manually verified the malicious actions and confirmed the samples' freshness.
- **Ground-Truth Benign**: We generated 80 obfuscated goodware and 156 real-world goodware. The obfuscated goodware was comparable to the ground-truth malware, while the real-world goodware included programs built from GNU Core Utilities, Mono, binutils, notepad++, and fleck projects.

#### Limitations

Our ground-truth sets are limited in scale and diversity, biased towards obfuscated files and seeded with ransomware. This is due to the difficulty in obtaining a large number of fresh files and the manual effort required to validate obfuscation. Despite these limitations, the ground-truth sets complement the main dataset, allowing us to measure fine-grained label dynamics and cross-examine engine correctness.

### Data Summary and Preprocessing

Across the five datasets, we collected 375,520,749 measurement points, characterized by file-ID, timestamp, engine name, and label. These points were generated by 78 different engines. We excluded 13 engines (nine newly added and four removed) and focused on the remaining 65 engines, resulting in 343,585,060 relevant measurement points.

### Measuring Label Dynamics

In this section, we formally model the label changes on VirusTotal. We characterize different types of temporal label changes and reason about their possible causes. We estimate the time required for labels to stabilize across engines and assess the impact of these dynamics on the labeling outcome, given that most researchers only submit files once for scanning.