### Data Handling and Analysis

Researchers must not treat privately shared data as a general resource that can be analyzed at will without the explicit consent of the provider. It is generally advisable for researchers to retain the data they have collected to address any concerns with their analysis or to follow up on questions generated by peer review or subsequent publication. However, it is also important to delete data provided for a specific purpose as soon as it is no longer needed. This ensures that the data does not fall into the wrong hands or lead to the temptation for reuse in another context. The tension between these competing goals is fundamental, and we encourage providers and users to explicitly address data retention as part of an Acceptable Use policy.

If a researcher wishes to use data for another task, they should seek permission from the provider. First and foremost, the provider may not want their data used for the new purpose the researcher has in mind. Additionally, the provider might already be engaged in research on the new topic, and being "scooped" by someone using their own data would be highly frustrating.

Furthermore, the provider may have transformed the shared data in a way that can (sometimes invisibly) render the researcher's results incorrect. For example, consider a packet trace where the provider removed large bulk transfers corresponding to backup traffic because these consumed a great amount of space in the trace yet had little bearing on the original analysis. If this data were subsequently studied for network utilization, it would show the network as much less loaded than it actually was.

Finally, the sensitivity "threat model" the provider had in mind when originally providing the trace may differ in the context of the new form of analysis. The researcher using the data has a strong obligation to honor the provider's concerns.

### De-anonymizing Data

Efforts to de-anonymize shared measurement data have the potential to cause serious problems for future data release across the community. Careless de-anonymization efforts can violate privacy, increase a site’s exposure to security problems, or potentially embarrass a data provider. Additionally, careless reporting on such activities from the research community itself can profoundly change the threat model applied to future data releases. Therefore, it is important to appreciate that such activities can have a chilling effect across the community, making potential providers reluctant to release data in the future.

De-anonymization of measurement data should not be undertaken lightly. There are scholarly reasons to attempt to de-anonymize data, such as illustrating how to leverage a modicum of information to untangle an anonymization scheme, which can point to better anonymization techniques. However, researchers wishing to engage in this sort of analysis must proceed carefully. They should undertake such an activity only with the consent of the data provider—either because such activity is part of the normal Acceptable Use policy provided with the data, or per a specific arrangement with the data provider. Reporting on such an investigation should refrain from openly publishing the specific, de-anonymized data.

The data provider often holds the ground truth, so they can inform researchers whether their de-anonymization schemes succeeded. As part of the scientific process, an investigator attempting to de-anonymize data for scholarly purposes should try to verify their results with the data provider. A natural hesitation to approaching providers in this way can arise due to a perceived conflict of interest. The data provider may simply indicate that the researcher did not properly de-anonymize the data, regardless of accuracy, in the hopes that the researcher will not publicize their efforts and keep unrevealed any information problematic for the provider. On the other hand, the data provider has a vested interest in understanding flaws in their anonymization scheme to fix problems before releasing more data.

We also note that reports of de-anonymization techniques should not directly unmask details of a dataset (e.g., IP addresses). It is significantly better to describe the process and note that the provider has verified that it indeed recovers sensitive data. Providing a fix for the problem in terms of a more secure way to perform the anonymization is also quite useful. We encourage reviewers, program committees, and editors to require authors to follow this path rather than publishing sensitive details of datasets.

To avoid the thorny issues of dealing with and reporting on others' data, researchers studying attacks on anonymization can focus on the anonymization techniques rather than the anonymized data. That is, the researchers can re-apply the anonymization used by a particular data provider but to data that they themselves capture. They then assess the possible attacks against the new data. Such an approach can completely factor out a provider’s sensitive data from the investigation. However, collecting data from a variety of sources inevitably yields different artifacts. Therefore, without using the provider’s data, the researcher may not get as full a picture of the strength of the provider’s anonymization techniques.

### Case Study

As a concrete example, [7] reports on an investigation into de-anonymizing several recently released datasets. The authors of the study do this with an eye towards enhancing the community’s understanding of anonymization techniques and where they break down. The study provides a useful illustration of several of the items discussed above.

The study reports apparent mappings between the IP addresses in anonymized datasets and the real IP addresses, as inferred by the authors’ techniques. However, the authors did not approach the data providers regarding this attempt, for fear of creating a conflict of interest [11], illustrating the uncertain community culture regarding how to undertake such studies. The downside to the study not including such interactions is that the authors were unable to compare their results with ground truth—and thus, in fact, ended up incorrectly de-anonymizing nearly all of the IP address mappings reported in the paper for our LBNL dataset [2], to the detriment of assessing the underlying scientific issues. On the other hand, the authors’ de-anonymization scheme clearly has significant merit, as they employed the same anonymization techniques as used on the LBNL traces on their own packet traces, for which their de-anonymization techniques worked well. Taken together, the above two points also nicely illustrate why a breadth of data can be highly beneficial when analyzing measurement data.

The final point we draw from this example is that the concerns expressed in this section are not theoretical. Exposing a purported mapping of anonymized IP addresses to real IP addresses risks making further release of data from LBNL more difficult. While the general form of the techniques presented in [7] is well known and was, in fact, taken into account by LBNL [13], the threat model used at LBNL was of a malicious attacker—not the research community—scouring the data for information. At a minimum, the actions of the research community will need to be explained and defended to LBNL’s decision-makers (likely the CIO) before additional data gains approval for release.

### Notification and Acknowledgment

Data providers should consider explicitly stating what sort of notification they would like when a researcher uses their data or when it later appears in a publication, and what sort of acknowledgment any such publications should include. Naturally, data users should honor these requests. If data users are uncertain regarding the provider’s desired notification policies, they should attempt to contact the provider to learn them. In the absence of explicit guidance, it is best to assume that the provider desires notification and an acknowledgment in publications, including the location of the data, if publicly available.

These points may seem obvious, but they frame an additional facet of such notification/credit of which data users are often unaware: some data providers find it highly beneficial—either internal to their organization or when interacting with their funders—to tabulate the uses that researchers have made of the released data. Since data release entails significant work for the provider in gaining the institute’s approval, obtaining funding to support an altruistic activity, and anonymizing the data, it behooves the community to give providers the necessary “ammo” for presenting a case that such release has broad benefit and results in positive public recognition for both the institute and those responsible for the data release.

### Interactions

In the previous sections, we have discussed what data providers should furnish to users and what responsibilities the users of the data have to the provider. In this section, we explore issues regarding subsequent interactions between data providers and data users. Analyzing measurement data is often a messy process, whereby additional context can often shed much light on the observed phenomena. Unfortunately, when using someone else’s data, the amount of additional context is often in short supply. Anonymizing data—nearly always a requirement when sharing—tends to introduce additional blind spots into the analysis process, which can leave researchers using shared data with lingering questions.

We advocate that researchers should ask data providers explicit questions when such situations arise, rather than making independent assumptions or assertions about the data. When doing so, researchers should temper their questions to the data providers to only those that are vital to their analysis. Of course, data providers may or may not be able to answer the questions for a variety of reasons (e.g., lack of time/energy, lack of additional context, privacy/competitive concerns, etc.). Further, seemingly mundane questions can result in a large amount of analysis to find an acceptable answer. While we do not wish to put providers on the hook for answering every question that comes their way, we suggest that providers make reasonable efforts to answer reasonable questions about data sets they release, to help foster an effective culture for sharing measurement data.

As discussed in § 3.3, one natural place where puzzles arise concerns working on de-anonymizing data. As sketched above, this activity should only be conducted under mutual agreement between the provider and the researcher. As part of this agreement, the parties should discuss information the provider will convey when checking the de-anonymized data against ground truth.

In cases where researchers ultimately must make assumptions about the data because they cannot get answers from the provider, they should explicitly note these assumptions when reporting on the data analysis. In addition, they should frame their efforts to work with the data provider. Peer reviewers should expect communication on key points of the analysis between providers and researchers and resist cases where researchers have seemingly not made efforts to validate their assumptions with data providers. We stress, however, that we advocate applying such a standard only for key analysis points; not for minor or tangential aspects of the data analysis.

### Summary

Our goal for this note is to help evolve the community’s understanding about the care required when releasing measurement data and the sensitivity of others using such data. We advocate that data providers be explicit in terms of a dataset’s acceptable use, and researchers thoughtful in the reporting of potentially sensitive information gleaned from others’ data. Data providers should also convey what interactions they desire or will accommodate, and researchers should comply with such interactions.

In general, measurement is a painfully laborious undertaking, and therefore, there is great benefit in leveraging others’ efforts in the form of shared measurement data. But providing such data is not without its own significant labors. Thus, it behooves the research community to foster a culture to support such sharing as best we can.

### Acknowledgments

We thank Paul Barford, Fabian Monrose, and Mike Reiter for their discussion of the issues presented in this note and for feedback on a draft version. This work was supported in DHS Award HSHQPA4X03322 and NSF Awards ITR/ANI-0205519 and NSF-0433702. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors or originators and do not necessarily reflect the views of the National Science Foundation.

### References

[1] Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD). http://crawdad.cs.dartmouth.edu/.

[2] Enterprise tracing project. http://www.icir.org/enterprise-tracing/.

[3] Protected Repository for the Defense of Infrastructure against CyberThreats. http://www.predict.org/.

[4] M. Allman, E. Blanton, and W. Eddy. A Scalable System for Sharing Internet Measurements. In Passive and Active Measurement Workshop, Mar. 2002.

[5] E. Blanton. tcpurify, May 2004. http://irg.cs.ohiou.edu/~eblanton/tcpurify/.

[6] E. Blanton. Personal communication, Apr. 2007.

[7] S. Coull, C. Wright, F. Monrose, M. Collins, and M. Reiter. Playing Devil’s Advocate: Inferring Sensitive Information from Anonymized Network Traces. In Proceedings of the Network and Distributed System Security Symposium, Feb. 2007.

[8] J. Heidemann. Personal communication, Apr. 2007.

[9] k. claffy, M. Crovella, T. Friedman, C. Shannon, and N. Spring. Community-Oriented Network Measurement Infrastructure (CONMI) Workshop Report. ACM Computer Communication Review, 36(2):41–48, Apr. 2006.

[10] G. Minshall. tcpdpriv, Aug. 1997. http://ita.ee.lbl.gov/html/contrib/tcpdpriv.html.

[11] F. Monrose and M. Reiter. Personal communication, Apr. 2007.

[12] R. Pang, M. Allman, M. Bennett, J. Lee, V. Paxson, and B. Tierney. A First Look at Modern Enterprise Traffic. In ACM SIGCOMM/USENIX Internet Measurement Conference, Oct. 2005.

[13] R. Pang, M. Allman, V. Paxson, and J. Lee. The Devil and Packet Trace Anonymization. ACM Computer Communication Review, 36(1), Jan. 2006.

[14] R. Pang and V. Paxson. A High-Level Programming Environment for Packet Trace Anonymization and Transformation. In ACM SIGCOMM, Aug. 2003.

[15] V. Paxson. Internet Traffic Archive. http://ita.ee.lbl.gov/.

[16] V. Paxson. Strategies for Sound Internet Measurement. In ACM SIGCOMM Internet Measurement Conference, Oct. 2004.

[17] C. Shannon, D. Moore, K. Keys, M. Fomenkov, B. Huffaker, and kc claffy. The Internet Measurement Data Catalog. ACM Computer Communication Review, 35(5), Oct. 2005.

[18] J. Xu, J. Fan, M. H. Ammar, and S. B. Moon. Prefix-Preserving IP Address Anonymization: Measurement-Based Security Evaluation and a New Cryptography-Based Scheme. In Proc. of the 10th IEEE International Conference on Network Protocols, 2002.