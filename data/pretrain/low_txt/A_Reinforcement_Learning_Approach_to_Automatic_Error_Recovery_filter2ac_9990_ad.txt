### Reinforcement Learning-Trained Policy and Its Exceptions

The reinforcement learning (RL)-trained policy generally performs well, with a few notable exceptions, such as error type 23 (Figure 11(a)). Upon closer examination of the training data for error type 23, it becomes evident that some new patterns in the test set are not represented in the training set. Consequently, the trained policy is suboptimal and may not perform consistently. As the size of the training data increases, the generated policy becomes more precise, and the hybrid approach nearly matches the performance of the trained policy, as shown in Figure 11(b).

### Time Cost Comparison

Figure 12 summarizes the total time cost for both the original user-defined policy and the hybrid policy. Similar to the trained policy, the hybrid policy achieves, on average, more than a 10% improvement over the original policy. Specifically, when the policy is trained with 40% of the log, the hybrid approach reduces the original downtime by 89.18%.

### Learning Rate Optimization

In this section, we discuss our efforts to improve the learning process and reduce training time. We employed a technique called a selection tree during the learning process. To construct the selection tree, we considered the top two repair actions each time we generated the policy from the Q-values. If the expected total cost of the second-best action is sufficiently close to that of the best action (based on a predefined threshold), both actions are chosen as candidates. Otherwise, only the best action is selected. The selection tree is then built iteratively by placing these candidate actions as children of the previous repair action, and the optimal policy is generated by scanning the tree.

Figure 13 compares the training time of this method (with a selection tree) to the standard RL training (without a selection tree), using a maximum of 160,000 sweeps (training set proportion = 0.4). Additionally, Figure 14 illustrates the performance of policies trained using both methods. It is clear that, with the standard RL method, some training processes do not converge to the optimal policy even after 160,000 sweeps. In contrast, using a selection tree, we can accelerate the learning rate and successfully find the optimal policy within 40,000 sweeps in our experiment.

### Related Work

Gray's classic text on failure analysis [14] provides an overview of failure statistics for commercially available fault-tolerant systems and discusses various approaches to software fault tolerance, focusing on preventing failures or reducing their frequency.

More recent work has explored the use of statistical learning techniques in automated fault diagnosis and performance management. For example, Ma and Hellerstein [19] presented an efficient algorithm for discovering infrequent Mutually Dependent Patterns (m-patterns) for system management, such as isolating problems in computer networks. Other tools, like Chronus [26] and PeerPressure [25], automate the task of searching for failure-inducing state changes and diagnosing misconfigurations, respectively.

Several projects, including Magpie [2] and Pinpoint [9], aim to associate failures or performance issues with specific components via request traces. Cohen et al. [11] proposed correlating low-level system metrics with high-level performance states using Tree-Augmented Naïve Bayesian networks, which can be used for clustering and retrieving signatures to provide insights into the causes of observed performance effects [12][28]. Yuan et al. [27] used pattern classification techniques to correlate known faults with system behaviors, enabling automatic recognition of future faults.

Compared to these research efforts, our approach focuses on automated error recovery rather than performance diagnosis. Tesauro et al. [24] also used a hybrid reinforcement learning approach for performance management, but our work specifically targets error recovery without setting limitations on the set of repair actions.

### Conclusion

In this paper, we introduced a novel reinforcement learning approach to enhance the framework for automatic error recovery. Our focus was on generating recovery policies when a system model is unavailable, an area that has not been fully explored. We investigated how to make appropriate decisions on which repair actions to choose when the root cause is only localized at a coarse level. Our method ensures that a locally optimal policy is found and can adapt to environmental changes without human intervention. Experimental results from a real cluster environment show that the automatically generated policy achieved more than a 10% reduction in machine downtime on average. Future work could include using generalization functions to approximate Q-learning values, introducing more complex relationships among actions, and designing initial policies that can be improved. We believe our approach will provide greater benefits as more information is gained from event monitoring and fault detection.

### Acknowledgments

We would like to thank Ken Cao, Peirong Liu, and Yi Li for clarifying the details of the user-defined policy and the recovery log. We also thank the anonymous reviewers for their helpful comments and Dwight Daniels for proofreading the paper.

### References

[1] M. Baker and M. Sullivan. The Recovery Box: Using fast recovery to provide high availability in the UNIX environment. In Proc. Summer USENIX Technical Conference, San Antonio, TX, 1992.

[2] P. Barham, A. Donnelly, R. Isaacs, and R. Mortier. Using Magpie for request extraction and workload modeling. In Proc. 6th Symposium on Operating Systems Design and Implementation (OSDI), Dec. 2004.

[3] J.F. Bartlett. A NonStop kernel. In Proc. 8th ACM Symposium on Operating Systems Principles, Pacific Grove, CA, 1981.

[4] A. Borg, W. Blau, W. Graetsch, F. Herrman, and W. Oberle. Fault Tolerance under UNIX. ACM Transactions on Computer Systems, 7(1): 1–24, Feb 1989.

[5] E. Brewer. Lessons from giant-scale services. IEEE Internet Computing, 5(4):46–55, July 2001.

[6] G. Candea and A. Fox. Crash-only software. In Proc. 9th Workshop on Hot Topics in Operating Systems, Lihue, Hawaii, 2003.

[7] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox. Microreboot – A Technique for Cheap Recovery. In Proc. 6th Symposium on Operating Systems Design and Implementation (OSDI), Dec 2004.

[8] G. Candea, E. Kiciman, S. Kawamoto, and A. Fox. Autonomous Recovery in Componentized Internet Applications. Cluster Computing Journal, 9(1), Feb 2006.

[9] M. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer. Pinpoint: Problem determination in large, dynamic systems. In Proc. 2002 Intl. Conf. on Dependable Systems and Networks, Washington, DC, June 2002.

[10] T.C. Chou. Beyond fault tolerance. IEEE Computer, 30(4):31–36, 1997.

[11] I. Cohen, M. Goldszmidt, T. Kelly, J. Symons, and J.S. Chase. Correlating instrumentation data to system states: A building block for automated diagnosis and control. In Proc. 6th Symposium on Operating Systems Design and Implementation, Dec. 2004.

[12] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly, and A. Fox. Capturing, Indexing, Clustering, and Retrieving System History. In Proceedings of the ACM Symposium on Operating Systems Principles (SOSP), Oct. 2005.

[13] A. Fox and D. Patterson. Self-repairing computers. Scientific American, June 2003.

[14] J. Gray. Why Do Computers Stop and What Can Be Done About It? 6th International Conference on Reliability and Distributed Databases, June 1987.

[15] G.J. Gordon. Stable Function Approximation in Dynamic Programming, tech report CMU-CS-95-103, 1995.

[16] K.R. Joshi, W.H. Sanders, M.A. Hiltunen, and R.D. Schlichting. Automatic Model-Driven Recovery in Distributed Systems. SRDS 2005: 25-38.

[17] K.R. Joshi, W.H. Sanders, M.A. Hiltunen, and R.D. Schlichting. Automatic Recovery Using Bounded Partially Observable Markov Decision Processes. In Proc. of the 2006 International Conference on Dependable Systems and Networks (DSN’06): 445-456.

[18] J.O. Kephart and D.M. Chess. The vision of autonomic computing. Computer, 36(1):41–50, 2003.

[19] S. Ma and J.L. Hellerstein. Mining Mutually Dependent Patterns for System Management. IEEE Journal on Selected Areas in Communications, VOL. 20, NO. 4, May 2002.

[20] T.M. Mitchell. Machine Learning. McGraw-Hill, 1997.

[21] S.A. Murphy. A Generalization Error for Q-Learning. Journal of Machine Learning Research, 6 (2005) 1073–1097.

[22] B. Murphy and T. Gent. Measuring system and software reliability using an automated data collection process. Quality and Reliability Engineering Intl., 11:341–353, 1995.

[23] R.S. Sutton. Learning to Predict by the Methods of Temporal Differences. Machine Learning 3: 9-44, 1988.

[24] G. Tesauro, R. Das, and N. Jong. Online Performance Management Using Hybrid Reinforcement Learning. First Workshop on Tackling Computer Systems Problems with Machine Learning Techniques (SysML’06), June 2006.

[25] H.J. Wang, J.C. Platt, Y. Chen, R. Zhang, and Y.M. Wang. Automatic Misconfiguration Troubleshooting with PeerPressure. In Proc. 6th Symposium on Operating Systems Design and Implementation, Dec. 2004.

[26] A. Whitaker, R.S. Cox, and S.D. Gribble. Configuration Debugging as Search: Finding the Needle in the Haystack. In Proc. 6th Symposium on Operating Systems Design and Implementation, Dec. 2004.

[27] C. Yuan, N. Lao, J.-R. Wen, J. Li, Z. Zhang, Y.-M. Wang, and W.-Y. Ma. Automated Known Problem Diagnosis with Event Traces. 1st EuroSys Conference, April 2006.

[28] S. Zhang, I. Cohen, M. Goldszmidt, J. Symons, and A. Fox. Ensembles of Models for Automated Diagnosis of System Performance Problems. In Proc. of the 2005 International Conference on Dependable Systems and Networks (DSN’05).