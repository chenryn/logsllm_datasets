### HVS-Based Authentication: Convenience, Security, and Social Acceptability

HVS (Human Visual System)-based authentication is widely regarded as more convenient and socially acceptable in public settings. This method also offers enhanced security, as it is more resistant to observation-based attacks compared to password-based and gesture-based approaches. In terms of reliability, most users still prefer password-based authentication, likely due to its proven practicality. However, we hypothesize that the reliability score for OcuLock could be significantly improved if users had more firsthand experience with the system over a longer period, given the high stability results reported in Section VIII.

### Related Work

#### HMD Authentication

Early research on HMD (Head-Mounted Display) authentication primarily focused on smart glasses for augmented reality. For instance, Chauhan et al. [6] proposed a gesture-based authentication system for Google Glass, utilizing the touchpad for input. Li et al. [28] developed a head movement biometric system triggered by auditory stimuli, while other studies used unique head movements in response to visual cues for authenticating smart glasses users [44].

More recently, efforts have been directed towards VR HMD (Virtual Reality Head-Mounted Display) authentication by adapting traditional methods. The Oculus Quest was the first commercial VR HMD to include a virtual PIN code [36]. George et al. [17] evaluated the security and usability of PINs and unlock patterns in VR HMDs using remote controllers. Other proposed methods include graphical passwords [13] and body motion biometrics [40].

Despite their effectiveness, these traditional methods are susceptible to observation-based attacks. Adversaries can observe and mimic user behavior or analyze it for side-channel attacks [30], [14]. While brain signal extraction for VR HMD authentication is relatively secure [29], it requires a cumbersome setup with multiple electrodes, making it impractical for current VR HMDs [37], [27].

In this paper, we leverage the fact that VR HMDs fully cover the user's eye area to propose an HVS-based biometric for unobservable authentication. Given that the foam face cover has direct contact with the skin around the eye sockets, we designed a usable and efficient system.

### Computation Time

Ensuring the usability of an authentication system requires minimizing computation time. The total computation time for OcuLock includes three main components: EOG (Electrooculogram) recording, signal processing, and authentication. Our measurements show that signal processing takes less than 1 ms, and authentication takes an average of 39 ms. The EOG recording time ranges from 3 to 10 seconds, as demonstrated in our experimental results. The total computation time is primarily determined by the EOG recording time, as the other components are negligible due to the use of efficient algorithms and the reuse of intermediate results, such as wavelet transform results for sympathetic energy.

While physical-world authentication systems typically take 1-2 seconds, VR interactions are generally slower due to the reliance on head and/or eye navigation in a virtual environment. User studies indicate that even simple authentication methods like PIN codes or unlock patterns take around 3 seconds in a VR setting [17]. Therefore, the 3-second authentication time of OcuLock is considered acceptable. Users can adjust the EOG recording time based on their preference for a tradeoff between authentication error and computation time.

The memory consumption for the entire authentication process is approximately 54 MB, which is reasonable for modern VR computing devices.

### Electrode Placement

In our prototype, we used conductive gel inside each electrode to measure EOG signals. The gel does not need frequent replacement (approximately every 30 minutes). For future real-world applications, dry electrodes can be used to enhance system usability, as demonstrated by JINS MEME, a commercial smart glasses device [32].

During our experiments, electrodes were transferred between subjects. We emphasize that the different EOG samples between subjects are not due to the electrode replacement. In most evaluations, electrodes were fixed at the HMD cover, ensuring consistent placement for all participants (see Figure 5). Even with minor placement differences, EOG measurements remained robust. In our temporal study (Section VIII-D), electrodes were repeatedly detached and reattached with about a 1 cm position change, yet the system could still recognize users, indicating that electrode position has a negligible effect.

### Conclusion

In this paper, we introduce OcuLock, a stable and unobservable system for authenticating users in VR HMDs. Unlike eye gaze-based systems, OcuLock leverages the HVS as a whole, extracting low-level physiological and behavioral features for biometric authentication. OcuLock is resistant to common and anticipated types of attacks, with EERs (Equal Error Rates) of 3.55% and 4.97% for impersonation and statistical attacks, respectively. The stable physiological features reduce the frequency of updating EOG templates. Our user study suggests that HVS-based authentication can simultaneously meet the requirements of convenience, security, and social comfort. Future work should focus on integrating the devices in our prototype into a unified VR HMD for more practical and large-scale user studies.

### Acknowledgment

This work was supported in part by the National Science Foundation Grants CCF-1852516, CNS-1718375, and the National Natural Science Foundation of China Grant 61972348.

### References

[1] M. Abo-Zahhad, S. M. Ahmed, and S. N. Abbas, “A novel biometric approach for human identification and verification using eye blinking signal,” IEEE Signal Processing Letters, vol. 22, no. 7, pp. 876–880, 2014.

[2] R. Barea, L. Boquete, M. Mazo, and E. López, “System for assisted mobility using eye movements based on electrooculography,” IEEE transactions on neural systems and rehabilitation engineering, vol. 10, no. 4, pp. 209–218, 2002.

[3] R. Bednarik, T. Kinnunen, A. Mihaila, and P. Fränti, “Eye-movements as a biometric,” in Scandinavian conference on image analysis. Springer, 2005, pp. 780–789.

[4] M. Brown, M. Marmor, E. Zrenner, M. Brigell, M. Bach et al., “ISCEV standard for clinical electro-oculography (EOG) 2006,” Documenta ophthalmologica, vol. 113, no. 3, pp. 205–212, 2006.

[5] A. Bulling, J. A. Ward, H. Gellersen, and G. Troster, “Eye movement analysis for activity recognition using electrooculography,” IEEE transactions on pattern analysis and machine intelligence, vol. 33, no. 4, pp. 741–753, 2010.

[6] J. Chauhan, H. J. Asghar, A. Mahanti, and M. A. Kaafar, “Gesture-based continuous authentication for wearable devices: The smart glasses use case,” in International Conference on Applied Cryptography and Network Security. Springer, 2016, pp. 648–665.

[7] Y. Chen and W. S. Newman, “A human-robot interface based on electrooculography,” in IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA’04. 2004, vol. 1. IEEE, 2004, pp. 243–248.

[8] V. Di Lollo, J.-i. Kawahara, S. S. Ghorashi, and J. T. Enns, “The attentional blink: Resource depletion or temporary loss of control?” Psychological research, vol. 69, no. 3, pp. 191–200, 2005.

[9] C. Ding and H. Peng, “Minimum redundancy feature selection from microarray gene expression data,” Journal of bioinformatics and computational biology, vol. 3, no. 02, pp. 185–205, 2005.

[10] Q. Ding, K. Tong, and G. Li, “Development of an EOG (electro-oculography) based human-computer interface,” in 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference. IEEE, 2006, pp. 6829–6831.

[11] A. T. Duchowski, “Eye tracking methodology,” Theory and practice, vol. 328, no. 614, pp. 2–3, 2007.

[12] S. Eberz, K. Rasmussen, V. Lenders, and I. Martinovic, “Preventing lunchtime attacks: Fighting insider threats with eye movement biometrics,” in Proceedings of Network and Distributed System Security Symposium. NDSS, 2015.

[13] M. Funk, K. Marky, I. Mizutani, M. Kritzler, S. Mayer, and F. Michahelles, “LookUnlock: Using spatial-targets for user-authentication on HMDs,” in CHI Conference on Human Factors in Computing Systems Late Breaking Work, 2019.

[14] D. Gafurov, E. Snekkenes, and P. Bours, “Spoof attacks on gait authentication system,” IEEE Transactions on Information Forensics and Security, vol. 2, no. 3, pp. 491–502, 2007.

[15] C. Galdi, M. Nappi, D. Riccio, and H. Wechsler, “Eye movement analysis for human authentication: a critical survey,” Pattern Recognition Letters, vol. 84, pp. 272–283, 2016.

[16] N. Garun, “Amazon will soon refund up to $70 million of in-app purchases made by children,” 2019, http://www.theverge.com/2017/4/4/15183254/amazon-ends-appeal-refund-70-million-in-app-purchases.

[17] C. George, M. Khamis, E. von Zezschwitz, M. Burger, H. Schmidt, F. Alt, and H. Hussmann, “Seamless and secure VR: Adapting and evaluating established authentication systems for virtual reality,” in Proceedings of Network and Distributed System Security Symposium. NDSS, 2017.

[18] Google, “Introducing Daydream standalone VR headsets,” https://vr.google.com/daydream/standalonevr/.

[19] C. Holland and O. V. Komogortsev, “Biometric identification via eye movement scanpaths in reading,” in 2011 International joint conference on biometrics (IJCB). IEEE, 2011, pp. 1–8.

[20] C. D. Holland and O. V. Komogortsev, “Complex eye movement pattern biometrics: Analyzing fixations and saccades,” in 2013 International conference on biometrics (ICB). IEEE, 2013, pp. 1–8.

[21] M. Joukal, Anatomy of the Human Visual Pathway. Springer, 04 2017, pp. 1–16.

[22] M. Juhola, Y. Zhang, and J. Rasku, “Biometric verification of a subject through eye movements,” Computers in biology and medicine, vol. 43, no. 1, pp. 42–50, 2013.

[23] T. Kinnunen, F. Sedlak, and R. Bednarik, “Towards task-independent person authentication using eye movement signals,” in Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications. ACM, 2010, pp. 187–190.

[24] C. Krapichler, M. Haubner, R. Engelbrecht, and K.-H. Englmeier, “VR imaging applications,” Computer interaction techniques for medical methods and programs in biomedicine, vol. 56, no. 1, pp. 65–74, 1998.

[25] D. Kumar and E. Poole, “Classification of EOG for human computer interface,” in Proceedings of the Second Joint 24th Annual Conference and the Annual Fall Meeting of the Biomedical Engineering Society][Engineering in Medicine and Biology, vol. 1. IEEE, 2002, pp. 64–67.

[26] T. B. Kuo and C. C. Yang, “Frequency domain analysis of electrooculogram and its correlation with cardiac sympathetic function,” Experimental neurology, vol. 217, no. 1, pp. 38–45, 2009.

[27] Lenovo, “Mirage Solo with Daydream,” https://www.lenovo.com/us/en/daydreamvr/.

[28] S. Li, A. Ashok, Y. Zhang, C. Xu, J. Lindqvist, and M. Gruteser, “Whose move is it anyway? Authenticating smart wearable devices using unique head movement patterns,” in 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom). IEEE, 2016, pp. 1–9.

[29] F. Lin, K. W. Cho, C. Song, W. Xu, and Z. Jin, “Brain password: A secure and truly cancelable brain biometrics for smart headwear,” in Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services. ACM, 2018, pp. 296–309.

[30] Z. Ling, Z. Li, C. Chen, J. Luo, W. Yu, and X. Fu, “I know what you enter on Gear VR,” in 2019 IEEE Conference on Communications and Network Security (CNS), June 2019, pp. 241–249.

[31] B. R. Manor and E. Gordon, “Defining the temporal threshold for ocular fixation in free-viewing visuocognitive tasks,” Journal of neuroscience methods, vol. 128, no. 1-2, pp. 85–93, 2003.

[32] J. MEME, “Jins Meme eye sensing,” 2019, https://jins-meme.com/en/products/es/.

[33] A. Nguyen, Z. Yan, and K. Nahrstedt, “Your attention is unique: Detecting 360-degree video saliency in head-mounted display for head movement prediction,” in 2018 ACM Multimedia Conference on Multimedia Conference. ACM, 2018, pp. 1190–1198.

[34] C. Nickel, T. Wirtl, and C. Busch, “Authentication of smartphone users based on the way they walk using k-NN algorithm,” in 2012 Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing. IEEE, 2012, pp. 16–20.

[35] R. F. Nogueira, R. de Alencar Lotufo, and R. C. Machado, “Fingerprint liveness detection using convolutional neural networks,” IEEE transactions on information forensics and security, vol. 11, no. 6, pp. 1206–1213, 2016.

[36] Oculus, “Getting started with your Oculus Quest,” https://support.oculus.com/855551644803876/.

[37] ——, “Our first all-in-one gaming headset,” https://www.oculus.com/quest/.

[38] ——, “Rift store: VR games, apps & more,” https://www.oculus.com/experiences/rift/.

[39] K. Pfeil, E. M. Taranta II, A. Kulshreshth, P. Wisniewski, and J. J. LaViola Jr, “A comparison of eye-head coordination between virtual and physical realities,” in Proceedings of the 15th ACM Symposium on Applied Perception, 2018, p. 18.

[40] K. Pfeuffer, M. J. Geiger, S. Prange, L. Mecke, D. Buschek, and F. Alt, “Behavioural biometrics in VR: Identifying people from body motion and relations in virtual reality,” in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 2019, p. 110.

[41] M. Porta, S. Ricotti, and C. J. Perez, “Emotional e-learning through eye tracking,” in Proceedings of the 2012 IEEE Global Engineering Education Conference (EDUCON). IEEE, 2012, pp. 1–6.

[42] P. Qvarfordt and S. Zhai, “Conversing with the user based on eye-gaze patterns,” in Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 2005, pp. 221–230.

[43] H. L. Ramkumar, “Electrooculogram,” 2019, https://eyewiki.aao.org/Electrooculogram#Testing process.

[44] C. E. Rogers, A. W. Witt, A. D. Solomon, and K. K. Venkatasubramanian, “An approach for user identification for head-mounted displays,” in Proceedings of the 2015 ACM International Symposium on Wearable Computers. ACM, 2015, pp. 143–146.

[45] I. Sluganovic, M. Roeschlin, K. B. Rasmussen, and I. Martinovic, “Using reflexive eye movements for fast challenge-response authentication,” in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 1056–1067.

[46] V. R. Society, “Virtual reality air force training,” https://www.vrs.org.uk/virtual-reality-military/air-force-training.html.

[47] C. Song, A. Wang, K. Ren, and W. Xu, “EyeVeri: A secure and usable approach for smartphone user authentication,” in IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications. IEEE, 2016, pp. 1–9.

[48] Y. Song, Z. Cai, and Z.-L. Zhang, “Multi-touch authentication using hand geometry and behavioral information,” in 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017, pp. 357–372.

[49] R. Steinberg, R. Linsenmeier, and E. Griff, “Retinal pigment epithelial cell contributions to the electroretinogram and electrooculogram,” Progress in retinal research, vol. 4, pp. 33–66, 1985.

[50] H. Steiner, S. Sporrer, A. Kolb, and N. Jung, “Design of an active multispectral SWIR camera system for skin detection and face verification,” Journal of Sensors, 2016.

[51] Viar360, “Virtual reality market size in 2018 with forecast for 2019,” 2019, https://www.viar360.com/virtual-reality-market-size-2018/.

[52] A. Walker, “Potential security threats with virtual reality technology,” 2017, https://learn.g2.com/security-threats-virtual-reality-technology.

[53] J.-G. Wang and E. Sung, “Study on eye gaze estimation,” IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 32, no. 3, pp. 332–350, 2002.

[54] Wikipedia, “Virtual reality therapy,” https://en.wikipedia.org/wiki/Virtual_reality_therapy.

[55] J. Yi, S. Luo, and Z. Yan, “A measurement study of YouTube 360 live video streaming,” in Proceedings of the 29th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video. ACM, 2019, pp. 49–54.

[56] Y. Zhang, W. Hu, W. Xu, C. T. Chou, and J. Hu, “Continuous authentication using eye movement response of implicit visual stimuli,” Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 1, no. 4, p. 177, 2018.