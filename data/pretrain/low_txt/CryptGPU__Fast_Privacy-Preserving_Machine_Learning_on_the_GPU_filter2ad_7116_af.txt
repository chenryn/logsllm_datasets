### GPU and Convolution Kernel Parameters
The parameters for the convolution kernels are selected based on those used in AlexNet [16]. The stride and padding parameters determine how the filter is applied to the input. All figures presented are log-log plots.

### Figure 2: Comparison of Total Protocol Execution Time
Figure 2 compares the total protocol execution time (in a LAN setting) on the CPU versus the GPU for point-wise evaluation of the private ReLU protocol on different-sized inputs.

### V. RELATED WORK
Privacy-preserving machine learning is a specialized area within secure computation, which can be addressed using general cryptographic methods such as secure 2-party computation (2PC) [28], secure multiparty computation [7, 8], or fully homomorphic encryption [44]. While these methods are powerful, they often introduce significant overhead. Much of the work in developing efficient protocols for scalable privacy-preserving machine learning has focused on more specialized approaches, although they still rely on the general building blocks for designing sub-protocols. Here, we survey some of these techniques.

#### Privacy-Preserving Inference
Recent works have developed specific protocols for private inference in deep learning models (e.g., [45, 1, 46, 47, 2, 48, 49, 3, 29, 50, 4, 5, 51, 30, 52, 53, 24, 54, 6]). These works operate in various models and architectures:
- Some consider a 2-party setting (e.g., [1, 48, 49, 4]).
- Others consider a 3-party (e.g., [2, 3, 6, 5, 29, 30]) or a 4-party setting (e.g., [52, 53]).
- Some frameworks assume the model is held in plaintext (e.g., [49, 4]), while others support secret-shared models (e.g., [6, 5]).

Until recently, with the exceptions of FALCON [6] and CRYPTFLOW [5], existing approaches only considered privacy-preserving inference using shallow neural networks (less than 10 layers) on relatively small datasets (like MNIST [11] or CIFAR [12]). Our focus is on designing privacy-preserving machine learning protocols that can support inference over modern deep learning models (with tens of millions of parameters and over a hundred layers) on large datasets (e.g., ImageNet [13]). As shown in Section IV-B, our system outperforms both FALCON and CRYPTFLOW for inference over sufficiently large models and datasets.

#### Privacy-Preserving Training
Compared to private inference, privacy-preserving training of deep neural networks is more challenging and computationally intensive, and has received less attention. Among the aforementioned works, only a few [1, 2, 29, 3, 52, 30, 53, 6] support privacy-preserving training. Of these, FALCON is the first system (to our knowledge) that supports privacy-preserving training at the scale of (Tiny) ImageNet and for models as large as AlexNet [16] and VGG-16 [17]. Our work is the first framework to leverage GPUs to demonstrate significantly better scalability for privately training deep networks over large datasets.

#### Privacy-Preserving Machine Learning Using GPUs
Most works on privacy-preserving machine learning are CPU-based and do not leverage GPU acceleration. Some notable exceptions include:
- Works [55, 54] use GPUs to accelerate homomorphic evaluation of convolutional neural networks on MNIST.
- DELPHI [4] uses GPUs to compute linear layers (i.e., convolutions) for private inference but performs non-linear operations (e.g., ReLU evaluation) on the CPU and assumes the model is public.
- Slalom [36] integrates a trusted computing base (e.g., Intel SGX) with GPUs to enable fast private inference by offloading convolutions to the GPU and performing non-linear operations within the trusted enclave.

Our design philosophy is to keep all computations on the GPU through a careful choice of "GPU-friendly" cryptographic protocols. Recent works proposing scalable private training and inference protocols highlight the use of GPUs as an important way for further scalability [6, 5]. Our system is the first to support private training and inference entirely on the GPU.

#### Model Stealing and Inversion Attacks
MPC protocols can only hide the inputs to the computation (e.g., the model or the dataset) up to what can be inferred from the output. Several recent works [56, 57, 58, 59, 60] have shown how black-box access to a model (in the case of a private inference service) can allow an adversary to learn information about the model or its training data. Differentially-private training algorithms [9, 10] provide one defense against certain types of these attacks. Our focus is on protecting the computation itself and ensuring no additional leakage about the inputs other than through the output. It is an interesting question to design a private training/inference protocol that also provides robustness against specific classes of model stealing/inversion attacks.

### VI. CONCLUSION
In this paper, we introduce CRYPTGPU, a new MPC framework that implements all cryptographic operations (both linear and non-linear) on the GPU. CRYPTGPU is built on top of PyTorch [21] and CRYPTEN [24] to make it easy to use for machine learning developers and researchers. Our experiments show that leveraging GPUs can significantly accelerate private training and inference for modern deep learning, making it practical to run privacy-preserving deep learning at the scale of ImageNet and with complex networks. Additionally, our systematic analysis of different cryptographic protocols provides new insights for designing "GPU-friendly" cryptographic protocols for deep learning, bridging the roughly 1000× gap between private and plaintext machine learning (on the GPU).

### ACKNOWLEDGMENTS
We thank Pavel Belevich, Shubho Sengupta, and Laurens van der Maaten for their feedback on system design and providing helpful pointers. D. J. Wu is supported by NSF CNS-1917414.

### REFERENCES
[1] P. Mohassel and Y. Zhang, “SecureML: A system for scalable privacy-preserving machine learning,” in IEEE Symposium on Security and Privacy, pp. 19–38, 2017.
[2] P. Mohassel and P. Rindal, “ABY3: A mixed protocol framework for machine learning,” in ACM CCS, pp. 35–52, 2018.
[3] S. Wagh, D. Gupta, and N. Chandran, “SecureNN: 3-party secure computation for neural network training,” Proc. Priv. Enhancing Technol., vol. 2019, no. 3, pp. 26–49, 2019.
[4] P. Mishra, R. Lehmkuhl, A. Srinivasan, W. Zheng, and R. A. Popa, “Delphi: A cryptographic inference service for neural networks,” in USENIX Security, pp. 2505–2522, 2020.
[5] N. Kumar, M. Rathee, N. Chandran, D. Gupta, A. Rastogi, and R. Sharma, “CrypTFlow: Secure TensorFlow inference,” in IEEE Symposium on Security and Privacy, pp. 336–353, 2020.
[6] S. Wagh, S. Tople, F. Benhamouda, E. Kushilevitz, P. Mittal, and T. Rabin, “FALCON: honest-majority maliciously secure framework for private deep learning,” Proc. Priv. Enhancing Technol., vol. 2021, 2021.
[24] B. Knott, S. Venkataraman, A. Hannun, S. Sengupta, M. Ibrahim, and L. van der Maaten, “CrypTen: Secure multi-party computation meets machine learning,” in Proceedings of the NeurIPS Workshop on Privacy-Preserving Machine Learning, 2020.
[25] M. Ito, A. Saito, and T. Nishizeki, “Secret sharing scheme realizing general access structure,” Electronics and Communications in Japan (Part III: Fundamental Electronic Science), vol. 72, no. 9, pp. 56–64, 1989.
[26] T. Araki, J. Furukawa, Y. Lindell, A. Nof, and K. Ohara, “High-throughput semi-honest secure three-party computation with an honest majority,” in ACM CCS, pp. 805–817, 2016.
[27] “CUDA libraries documentation.” https://docs.nvidia.com/cuda-libraries/index.html.
[28] A. C. Yao, “How to generate and exchange secrets (extended abstract),” in FOCS, pp. 162–167, 1986.
[29] H. Chaudhari, A. Choudhury, A. Patra, and A. Suresh, “ASTRA: high throughput 3PC over rings with application to secure prediction,” in ACM CCS, pp. 81–92, 2019.
[30] A. Patra and A. Suresh, “BLAZE: blazing fast privacy-preserving machine learning,” in NDSS, 2020.
[31] S. Kamara, P. Mohassel, and M. Raykova, “Outsourcing multi-party computation,” IACR Cryptol. ePrint Arch., vol. 2011, p. 272, 2011.
[36] F. Tramèr and D. Boneh, “Slalom: Fast, verifiable, and private execution of neural networks in trusted hardware,” in ICLR, 2019.
[32] “cuBLAS.” https://docs.nvidia.com/cuda/cublas/index.html.
[33] “cuDNN.” https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html.
[34] D. Demmler, T. Schneider, and M. Zohner, “ABY - A framework for efficient mixed-protocol secure two-party computation,” in NDSS, 2015.
[35] D. Beaver, “Efficient multiparty protocols using circuit randomization,” in CRYPTO, pp. 420–432, 1991.
[37] J. Furukawa, Y. Lindell, A. Nof, and O. Weinstein, “High-throughput secure three-party computation for malicious adversaries and an honest majority,” in EUROCRYPT, pp. 225–255, 2017.
[38] R. Canetti, “Security and composition of multiparty cryptographic protocols,” J. Cryptol., vol. 13, no. 1, pp. 143–202, 2000.
[39] O. Goldreich, The Foundations of Cryptography - Volume 2: Basic Applications. Cambridge University Press, 2004.
[40] V. Nair and G. E. Hinton, “Rectified linear units improve restricted Boltzmann machines,” in ICML, pp. 807–814, 2010.
[41] “PyTorch/CSPRNG.” https://github.com/pytorch/csprng.
[42] J. K. Salmon, M. A. Moraes, R. O. Dror, and D. E. Shaw, “Parallel random numbers: as easy as 1, 2, 3,” in Conference on High Performance Computing Networking, Storage and Analysis, SC, pp. 16:1–16:12, 2011.
[43] Sameer Wagh and Shruti Tople and Fabrice Benhamouda and Eyal Kushilevitz and Prateek Mittal and Tal Rabin, “Falcon: Honest-majority maliciously secure framework for private deep learning.” Available at https://github.com/snwagh/falcon-public.
[44] C. Gentry, A fully homomorphic encryption scheme. PhD thesis, Stanford University, 2009. crypto.stanford.edu/craig.
[45] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig, and J. Wernsing, “Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy,” in ICML, pp. 201–210, 2016.
[46] J. Liu, M. Juuti, Y. Lu, and N. Asokan, “Oblivious neural network predictions via MiniONN transformations,” in ACM CCS, pp. 619–631, 2017.
[47] N. Chandran, D. Gupta, A. Rastogi, R. Sharma, and S. Tripathi, “EzPC: Programmable, efficient, and scalable secure two-party computation for machine learning.” Cryptology ePrint Archive, Report 2017/1109, 2017. https://eprint.iacr.org/2017/1109.
[48] M. S. Riazi, C. Weinert, O. Tkachenko, E. M. Songhori, T. Schneider, and F. Koushanfar, “Chameleon: A hybrid secure computation framework for machine learning applications,” in ACM CCS, pp. 707–721, 2018.
[49] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan, “GAZELLE: A low latency framework for secure neural network inference,” in USENIX Security Symposium, pp. 1651–1669, 2018.
[50] M. S. Riazi, M. Samragh, H. Chen, K. Laine, K. E. Lauter, and F. Koushanfar, “XONN: XNOR-based oblivious deep neural network inference,” in USENIX Security Symposium, pp. 1501–1518, 2019.
[51] A. P. K. Dalskov, D. Escudero, and M. Keller, “Secure evaluation of quantized neural networks,” Proc. Priv. Enhancing Technol., vol. 2020, no. 4, pp. 355–375, 2020.
[52] M. Byali, H. Chaudhari, A. Patra, and A. Suresh, “FLASH: Fast and robust framework for privacy-preserving machine learning,” Proc. Priv. Enhancing Technol., vol. 2020, no. 2, pp. 459–480, 2020.
[53] H. Chaudhari, R. Rachuri, and A. Suresh, “Trident: Efficient 4PC framework for privacy-preserving machine learning,” in NDSS, 2020.
[54] A. A. Badawi, J. Chao, J. Lin, C. F. Mun, S. J. Jie, B. H. M. Tan, X. Nan, A. M. M. Khin, and V. Chandrasekhar, “Towards the AlexNet moment for homomorphic encryption: HCNN, the first homomorphic CNN on encrypted data with GPUs,” IEEE Transactions on Emerging Topics in Computing, 2020.
[55] A. A. Badawi, B. Veeravalli, C. F. Mun, and K. M. M. Aung, “High-performance FV somewhat homomorphic encryption on GPUs: An implementation using CUDA,” IACR Trans. Cryptogr. Hardw. Embed. Syst., vol. 2018, no. 2, pp. 70–95, 2018.
[56] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici, “Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers,” Int. J. Secur. Networks, vol. 10, no. 3, pp. 137–150, 2015.
[57] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that exploit confidence information and basic countermeasures,” in ACM CCS, pp. 1322–1333, 2015.
[58] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing machine learning models via prediction APIs,” in USENIX Security Symposium, pp. 601–618, 2016.

\[ \mathcal{L}_{\text{CE}}(x; y) := -\sum_{i \in [d]} y_i \log \tilde{z}_i, \]
(A.1)