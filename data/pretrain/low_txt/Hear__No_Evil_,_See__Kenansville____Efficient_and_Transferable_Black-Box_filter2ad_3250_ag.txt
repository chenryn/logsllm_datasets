以下是经过优化后的文本，使其更加清晰、连贯和专业：

---

**参考文献**

[44] S. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky, Y. Kang, X. Li, J. Miller, A. Ng, J. Raiman 等人，“Deep Voice: 实时神经文本到语音转换”，第 34 届国际机器学习会议论文集，2017 年。

[45] M. Ito 和 R. Donaldson，“用于语音声音分析和识别的过零测量”，IEEE 音频与电声学汇刊，卷 19，第 3 期，页 235–242，1971 年。

[46] E. Bursztein, R. Beauxis, H. Paskov, D. Perito, C. Fabry 和 J. Mitchell，“基于噪声的非连续音频验证码的失效”，2011 IEEE 安全与隐私研讨会，2011 年，页 19–31。

[47] J. Tam, J. Simsa, S. Hyde 和 L. V. Ahn，“破解音频验证码”，神经信息处理系统进展，2009 年，页 1625–1632。

[48] S. Sano, T. Otsuka 和 H. G. Okuno，“使用 HMM 基于自动语音识别解决 Google 的连续音频验证码”，国际安全研讨会，Springer，2013 年，页 36–52。

[49] S. Solanki, G. Krishnan, V. Sampath 和 J. Polakis，“在（网络）空间中，机器人可以听到你的声音：使用 OTS 语音识别破解音频验证码”，第 10 届 ACM 人工智能与安全研讨会，ACM，2017 年，页 69–80。

[50] K. Bock, D. Patel, G. Hughey 和 D. Levin，“uncaptcha: recaptcha 音频挑战的低资源破解”，第 11 届 USENIX 攻击技术大会，USENIX 协会，2017 年，页 7–7。

[51] A. Krizhevsky, I. Sutskever 和 G. E. Hinton，“使用深度卷积神经网络进行 ImageNet 分类”，神经信息处理系统，卷 25，2012 年 1 月。

[52] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke 和 A. Rabinovich，“通过卷积深入研究”，2015 IEEE 计算机视觉与模式识别会议 (CVPR)，2015 年 6 月，页 1–9。

[53] K. He, X. Zhang, S. Ren 和 J. Sun，“图像识别中的深度残差学习”，2016 年 IEEE 计算机视觉与模式识别会议论文集，2016 年，页 770–778。

[54] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlíček, Y. Qian, P. Schwarz, J. Silovský, G. Stemmer 和 K. Veselý，“Kaldi 语音识别工具包”，IEEE 2011 自动语音识别与理解研讨会，IEEE 信号处理学会，2011 年，IEEE 目录号：CFP11SRW-USB。

[55] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein 和 J. D. Tygar，“对抗性机器学习”，第 4 届 ACM 安全与人工智能研讨会，AISec '11，纽约，美国：ACM，2011 年，页 43–58。在线可访问：http://doi.acm.org/10.1145/2046684.2046692

[56] A. Kurakin, I. Goodfellow 和 S. Bengio，“物理世界中的对抗性示例”，arXiv 预印本 arXiv:1607.02533，2016 年。

[57] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow 和 R. Fergus，“神经网络的有趣特性”，arXiv 预印本 arXiv:1312.6199，2013 年。

[58] I. J. Goodfellow, J. Shlens 和 C. Szegedy，“解释和利用对抗性示例”，arXiv 预印本 arXiv:1412.6572，2014 年。

[59] S. Baluja 和 I. Fischer，“对抗性变换网络：学习生成对抗性示例”，arXiv 预印本 arXiv:1703.09387，2017 年。

[60] J. Su, D. V. Vargas 和 S. Kouichi，“单像素攻击以欺骗深度神经网络”，arXiv 预印本 arXiv:1710.08864，2017 年。

[61] S. M. Moosavi Dezfooli, A. Fawzi 和 P. Frossard，“DeepFool：一种简单且准确的方法来欺骗深度神经网络”，2016 IEEE 计算机视觉与模式识别会议论文集，EPFL-CONF-218057，2016 年。

[62] T. B. Brown, D. Mané, A. Roy, M. Abadi 和 J. Gilmer，“对抗性补丁”，arXiv 预印本 arXiv:1712.09665，2017 年。

[63] M. Sharif, S. Bhagavatula, L. Bauer 和 M. K. Reiter，“配饰犯罪：对最先进面部识别的真实且隐蔽的攻击”，2016 ACM SIGSAC 计算机与通信安全会议论文集，ACM，2016 年，页 1528–1540。

[64] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik 和 A. Swami，“对抗环境下深度学习的局限性”，2016 IEEE 欧洲安全与隐私研讨会，IEEE，2016 年，页 372–387。

[65] N. Carlini 和 D. Wagner，“评估神经网络的鲁棒性”，2017 IEEE 安全与隐私研讨会，IEEE，2017 年，页 39–57。

[66] A. Nguyen, J. Yosinski 和 J. Clune，“深度神经网络容易被愚弄：对无法识别图像的高置信度预测”，2015 IEEE 计算机视觉与模式识别会议论文集，2015 年，页 427–436。

[67] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang 和 X. Zhang，“神经网络的特洛伊攻击”，2017 年网络与分布式系统安全研讨会论文集，2017 年。

[68] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang 和 W. Xu，“DolphinAttack：不可听的声音命令”，2017 ACM SIGSAC 计算机与通信安全会议论文集，ACM，2017 年，页 103–117。

[69] T. Vaidya, Y. Zhang, M. Sherr 和 C. Shields，“Cocaine Noodles：利用人类与机器语音识别之间的差距”，WOOT，卷 15，页 10–11，2015 年。

授权许可限于清华大学。从 IEEE Xplore 下载于 2022 年 2 月 25 日 12:09:39 UTC。适用限制。

[70] W. Cai, A. Doshi 和 R. Valle，“使用深度生成模型攻击说话人识别”，arXiv 预印本 arXiv:1801.02384，2018 年。

[71] Y. Gong 和 C. Poellabauer，“为语音副语言应用制作对抗性示例”，arXiv 预印本 arXiv:1711.03280，2017 年。

[72] C. Kereliuk, B. L. Sturm 和 J. Larsen，“深度学习与音乐对手”，IEEE 多媒体汇刊，卷 17，第 11 期，页 2059–2071，2015 年。

[73] D. Kumar, R. Paccagnella, P. Murley, E. Hennenfent, J. Mason, A. Bates 和 M. Bailey，“针对 Amazon Alexa 的技能抢占攻击”，第 27 届 USENIX 安全研讨会 (USENIX Security 18)，USENIX 协会，2018 年。

[74] M. K. Bispham, I. Agrafiotis 和 M. Goldsmith，“通过语音接口的攻击分类”，2018 年。

[75] “Google Cloud Speech-to-Text API”，最后访问于 2019 年，网址：https://cloud.google.com/speech-to-text/。

[76] “使用增强模型转录音频电话”，最后访问于 2019 年，网址：https://cloud.google.com/speech-to-text/docs/phone-model。

[77] “Wit.ai 开发者自然语言处理”，最后访问于 2019 年，网址：https://wit.ai/。

[78] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates 等人，“Deep Speech：端到端语音识别的大规模扩展”，arXiv 预印本 arXiv:1412.5567，2014 年。

[79] “Mozilla DeepSpeech 项目”，最后访问于 2019 年，网址：https://azure.microsoft.com/en-us/services/cognitive-services/speaker-recognition/。

[80] S. Naren，“使用 DeepSpeech-2 进行语音识别”，最后访问于 2019 年，网址：https://github.com/SeanNaren/deepspeech.pytorch。

[81] A. W. Rix, J. G. Beerends, M. P. Hollier 和 A. P. Hekstra，“感知语音质量评估 (PESQ) —— 一种新的电话网络和编解码器语音质量评估方法”，2001 IEEE 国际声学、语音和信号处理会议论文集 (Cat. No. 01CH37221)，卷 2，IEEE，2001 年，页 749–752。

[82] “简单的音频识别”，最后访问于 2019 年，网址：https://www.tensorflow.org/tutorials/sequences/audio_recognition。

[83] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner 和 A. Madry，“鲁棒性可能与准确性相悖”，arXiv 预印本 arXiv:1805.12152，卷 1，2018 年。

[84] E. Dohmatob，“对抗鲁棒性的局限性：强无免费午餐定理”，arXiv 预印本 arXiv:1810.04065，2018 年。

[85] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran 和 A. Madry，“对抗性示例不是错误，而是特征”，神经信息处理系统进展，2019 年，页 125–136。

**附录**

我们提供了额外的讨论和结果，这些内容虽然与本文的主要贡献关系不大，但仍然具有启发性。

**A. 模型**

**Google（正常）**：为了在真正的黑盒场景中展示我们的攻击，我们针对 Google 提供的语音转录 API 进行了测试。“正常”模型由 Google 提供，适用于“干净”的使用场景，例如家庭助手，其中语音不期望经过蜂窝网络 [75]。

**Google（手机）**：为了展示针对训练用于嘈杂音频的模型的攻击，我们测试了针对“手机”模型的攻击。Google 提供此模型用于蜂窝使用场景，并对其进行了训练，使其能够代表蜂窝网络压缩 [76]。我们还假设 Google 的“手机”模型将对由于音频样本通过电话网络传输而引入的噪声、抖动、丢失和压缩具有鲁棒性。

**Facebook Wit**：为了更好地覆盖专有语音转录服务的空间，我们还针对 Facebook Wit 进行了测试，它提供了一个“干净”的语音转录模型 [77]。同样，由于其专有性质，对该模型没有任何已知信息。

**Deep Speech-1**：Deep Speech-1 的目标是通过端到端学习范式消除手工设计的特征管道 [24], [78]。这使得即使在嘈杂环境中也能实现稳健的语音转录，前提是提供了足够的训练数据。在我们的实验中，我们使用了 Mozilla 提供的开源实现和检查点，并使用 MFCC 特征 [79]。

**Deep Speech-2**：Deep Speech-2 引入了针对非常大的训练集的架构优化。它被训练成将原始音频频谱图映射到其正确的转录，并展示了当前在嘈杂、端到端音频转录方面的最新技术水平 [26]。我们使用 GitHub 用户 SeanNaren 提供的开源实现 [32], [80]。我们测试的两个版本的主要区别在于特征预处理：测试的 Deep Speech-1 版本使用 MFCC 特征，而测试的 Deep Speech-2 版本使用原始音频频谱图。

**CMU Sphinx**：CMU Sphinx 项目是一个代表超过二十年语音转录任务研究的开源语音转录库 [25]。Sphinx 不严重依赖深度学习技术，而是结合统计方法来建模语音转录和高级语言概念。我们使用 CMU Sphinx 库提供的 PocketSphinx 实现和检查点 [25]。

**Microsoft Azure**：为了在黑盒环境中展示我们的攻击，我们攻击了 Microsoft Azure 提供的说话人识别 API [38]。该系统是专有的，因此完全是黑盒的。关于系统的内部结构没有公开可用的信息。

**B. 简单的白噪声攻击**

1. **动机**：读者可能会倾向于使用简单的攻击手段来破坏 ASR 和 AVI 模型。这包括向良性音频样本添加白噪声。然而，在以下小节中，我们将展示任何这样的简单技术都无法达到攻击者的目标：在不影响人类对音频样本的可解释性的情况下欺骗模型。

2. **方法和设置**：我们通过攻击一组随机选择的 100 个包含说话人说出单个单词的音频文件来测试这种白噪声方法。我们在这些样本中添加白噪声以生成对抗性音频样本。然后，我们将原始音频和添加了白噪声的音频样本传递给 Google 语音 API，并记录 API 错误转录的样本数量。

接下来，为了衡量对人类可解释性的影响，我们使用了感知语音质量评估 (PESQ) 标准 [81]。这是用于衡量语音质量的全球标准。

注：在运行我们的实验时，该实现尚未包括用于辅助束搜索解码的语言模型。

---

希望这些修改能帮助您更好地表达您的意图。如果您有任何进一步的要求或需要更多的调整，请告诉我！