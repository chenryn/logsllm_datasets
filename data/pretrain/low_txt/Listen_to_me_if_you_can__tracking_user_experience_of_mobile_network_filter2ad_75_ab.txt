### Retrieving Location Information from Tweets

There are two primary methods for obtaining location information from tweets: through user profiles and directly from the message content. The latter is generally more valuable, as it provides a direct link to the location where a performance issue occurred. Therefore, we prioritize extracting location data from the message content. If this information is not available, we then turn to the user profile. Given that both profiles and messages are unstructured human language inputs, the location information may be missing or incomplete.

Our analysis identified three main categories of location information: city + state, state only, and other (e.g., city name only, street). Table 1 summarizes the distribution of these categories in the analyzed user profiles and tweets. We found that a significant portion of tweets lack location information—37% of profiles and 69% of tweets do not contain specific location data. This reduces the number of valid tweets for further correlation. In contrast, the location information in customer tickets is well-organized by market regions, which can span multiple states or parts of a single state. Different data sources provide varying levels of location granularity, so we use the most detailed common location for correlation purposes in Section 3.3.

### Timeliness of Incident Reporting

There is inherently a delay between when a customer experiences an incident and when they report it. The reporting time is defined as the timestamp of the Twitter message or the call to customer care. This information can be directly retrieved from the data. However, determining the exact time of the incident is often challenging. If available, it is usually embedded within the tweet or ticket description. For example, a tweet might mention "a couple of call drops today," while a ticket might state "no service since yesterday."

We extracted timing information from tweets and tickets using simple pattern recognition, such as "now, today, yesterday, 3 days, 2010-04-03, this morning." Over 90.1% of our collected tweets contained such timing information, compared to only 23.7% of ticket descriptions. We then calculated the differences between the reported times and the estimated incident times. Table 2 illustrates the resulting distribution, showing that most tweets are posted on the same day as the incident, while most tickets are opened one or more days later. This suggests that Twitter users respond more quickly than those who report issues via customer care. However, it is important to note that these two groups may have different priorities regarding performance issues, so we cannot conclude that tweets are universally faster for all incidents.

### Correlation Results

In this section, we correlate performance-related tweets and tickets with major network incident reports. Figure 3 shows the time series of incidents, tweets, and tickets. Unlike the diurnal pattern observed in Figure 1, performance-related tweets exhibit a weak diurnal pattern, indicating that many are driven by specific incidents. Customer tickets, however, show a strong diurnal pattern due to calling behavior and the delay in reporting.

To assess the correlation between incident reports and customer feedback, we conducted a statistical correlation [12] for each location, computing Pearson's coefficient and performing significance tests. Unfortunately, the results showed no strong correlation, likely due to the time lag between the start of an incident and the reporting of performance issues. Addressing this time lag through time shifting is a challenge, as the lags vary case by case and cannot be systematically compensated. Future work will focus on developing new statistical significance test methodologies for this scenario.

Instead of relying on statistical correlation, we used the incidents in the report as the ground truth and investigated whether these incidents were reported in the collected tweets and tickets. Specifically, if tweets or tickets were observed during an incident and within the same region, we deemed them associated with the incident. We focused on verifying short-term incidents (less than 3 hours) to minimize the risk of false associations. Our findings indicate that 55.6% of incidents were found in tweets, and only 37.0% in customer care tickets. Interestingly, all matches found in tickets were also found in tweets. We also measured the likelihood of observing tweets/tickets during incidents (c1) and during non-incidents (c2). The ratio c1/c2 was 8.3 for tweets and 6.8 for tickets, suggesting that user feedback is significantly more likely during incidents.

### Total Delay Analysis

In Section 3.2.3, we quantitatively studied the delay from when customers experience an issue to when they report it. There is also a delay between when an incident occurs and when users experience it. However, the exact time of user experience is not always accurately recorded. Therefore, we analyzed the total delay from the start of an incident to the time of reporting. Figure 4 shows the box statistics of the delay distributions. The median response time for Twitter users is approximately 10 minutes faster than for those who call customer care. The fastest Twitter responses were in several minutes. This suggests that Twitter feedback could be a more timely source for observing network performance issues.

### Revisiting the Time Series Data

Figure 3 shows obvious spikes on Day 11 for both Twitter and customer care data, even though there were few recorded incidents during that period. Specifically, there were many complaints about call drops in the central area between 8 PM and 10 PM. This may indicate a short-term network problem, but it was not reported in the major incident reports.

### Related Work

Several studies have focused on social networks, including user behavior, network performance impact, community evolution, information propagation, and privacy issues. Few studies, however, have explored the value of social network content. Vieweg et al. [22] showed that microblogging platforms like Twitter can contribute to situational awareness during natural hazards. Motoyama et al. [19] used Twitter data to infer online service availability.

Correlating across data sources is a common method in anomaly detection and network problem diagnosis. Most studies focus on statistical methods. Our study is the first to explore the potential of social media content as a new source for understanding mobile network performance and its impact on user experience.

### Conclusion and Future Work

This study demonstrates the potential of using social network content for monitoring network performance. Our data suggest that Twitter messages often provide timely feedback on network incidents, making them a complementary source for understanding performance issues and their impact on user experience.

Future work will involve applying advanced natural language processing techniques to better understand tweets, tickets, and incident reports. This includes improving the extraction of performance-related issues from tweets and quantifying the severity level of these issues based on the scale of responses and sentiment in the messages.

### References

[1] comScore 2009 US Digital Year in Review, 2009.
http://www.comscore.com/Press_Events/Presentations_Whitepapers/2010/The_2009_U.S._Digital_Year_in_Review.

[2] Fabrício Benevenuto, Tiago Rodrigues, Meeyoung Cha, and Virgílio A. F. Almeida. Characterizing user behavior in online social networks. In Internet Measurement Conference, pages 49–62, 2009.

[3] Daniela Brauckhoff, Xenofontas Dimitropoulos, Arno Wagner, and Kavè Salamatian. Anomaly extraction in backbone networks using association rules. In Proc. ACM IMC, 2009.

[4] Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and Krishna P. Gummadi. Measuring User Influence in Twitter: The Million Follower Fallacy. In In Proceedings of the 4th International AAAI Conference on Weblogs and Social Media (ICWSM).

[5] Meeyoung Cha, Alan Mislove, and Krishna P. Gummadi. A measurement-driven analysis of information propagation in the Flickr social network. In WWW ’09: Proceedings of the 18th international conference on World wide web, pages 721–730, New York, NY, USA, 2009. ACM.

[6] Catherine Dwyer, Starr Roxanne Hiltz, and Katia Passerini. Trust and privacy concern within social networking sites: A comparison of Facebook and MySpace. In Proceedings of the Thirteenth Americas Conference on Information Systems (AMCIS 2007), 2007. Paper 339.

[7] Sanchit Garg, Trinabh Gupta, Niklas Carlsson, and Anirban Mahanti. Evolution of an online social aggregation network: an empirical study. In IMC ’09: Proceedings of the 9th ACM SIGCOMM conference on Internet measurement conference, pages 315–321, New York, NY, USA, 2009. ACM.

[8] Ralph Gross and Alessandro Acquisti. Information revelation and privacy in online social networks. In WPES ’05: Proceedings of the 2005 ACM workshop on Privacy in the electronic society, pages 71–80, New York, NY, USA, 2005. ACM.

[9] Yiyi Huang, Nick Feamster, Anukool Lakhina, and Jim (Jun) Xu. Diagnosing network disruptions with network-wide analysis. SIGMETRICS Perform. Eval. Rev., 35(1):61–72, 2007.

[10] Srikanth Kandula, Ranveer Chandra, and Dina Katabi. What is going on? learning communication rules in edge networks. In Proc. ACM SIGCOMM, 2008.

[11] Srikanth Kandula, Ratul Mahajan, Patrick Verkaik, Sharad Agarwal, Jitendra Padhye, and Paramvir Bahl. Detailed diagnosis in enterprise networks. In Proc. ACM SIGCOMM, 2009.

[12] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren. Detection and localization of network blackholes. In Proc. INFOCOM, 2007.

[13] Balachander Krishnamurthy, Phillipa Gill, and Martin Arlitt. A few chirps about Twitter. In WOSP ’08: Proceedings of the first workshop on Online social networks, pages 19–24, New York, NY, USA, 2008. ACM.

[14] Haewoon Kwak, Yoonchan Choi, Young-Ho Eom, Hawoong Jeong, and Sue Moon. Mining communities in networks: a solution for consistency and its evaluation. In IMC ’09: Proceedings of the 9th ACM SIGCOMM conference on Internet measurement conference, pages 301–314, New York, NY, USA, 2009. ACM.

[15] Anukool Lakhina, Mark Crovella, and Christophe Diot. Mining anomalies using traffic feature distributions. In SIGCOMM ’05: Proceedings of the 2005 conference on Applications, technologies, architectures, and protocols for computer communications, pages 217–228, New York, NY, USA, 2005. ACM.

[16] A. Mahimkar, Z. Ge, A. Shaikh, J. Wang, J. Yates, Y. Zhang, and Q. Zhao. Towards Automated Performance Diagnosis in a Large IPTV Network. In Proc. ACM SIGCOMM, 2009.

[17] A. Mahimkar, J. Yates, Y. Zhang, A. Shaikh, J. Wang, Z. Ge, and C. T. Ee. Troubleshooting chronic conditions in large IP networks. In Proc. ACM CoNEXT, 2008.

[18] Alan Mislove, Massimiliano Marcon, Krishna P. Gummadi, Peter Druschel, and Bobby Bhattacharjee. Measurement and analysis of online social networks. In IMC ’07: Proceedings of the 7th ACM SIGCOMM conference on Internet measurement, pages 29–42, New York, NY, USA, 2007. ACM.

[19] Marti Motoyama, Brendan Meeder, Kirill Levchenko, Geoffrey M. Voelker, and Stefan Savage. Measuring online service availability using Twitter. In USENIX 3rd Workshop on Online Social Networks (WOSN), 2010.

[20] Atif Nazir, Saqib Raza, Dhruv Gupta, Chen-Nee Chuah, and Balachander Krishnamurthy. Network level footprints of Facebook applications. In IMC ’09: Proceedings of the 9th ACM SIGCOMM conference on Internet measurement conference, pages 63–75, New York, NY, USA, 2009. ACM.

[21] Fabian Schneider, Anja Feldmann, Balachander Krishnamurthy, and Walter Willinger. Understanding online social network usage from a network perspective. In Anja Feldmann and Laurent Mathy, editors, Internet Measurement Conference, pages 35–48. ACM, 2009.

[22] Sarah Vieweg, Amanda L. Hughes, Kate Starbird, and Leysia Palen. Microblogging during two natural hazards events: what Twitter may contribute to situational awareness. In CHI ’10: Proceedings of the 28th international conference on Human factors in computing systems, pages 1079–1088, New York, NY, USA, 2010. ACM.

[23] Bimal Viswanath, Alan Mislove, Meeyoung Cha, and Krishna P. Gummadi. On the evolution of user interaction in Facebook. In WOSN ’09: Proceedings of the 2nd ACM workshop on Online social networks, pages 37–42, New York, NY, USA, 2009. ACM.