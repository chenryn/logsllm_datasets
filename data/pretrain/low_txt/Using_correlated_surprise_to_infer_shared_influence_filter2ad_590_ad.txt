### Extremum on the Delay Axis

- The extremum occurs at a delay of approximately 100-200 samples, which corresponds to an interval of about 4-8 seconds (with a sampling rate of 0.04 seconds per sample).
- Anomalies in \( \text{LASER1}(t) \) tend to precede those in \( \text{PLANNER\_TRAJ} \) by 4-8 seconds.
- This temporal relationship is more important than the specific values of the anomalies and provides crucial information for isolating the bug mentioned in Section 5.1.

### Directed Graph Representation

- In Stanley's diagram, each laser component (denoted as \( \text{LASER*} \)) has a directed arrow to the box containing the planner software (denoted as \( \text{PLANNER*} \)).
- An arrow originating from a box indicates an influence on the vertex inside that box; an arrow terminating at a box indicates that the component within the box is influenced.
- Consequently, the SIG (Structure-of-Influence Graph) reveals that the \( \text{LASER*} \) components influence the planner, with a time delay.
- The edges in the dependency diagram indicate these influences, rather than functional dependencies. For example, the strongest interactions are plotted as red rectangles, indicating a time-delayed communication between \( \text{LASER*} \) and \( \text{PLANNER*} \).

### Dynamic Changes in Influence

- SIGs are dynamic and can be used to examine changes over time. For instance, a SIG for Junior during the second race mission (Figure 12) shows how the structure of influence changes.
- Dashed grey edges indicate new connections, and open arrowheads denote changes in the direction of influence.
- By using a sliding window approach, we can observe how the influence structure evolves over different periods, such as the first and second thirds of the Urban Challenge missions.

### Anomaly Detection and Bug Diagnosis

- Using the method described in Section 3.4, we distill the data into SIGs and compute a statistical baseline with parameters \( c = 0.15 \) and \( \alpha = 90 \) (Figure 11).
- For Stanley, the SIG highlights that the four laser sensors form a clique, and their anomalies precede those in the planner by several seconds.
- Observation (i) shows that the directed arrows from the lasers to the planner indicate that the laser anomalies precede the planner's anomalies.
- Observation (ii) suggests that another component, shared by the lasers, may be influencing the swerving behavior.

### Case Study: Swerving Bug

- Starting with logs from the Grand Challenge race, we construct a synthetic component called \( \text{SWERVE} \), whose anomaly signal is nonzero only during the period of swerving behavior.
- Updating Stanley's SIG with \( \text{SWERVE} \) as a new component (Figure 11) shows that the correlation values indicate a strong influence from the laser sensors and the temperature sensor.
- The spurious correlation with the temperature sensor is due to the timing of the anomaly signals. The bug, triggered by a buffer shared by the laser sensors, was a non-deterministic timing dependency.

### Application to Thunderbird Supercomputer

- For the Thunderbird supercomputer, we use SIGs to localize a non-performance-related bug.
- The SIG method, based on term frequency in log messages, helps identify clusters of components that generate the "CPU error" message.
- These clusters suggest that the bug is related to job scheduling and workload under heavy network activity, leading to insights that help isolate the cause and rule out other potential issues.

### Contributions

- We propose using influence to characterize the interactions among components in a system and present a method for constructing Structure-of-Influence Graphs (SIGs).
- Our simulation experiments and case studies with autonomous vehicles and a production supercomputer demonstrate the benefits of using influence over traditional dependency modeling.
- SIGs enable users to better identify the sources of problems and understand the interactions among components in heterogeneous systems.

### Acknowledgments

- The authors thank Xuan Vii, Jon Stearley, Peter Hawkins, Randall LaViolette, Mike Montemerlo, and the rest of the Stanford Racing Team for their contributions and expertise.

### References

- [1] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and A. Methitacharoen. Performance debugging for distributed systems of black boxes. In SOSP, pages 74-89, 2003.
- [2] P. Bahl, R. Chandra, A. Greenberg, and M. Zhang. Towards highly reliable services via inference of multi-level enterprise network dependencies. In SIGCOMM, 2007.
- [3] P. Barham, A. Donnelly, R. Isaacs, S. Kandula, D. A. Maltz, and R. Mortier. Using Magpie for request extraction and workload modelling. In OSDI, 2004.
- [4] P. C. Bates. Debugging heterogeneous distributed systems using event-based models of behavior. ACM Transactions on Computer Systems, 13(1):1-31, 1995.
- [5] M. Brodie, I. Rish, and S. Ma. Optimizing probe selection for fault localization. In Int'l Workshop on Distributed Systems: Operations and Management (DSOM), October 2001.
- [6] A. Brown, G. Kar, and A. Keller. An active approach to characterizing dynamic dependencies for problem determination in a distributed environment. In IEEE IM, pages 377-390, Seattle, WA, 2001.
- [7] M. Y. Chen, A. Accardi, E. Kiciman, J. Lloyd, D. Patterson, A. Fox, and E. Brewer. Path-based failure and evolution management. In NSDI, 2004.
- [8] M. Y. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer. Pinpoint: Problem determination in large, dynamic internet services. In DSN, June 2002.
- [9] S. Chutani and H. Nussbaumer. On the distributed fault diagnosis of computer networks. In IEEE Symposium on Computers and Communications, pages 71-77, Alexandria, Egypt, June 1995.
- [10] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly, and A. Fox. Capturing, indexing, clustering, and retrieving system history. In SOSP, 2005.
- [11] S. Kandula, D. Katabi, and J.-P. Vasseur. Shrink: A tool for failure diagnosis in IP networks. In MineNet Workshop at SIGCOMM, 2005.
- [12] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren. IP fault localization via risk modeling. In NSDI, pages 57-70, 2005.
- [13] S. Kullback. The Kullback-Leibler distance. The American Statistician, 41:340-341, 1987.
- [14] M. Montemerlo et al. Junior: The Stanford entry in the Urban Challenge. Journal of Field Robotics, 25(9):569-597, 2008.
- [15] A. J. Oliner, A. Aiken, and J. Stearley. Alert detection in system logs. In ICDM, December 2008.
- [16] A. J. Oliner and J. Stearley. What supercomputers say: A study of five system logs. In DSN, 2007.
- [17] P. Reynolds, C. Killian, J. L. Wiener, J. C. Mogul, M. A. Shah, and A. Vahdat. Pip: Detecting the unexpected in distributed systems. In NSDI, 2006.
- [18] P. Reynolds, J. L. Wiener, J. C. Mogul, M. K. Aguilera, and A. Vahdat. WAP5: Black-box performance debugging for wide-area systems. In WWW, 2006.
- [19] I. Rish, M. Brodie, N. Odintsova, S. Ma, and G. Grabarnik. Real-time problem determination in distributed systems using active probing. In NOMS, 2004.
- [20] H. A. Sturges. The choice of a class interval. Journal of the American Statistical Association, 1926.
- [21] S. Thrun and M. Montemerlo, et al. Stanley: The robot that won the DARPA Grand Challenge. Journal of Field Robotics, 23(9):661-692, June 2006.