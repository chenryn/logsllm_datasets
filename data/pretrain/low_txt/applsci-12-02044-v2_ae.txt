### 5.2.8. Experiment 8: Comparison of Compression Rates Achieved by the Proposed Algorithm and Logzip

It is crucial to compare the compression rates of our proposed method with those of other algorithms that use a similar approach. Like our algorithm, Logzip [19] also utilizes hidden structures (templates) to reduce file size and employs the general compressor Bzip2 to further decrease the size. In this experiment, we compared the achieved compression rates for datasets A, B, and C. The results are presented in Figure 43.

**Figure 43.** The compression rates achieved by our method and Logzip on datasets A, B, and C.

Our method achieves a compression rate approximately 1% higher than Logzip's. Unlike Logzip, our algorithm does not incorporate the use of general compressors. With the joint use, our method could achieve even higher compression rates, as explained in Section 5.2.5.

### 5.2.9. Experiment 9: Investigating the Memory Usage of the Proposed Algorithm and Logzip

Memory usage is a significant aspect of a compressor. We investigated the average and maximum memory usages of the proposed algorithm and Logzip [19], as well as the duration of time that the compressors used the memory. Dataset A was used for the experiment, and the computer used for the measurements had 16 GB of DDR4 RAM. The results are shown in Figure 44.

**Figure 44.** The memory usage of the proposed method and Logzip.

Our algorithm uses 44% less memory on average, and the maximum memory used is 2.9 times less than in the case of Logzip. This can be attributed to the different methods of loading messages. Our algorithm reads lines sequentially, similar to a stream, while Logzip loads the entire file into a dataframe in memory. Additionally, Logzip consumes memory for four times as long as the proposed algorithm. It is evident that memory usage scales with the size of the input, and the available memory must be at least 2.2 times the input size.

### 5.2.10. Experiment 10: Generating Log Messages with Different Distributions and Evaluating the Compression Rates

In the final experiment, we investigated whether our enhanced algorithm maintains high compression rates for distributions other than the power law. We created four datasets with varying sizes and distributions. For the "random" datasets, each template had a 5% probability of being created by the sampling algorithm. We generated a file with 1 million entries and another with 50 million entries based on this principle. For the other two datasets, we used a normal distribution instead of equal 5% probabilities. The distribution of templates in the generated files is shown in Figures 45–48.

**Figure 45.** The template distribution of the 1 million randomly generated messages dataset.

**Figure 46.** The template distribution of the 50 million randomly generated messages dataset.

**Figure 47.** The template distribution of the 1 million messages dataset generated based on normal distribution.

**Figure 48.** The template distribution of the 50 million messages dataset generated based on normal distribution.

After creating the custom datasets, we measured the compression rates of our enhanced algorithm. The results are shown in Figure 49.

**Figure 49.** The compression rates achieved by our method on the generated datasets.

Our algorithm achieves high compression rates regardless of the distribution of the templates. For all datasets, at least a 94% compression rate was achieved, indicating the robustness of our algorithm.

### 6. Discussion and Conclusions

In this paper, we evaluated the compression capacity of an enhanced version of the algorithm proposed in [21]. The original algorithm uses template miners to identify templates and creates a dictionary where each ID represents an event type. Log lines are then represented using the corresponding ID and parameter list, achieving around 67% compression rates. To improve performance, we introduced several enhancements:

1. Templates were ordered based on their frequency, with smaller IDs assigned to more frequent templates.
2. A dictionary for templates was created, resulting in encoded log messages containing only numbers.
3. Huffman coding was applied to further compress the file.

We conducted several experiments to analyze the performance of the enhanced algorithm. The results showed that each enhancement improved the compression capacity. The joint use of the parameter dictionary and Huffman coding achieved an average of 92% compression rate, which is 25% more than the original algorithm. In terms of speed, our algorithm is fast, taking only seconds to compress and decompress the investigated log files.

We also compared our algorithm with general compressors. While general compressors are faster and achieve better compression rates (around 98%), they are not well suited for statistical applications. Our algorithm allows for easy answers to statistical questions such as the distribution of templates or the frequency of different parameters of a message type. Instances of given templates can also be found faster than with general compressors.

Based on our experiments, we recommend the joint use of our algorithm and general compressors, as it improves compression rates and serves as a wrapper for the created templates and the encoded file. Future work will focus on evaluating the performance on stream-like data and comparing our method with other general compressors. We also plan to investigate the relationship between the compression rate and the k-th order empirical entropy.

### Author Contributions

- **Conceptualization:** P.M., P.L.-K., A.K.
- **Methodology:** P.M., P.L.-K., A.K.
- **Software:** P.M., P.L.-K.
- **Validation:** P.M., P.L.-K., A.K.
- **Investigation:** P.M., P.L.-K., A.K.
- **Writing—Original Draft Preparation:** P.M., P.L.-K., A.K.
- **Writing—Review and Editing:** P.M., P.L.-K., A.K.
- **Supervision:** A.K.
- **Project Administration:** A.K.

All authors have read and agreed to the published version of the manuscript.

### Funding

This project has been supported by grants from the "Application Domain Specific Highly Reliable IT Solutions" project, implemented with support from the National Research, Development, and Innovation Fund of Hungary, financed under the Thematic Excellence Program TKP2020-NKA-06 (National Challenges Subprogram) funding scheme.

### Institutional Review Board Statement

Not applicable.

### Informed Consent Statement

Not applicable.

### Data Availability Statement

The data was provided by the Ericsson-ELTE Software Technology Lab.

### Acknowledgments

This publication is a partial result of the Research and Development Operational Program for the project "Modernisation and Improvement of Technical Infrastructure for Research and Development of J. Selye University in the Fields of Nanotechnology and Intelligent Space," ITMS 26210120042, co-funded by the European Regional Development Fund and supported by the ÚNKP-21-3 New National Excellence Program of the Ministry for Innovation and Technology from the source of the National Research, Development, and Innovation Fund. The project was also supported by the Ericsson-ELTE Software Technology Lab.

### Conflicts of Interest

The authors declare no conflict of interest.

### Abbreviations

- **IPLoM:** Iterative Partitioning Log Mining
- **MoLFI:** Multi-objective Log message Format Identification
- **NSGA-II:** Non-dominated Sorting Genetic Algorithm II
- **BWT:** Burrows–Wheeler Transformation
- **LZMA:** Lempel–Ziv–Markov-chain Algorithm
- **PPM:** Prediction by Partial Matching
- **Enh:** Enhanced version of our algorithm
- **Huff:** Huffman coding
- **WPE:** Without Parameter Encoding

### References

1. Landauer, M.; Wurzenberger, M.; Skopik, F.; Settanni, G.; Filzmoser, P. Dynamic log file analysis: An unsupervised cluster evolution approach for anomaly detection. *Computers & Security* 2018, 79, 94–116.
2. Aivalis, C.; Blas, A.C. Log File Analysis of E-commerce Systems in Rich Internet Web 2.0 Applications. In Proceedings of the PCI 2011—15th Panhellenic Conference on Informatics, Kastoria, Greece, 30 September–2 October 2011; pp. 222–226.
3. Nagaraj, K.; Neville, J. Structured comparative analysis of systems logs to diagnose performance problems. In Proceedings of the 9th USENIX Symposium on Networked Systems Design and Implementation, San Jose, CA, USA, 25–27 April 2012; pp. 353–366.
4. Logothetis, D.; Trezzo, C.; Webb, K.C.; Yocum, K. In-situ MapReduce for log processing. In Proceedings of the USENIX ATC, Portland, OR, USA, 14–15 June 2011; p. 115.
5. Li, H.; Shang, W.; Hassan, A.E. Which log level should developers choose for a new logging statement? *Empirical Software Engineering* 2017, 22, 1684–1716.
6. Lin, H.; Zhou, J.; Yao, B.; Guo, M.; Li, J. Cowic: A column-wise independent CN for log stream analysis. In Proceedings of the 2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, Shenzhen, China, 4–7 May 2015; pp. 21–30.
7. Yao, K.; Li, W.; Hassan, A.E. A study of the performance of general compressors on log files. *Empirical Software Engineering* 2020, 25, 3043–3085.
8. Du, M.; Li, F. Spell: Streaming parsing of system event logs. In Proceedings of the 16th International Conference on Data Mining, Barcelona, Spain, 12–15 December 2016; pp. 859–864.
9. Shima, K. Length matters: Clustering system log messages using LENRDS. arXiv 2016, arXiv:1611.03213.
10. He, P.; Zhu, J.; Zheng, Z.; Lyu, M.R. Drain: An online log parsing approach with fixed depth tree. In Proceedings of the 2017 IEEE International Conference on Web Services (ICWS), Honolulu, HI, USA, 25–30 June 2017; pp. 33–40.
11. Christensen, R.; Li, F. Adaptive log compression for massive log data. In Proceedings of the SIGMOD, New York, NY, USA, 22–27 June 2013; pp. 1283–1284.
12. Feng, B.; Wu, C.; Li, J. MLC: An efficient multi-level log compression method for cloud backup systems. In Proceedings of the 2016 IEEE Trustcom/BigDataSE/ISPA, Tianjin, China, 23–26 August 2016; pp. 1358–1365.
13. Mell, P.; Harang, R.E. Lightweight packing of log files for improved compression in mobile networks. In Proceedings of the 2014 IEEE Military Communications Conference, Baltimore, MD, USA, 6–8 October 2014; pp. 192–197.
14. Grabowski, S.; Deorowicz, S. Web log compression. *Automatyka/Akademia Górniczo-Hutnicza im. Stanisława Staszica w Krakowie* 2007, 11, 417–424.
15. Lloyd, T.; Barton, K.; Tiotto, E.; Amaral, J.N. Run-length base-delta encoding for high-speed compression. In Proceedings of the 47th International Conference on Parallel Processing Companion, Eugene, OR, USA, 13–16 August 2018; pp. 1–9.
16. Tan, H.; Zhang, Z.; Zou, X.; Liao, Q.; Xia, W. Exploring the Potential of Fast Delta Encoding: Marching to a Higher Compression Ratio. In Proceedings of the 2020 IEEE International Conference on Cluster Computing (CLUSTER), Kobe, Japan, 14–17 September 2020; pp. 198–208.
17. Skibiński, P.; Świercz, A. Fast and efficient log file compression. In Proceedings of the CEUR Workshop, 11th East-European Conference on Advances in Databases and Information Systems, Varna, Bulgaria, 29 September–3 October 2007; pp. 56–69.
18. Otten, F.; Irwin, B.; Thinyane, H. Evaluating text preprocessing to improve compression on maillogs. In Proceedings of the 2009 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists, Emfuleni, South Africa, 12–14 October 2009; pp. 44–53.
19. Liu, J.; Zhu, J.; He, S.; He, P.; Zheng, Z.; Lyu, M. Extracting hidden structures via iterative clustering for log compression. In Proceedings of the 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), San Diego, CA, USA, 11–15 November 2019; pp. 863–873.
20. Hätönen, K.; Boulicaut, J.F.; Klemettinen, M.; Masson, C. Comprehensive log compression with frequent patterns. In International Conference on Data Warehousing and Knowledge Discovery; Springer: Berlin/Heidelberg, Germany, 2003; pp. 360–370.
21. Marjai, P.; Lehotay-Kéry, P.; Kiss, A. The Use of Template Miners and Encryption in Log Message Compression. *Computers* 2021, 10, 83.
22. He, P.; Zhu, J.; He, S.; Li, J.; Lyu, M.R. An evaluation study on log parsing and its use in log mining. In Proceedings of the 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, Toulouse, France, 28 June–1 July 2016; pp. 654–661.
23. Makanju, A.; Ayodele, A.N.; Milios, E.E. A lightweight algorithm for message type extraction in system application logs. *IEEE Trans. Knowl. Data Eng.* 2011, 24, 1921–1936.
24. Messaoudi, S.; Panichella, A.; Bianculli, D.; Briand, L.C.; Štuikys, R. A search-based approach for accurate identification of log message formats. In Proceedings of the 26th Conference on Program Comprehension, Gothenburg, Sweden, 27 May–3 June 2018; pp. 167–177.
25. Deb, K.; Pratap, A.; Agarwal, S.; Meyarivan, T.A.M.T. A fast and elitist multiobjective genetic algorithm: NSGA-II. *IEEE Trans. Evol. Comput.* 2002, 6, 182–197.
26. Sivanandam, S.N.; Deepa, S.N. Introduction to Genetic Algorithms; Springer: Berlin/Heidelberg, Germany, 2008; pp. 15–37.
27. Syswerda, G. Schedule optimization using genetic algorithms. In Proceedings of the Third International Conference on Genetic Algorithms; Morgan Kaufmann Publishers: Burlington, MA, USA, 1989; pp. 2–9.
28. Branke, J.; Deb, K.; Dierolf, H.; Osswald, M. Finding knees in multi-objective optimization. In International Conference on Parallel Problem Solving from Nature; Springer: Berlin/Heidelberg, Germany, 2004; pp. 722–731.
29. Burrows, M.; Wheeler, D. A block-sorting lossless data compression algorithm. Digital SRC Research Report; Digital Systems Research Center: Palo Alto, CA, USA, 1994.
30. Ziv, J.; Lempel, A. A universal algorithm for sequential data compression. *IEEE Trans. Inf. Theory* 1977, 23, 337–343.
31. Bell, T.; Witten, I.H.; Cleary, J.G. Modeling for text compression. *ACM Comput. Surv.* 1989, 21, 557–591.
32. Cleary, J.; Witten, I. Data compression using adaptive coding and partial string matching. *IEEE Trans. Commun.* 1984, 32, 396–402.
33. Huffman, D.A. A method for the construction of minimum-redundancy codes. *Proc. IRE* 1952, 40, 1098–1101.
34. Moffat, A.; Zobel, J.; Sharman, N. Text compression for dynamic document databases. *IEEE Trans. Knowl. Data Eng.* 1997, 9, 302–313.
35. Shannon, C.E. A mathematical theory of communication. *Bell Syst. Tech. J.* 1948, 27, 379–423.
36. Ferragina, P.; González, R.; Navarro, G.; Venturini, R. Compressed text indexes: From theory to practice. *J. Exp. Algorithmics* 2009, 13, 1.12–1.31.
37. Dahuffman Python Library. Available online: <https://pypi.org/project/dahuffman/> (accessed on 22 September 2021).
38. 7-Zip. Available online: <https://www.7-zip.org/>