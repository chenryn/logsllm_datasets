# Availability Measurement and Modeling for an Application Server

**Authors:**
- Dong Tang
- Dileep Kumar
- Sreeram Duvur
- Øystein Torbjørnsen

**Affiliation:**
Sun Microsystems, Inc.
4150 Network Circle, Santa Clara, CA 95054
Email: {dong.tang, dileep.kumar, sreeram.duvur, oystein.torbjornsen}@sun.com

## Abstract
Application servers are standard middleware platforms for deploying web-based business applications, which require high system availability to minimize transaction loss. This paper presents a measurement-based availability modeling and analysis for a fault-tolerant application server system—Sun Java System Application Server, Enterprise Edition. The study employs hierarchical Markov reward modeling techniques, with model parameters conservatively estimated from lab or field measurements. Uncertainty analysis is used to obtain average system availability and confidence intervals by randomly sampling from possible parameter ranges. This combined approach of lab measurement, analytical modeling, and uncertainty analysis provides a conservative availability assessment at stated confidence levels for new software products.

## 1. Introduction
Application servers have become essential middleware for deploying web-based business applications such as online banking, stock trading, merchandise purchasing, and auction services. System availability is a critical metric for evaluating these software systems. Although recent research has focused on benchmarking dependability for OLTP systems and evaluating user-perceived availability for specific web-based applications, there has been no published study demonstrating how to evaluate the availability of application server middleware using current modeling and analysis techniques combined with measurement.

In this paper, we present an availability evaluation study that combines measurement, modeling, and statistical analysis techniques for Sun Java System Application Server, Enterprise Edition 7 (JSAS EE7). Model parameters are conservatively estimated based on data collected from lab or field measurements. For parameters that cannot be accurately measured in limited time frames or may vary widely in customer sites, we apply uncertainty analysis by randomly sampling these parameters to obtain average system availability and confidence intervals.

Previous studies [7, 10] have shown that combining measurement and mathematical models can provide a quantitative dependability assessment for the target software system. Hsueh [4] was the first to use Markov reward models, combined with operational failure data, to model an IBM/MVS operating system. Similar techniques were applied to the Tandem Guardian and VAX/VMS operating systems [9]. The methodology was further extended to using test data in evaluating availability for air traffic control software [16]. These studies rely on failure data to estimate parameters for the models, which can generate system availability or performability measures. Methods to estimate parameters and associated confidence levels, including situations where no failures were observed during the measurement period, have been addressed in [8].

Markov reward models are powerful and widely accepted in availability and reliability analysis [3, 19]. The state space-based model structure and reward rates associated with each state provide capabilities to evaluate availability, performability, service cost, and various metrics of the modeled system. However, real systems often have a large number of states, leading to the state explosion problem. To simplify model specification, stochastic Petri nets have been used, supported by well-known tools [2, 14]. Hierarchical modeling approaches have been proposed to decompose complex models into submodels, implemented in modern modeling tools [13, 18].

The software tool used in this analysis is RAScad [17, 18], a Sun internal web-based Reliability, Availability, and Serviceability (RAS) architecture modeling tool for system design and development phases. RAScad has been extensively used in designing new Sun hardware products and developing availability models for Sun Cluster software systems [12]. The model and analysis presented in this paper were developed using RAScad's hierarchical Markov modeling and uncertainty analysis capabilities. The system metrics of interest include availability, yearly downtime, and mean time between system failures.

The rest of the paper is organized as follows:
- Section 2 introduces the architecture of the target system.
- Section 3 describes the test environment and measurements.
- Sections 4 and 5 discuss the assumptions and parameters used in the model.
- Section 6 presents the model.
- Section 7 analyzes the results obtained from the model and conducts uncertainty analysis.
- Section 8 concludes the study.

## 2. System Architecture
Figure 1 shows a general configuration for JSAS EE7 [15]. A J2EE technology-based web services deployment model typically consists of three tiers: web server, application server, and database. The web server tier, including load balancers, communicates with the Internet and distributes incoming requests to application server instances, performing a reverse proxy function. The web server tier may also serve static content and dynamic content that does not require security. The web server tier is stateless, as it does not retain memory of prior requests.

An application server-specific Load Balancer Plugin (LBP) is installed on the web server. This plugin is aware of the state of application servers and performs user application session load balancing and proxy functions. Load balancing decisions are recorded as HTTP cookies, keeping the web server stateless.

The application server tier, including its data persistence support, processes user requests in a stateful context and carries out single- or multi-step business transactions. A transaction typically involves a series of dependent business logic functions that communicate with the database tier. The database tier contains various business data and user information and may span multiple databases. The database tier is not necessarily a relational database; it can be any transactional data repository or even a message broker that can store or transmit application data reliably.

To focus on the application server tier, the database tier is omitted from the figure, although it is required and used by our test applications.

### Figure 1. General Configuration of JSAS EE7

As shown in the figure, multiple Application Server (AS) instances are organized as a cluster to serve user requests from load balancers. The session conversational state of AS instances is written to a highly available database (HADB), developed based on the "Always-On" availability technology [11]. Multiple pairs of HADB nodes are organized as Database Redundancy Units (DRU) to provide redundancy. When an AS instance fails, the LBP plugin in web servers can detect the failure and forward subsequent requests to another AS instance, which can then restore the last saved conversational state from HADB. The AS instances and HADB pairs constitute the target system to be analyzed in this paper.

The target system provides automatic recovery and self-repair features, including HTTP session failover, which is necessary for tolerating faults in a stateful environment. For example, an insurance quote or loan application typically requires multiple screens of user input. A failure at any step of the process results in the loss of state information and a complete transaction loss, requiring the transaction to restart from the beginning. JSAS EE7 supports the persistence of HTTP session data in its bundled HADB, which is available to all instances in a cluster. In the event of an instance failure, session data can be recovered by other instances in the cluster. In addition to unplanned events, this capability, along with the HADB self-repair capability described below, also provides minimal impact on planned maintenance.

HADB is a highly scalable and available distributed database. It consists of two mirrored DRUs, which are logical groupings of nodes. Each DRU contains the complete set of session data evenly distributed over multiple nodes, ensuring optimal throughput and response time. A node is a collection of processes, a dedicated area of main memory, and some physical disk space. Two different nodes normally do not share a physical host. A node may be active, providing data access and update, or spare, ready to take over for the failure of an active node. Nodes are grouped and provide a mutual watchdog service in the group.

For each node in a DRU, there is a mirrored node in the other DRU. Should a node fail, the failed node attempts to restart itself and recover data from the companion node. If the restart procedure is successful, the recovered node will return the system to its pre-failure state. If the restart attempt is not successful, a repair procedure is initiated on a spare node by reconstructing data on the spare. Completion of this repair procedure will convert the spare node to an active node. The failed node will become a spare node after a physical repair action is fulfilled.

## 3. Measurements
To assess the stability of the target software system, multiple longevity tests were performed by running representative application benchmarks on the following configuration: Two Application Server instances running on two 4-CPU Sun E450 systems and two pairs of HADB nodes running on four Sun Ultra 80 systems. Table 1 shows the test configuration and their logical relationship between layers. System load was generated through a commercial workload generator running on a Microsoft Windows machine. A load balancer was included in the test configuration to perform sticky round-robin load balancing between multiple server instances. In these tests, the systems were utilized at a load factor of 60-70%, and multiple 7-day duration runs were performed. Roughly seven million requests were processed by the system in each run.

### Table 1. Test Environment

| Layer | Components |
|-------|------------|
| Load Balancers | Sticky Round-Robin Load Balancing |
| AS Instance 1 | J2EE Web App/Nile Bookstore |
| AS Instance 2 | J2EE Web App/Nile Bookstore |
| HADB Pair 1 (2 Nodes) | - |
| HADB Pair 2 (2 Nodes) | - |
| Oracle Database & Sun Java System Directory Server | - |
| Solaris™ 9 OS running on Sun Enterprise 450 server | - |

The stability test was conducted on two large applications. The first application is a sophisticated, real-world J2EE Web Application benchmark for running digital marketplaces. It includes Catalog, Auction, Pricing, and Order Management modules and is used in many customer deployments. The application uses pooled JDBC technology to access an Oracle database and the Java LDAP Software Developer Kit (SDK) to access the Sun Java System Directory Server. The average session size is 50KB, which is larger than the typical size of HTTP sessions.

The second test application is the Nile Bookstore benchmark, which uses the JSAS EE7 connection pooling capabilities to access an Oracle database. The average session size is roughly 30KB. This application is a complete, end-to-end e-commerce application server benchmark that has been widely used by independent testing laboratories. Both applications were stable during the course of the test. Application redeployment or server restart was not necessary. One of the tests, which was continued for 24 days for sanity checking and availability demonstration purposes, survived a system reboot due to a recoverable hardware problem.

To assess fault tolerance, a number of manual fault injection tests were performed to ensure that the system can tolerate single faults and behaves as expected. Some of the faults injected include:

- Bringing down an HADB node by killing all related processes
- Disrupting HADB node communication by unplugging the network cable
- Unplugging the HADB node hardware power
- Bringing down an Application Server node by killing processes
- Unplugging the Application Server node host network cable
- Unplugging the Application Server node host power

For all the fault injection tests listed above, the system continued functioning without any major departure from the expected performance. In addition to these manual fault injection tests, automated fault injections were conducted extensively on the HADB system. Some of the faults injected include:

- Simultaneously killing all processes in a node to simulate a full node failure
- Randomly killing one of the processes to simulate software bugs
- Asking processes to terminate immediately to simulate fast fail scenarios

Both single-node and multi-node (not in a pair) failures were induced in the fault injections. Workloads were fluctuated from idle to fully loaded states, in combination with rare conditions such as repair and data reorganization modes, during the fault injection process. For over 3,000 fault injections covering a variety of failure scenarios, all recoveries were successful.

During these fault injection tests, measurements were made to determine AS/HADB node recovery/restart times under different failure scenarios.