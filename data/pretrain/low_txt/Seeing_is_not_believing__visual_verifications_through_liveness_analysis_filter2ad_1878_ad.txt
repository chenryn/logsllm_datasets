### 6.4 Experimental Results

#### 6.4.1 Movee Server Overhead
Figure 8 illustrates the per-module overhead of the liveness analysis on the server, running on a Dell laptop, for 6-second videos. The values are averaged over 10 experimental runs. The Video Motion Analysis (VMA) module is the most time-consuming, slightly exceeding 1 second. The Image Motion Analysis (IMA) and Classification components (using the J48 classifier) impose the smallest overheads, totaling 110 milliseconds. The Multilayer Perceptron (MLP) takes an average of 940 milliseconds, while the Random Forest (RF) takes an average of 140 milliseconds. The Sensor Consistency (SC) module has an overhead of around 150 milliseconds, with the stretching step being the least costly and the penalty-based Dynamic Time Warping (DTW) being the most costly.

#### 6.4.2 Attack Detection Analysis
Movee effectively prevents "Copy-Paste" and "Replay" attacks, as no sensor stream exists in these cases. However, it does not detect "Projection" attacks, where the video and sensor streams are captured during the same user hand movement, although a human observer can easily recognize that the video is of a poster. 

In the following, we analyze Movee's ability to detect the more complex "Random" and "Direction Sync" attacks using the mentioned classifiers. The results are shown in Figure 9. For the random dataset, the MLP provides the highest accuracy at 92%, followed by RF at 91% and J48 at 90%. Figure 10(a) displays the ROC curve and the computed Equal Error Rate (EER) value for the MLP and the random dataset, with an EER as low as 0.08.

We also evaluated the impact of each step in the SC module on the accuracy of Movee for both test datasets. For each dataset, we measured the accuracy of the three classifiers under different conditions: (i) no alignment phase, (ii) stretching and DTW, (iii) stretching, calibration, and DTW, and (iv) stretching, calibration, and penalty-based DTW. Figures 10(b) and 10(c) show the results for the random and direction sync datasets, respectively. The stretching and DTW steps contribute the most to the accuracy, providing almost a 12% improvement for all three classifiers on the random attack and an 8% improvement for the direction sync attack. For the direction sync attack, the penalization step brings the most significant accuracy improvement, nearly 11% for all three classifiers. 

Notably, MLP performs best for the random attack, while C4.5 (J48) outperforms MLP and RF for the direction sync attack. When all processing steps are applied, C4.5 should be chosen as it outperforms MLP and RF's accuracy by 6% for the direction sync attack, while it lags only 2% behind MLP's accuracy for the random attack.

### 6.5 Limitations
Our experiments did not include very short videos (less than 6 seconds) or videos shot in unusual circumstances, such as those involving very high accelerometer activity (e.g., running) or when the user is in a moving vehicle. Additionally, due to the lack of gyroscope sensors in the Samsung Admire device, we did not integrate gyroscope readings to verify camera rotation movements.

Furthermore, we did not experiment with doctored video and accelerometer streams. For instance, an attacker could use the work of Davison et al. [22] to recover the 3D trajectory of the camera and then create a corresponding accelerometer sample to feed into Movee. We defer the task of ensuring the integrity of the mobile app and the device’s connection to its camera and accelerometer sensors to the providers of Movee. Establishing the integrity of a mobile platform and mobile apps is currently an active area of research [38, 1, 39].

Finally, we did not explore "green screen" attacks, where the attacker captures a video with a portion of the scene being a green screen and overlays additional video footage or static images. While Movee raises the bar, an attacker would need to invest in additional equipment to thwart its defenses, and the quality of this equipment determines the (in)ability of a human observer to detect the attack.

### 7. Related Work
The combination of video and accelerometer data has been studied by Hong et al. [28] to improve motion estimation in video encoding. They demonstrated that using accelerometer data can speed up the encoding process by a factor of 2-3. Moiz et al. [33] developed a wearable, multi-modality motion capture platform using inertial and ultrasonic sensors to estimate position. Our focus, however, is on verifying the liveness of a video through the consistency of its video and accelerometer data.

Indyk et al. [29] addressed the problem of finding pirated video on the Internet by extracting temporal fingerprints based on shot boundaries and matching them against a database. Our work, on the other hand, focuses on verifying the liveness of a video claimed to have been taken by a mobile device user, which is an orthogonal problem but can complement their work.

A similar approach to liveness analysis is used in biometric liveness verification. Kollreider et al. [31] proposed a lightweight optical flow method to distinguish between a live face and a photograph. Park et al. [36] introduced a liveness detection method to differentiate between 2D and 3D objects using video sequences without additional hardware. Chetty [18] proposed liveness checking techniques for multimodal biometric authentication systems, fusing acoustic and visual speech features to measure synchronization.

Accelerometers have been used for gait or gesture recognition. Mantyjarvi et al. [32] and Pylvänen [37] used 3D accelerometers and hidden Markov models to identify users and gestures, respectively.

### Summary
Our work introduces novel techniques for combining video and inertial sensor data to verify the liveness of a video stream. Movee ensures that the video was indeed shot as claimed by the user, using her mobile device. It does not require additional equipment but requires the user to install and shoot the video using Movee's client application.

### 8. Conclusions
In this paper, we introduced the concept of "liveness" analysis to verify that a video has been shot on a claimed mobile device. We proposed Movee, a system that uses accelerometer sensors to verify the ownership of a simultaneously captured video stream. Through extensive experiments, we showed that Movee is efficient in differentiating fraudulent and genuine videos and imposes reasonable overheads on the server. Future work will integrate more sensors (e.g., gyroscope) and alternative VMA implementations to improve accuracy.

### 9. Acknowledgments
We thank the shepherd and the anonymous reviewers for their excellent feedback.

### 10. References
[1] Arxan: Protecting the App Economy. http://www.arxan.com/.
[2] Chicago Works. http://www.chicagoworksapp.com/.
[3] Kickstarter. http://www.kickstarter.com/.
[4] NYC 311: Pothole or Other Street Surface Complaint. http://www.nyc.gov/apps/311/allServices.htm?requestType=topService&serviceName=Pothole+or+Other+Street+Surface+Complaint.
[5] Open Source Computer Vision. http://opencv.org/.
[6] Optical mouse. https://en.wikipedia.org/wiki/Optical_mouse.
[7] Root and Me. https://play.google.com/store/apps/details?id=com.iamjake.root&hl=en.
[8] Sensor Delay. http://developer.android.com/reference/android/hardware/SensorManager.html.
[9] Unlock Root. http://www.unlockroot.com/.
[10] Vine. http://vine.co/.
[11] Weka. http://www.cs.waikato.ac.nz/ml/weka/.
[12] XPrivacy 1.9.5: The ultimate privacy manager. http://forum.xda-developers.com/showthread.php?t=2320783.
[13] YouTube. http://www.youtube.com.
[14] A. Ali, F. Deravi, and S. Hoque. Liveness detection using gaze collinearity. In Emerging Security Technologies (EST), pages 62–65, 2012.
[15] A. Anjos and S. Marcel. Counter-measures to photo attacks in face recognition: A public database and a baseline. In Biometrics (IJCB), pages 1–7, 2011.
[16] L. Breiman. Random forests. Machine Learning, 45:5–32, 2001.
[17] S. Capkun, K. B. Rasmussen, M. Cagalj, and M. B. Srivastava. Secure location verification with hidden and mobile base stations. IEEE Trans. Mob. Comput., 7(4):470–483, 2008.
[18] G. Chetty. Biometric liveness detection based on cross modal fusion. In Information Fusion, 2009. FUSION '09. 12th International Conference on, pages 2255–2262, July.
[19] G. Chetty and M. Wagner. Multi-level liveness verification for face-voice biometric authentication. In Biometric Symposium, 2006.
[20] D. Clifford and G. Stone. Variable penalty dynamic time warping code for aligning mass spectrometry chromatograms in R. Journal of Statistical Software, 47(8):1–17, April 2012.
[21] I. Coope. Circle fitting by linear and nonlinear least squares. Journal of Optimization Theory and Applications, 76:381–388, 1993.
[22] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse. MonoSLAM: Real-time single camera SLAM. IEEE Trans. Pattern Anal. Mach. Intell., 29(6):1052–1067, June 2007.
[23] E. De Castro and C. Morandi. Registration of translated and rotated images using finite Fourier transforms. IEEE Trans. Pattern Anal. Mach. Intell., 9(5):700–703, May 1987.
[24] J. B. J. Fourier and A. Freeman. The Analytical Theory of Heat. Cambridge University Press, 2009.
[25] R. Frischholz and U. Dieckmann. BioID: A multimodal biometric identification system. IEEE Computer, 33(2):64–68, 2000.
[26] S. I. Gallant. Perceptron-based learning algorithms. Trans. Neur. Netw., 1(2):179–191, June 1990.
[27] B. F. Hildebrand. Introduction to numerical analysis: 2nd edition. Dover Publications, Inc., 1987.
[28] G. Hong, A. Rahmati, Y. Wang, and L. Zhong. Sensecoding: Accelerometer-assisted motion estimation for efficient video encoding. MM '08, pages 749–752. ACM, 2008.
[29] P. Indyk, G. Iyengar, and N. Shivakumar. Finding pirated video sequences on the Internet. Technical report, Stanford University, 1999.
[30] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. Pages 1137–1143, 1995.
[31] K. Kollreider, H. Fronthaler, and J. Bigün. Non-intrusive liveness detection by face images. Image Vision Comput., 27(3):233–244, 2009.
[32] J. Mantyjarvi, M. Lindholm, E. Vildjiounaite, S.-M. Makela, and H. Ailisto. Identifying users of portable devices from gait pattern with accelerometers. In ICASSP '05, volume 2, pages ii/973–ii/976 Vol. 2, March.
[33] F. Moiz, D. Leon-Salas, and Y. Lee. A wearable motion tracker. BodyNets '10, pages 214–219.
[34] M. Müller. Dynamic time warping. In Information Retrieval for Music and Motion, pages 69–84. Springer Berlin Heidelberg, 2007.
[35] G. Pan, L. Sun, Z. Wu, and S. Lao. Eyeblink-based anti-spoofing in face recognition from a generic webcamera. In ICCV 2007., pages 1–8.
[36] G.-t. Park, H. Wang, and Y.-s. Moon. Liveness detection method and apparatus of video image, August 2007.
[37] T. Pylvänen. Accelerometer-based gesture recognition using continuous HMMs. IbPRIA'05, pages 639–646. Springer-Verlag, 2005.
[38] A. Shabtai, Y. Fledel, and Y. Elovici. Securing Android-powered mobile devices using SELinux. Security & Privacy, IEEE, 8(3):36–44, 2010.
[39] J. Six. Application Security for the Android Platform: Processes, Permissions, and Other Safeguards. O’Reilly, 2011.
[40] J. O. Smith. Spectral Audio Signal Processing. W3K Publishing, 2011. Online book.
[41] N. P. H. Thian and S. Bengio. Evidences of equal error rate reduction in biometric authentication fusion, 2004.
[42] L. von Ahn, M. Blum, N. J. Hopper, and J. Langford. CAPTCHA: Using hard AI problems for security. In Proceedings of EUROCRYPT, pages 294–311. Springer-Verlag, 2003.
[43] Wikipedia. Receiver operating characteristic. http://en.wikipedia.org/wiki/Receiver_operating_characteristic.