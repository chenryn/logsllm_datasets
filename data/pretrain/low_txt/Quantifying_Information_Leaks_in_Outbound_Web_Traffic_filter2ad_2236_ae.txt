### Request and Response Stream Analysis

The request and response stream operated at a rate of 1.7 Mbps. The primary bottleneck during processing was the CPU. The real web traffic consisted of 3.04 GB of total data, with 15% (475 MB) being request data and 85% (2.58 GB) being response data. The engine processed requests at an average rate of 0.25 Mbps and responses at an average rate of 10.9 Mbps. This performance disparity is attributed to the time required to compute the edit distance for request URLs. JavaScript execution was included in the response processing time. None of the scripts were given a time limit, and none entered infinite loops.

### Performance Improvement for Intrusion Detection Systems

For the prototype implementation to be viable in an intrusion detection system that inspects large volumes of network traffic, several performance improvements are necessary. One area for optimization is reducing the number of edit distance comparisons by only considering multi-byte chunks. Another approach is to use a string co-processor specifically designed for edit distance computations. Exploring CPU performance optimizations and maximizing the throughput of the unconstrained bandwidth measurement engine is also future work.

### Memory Footprint and Optimization

The memory footprint during analysis was substantial for the prototype implementation. It retained all observed links in memory without attempting to free older, less relevant data. Processing 20 MB of web browsing traffic from one user over a single day required 120 MB of RAM. While this is impractical for an intrusion detection application, the memory usage can be significantly reduced by discarding links from old pages. Although this may slightly degrade analysis results, the number of links loaded from recent pages is much smaller than those from older pages. Another possible optimization is sharing link information across users.

### Entropy Mitigation Strategies

#### 8.1. System Information and Human Input

The current leak measurement engine cannot access actual system information or human input on the client; it only observes the resulting requests. Due to the complexity of active content on websites, system information and human input can generate a chain of events leading to larger output. For example, clicking on a web page may trigger an AJAX request containing a few hundred bytes of XML. Speculatively firing events could help, but this approach would lead to exponential growth. A better solution is to obtain system information (e.g., screen resolution, OS, installed plugins) and human input hints from an agent running on the end host. This agent, such as a browser plugin, could record and send all system information and human input events to the analysis engine, allowing it to replay the exact sequence and verify the output. Incorrect data from the agent would only increase the unconstrained bandwidth measurement and raise suspicion.

In some threat models, it may be possible to reduce unconstrained bandwidth measurements by discounting human input entirely. A trusted device, similar to a hardware key-logger, could intercept mouse and keyboard events before they reach the computer and report them to the leak measurement engine. This would aid analysis similarly to a browser plugin, except that the size of the original human input could also be discounted if the user is trusted.

#### 8.2. Timing

The timing of each request can leak several bits of information to an observer outside the network. Traditional methods for mitigating timing channels involve adding entropy to each request. For web traffic, this can be achieved by using a trusted proxy server between the client and the web server. The proxy can add jitter to each web request by delaying it a random amount of time, increasing the timing interval from 0.06 seconds to 1 second. Randomly delaying requests up to 1 second would reduce the amount of timing information in each request by 5 bits, which can be significant for a large number of requests.

Another option is to reduce the total number of requests. A smart caching proxy could pre-fetch all mandatory links when a client requests a web page. When the client requests a resource from a mandatory link, the proxy can return the result without any information leaving the network, thus preventing leakage through those requests.

Some requests include an explicit time value, which is the system time at which a script executed on the end host. Websites may include this time value to prevent caching or collect latency statistics. This time value differs slightly from the time the request appears on the network, has high precision, and can leak information. A proxy server can eliminate this timing information by discovering it with the edit distance algorithm and overwriting it with the time the proxy sends the request.

#### 8.3. Random Number Generator

Many websites include random numbers in link URLs to prevent caching, but these requests also leak data. One way to reduce entropy from the random number generator (RNG) is to have a network service handle random number generation. When a script calls for a random number, the JavaScript engine could request a new random number from a trusted central location instead of using the local RNG. This would move random numbers from the set U of UI-layer input to the set I of network inputs, allowing the analysis engine to discount them from the information measurement in outbound web requests (assuming they are not modified by malware).

### Conclusions and Future Work

This paper introduces a new approach for quantifying information leaks in web traffic. Instead of inspecting a message's data, the goal is to quantify its information content. The algorithms achieve precise results by discounting fields that are repeated or constrained by the protocol. The work focuses on web traffic but can apply to other protocols. The analysis engine processes static fields in HTTP, HTML, and JavaScript to create a distribution of expected request content and executes dynamic scripts in an emulated browser environment to obtain complex request values.

We evaluated our analysis techniques on controlled test cases and real web traffic from 10 users over a 30-day period. For controlled tests, the measurement techniques yielded byte counts ranging from 0.32% to 1.12% of the raw message size. These tests highlighted limitations, such as the inability to filter parts of URLs containing random numbers. For real web traffic, the precise unconstrained byte counts averaged 1.48% of the corresponding raw values, significantly better than a generic compression algorithm, which averaged 9.87%.

In the future, we plan to implement similar leak measurement techniques for other protocols, such as SMTP. We also aim to improve dynamic content analysis techniques, obtain user input hints from clients, and execute plug-in objects to extract additional request URLs. Finally, we hope to optimize and integrate these techniques into a network intrusion detection system that uses bandwidth thresholds to discover information leaks.

### Acknowledgements

We thank friends and students at the University of Michigan who participated in this study. Special thanks to Peter Chen, Myron Gutmann, Morley Mao, and Patrick McDaniel for their feedback. The web traffic study was conducted with IRB approval under project HUM00024168 at the University of Michigan.

### References

[1] Adobe Systems Incorporated. Adobe Flash Player. http://www.macromedia.com/software/flash/about, 2008.
[2] R. Anderson and F. Petitcolas. On the Limits of Steganography. IEEE Journal of Selected Areas in Communications, 16(4):474-481, 1998.
[3] K. Borders and A. Prakash. Web Tap: Detecting Covert Web Traffic. In Proc. of the 11th ACM Conference on Computer and Communications Security (CCS), 2004.
[4] K. Borders and A. Prakash. Towards Quantification of Network-Based Information Leaks Via HTTP. In Proc. of the 3rd USENIX Workshop on Hot Topics in Security, 2008.
[5] S. Brand. DoD 5200.28-STD Department of Defense Trusted (Orange Book). National Computer Security Center, 1985.
[6] S. Cabuk, C. Brodley, and C. Shields. IP Covert Timing Channels: Design and Detection. In Proc. of the 11th ACM Conference on Computer and Communications Security (CCS), 2004.
[7] S. Castro. How to Cook a Covert Channel. hakin9, http://www.gray-world.net/projects/cooking_channels/hakin9_cooking_channels_en.pdf, 2006.
[8] J. Gailly and M. Adler. The gzip Home Page. http://www.gzip.org/, 2008.
[9] J. Giles and B. Hajek. An Information-Theoretic and Game-Theoretic Study of Timing Channels. IEEE Transactions on Information Theory, 48:2455â€“2477, 2003.
[10] M. Handley, V. Paxson, and C. Kreibich. Network Intrusion Detection: Evasion, Traffic Normalization, and End-to-End Protocol Semantics. In Proc. of the 10th USENIX Security Symposium, 2001.
[11] M. Kang, I. Moskowitz, and D. Lee. A Network Version of the Pump. In Proc. of the 1995 IEEE Symposium in Security and Privacy, 1995.
[12] G. Malan, D.Watson, F. Jahanian, and P. Howell. Transport and Application Protocol Scrubbing. In Proc. of the IEEE INFOCOM 2000 Conference, 2000.
[13] S. McCamant and M. Ernst. Quantitative Information Flow as Network Flow Capacity. In Proc. of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 2008.
[14] Mozilla. The Firefox Web Browser. http://www.mozilla.com/firefox/, 2008.
[15] Mozilla. SpiderMonkey Engine (JavaScript-C). http://www.mozilla.org/js/spidermonkey/, 2008.
[16] A. Myers, N. Nystrom, L. Zheng, and S. Zdancewic. Jif: Java information flow. http://www.cs.cornell.edu/jif, 2001.
[17] R. Richardson. CSI Computer Crime and Security Survey. http://i.cmpnet.com/v2.gocsi.com/pdf/CSISurvey2007.pdf, 2007.
[18] RSA Security, Inc. RSA Data Loss Prevention Suite. RSA Solution Brief, http://www.rsa.com/products/EDS/sb/DLPST_SB_1207-lowres.pdf, 2007.
[19] N. Schear, C. Kintana, Q Zhang, and A. Vahdat. Glavlit: Preventing Exfiltration at Wire Speed. In Proc. of the 5th Workshop on Hot Topics in Networks (HotNets), 2006.
[20] J. Seward. bzip2 and libbzip2, version 1.0.5 â€“ A Program and Library for Data Compression. http://www.bzip.org/1.0.5/bzip2-manual-1.0.5.html, 2007.
[21] C. Shannon. Prediction and Entropy of Printed English. Bell System Technical Journal, 30:50â€“64, 1951.
[22] S. Servetto and M. Vetterli. Communication Using Phantoms: Covert Channels in the Internet. In Proc. of the IEEE International Symposium on Information Theory, 2001.
[23] Sun Microsystems. Java. http://www.java.com, 2008.
[24] VONTU. Data Loss Prevention, Confidential Data Protection â€“ Protect Your Data Anywhere. http://www.vontu.com, 2008.
[25] R. Wagner and M. Fischer. The String-to-String Correction Problem. Journal of the ACM, 21(1):168â€“173, 1974.
[26] Websense, Inc. Web Security, Internet Filtering, and Internet Security Software. http://www.websense.com/global/en/, 2008.
[27] A. Yumerefendi, B. Mickle, and L. Cox. TightLip: Keeping applications from spilling the beans. In Proc. of the 4th USENIX Symposium on Networked Systems Design and Implementation (NSDI), 2007.