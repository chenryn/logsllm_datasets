### Client’s TLS Handshake with OD

When the Service Provider (SP) detects the completion of the TLS handshake between the client and the Origin Destination (OD), it changes the CipherSpec of the TLS connection with the client to use the stream cipher RC4. (Note: A real implementation should use the same cipher agreed upon by the client and OD.) The CipherSpec, which includes the cipher key and MAC secrets, is derived from `kC,RS`. The SP also resets the read and write TLS sequence numbers to 0. Optionally, the SP can close its TCP connection with the OD and immediately create a new TCP connection to a local SOCKS proxy, such as 3proxy version 0.6.1. Alternatively, one could integrate SOCKS protocol support directly into Squid, eliminating the need for a separate SOCKS proxy. Subsequently, Squid simply tunnels traffic between the client and the SOCKS proxy.

### 5.4 Client-side Proxy

To avoid requiring modifications to existing applications, we employ a local proxy at the client host, similar to the Tor proxy. This local proxy, referred to as the Client Proxy (CP), exposes an apparent SOCKS interface. The CP is configured with the hostnames/IP addresses of two servers: one for the registration phase and the other for the OD. Upon starting, the CP generates TCP traffic towards the DR to register itself with Cirripede. The generated TCP traffic must contain special Initial Sequence Numbers (ISNs), which requires either kernel support or a userspace TCP stack. Our prototype CP generates SYN packets using raw sockets without establishing a full application connection.

Once registered, applications on the client host can use the CP as a regular SOCKS proxy. However, the CP does not interpret the SOCKS requests. Instead, upon receiving a TCP connection request, it initiates an HTTPS connection to the OD and completes the TLS handshake. It then changes the CipherSpec and resets the TLS sequence numbers, similar to the SP. The CP expects the next TLS record (of type "application data") to contain a confirmation message. If this is the case, the CP tunnels traffic between the application and the SP over the TLS channel without interpreting the SOCKS protocol. Otherwise, it rejects the SOCKS request.

### 6. Evaluation

We evaluate the registration component and throughput performance of Cirripede using experiments on the University of Utah’s Emulab testbed and simulations to study the effect of DR deployment on client registration.

#### 6.1 Registration Performance

##### 6.1.1 Metrics

We are interested in the RS's ability to handle real traffic load. The main metrics are:
1. The fraction of registration signals detected by the RS.
2. The load on the RS, specifically CPU and memory utilization.

##### 6.1.2 Experiment Setup and Topology

For this experiment, we use an existing packet trace and embed registration signals into the existing packets without introducing new packets. We use a one-hour trace captured in March 2011 at CAIDA’s equinix-sanjose monitor, filtering it to keep only TCP SYN packets. We assume all clients in the trace want to register, but each client registers only once, at the earliest opportunity. Since no new packets are injected, a client needs at least 12 SYN packets to register. Out of over 94 million SYN packets from over 6.4 million unique client IP addresses, we can embed 1,069,318 complete registrations.

The experiment involves two machines: the DR and the RS. Both are 2.4 GHz 64-bit Quad Core Xeon E5530 machines with 12 GB of RAM, running Ubuntu 10.04 64-bit. The machines are connected via a 1 Gbps Ethernet link with zero latency. The DR uses tcpreplay to replay the processed packet trace (containing only TCP SYN packets) against the RS. The replay speed is about 41,000 packets/second, resulting in a replay duration of 2300 seconds, which puts more pressure on the RS than a live capture would. The RS uses four threads, each handling a different set of clients, partitioned by the hash of the client IP address. The validation interval is one hour, effectively meaning the RS does not time out any client during the experiment. The RS uses sar to collect CPU and memory utilizations at two-second intervals.

##### 6.1.3 Results

The RS successfully receives 100% of the packets sent by the DR and detects 1,038,689 registrations, achieving a 97% success rate. Manual inspection of missed registrations reveals that out-of-order packets are the cause. Further investigation with tcpdump shows that the network reorders packets, even though the DR sends them in the correct order.

In terms of load, the average CPU utilization is 56% with a maximum of 73%, and the average memory utilization is 1.1 GB with a maximum of 1.6 GB. Memory utilization increases through the experiment, as expected, since the RS does not time out any client. These results indicate that the registration component of Cirripede scales well.

#### 6.2 Throughput Performance

##### 6.2.1 Metrics

For these experiments, we measure the performance of downloading data from a web server. All clients are pre-registered with Cirripede before each experiment. The first metric is the time to download the first byte, which measures the perceived responsiveness of loading a website, especially for small data like static web pages. The second metric is the full page download time, though we only download a single file in our experiments.

##### 6.2.2 Experiment Setup and Topology

All hosts in the experiments, including routers, are 2.4 GHz 64-bit Quad Core Xeon E5530 machines with 12 GB of memory, running CentOS 5.5 64-bit. Links are 1 Gbps unless specified otherwise. Five servers run the Apache web server version 2.2.3 with SSL support enabled. Five client hosts use curl version 7.15.5 to fetch files from the servers using HTTP. Due to NIC limits, the clients are connected to the DR via two intermediate routers, but the link bandwidths at the client hosts are the bottleneck. The DR is on the path between all client-server pairs, and both the SP and DR have RTT of 150 ms to all clients and 50 ms to all servers, resulting in an effective RTT of 200 ms between all client-server pairs.

In the first set of experiments, all five client hosts have link bandwidths of 100 Mbps. On each client host, we launch 20 simultaneous "client" instances, each using curl to download a 10 MB file from a particular server 100 times over HTTP. Across all five client hosts, we have 100 "client" instances. We compare results from using Cirripede and without using Cirripede.

In the second set of experiments, we use four client hosts with different link bandwidths: 2 Mbps, 10 Mbps, 50 Mbps, and 100 Mbps. Each client host runs only one client instance, using curl to download a 1 MB file from a server 100 times. All clients use the Cirripede service.

##### 6.2.3 Results

Figure 3(a) shows the results for the time to the first byte. Cirripede adds a delay of no more than a few seconds, primarily due to the extra round-trips of the TLS handshake and the SOCKS request-response. Figure 3(b) compares the total download times. The main takeaway is that Cirripede provides comparable performance to the baseline of not using Cirripede. In some cases, Cirripede can result in faster download times because high latencies negatively affect TCP throughput. By splitting the original TCP connection into two separate connections with lower RTTs, each connection has higher throughput, improving the overall end-to-end throughput.

Figure 4 shows the results of the second set of experiments. As expected, higher bandwidths improve performance, but high RTTs between clients and servers yield diminishing returns for standard TCP.

#### 6.3 DR Deployment Simulation

To use Cirripede, a client must discover a path to a website that traverses a DR. To ensure this happens commonly, the provider of the Cirripede service needs to strategically deploy DRs.