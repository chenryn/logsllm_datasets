### 6.3.2 Packet Rate and Migration Delay

As the packet rate increases, the additional delay introduced by the migration process to packet forwarding decreases. This explains why, at a packet rate of 25,000 packets per second, the delay increase caused by migration becomes negligible. Additionally, our experiments show that migration does not cause any packet drops. Finally, our results indicate that link migration does not affect forwarding delay.

### 6.3.3 Importance of Reserved Migration Bandwidth

**Figure 9: Delay Increase of Data Traffic Due to Bandwidth Contention with Migration Traffic**

In Sections 6.3.1 and 6.3.2, migration traffic was given its own dedicated link (i.e., separate bandwidth). Here, we explore the importance of this requirement and the performance implications for data traffic if it is not met.

#### Experiment Setup

We used a dumbbell testbed in this experiment, where migration traffic and data traffic share the same bottleneck link. We loaded the OSPF daemon of a virtual router with 250,000 routes. The data traffic rate was initially set to 500 Mbps and gradually increased to 900 Mbps. Since OpenVZ uses TCP (scp) for memory copy, the migration traffic only receives the leftover bandwidth after the UDP data traffic.

#### Results

As the available bandwidth decreases below 300 Mbps, the migration time increases, leading to a longer control-plane downtime for the virtual router. Figure 9 compares the delay increase of the data traffic at different rates. Both the average delay and the delay jitter increase dramatically as the bandwidth contention becomes severe. Table 3 compares the packet loss rates of the data traffic at different rates, with and without migration traffic. As expected, bandwidth contention (i.e., data traffic rate ≥ 700 Mbps) causes data packet loss.

The above results indicate that to minimize the control-plane downtime of the virtual router and to eliminate the performance impact on data traffic, operators should provide separate bandwidth for the migration traffic.

### 6.4 Control Plane Impact

In this subsection, we investigate the control plane dynamics introduced by router migration, particularly how migration affects protocol adjacencies. We assume a backbone network running MPLS, where edge routers run OSPF and BGP, and core routers run only OSPF. Our results show that, with default timers, protocol adjacencies of both OSPF and BGP are maintained, and at most one OSPF LSA retransmission is needed in the worst case.

#### 6.4.1 Core Router Migration

We configured virtual routers VR1, VR6, VR8, and VR10 on the Abilene testbed (Figure 7) as edge routers, and the remaining virtual routers as core routers. By migrating VR5 from physical node Chicago-1 to Chicago-2, we observed the impact of migrating a core router on OSPF dynamics.

- **No Events During Migration:**
  - We first examined the case where there are no network events during the migration. Our experiment results show that the control-plane downtime of VR5 is between 0.924 and 1.008 seconds, with an average of 0.972 seconds over 10 runs.
  - We started with the default OSPF timers of Cisco routers: hello-interval of 10 seconds and dead-interval of 40 seconds. We then reduced the hello-interval to 5, 2, and 1 second in subsequent runs, while keeping the dead-interval equal to four times the hello-interval. In all cases, the OSPF adjacencies between the migrating VR5 and its neighbors (VR4 and VR6) remained up. Even in the most restrictive 1-second hello-interval case, at most one OSPF hello message was lost, and VR5 came back up on Chicago-2 before its neighbors' dead timers expired.

- **Events During Migration:**
  - We then investigated the case where there are events during the migration and the migrating router VR5 misses the LSAs triggered by these events. We triggered new LSAs by flapping the link between VR2 and VR3. We observed that VR5 missed an LSA when the LSA was generated during VR5’s 1-second downtime. In such a case, VR5 received a retransmission of the missing LSA 5 seconds later, which is the default LSA retransmit-interval.
  - We then reduced the LSA retransmit-interval from 5 seconds to 1 second to reduce the time VR5 may have a stale view of the network. This change brought down the maximum interval between the occurrence of a link flap and VR5’s reception of the resulting LSA to 2 seconds (i.e., the 1-second control plane downtime plus the 1-second LSA retransmit-interval).

#### 6.4.2 Edge Router Migration

We configured VR5 as the fifth edge router in the network, running BGP in addition to OSPF. VR5 received a full Internet BGP routing table with 255,000 routes (obtained from RouteViews on Dec 12, 2007) from an eBGP peer not included in Figure 7, and it formed an iBGP full mesh with the other four edge routers.

With the addition of a full BGP table, the memory dump file size grew from 3.2 MB to 76.0 MB. Consequently, it took longer to suspend/dump the virtual router, copy over its dump file, and resume it. The average downtime of the control plane during migration increased to between 3.484 and 3.594 seconds, with an average of 3.560 seconds over 10 runs. We observed that all of VR5’s BGP sessions remained intact during its migration. The minimal integer hello-interval VR5 can support without breaking its OSPF adjacencies during migration is 2 seconds (with dead-interval set to 8 seconds). In practice, ISPs are unlikely to set the timers much lower than the default values to shield themselves from faulty links or equipment.

### 7. Migration Scheduling

This paper primarily discusses the question of migration mechanisms ("how to migrate") for VROOM. Another important question is the migration scheduling ("where to migrate"). Here, we briefly discuss the constraints that need to be considered when scheduling migration and several optimization problems that are part of our ongoing work on VROOM migration scheduling.

When deciding where to migrate a virtual router, several physical constraints must be taken into consideration. First, an "eligible" destination physical router for migration must use a software platform compatible with the original physical router and have similar (or greater) capabilities (such as the number of access control lists supported). Additionally, the destination physical router must have sufficient resources available, including processing power (whether the physical router is already hosting the maximum number of virtual routers it can support) and link capacity (whether the links connected to the physical router have enough unused bandwidth to handle the migrating virtual router’s traffic load). Furthermore, the redundancy requirement of the virtual router needs to be considered—today, a router is usually connected to two different routers (one as primary and the other as backup) for redundancy. If the primary and backup are migrated to the same node, physical redundancy will be lost.

Fortunately, ISPs typically leave enough "head room" in link capacities to absorb increased traffic volume. Additionally, most ISPs use routers from one or two vendors, with a small number of models, which leaves a large number of eligible physical routers to be chosen for the migration.

Given a physical router that requires maintenance, the question of where to migrate the virtual routers it currently hosts can be formulated as an optimization problem, subject to all the above constraints. Depending on the operator's preference, different objectives can be used to pick the best destination router, such as minimizing the overall CPU load of the physical router, minimizing the maximum load of physical links in the network, minimizing the stretch (i.e., latency increase) of virtual links introduced by the migration, or maximizing the reliability of the network (e.g., the ability to survive the failure of any physical node or link). However, finding optimal solutions to these problems may be computationally intractable. Fortunately, simple local-search algorithms should perform reasonably well, since the number of physical routers to consider is limited (e.g., to hundreds or small thousands, even for large ISPs), and finding a "good" solution (rather than an optimal one) is acceptable in practice.

Besides migration scheduling for planned maintenance, we are also working on the scheduling problems of power savings and traffic engineering. In the case of power savings, we take the power prices in different geographic locations into account and try to minimize power consumption with a certain migration granularity (e.g., once every hour, according to the hourly traffic matrices). In the case of traffic engineering, we migrate virtual routers to shift load away from congested physical links.

### 8. Conclusions

VROOM is a new network-management primitive that supports live migration of virtual routers from one physical router to another. To minimize disruptions, VROOM allows the migrated control plane to clone the data-plane state at the new location while continuing to update the state at the old location. VROOM temporarily forwards packets using both data planes to support asynchronous migration of the links. These designs are readily applicable to commercial router platforms. Experiments with our prototype system demonstrate that VROOM does not disrupt the data plane and only briefly freezes the control plane.

In the unlikely scenario that a control-plane event occurs during the freeze, the effects are largely hidden by existing mechanisms for retransmitting routing-protocol messages.

Our research on VROOM raises several broader questions about the design of future routers and the relationship with the underlying transport network. Recent innovations in transport networks support rapid set-up and tear-down of links, enabling the network topology to change underneath the IP routers. Dynamic topologies coupled with VROOM’s migration of the control plane and cloning of the data plane make the router an increasingly ephemeral concept, not tied to a particular location or piece of hardware. Future work on router hypervisors could take this idea one step further. Just as today’s commercial routers have a clear separation between the control and data planes, future routers could decouple the control-plane software from the control-plane state (e.g., routing information bases). Such a "control-plane hypervisor" would make it easier to upgrade router software and for virtual routers to migrate between physical routers that run different code bases.

### 9. References

[1] The Internet2 Network. http://www.internet2.edu/.

[2] T. Afferent, R. Doverspike, C. Kalmanek, and K. K. Ramakrishnan. Packet-aware transport for metro networks. IEEE Communication Magazine, March 2004.

[3] M. Agrawal, S. Bailey, A. Greenberg, J. Pastor, P. Sebos, S. Seshan, J. van der Merwe, and J. Yates. RouterFarm: Towards a dynamic, manageable network edge. In Proc. ACM SIGCOMM Workshop on Internet Network Management (INM), September 2006.

[4] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebar, I. Pratt, and A. Warfield. Xen and the Art of Virtualization. In Proc. SOSP, October 2003.

[5] O. Bonaventure, C. Filsfils, and P. Francois. Achieving sub-50 milliseconds recovery upon BGP peering link failures. IEEE/ACM Trans. Networking, October 2007.

[6] S. Bryant and P. Pate. Pseudo wire emulation edge-to-edge (PWE3) architecture. RFC 3985, March 2005.

[7] J. Chabarek, J. Sommers, P. Barford, C. Estan, D. Tsiang, and S. Wright. Power awareness in network design and routing. In Proc. IEEE INFOCOM, 2008.

[8] E. Chen, R. Fernando, J. Scudder, and Y. Rekhter. Graceful Restart Mechanism for BGP. RFC 4724, January 2007.

[9] Ciena CoreDirector Switch. http://www.ciena.com.

[10] MPLS VPN Carrier Supporting Carrier. http://www.cisco.com/en/US/docs/ios/12_0st/12_0st14/feature/guide/csc.html.

[11] Cisco Logical Routers. http://www.cisco.com/en/US/docs/ios_xr_sw/iosxr_r3.2/interfaces/command/reference/hr32lr.html.

[12] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul, C. Limpach, I. Pratt, and A. Warfield. Live Migration of Virtual Machines. In Proc. NSDI, May 2005.

[13] B. Cully, G. Lefebvre, D. Meyer, M. Feeley, N. Hutchinson, and A. Warfield. Remus: High availability via asynchronous virtual machine replication. In Proc. NSDI, April 2008.

[14] D-ITG. http://www.grid.unina.it/software/ITG/.

[15] Emulab. http://www.emulab.net.

[16] N. Feamster, L. Gao, and J. Rexford. How to lease the Internet in your spare time. ACM SIGCOMM Computer Communications Review, Jan 2007.

[17] P. Francois, M. Shand, and O. Bonaventure. Disruption-free topology reconfiguration in OSPF networks. In Proc. IEEE INFOCOM, May 2007.

[18] M. Gupta and S. Singh. Greening of the Internet. In Proc. ACM SIGCOMM, August 2003.

[19] G. Iannaccone, C.-N. Chuah, S. Bhattacharyya, and C. Diot. Feasibility of IP restoration in a tier-1 backbone. IEEE Network Magazine, Mar 2004.

[20] Juniper Logical Routers. http://www.juniper.net/techpubs/software/junos/junos85/feature-guide-85/id-11139212.html.

[21] Z. Kerravala. Configuration Management Delivers Business Resiliency. The Yankee Group, November 2002.

[22] M. McNett, D. Gupta, A. Vahdat, and G. M. Voelker. Usher: An extensible framework for managing clusters of virtual machines. In Proc. USENIX LISA Conference, November 2007.

[23] NetFPGA. http://yuba.stanford.edu/NetFPGA/.

[24] OpenVZ. http://openvz.org.

[25] Average retail price of electricity. http://www.eia.doe.gov/cneaf/electricity/epm/table5_6_a.html.

[26] Quagga Routing Suite. http://www.quagga.net.

[27] A. Rostami and E. Sargent. An optical integrated system for implementation of NxM optical cross-connect, beam splitter, mux/demux and combiner. IJCSNS International Journal of Computer Science and Network Security, July 2006.

[28] K. Roth, F. Goldstein, and J. Kleinman. Energy Consumption by Office and Telecommunications Equipment in commercial buildings Volume I: Energy Consumption Baseline. National Technical Information Service (NTIS), U.S. Department of Commerce, Springfield, VA 22161, NTIS Number: PB2002-101438, 2002.

[29] A. Shaikh, R. Dube, and A. Varma. Avoiding instability during graceful shutdown of multiple OSPF routers. IEEE/ACM Trans. Networking, 14(3):532–542, June 2006.

[30] R. Teixeira, A. Shaikh, T. Griffin, and J. Rexford. Dynamics of hot-potato routing in IP networks. In Proc. ACM SIGMETRICS, June 2004.

[31] J. van der Merwe and I. Leslie. Switchlets and dynamic virtual ATM networks. In Proc. IFIP/IEEE International Symposium on Integrated Network Management, May 1997.

[32] VINI. http://www.vini-veritas.net.

[33] Y. Wang, J. van der Merwe, and J. Rexford. VROOM: Virtual ROuters On the Move. In Proc. ACM SIGCOMM Workshop on Hot Topics in Networking, Nov 2007.

[34] J. Wei, K. Ramakrishnan, R. Doverspike, and J. Pastor. Convergence through packet-aware transport. Journal of Optical Networking, 5(4), April 2006.

[35] T. Wood, P. Shenoy, A. Venkataramani, and M. Yousif. Black-box and Gray-box Strategies for Virtual Machine Migration. In Proc. NSDI, April 2007.