### Hazard Values for ISP Topologies

We analyze the hazard values for ISP topologies from the Topology Zoo, considering up to three (k = 3) failed links. The distribution of these hazard values is illustrated in Figure 5. Notably, a few highly connected networks exhibit a hazard value of zero, indicating that no failure scenario with up to three failed links can disconnect any demand. This suggests high resilience to failures. An example is the GlobalCenter network, which has a hazard value of zero, as shown in Figure 6.

Over 200 out of 260 topologies have a hazard value below 0.002, meaning they experience only a 0.2% loss in value compared to a fully connected network. However, some topologies, such as the Sogo network, have a higher hazard value. For Sogo, the hazard value is 0.01, indicating a 1% loss in reward value due to connection failures.

**Figure 5.** Hazard values for the Topology Zoo (k = 3)

**Figure 6.** GlobalCenter (left) with γ=0.00000 for k = 3 and Sogo (right) with γ=0.01134 for k = 3

For valley-free routing, the two curves diverge more rapidly than in the case of ISP topologies.

### Conclusion

Our strategic algorithm is applicable to real ISP and datacenter topologies. The current implementation (in Python) allows us to compute the hazard value for 2 to 3 failed links on Topology Zoo networks within about 5 minutes. This is clear evidence that in certain network topologies, there are numerous failure scenarios that can completely disconnect the end-points, and the computed hazard values align with the intuitive understanding of more or less resilient types of network topologies.

### Related Work

There is extensive literature on the empirical characteristics of failures in datacenters [16], [37], state-wide networks [34], and IP backbones [20]. This research is valuable for comparing existing networks but does not directly address the problem of comparing unimplemented network designs. Empirical studies provide valuable inputs to the method described in this paper.

In the graph-theory community, connectivity is often measured by the minimum cut or the number of disjoint paths, which are common metrics for both throughput and resilience [2], [3], [35]. Cuts also form the basis for expansion measures [19]. Another approach, used by the parallel-computing community, measures how many failures a network can sustain while still emulating its ideal counterpart with a certain maximal overhead [24], [29]. Both worst-case and average-case scenarios are well understood [2]. However, these generic graph-theoretic metrics treat all connections equally and do not account for the varying importance of connections to network operators.

Within the networking community, additional specific metrics are considered. Some, like the protection ratio [15], focus on single failures, while others, such as the loop ratio [9], consider detailed network behavior during convergence after failures. In our work, we assume fast rerouting. Two relevant connectivity measures in the context of multiple failures and locality constraints are perfect [11]–[13] and ideal [8], [14] resilience. However, these measures do not account for other aspects considered in this paper.

Our work is also related to survivable network design, a classic topic in operations research [17], [28]. A general and powerful approach to designing robust networks uses mathematical programming, particularly (integer) linear programming. SNDlib [27] is a well-known platform providing benchmarks for telecommunication networks. However, we are not aware of any existing approach that accounts for alternatives or more complex routing constraints and DFPs.

There is also literature on the impact of routing constraints on connectivity, such as in inter-domain routing [22] and fast rerouting [7]. Our approach explicitly models the physical topology and routing with constraints, making it orthogonal to work on resilient routing mechanisms [30] and verifying and maintaining routing and policy constraints under failures [21], [32].

To our knowledge, no connectivity measure explicitly accounts for differences in demand, choice between connections, preferences, and specific failures. With this work, we aim to fill this gap and propose a measure that meets several desirable properties and can be computed efficiently.

### Acknowledgments

We would like to thank Henrik Thostrup Jensen from NORDUnet for his valuable inputs and discussions. This research was supported by the Vienna Science and Technology Fund (WWTF), project WHATIF, ICT19-045, 2020-2024, DFF project QASNET, the Villum Investigator Project S4OS, and the ERC Advanced Grant Project LASSO.

**References:**

[1] A. Bagchi, A. Bhargava, A. Chaudhary, D. Eppstein, and C. Scheideler. The effect of faults on network expansion. Theory of Computing Systems, 39(6):903–928, 2006.
[2] A. Bagchi, A. Bhargava, A. Chaudhary, D. Eppstein, and C. Scheideler. The effect of faults on network expansion. Theory of Computing Systems, 39(6):903–928, 2006.
[3] A. Bagchi, A. Chaudhary, C. Scheideler, and P. Kolman. Algorithms for fault-tolerant routing in circuit switched networks. In Proc. 14th Annual ACM symposium on Parallel Algorithms and Architectures (SPAA), pages 265–274, 2002.
[4] T. Benson, A. Anand, A. Akella, and M. Zhang. Understanding data center traffic characteristics. ACM SIGCOMM Computer Communication Review, 40(1):92–99, 2010.
[5] M. N. Brain, J. H. Davenport, and A. Griggio. Benchmarking solvers, SAT-style. In Proceedings of the 2nd International Workshop on Satisfiability Checking and Symbolic Computation co-located with the 42nd International Symposium on Symbolic and Algebraic Computation (ISSAC’17), volume 1974 of CEUR, pages 1–15. CEUR-WS.org, 2017.
[6] Charles E. Leiserson. Fat-trees: Universal networks for hardware-efficient supercomputing. IEEE Transactions on Computers, 34(10):892–901, 1985.
[7] M. Chiesa, A. Kamisinski, J. Rak, G. Retvari, and S. Schmid. A survey of fast-recovery mechanisms in packet-switched networks. IEEE Communications Surveys and Tutorials (COMST), 2021.
[8] M. Chiesa, I. Nikolaevskiy, S. Mitrović, A. Gurtov, A. Madry, M. Schapira, and S. Shenker. On the resiliency of static forwarding tables. IEEE/ACM Transactions on Networking, 25(2):1133–1146, 2016.
[9] F. Clad. Disruption-free routing convergence: computing minimal link-state update sequences. PhD thesis, Strasbourg, 2014.
[10] P. Cuijpers, S. Schmid, N. Schnepf, and J. Srba. Reproducibility package for: The hazard value: A quantitative network connectivity measure accounting for failures, Mar. 2022. https://doi.org/10.5281/zenodo.6394782.
[11] J. Feigenbaum, B. Godfrey, A. Panda, M. Schapira, S. Shenker, and A. Singla. Brief announcement: On the resilience of routing tables. In Proceedings of the 2012 ACM symposium on Principles of distributed computing, pages 237–238, 2012.
[12] K.-T. Foerster, J. Hirvonen, Y.-A. Pignolet, S. Schmid, and G. Tredan. On the feasibility of perfect resilience with local fast failover. In Proc. SIAM Symposium on Algorithmic Principles of Computer Systems (APOCS), 2021.
[13] K.-T. Foerster, J. Hirvonen, Y.-A. Pignolet, S. Schmid, and G. Tredan. On the price of locality in static fast rerouting. In Proc. 52nd IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 2022.
[14] K.-T. Foerster, A. Kamisinski, Y.-A. Pignolet, S. Schmid, and G. Tredan. Improved fast rerouting using postprocessing. IEEE Transactions on Dependable and Secure Computing, 2020.
[15] P. François and O. Bonaventure. An evaluation of IP-based fast reroute techniques. In Proceedings of the 2005 ACM conference on emerging network experiment and technology, pages 244–245, 2005.
[16] P. Gill, N. Jain, and N. Nagappan. Understanding network failures in data centers: measurement, analysis, and implications. In Proceedings of the ACM SIGCOMM 2011 conference, pages 350–361, 2011.
[17] M. Grötschel, C. L. Monma, and M. Stoer. Design of survivable networks. Handbooks in operations research and management science, 7:617–672, 1995.
[18] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, S. Lu, and G. Lv. BCube: A high performance, server-centric network architecture for modular data centers. In ACM SIGCOMM. Association for Computing Machinery, Inc., August 2009.
[19] S. Hoory, N. Linial, and A. Wigderson. Expander graphs and their applications. Bulletin of the American Mathematical Society, 43(4):439–561, 2006.
[20] G. Iannaccone, C.-n. Chuah, R. Mortier, S. Bhattacharyya, and C. Diot. Analysis of link failures in an IP backbone. In Proceedings of the 2nd ACM SIGCOMM Workshop on Internet measurment, pages 237–242, 2002.
[21] P. G. Jensen, M. Konggaard, D. Kristiansen, S. Schmid, B. C. Schrenk, and J. Srba. Aalwines: A fast and quantitative what-if analysis tool for MPLS networks. In Proc. 16th ACM International Conference on emerging Networking EXperiments and Technologies (CoNEXT), 2020.
[22] R. Klöti, V. Kotronis, B. Ager, and X. Dimitropoulos. Policy-compliant path diversity and bisection bandwidth. In 2015 IEEE Conference on Computer Communications (INFOCOM), pages 675–683. IEEE, 2015.
[23] S. Knight, H. Nguyen, N. Falkner, R. Bowden, and M. Roughan. The Internet topology zoo. Selected Areas in Communications, IEEE Journal on, 29:1765 – 1775, 11 2011.
[24] F. T. Leighton, B. M. Maggs, and R. K. Sitaraman. On the fault tolerance of some popular bounded-degree networks. SIAM Journal on computing, 27(5):1303–1333, 1998.
[25] B. M. Maggs and R. K. Sitaraman. Algorithmic nuggets in content delivery. ACM SIGCOMM Computer Communication Review, 45(3):52–66, 2015.
[26] W. Najjar and J.-L. Gaudiot. Network resilience: A measure of network fault tolerance. IEEE Transactions on Computers, 39(2):174–181, 1990.
[27] S. Orlowski, M. Pióro, A. Tomaszewski, and R. Wessäly. SNDlib 1.0–Survivable Network Design Library. In Proceedings of the 3rd International Network Optimization Conference (INOC 2007), Spa, Belgium, April 2007. http://sndlib.zib.de, extended version accepted in Networks, 2009.
[28] P. Pavon-Marino and J.-L. Izquierdo-Zaragoza. Net2plan: an open source network planning tool for bridging the gap between academia and industry. IEEE Network, 29(5):90–96, 2015.
[29] C. Scheideler. Models and techniques for communication in dynamic networks. In Annual Symposium on Theoretical Aspects of Computer Science, pages 27–49. Springer, 2002.
[30] S. Schmid, N. Schnepf, and J. Srba. Resilient capacity-aware routing. In Proceedings of the 25th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS’21), volume 12651 of LNCS, pages 411–429. Springer-Verlag, 2021.
[31] P. Sebos, J. Yates, G. Hjalmtysson, and A. Greenberg. Auto-discovery of shared risk link groups. In OFC 2001. Optical Fiber Communication Conference and Exhibit. Technical Digest Postconference Edition (IEEE Cat. 01CH37171), volume 3, pages WDD3–WDD3. IEEE, 2001.
[32] S. Steffen, T. Gehr, P. Tsankov, L. Vanbever, and M. Vechev. Probabilistic verification of network configurations. In Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication, pages 750–764, 2020.
[33] L. Suresh, M. Canini, S. Schmid, and A. Feldmann. C3: Cutting tail latency in cloud data stores via adaptive replica selection. In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI), pages 513–527, 2015.
[34] D. Turner, K. Levchenko, A. C. Snoeren, and S. Savage. California fault lines: understanding the causes and impact of network failures. In Proceedings of the ACM SIGCOMM 2010 Conference, pages 315–326, 2010.
[35] E. Upfal. Tolerating linear number of faults in networks of bounded degree. In Proceedings of the eleventh annual ACM symposium on Principles of distributed computing, pages 83–89, 1992.
[36] K. Vajanapoom, D. Tipper, and S. Akavipat. A risk management approach to resilient network design. In International Congress on Ultra Modern Telecommunications and Control Systems, pages 622–627. IEEE, 2010.
[37] D. Zhuo, M. Ghobadi, R. Mahajan, K.-T. Förster, A. Krishnamurthy, and T. Anderson. Understanding and mitigating packet corruption in data center networks. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, pages 362–375, 2017.