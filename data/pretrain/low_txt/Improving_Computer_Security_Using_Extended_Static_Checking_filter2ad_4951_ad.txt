### Pseudo-Random Number Routines
**Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P'02)**
*1081-6011/02 $17.00 © 2002 IEEE*

### 4.2 Run-time Checking
Most operating systems prevent programs from performing grossly aberrant behavior, such as illegal memory access. Some programming languages and environments provide advanced monitoring features. For example, Perl uses a data tainting model to ensure that user-supplied commands are not executed directly in UNIX setuid scripts [31]. Similarly, JavaScript employs a similar approach to protect user privacy by preventing user-specific data from being transmitted over the network [12]. Java provides a flexible mechanism for controlling the run-time security policy enforced by the Java virtual machine [15].

Some of C's inherent weaknesses can be mitigated through runtime checking. Jones and Kelly [16] propose a scheme for adding object bounds checks at compile time. However, standard libraries that are not recompiled remain vulnerable to buffer overflow attacks, and the performance of checked programs may suffer. Program instrumentation approaches, like those used by Purify [14], are often impractical due to performance degradation and increased memory usage. Format string vulnerabilities, where an attacker exploits a function with a `printf`-style format string argument, can be thwarted at runtime with minimal computational overhead [5, 26].

### 4.3 Static Checking
Necula [22] proposes a combination of formal reasoning and load-time checking with his system for packaging proofs with executable programs. Before a program is executed, its proof can be checked to ensure that the code maintains the system’s safety and security policies. Since the proof pertains to assembly code, the properties that can be easily proven are at a low level. Necula suggests that his approach would be useful for determining whether to allow code to execute in the kernel's address space by requiring a proof that the program will maintain the consistency of the kernel's data structures. This method is efficient because proof checking is much faster than proof construction, and once a piece of code is approved, no further effort to constrain its behavior is necessary. In Necula’s view, producing proofs should be part of the compiler's function.

LCLint is a C program checker [10]. Without additional specifications, its ability to find errors is limited, similar to most lint programs. With programmer-supplied specifications, it can perform additional checks using compiler flow analysis techniques. It can detect abstraction violations, unannounced modifications to global variables, and possible use-before-initialization errors. While these are common issues in C programs, they are not direct widespread causes of security flaws.

Larochelle and Evans have modified LCLint to statically detect buffer overflow vulnerabilities [18]. Their method involves reasoning about minimum and maximum array indices that may be read or written. Programmer-provided preconditions and postconditions, combined with built-in specifications for standard libraries, are used to generate a set of constraints. If the constraints cannot be resolved, a buffer overflow flaw may be present. Unlike the approach taken here, a programmer cannot write additional specifications to find new types of vulnerabilities using the modified version of LCLint.

Wagner et al. have developed a static checker that uses integer range analysis to determine whether a C program contains potential buffer overflow errors [30]. While capable of finding many errors that lexical analysis tools would miss, the checker is still somewhat imprecise: it ignores statement order, cannot model interprocedural dependencies, and ignores pointer aliasing.

Inspired by Perl’s taint mode, Shankar et al. use type qualifiers to perform a taint analysis for the purpose of statically detecting format string vulnerabilities in C programs [27]. Their system requires a programmer to annotate a small number of variables as either tainted or untainted and then uses type inference rules (along with pre-annotated system libraries) to propagate the qualifiers. Once the qualifiers are propagated, the system can detect format string vulnerabilities by performing type checking.

### 5 Conclusions
While it is inconceivable that any single method will solve all computer security problems, we have shown that significant classes of vulnerabilities can be detected in source code using extended static checking. Although we have focused much of this paper on demonstrating this point, another measure of the utility of the approach is the range of security vulnerabilities it cannot detect.

Flaws may go undetected due to the way Eau Claire models C programs. A vulnerability may depend on the execution of a function pointer (an unmodeled construct) or the way the program behaves when an illegal operation occurs (Eau Claire assumes that illegal operations do not occur). While such shortcomings will probably always exist, they primarily reflect limitations in our implementation rather than in our method.

Eau Claire can only find security flaws for which it has specifications. Since new types of flaws are routinely discovered, a program that meets all available specifications today may prove to be insecure tomorrow. Simplify, the theorem prover Eau Claire uses, does not reason about induction. Consequently, Eau Claire’s specification language does not contain any recursive constructs. We have not encountered any real-world vulnerabilities that require recursion in their specifications, so this has not yet been a problem.

Many security vulnerabilities are not the result of programming logic errors. If a system’s design is fundamentally insecure, a static checker is unlikely to reveal the problem. For example, if a system design does not require users to select strong passwords, a static checker cannot determine that, although password authentication is implemented correctly, the passwords might be easy to guess.

Finally, if tools like Eau Claire are not put to use, they will not find flaws. The most likely reason for a programmer to ignore extended static checking is that they do not believe their efforts will be rewarded. However, one form of static checking, type checking, has long been embraced by programmers. Although it is imperfect, type checking has been successful because the rewards it provides far outweigh the cost of its use. Program verification technology has not fared so well, and perhaps part of the reason is that the rewards do not appear to outweigh the cost. Conventional wisdom holds that the undecidable nature of most static analysis questions implies that program verification techniques are a mirage, that there is no reward to be had. On the other side of the equation, fully specifying the behavior of a program is a daunting task.

In this case, conventional wisdom is wrong. Extended static checking makes use of program verification techniques and, as we have demonstrated, is capable of detecting common types of security vulnerabilities. This is a significant reward because security problems are difficult to detect by other means, and the penalty for missing them can be severe. The cost of extended static checking is variable. At the low end, a programmer can use pre-existing specification libraries to check for common flaws. The checker can be invoked much like a compiler, and the time required to perform the checking is typically a small multiple of a compiler’s execution time. A user who is willing to invest effort in writing specifications can improve the precision and accuracy of the checker and also check for new types of flaws or flaws specific to their problem domain.

### Acknowledgements
We would like to thank Greg Nelson for his guidance and patience and the anonymous referees for their constructive feedback.

### References
[1] R. Back and J. von Wright. *Reﬁnement Calculus: A Systematic Introduction*. Graduate Texts in Computer Science. Springer-Verlag, 1998.
[2] M. Bishop and M. Dilger. "Checking for race conditions in file accesses." *Computing Systems*, 9(2):132–152, Spring 1996.
[3] RSAREF buffer overflow. The bugtraq mailing list: http://www.securityfocus.com. Vulnerability 843.
[4] Linux capabilities vulnerability. The bugtraq mailing list: http://www.securityfocus.com. Vulnerability 1322.
[5] C. Cowan, M. Barringer, S. Beattie, and G. Kroah-Hartman. "FormatGuard: Automatic protection from printf format string vulnerabilities." In *Proceedings of the USENIX Security Symposium*, 2001.
[6] D. Detlefs. "An overview of the extended static checking system." In *The first Formal Methods in Software Practice workshop collocated with ISSTA 96*, 1995.
[7] D. L. Detlefs, K. R. M. Leino, G. Nelson, and J. B. Saxe. *Extended Static Checking*. Technical Report 159, Compaq Systems Research Center, December 1998.
[8] D. L. Detlefs, G. Nelson, and J. B. Saxe. *Simplify: The ESC Theorem Prover*. Unpublished manuscript, November 1996.
[9] E. W. Dijkstra. *A Discipline of Programming*. Prentice-Hall, Englewood Cliffs, NJ, 1976.
[10] D. Evans, J. Guttag, J. Horning, and Y. M. Tan. "LCLint: A tool for using specifications to check code." In *Symposium on the Foundations of Software Engineering*. SIGSOFT, December 1994.
[11] C. Flanagan and J. B. Saxe. "Avoiding exponential explosion: Generating compact verification conditions." In *Symposium on the Principles of Programming Languages*. ACM, 2001.
[12] D. Flanagan. *JavaScript: The Definitive Guide*. O’Reilly & Associates, 1996.
[13] S. P. Harbison and G. L. Steele Jr. *C: A Reference Manual*. Prentice-Hall, Englewood Cliffs, NJ, 1995.
[14] R. Hastings and B. Joyce. "Purify: Fast detection of memory leaks and access errors." In *Proceedings of the Winter USENIX Conference*, pages 125–136, 1992.
[15] The Java Security Home Page. On the web as http://java.sun.com/security/.
[16] R. W. M. Jones and P. H. J. Kelly. "Backwards-compatible bounds checking for arrays and pointers in C programs." In *Third International Workshop on Automated Debugging*, 1997.
[17] C. E. Landwehr, A. R. Bull, J. P. McDermott, and W. S. Choi. "A taxonomy of computer security flaws." *ACM Computing Surveys*, 26(3):211–254, September 1994.
[18] D. Larochelle and D. Evans. "Statically detecting likely buffer overflow vulnerabilities." In *2001 USENIX Security Symposium*, August 2001.
[19] K. R. M. Leino. *Toward Reliable Modular Programs*. PhD thesis, California Institute of Technology, 1995.
[20] K. R. M. Leino, J. B. Saxe, and Raymie Stata. "Checking Java programs via guarded commands." Technical Report 1999-02, Compaq Systems Research Center, May 1999.
[21] M. S. Manasse and C. G. Nelson. *Correct Compilation of Control Structures*. Technical report, AT&T Bell Laboratories, September 1984.
[22] G. Necula. "Proof-carrying code." In *Proceedings of the Symposium on Principles of Programming Languages*. ACM, 1997.
[23] G. Nelson. *Pointers are bad, records are bad, but record-pointers are good*. Technical report, Xerox, Palo Alto Research Center, November 1982.
[24] M. Norrish. *C Formalised in HOL*. PhD thesis, University of Cambridge, 1998.
[25] File access problems in lpr/lpd. Redhat Security Advisory, October 1999. RHSA-1999:041-01.
[26] T. J. Robbins. *libformat*. On the web as http://box3n.gumbynet.org/fyre/software/libformat.pdf.
[27] U. Shankar, K. Talwar, J. S. Foster, and D. Wagner. "Detecting format string vulnerabilities with type qualifiers." In *Proceedings of the 10th USENIX Security Symposium*, August 2001.
[28] The Simplify Home Page, Compaq Systems Research Center. On the web as http://research.compaq.com/SRC/esc/Simplify.html.
[29] J. Viega, J. T. Bloch, T. Kohno, and G. McGraw. "ITS4: A static vulnerability scanner for C and C++ code." In *Proceedings of the Annual Computer Security Applications Conference*. Applied Computer Security Associates, 2000.
[30] D. Wagner, J. S. Foster, E. A. Brewer, and A. Aiken. "A first step towards automated detection of buffer overrun vulnerabilities." In *Proceedings of the Network and Distributed System Security Symposium*, February 2000.
[31] L. Wall, T. Christiansen, and R. Schwartz. *Programming Perl*. O’Reilly & Associates, 1996.

**Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P'02)**
*1081-6011/02 $17.00 © 2002 IEEE*