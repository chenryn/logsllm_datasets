# 深度学习中的降维攻击：数据流处理中的安全风险
##### 译文声明
本文为翻译文章，仅供参考。具体表达及含义以原文为准。

深度学习在许多领域受到了广泛关注，尤其是在图像识别（如人脸识别和自动驾驶）方面，这些应用正逐渐融入我们的生活。随着深度学习的普及，其安全性问题也日益凸显。
目前对深度学习的安全讨论主要集中在平台漏洞、模型错误以及逃逸攻击等方面。
近期，360安全团队发现，在深度学习的数据处理流程中同样存在安全风险。攻击者可以利用数据流处理中的问题，无需借助软件实现漏洞或模型弱点，即可实施逃逸或数据污染攻击。
## 攻击实例
以下是一些针对深度学习图像识别应用的降维攻击案例。

### 示例一：灰太狼与喜羊羊
下图中展示了一张图片，从人类视角来看，这可能是著名的卡通角色“灰太狼”。然而，现有的深度学习系统却将其误判为“喜羊羊”。

这种错误并非玩笑。深度学习系统的识别结果依赖于模型的质量和训练数据的数量。由于经典模型并未针对卡通图片进行专门训练，因此我们尝试了自然界场景的图片。

### 示例二：羊群与狼
下面这张图片看起来像是一群羊。但当我们将该图片输入到基于Caffe平台的经典图像识别应用程序时（使用GoogleNet神经网络和ImageNet数据集训练），系统却将其识别为“狼”。
同样的情况也发生在其他流行的图像识别框架（如TensorFlow）上。

### 示例三：小羊与猞猁猫
另一张图片中，人类能够轻易地辨认出这是一只小羊。但在Caffe的机器学习应用中，它被错误地归类为ImageNet数据库中的“猞猁猫”。其他平台如TensorFlow和Torch也有类似的表现。

## 降维攻击原理
此前我们曾探讨过关于深度学习逃逸攻击的各种方法，其中大多数研究集中于对抗样本生成技术。而本文所介绍的降维攻击则不同，它直接针对深度学习应用的数据流处理过程。

深度学习的核心在于神经网络，而静态神经网络通常假定输入具有固定维度。但在实际应用中，输入数据往往与模型预期不符。为此，有两种解决方案：一是仅接受符合要求的输入；二是调整输入维度以适应模型需求。后者在广泛图像识别任务中更为常见。

下图展示了典型深度学习应用的数据流处理过程：

【 深度学习应用的数据流程图 】

常用的降维算法包括最近邻插值法、双线性插值等，旨在保持原图特征的同时缩小尺寸。然而，这些算法并未充分考虑恶意构造的输入数据。上述示例即利用了双线性插值算法中存在的漏洞。

通过对比原始输入与降维后图片，我们可以更直观地理解这一过程（左列为原始输入，右列为经处理后的输入）。例如，“羊群”图片经过缩放变成了雪地里的白狼；而“小羊”则被误认为是可爱的小猫。

当然，这些输入图片都是经过特殊设计以触发异常结果的。

此外，我们还测试了其他深度学习应用，比如MNIST手写数字识别任务。实验表明，即使对于清晰可读的手写数字，也可以构造出导致系统误判的图片。

## 影响范围及防范措施
降维攻击可能影响所有采用维度调整机制的深度学习系统。主流平台如TensorFlow、Caffe和Torch均提供了相应的函数支持。根据初步分析，几乎所有在线可用的深度学习图像识别程序都面临此类威胁。

为了抵御降维攻击，建议采取以下措施：
- 过滤异常尺寸的图片；
- 对比降维前后图像差异；
- 使用更加鲁棒的降维算法。

## 结论
本文旨在揭示一种常被忽视的人工智能安全问题——降维攻击。这类新型攻击手段主要针对任意图像识别的深度学习应用。希望通过本文提醒公众，在享受人工智能带来的便利同时，也要持续关注相关系统的安全性问题。未来我们将继续深入探讨人工智能领域的安全挑战。

## 参考文献
【1】对深度学习的逃逸攻击 — 探究人工智能系统中的安全盲区  
【2】深度学习框架中的魔鬼 — 探究人工智能系统中的安全问题  
【3】Weilin Xu, Yanjun Qi, and David Evans, “Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers”, NDSS, 2016