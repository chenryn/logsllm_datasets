### Estimating Hrep(P*) for P*norep

To estimate \( H_{\text{rep}}(P^*) \) for the optimal processor count without replication (\( P^*_{\text{norep}} \)), we use our simulator. Figure 9 compares \( H_{\text{rep}}(P^*_{\text{norep}}) \) and \( H_{\text{norep}}(P^*_{\text{norep}}) \) for two cases of workload parallelism: perfectly parallel (α = 0) and α > 0. The figure shows that, with all other parameters being equal, replication always has a lower normalized expected completion time than no-replication.

For perfectly parallel jobs (α = 0), our theoretical analysis was conducted under the assumption that \( R = D = 0 \). While this assumption is validated by the figure, we also observe that the performance difference between replication and no-replication is even more pronounced when \( R \) and \( D \) are non-zero. This empirical analysis further supports our conclusion that replication outperforms no-replication at the optimal processor counts for no-replication.

Based on the theoretical and empirical results in this section, we can conclude that, for all practical values of platform parameters, replication offers better performance by the time the optimal number of processors for no-replication is reached. Thus, the global speedup, defined as \( \min(H_{\text{norep}}(P), H_{\text{rep}}(P)) \), is monotonic with respect to the number of processors until the optimal count for replication is reached. Furthermore, the global optimum is achieved at processor counts that are optimal for replication.

### Overhead of Replication

So far, we have assumed that the only cost of replication is the doubling of the failure-free execution time of the parallel part of a job, as replication duplicates work on half the system nodes. However, prior studies [4, 13-15] have noted that replication also introduces additional overheads, such as increased communication between replicas to maintain consistency, higher memory utilization, and network congestion.

In this section, we assess how these additional overheads affect the reliability-aware speedup of replication compared to no-replication, which does not suffer from such overheads. We use the model from [4], which analyzed the overhead of replication on several message-passing applications and found that the overhead grows proportionally to the logarithm of the number of processors, \( P \). Specifically, the extra overhead induced by replication, as a fraction of the original time, is given by \( \delta \log P \), where \( \delta \) is an application-specific constant.

To derive the speedup formula, assume the original work takes \( W \) units of time on a single processor without failures or fault tolerance. Without the additional overhead of replication, the same work using \( P \) total processors would take \( W \left( \alpha + \frac{2(1 - \alpha)}{P} \right) \) in the absence of failures and without C/R. With the additional overhead of replication, this time becomes \( W \left( \alpha + \frac{2(1 - \alpha)}{P} \right) (1 + \delta \log P) \). The failure-free speedup can then be expressed as:
\[ S_{\text{rep}}(P) = \frac{W}{W \left( \alpha + \frac{2(1 - \alpha)}{P} \right) (1 + \delta \log P)} = \frac{1}{\left( \alpha + \frac{2(1 - \alpha)}{P} \right) (1 + \delta \log P)} \]

We update \( H_{\text{rep}}(P) \) to use this expression for \( S_{\text{rep}}(P) \) to determine the reliability-aware speedup that factors in the cost of replication.

Figure 10 compares the normalized expected completion time of no-replication and replication for different values of \( \delta \). For no-replication, the curve is generated using the exact expression for \( H_{\text{norep}}(P) \). For replication, we evaluate \( H_{\text{rep}}(P) \) using our simulator, with \( S_{\text{rep}}(P) \) updated as described above. The figure shows that, for values of \( \delta \leq 10^{-2} \), replication still outperforms no-replication before no-replication reaches its optimal. The value of \( \delta \) determined in [4] for the application with the highest overhead of replication was of the order \( 10^{-3} \). This value can be further reduced by leveraging application-specific properties of most message-passing applications [16]. Therefore, the overhead of replication, as long as it is not unreasonably high, does not change the form of the global speedup function, which still behaves according to scenario (a) in Figure 8.

### Weibull Distribution

Throughout this paper, we have assumed that the failure distribution of individual processors follows an exponential distribution, which simplifies the theoretical development, especially the derivation of MTTI as carried out in [5]. However, several studies [17-20] on real-world HPC systems have noted that the closest fit to actual failure data is given by the Weibull distribution.

In this section, we use our simulator to assess whether our conclusions hold for Weibull distributions. The Weibull distribution, in addition to the parameter \( \lambda \), has a shape parameter \( m > 0 \). The Cumulative Distribution Function (CDF) of the Weibull distribution is defined as:
\[ P(X \leq t) = 1 - e^{-(\lambda t)^m} \]

In practice, the value for \( m \) is usually found to be between 0.5 and 0.7 [19-20]. With dual replication, we define the reliability \( R(t) \) as the probability that all \( P/2 \) pairs of processor-replicas survive until time \( t \). The probability of survival of a pair of replicas is obtained by subtracting from 1 the probability that both replica-processors fail within time \( t \). Under the Weibull failure distribution, this is given by:
\[ R(t) = \left( 2e^{-(\lambda t)^m} - e^{-2(\lambda t)^m} \right)^{P/2} \]

The mean of non-negative random variables is equal to the integral of the reliability function. Therefore, the MTTI is given by:
\[ \text{MTTI} = \int_0^\infty R(t) \, dt = \int_0^\infty \left( 2e^{-(\lambda t)^m} - e^{-2(\lambda t)^m} \right)^{P/2} \, dt \]

A closed-form solution for the above integral is known only for \( m = 1 \), which reduces to the exponential distribution. We numerically compute the integral to determine the checkpoint interval \( \sqrt{2 \times C \times \text{MTTI}} \). The simulator then generates interrupts based on the distribution defined by the above equation. We compare the resulting completion time with no-replication, for which the interrupts are generated according to the reliability function \( R(t) = e^{-(\lambda t)^m P} \).

The results, accounting for the overhead of replication, are shown in Figure 11. We observe that the optimal processor counts using Weibull failure distribution are generally lower than those with exponential failures. Comparing replication and no-replication, we see that, similar to Figure 10, the optimal number of processors for replication is much higher than for no-replication. Furthermore, the optimal speedup of replication is superior to the optimal achievable by no-replication. The crossover in performance happens at almost the same point as the optimal processor counts of no-replication. This confirms that, even with Weibull failure distributions, the overall speedup profile follows the form depicted in scenario (a) in Figure 8, assuming practical values for the overhead of replication. Therefore, the general conclusions made in this paper using exponential distributions, i.e., replication has much higher optimal processor counts than no-replication and that the optimal speedup of replication is superior to that possible without replication, hold for Weibull failure distributions as well.

### Related Work

The most closely related body of work to this paper is the study of reliability-aware speedups. In this domain, [2] and [3] formulated reliability-aware speedups for checkpoint-restart (C/R) and [3] numerically computed the optimal number of processors. [1] provided theoretical results on the optimal number of processors for non-perfectly parallel jobs. We follow their approach in formulating the optimal processor count problem and extend their results to cover the case of perfectly parallel jobs. However, these works consider C/R only without replication and do not address the impact of replication on reliability-aware speedups, which is the main focus of our paper.

Several studies in the HPC domain have explored the idea of combining replication with C/R. [4] suggested replication as a viable fault-tolerance scheme for exascale HPC systems, showing that, at sufficiently large scales, C/R alone will be less efficient than C/R with replication. [12] theoretically studied replication and derived a summation-based formula for the mean-time-to-interrupt (MTTI) of a replicated execution. [5] recently derived a closed-form expression for the MTTI in the case of dual replication. There have been several other works investigating both the implementation [13, 16] and theoretical [21] issues surrounding replication. These works do not consider the problem of finding the optimal number of processors using replication, with the exception of [21], which focuses primarily on silent errors and considers the failure of even one processor in a replica-pair as a failure for the entire job. Thus, their results are not applicable to the fail-stop model, which is what we study in this paper. Additionally, none of the above works on replication assess how replication can impact the overall form of reliability-aware speedups, which we do in this paper by showing that replication outperforms the optimal achievable by no-replication and that the crossover happens before or close to the optimal system scale for no-replication.

### Conclusion

In this paper, we studied the reliability-aware speedup of a replicated execution and contrasted it with the reliability-aware speedup without replication. We derived novel results on how the optimal processor counts of replication and no-replication relate to the individual node failure rate \( \lambda \). We further showed that replication generally starts outperforming no-replication before or close to the point where no-replication reaches its optimal processor counts. Collectively, the results in this paper indicate that replication significantly enhances reliability-aware speedup beyond what is possible without replication.

There are several directions for future work. One direction would be to analyze higher degrees of replication, such as triple redundancy, to see if they can further improve the optimal performance achievable by dual replication. Another interesting direction would be to explore platforms where node failure rates are not identical, requiring further generalization of the results in this paper.

### Acknowledgment

We are thankful to the reviewers and our shepherd, Devesh Tiwari, for helping us improve the quality of this paper. This research is based in part upon work supported by the Department of Energy under contract DE-SC0014376. This research was supported in part by the University of Pittsburgh Center for Research Computing through the resources provided. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by the National Science Foundation grant number OCI-1053575. Specifically, it used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).

### References

[1] A. Cavelan, J. Li, Y. Robert, and H. Sun, “When Amdahl meets Young/Daly,” in 2016 IEEE International Conference on Cluster Computing (CLUSTER). IEEE, 2016, pp. 203–212.

[2] H. Jin, Y. Chen, H. Zhu, and X.-H. Sun, “Optimizing HPC fault-tolerant environment: An analytical approach,” in 2010 39th International Conference on Parallel Processing. IEEE, 2010, pp. 525–534.

[3] Z. Zheng, L. Yu, and Z. Lan, “Reliability-aware speedup models for parallel applications with coordinated checkpointing/restart,” IEEE Transactions on Computers, vol. 64, no. 5, pp. 1402–1415, 2015.

[4] K. Ferreira, J. Stearley, J. H. Laros III, R. Oldfield, K. Pedretti, R. Brightwell, R. Riesen, P. G. Bridges, and D. Arnold, “Evaluating the viability of process replication reliability for exascale systems,” in Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis. ACM, 2011, p. 44.

[5] A. Benoit, T. Hérault, V. L. Fèvre, and Y. Robert, “Replication is more efficient than you think,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. ACM, 2019, p. 89.

[6] J. W. Young, “A first order approximation to the optimum checkpoint interval,” Communications of the ACM, vol. 17, no. 9, pp. 530–531, 1974.

[7] G. M. Amdahl, “Validity of the single processor approach to achieving large scale computing capabilities,” in Proceedings of the April 18-20, 1967, spring joint computer conference. ACM, 1967, pp. 483–485.

[8] J. T. Daly, “A higher order estimate of the optimum checkpoint interval for restart dumps,” Future Generation Computer Systems, vol. 22, no. 3, pp. 303–312, 2006.

[9] Y. Liu, R. Nassar, C. Leangsuksun, N. Naksinehaboon, M. Paun, and S. L. Scott, “An optimal checkpoint/restart model for a large-scale high-performance computing system,” in 2008 IEEE International Symposium on Parallel and Distributed Processing. IEEE, 2008, pp. 1–9.

[10] O. Subasi, G. Kestor, and S. Krishnamoorthy, “Toward a general theory of optimal checkpoint placement,” in 2017 IEEE International Conference on Cluster Computing (CLUSTER). IEEE, 2017, pp. 464–474.

[11] A. Agrawal, G. H. Loh, and J. Tuck, “Leveraging near data processing for high-performance checkpoint/restart,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. ACM, 2017, p. 60.

[12] H. Casanova, Y. Robert, F. Vivien, and D. Zaidouni, “Combining process replication and checkpointing for resilience on exascale systems,” Ph.D. dissertation, INRIA, 2012.

[13] C. Engelmann and S. Böhm, “Redundant execution of HPC applications with MR-MPI.”

[14] J. Elliott, K. Kharbas, D. Fiala, F. Mueller, K. Ferreira, and C. Engelmann, “Combining partial redundancy and checkpointing for HPC,” in 2012 IEEE 32nd International Conference on Distributed Computing Systems. IEEE, 2012, pp. 615–626.

[15] Z. Hussain, T. Znati, and R. Melhem, “Partial redundancy in HPC systems with non-uniform node reliabilities,” in SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2018, pp. 566–576.

[16] A. Lefray, T. Ropars, and A. Schiper, “Replication for send-deterministic MPI HPC applications,” in Proceedings of the 3rd Workshop on Fault-tolerance for HPC at extreme scale. ACM, 2013, pp. 33–40.

[17] N. Raju, Y. L. Gottumukkala, C. B. Leangsuksun, R. Nassar, and S. Scott, “Reliability analysis in HPC clusters,” in Proceedings of the High Availability and Performance Computing Workshop, 2006, pp. 673–684.

[18] B. Schroeder and G. Gibson, “A large-scale study of failures in high-performance computing systems,” IEEE transactions on Dependable and Secure Computing, vol. 7, no. 4, pp. 337–350, 2009.

[19] T. Hérault and Y. Robert, Fault-tolerance techniques for high-performance computing. Springer, 2015.

[20] D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai, D. Oliveira, D. Londo, N. DeBardeleben, P. Navaux et al., “Understanding GPU errors on large-scale HPC systems and the implications for system design and operation,” in 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2015, pp. 331–342.

[21] A. Benoit, A. Cavelan, F. Cappello, P. Raghavan, Y. Robert, and H. Sun, “Coping with silent and fail-stop errors at scale by combining replication and checkpointing,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. ACM, 2019, p. 89.