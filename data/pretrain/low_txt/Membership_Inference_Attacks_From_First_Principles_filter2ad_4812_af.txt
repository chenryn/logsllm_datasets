### Optimized Text

All of our experiments to date have focused on attacking models that we have trained ourselves. To ensure that these models were not inadvertently weak in terms of privacy (or non-private), we now demonstrate the effectiveness of our attacks on existing, pre-trained state-of-the-art models. For this purpose, we use standard models pre-trained by Phan [51] on the complete CIFAR-10 training set, which consists of 50,000 examples.

We train 256 shadow models using the same training code and randomly subsampling 50,000 points from the entire CIFAR-10 dataset (which contains 60,000 examples). On average, we have 213 IN models and 43 OUT models per example. Figure 12 illustrates the true-positive rate (TPR) of our attack at a false-positive rate (FPR) of 0.1% for various canonical model architectures. We consider two attack variants:
1. The adversary knows the target model's architecture and uses it to train the shadow models.
2. The shadow models use a different architecture than the target model.

Given that we only have 43 models to estimate the distribution \(\tilde{Q}_{\text{out}}\), estimating a global variance for all examples yields the best results. The outcomes of this experiment are qualitatively similar to those in Section VI-E:
1. The model architecture has a minor impact on privacy leakage (e.g., the attack performs better against a ResNet-18 than a MobileNet-v2).
2. The attack is most effective when the shadow models share the same architecture as the target model, but it remains robust even with architecture mismatches. For instance, attacking a ResNet-34 model with either ResNet-18 or ResNet-50 shadow models results in a slight decrease in the attack success rate (from 5% TPR to 4% TPR).

### Conclusion

Throughout this paper, we have argued that membership inference attacks should aim to achieve high true-positive rates at low false-positive rates. Our attack provides one method to achieve this goal. There are several evaluation directions that future work could explore under this framework.

#### Membership Inference Attacks as a Privacy Metric
Both researchers [42] and practitioners [63] use membership inference attacks to measure the privacy of trained models. We argue that these metrics should employ strong attacks (like ours) to accurately measure privacy leakage. Future work using membership inference attacks should focus on the low false-positive rate regime to better understand if the privacy of even a few users can be confidently breached.

#### Usability Improvements to Membership Inference Attacks
The key limitation of per-example membership inference attacks is the need for new hyperparameters that must be learned from the data. While it is more important that attacks are strong (even if slow) rather than fast (but weak), we hope that future work will improve the computational efficiency of our attack approach to allow it to be deployed in more settings.

#### Improving Other Privacy Attacks with Our Method
Membership inference attacks form the basis for many other privacy attack methods [3, 4, 17]. In principle, our membership inference method should be able to directly enhance these attacks.

#### Rethinking Our Current Understanding of MIA Results
The literature on membership inference attacks has addressed several memorization questions. However, many prior studies focused on the inadequate metric of average-case attack success rates rather than the low false-positive rate regime. As a result, it is necessary to re-evaluate prior results from this perspective:
- Do previously "broken" [26, 45] defenses prevent our attack? Prior defenses were only shown to be ineffective at preventing an adversary from succeeding on average, not confidently at low false-positive rates.
- How does differential privacy interact with our improved attacks? We have preliminary evidence that vacuous guarantees might prevent our low-FPR attacks (Section A-A).
- Are attacks with reduced capabilities possible? For example, label-only attacks [6, 34, 54] can match the balanced accuracy of shadow-model approaches. But do these attacks work at low false-positive rates?
- Are attacks with extra capabilities more effective? Prior work has shown that access to gradient queries [46] or intermediate models [58] improves attack AUC. However, does this observation hold at low false-positive rates?

We hope that future work will address these questions, among others, to better evaluate and develop techniques that preserve the privacy of training data. By developing attacks that succeed at low false-positive rates, we can assess privacy not just as a measurement of the average user but of the most vulnerable.

### Why Some Examples Are Less Private

While our average attack success rate is modest, the success rate at low false-positive rates can be very high. This suggests that there is a subset of examples that are easier to attack than others. One important factor behind why some samples are less private is that they are out-of-distribution.

To support this argument, we intentionally inject out-of-distribution examples into a model’s training dataset and compare the difficulty of attacking these newly inserted samples versus typical examples. Specifically, we insert 1,000 examples from various out-of-distribution sources into the 50,000-example CIFAR-10 training dataset to form a new augmented 51,000-example dataset. We then train shadow models on this dataset, run our attack, and measure the distinguishability of distributions of losses for IN and OUT models for each of the 1,000 newly inserted examples (using a simple measure of distance between distributions defined as \(d = \frac{|\mu_{\text{in}} - \mu_{\text{out}}|}{\sigma_{\text{in}} + \sigma_{\text{out}}}\)).

Figure 13 shows the distribution of these "privacy scores" assigned to each example. As a baseline, in blue, we show the distribution of privacy scores for the standard CIFAR-10 dataset, which are tightly concentrated around 0. Next, we show the privacy scores of examples inserted from the CINIC-10 dataset, which are drawn from ImageNet. Due to this slight distribution shift, the CINIC-10 images have a larger privacy score on average, making it easier to detect their presence in the dataset because they are slightly out-of-distribution.

We can extend this further by inserting intentionally mislabeled images that are extremely out-of-distribution. If we choose 1,000 images (shown in red) from the CIFAR-10 test set and assign new random labels to each image, we get a much higher privacy score for these images. Finally, we interpolate between the extreme OOD setting of random (and thus incorrectly) labeled CIFAR-10 images and correctly-labeled CINIC-10 by inserting randomly labeled images from CIFAR-100 (shown in green). Because these images come from a disjoint class distribution, models will not typically be confident on their label unless they are seen during training. The privacy scores here fall between correctly labeled CINIC-10 and incorrectly labeled CIFAR-10.

### Acknowledgements

We are grateful to Thomas Steinke, Dave Evans, Reza Shokri, Sanghyun Hong, Alex Sablayrolles, Liwei Song, and the anonymous reviewers for their comments on drafts of this paper.

### References

[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMaham, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, page 308–318. ACM, 2016.

[2] Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 123–132, 2021.

[3] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19), pages 267–284, 2019.

[4] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), 2021.

[5] Mia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan Cao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan Wang, Andrew M Dai, Zhifeng Chen, et al. Gmail smart compose: Real-time assisted writing. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2287–2295, 2019.

[6] Christopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only membership inference attacks. In International Conference on Machine Learning, pages 1964–1974. PMLR, 2021.

[7] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data, 2018.

[8] Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. CINIC-10 is not Imagenet or CIFAR-10. arXiv preprint arXiv:1810.03505, 2018.

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In IEEE conference on computer vision and pattern recognition, pages 248–255. IEEE, 2009.

[10] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211–407, 2014.

[11] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust traceability from trace amounts. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 650–669. IEEE, 2015.

[12] Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! a survey of attacks on private data. Annual Review of Statistics and Its Application, 4:61–84, 2017.

[13] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639):115–118, 2017.

[14] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 954–959, 2020.

[15] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. arXiv preprint arXiv:2008.03703, 2020.

[16] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1322–1333, 2015.

[17] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks on fully connected neural networks using permutation invariant representations. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security, pages 619–633, 2018.

[18] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. LOGAN: Membership inference attacks against generative models. In Proceedings on Privacy Enhancing Technologies (PoPETs), pages 133–152. De Gruyter, 2019.

[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.

[20] Xinlei He, Jinyuan Jia, Michael Backes, Neil Zhenqiang Gong, and Yang Zhang. Stealing links from graph neural networks. In 30th USENIX Security Symposium (USENIX Security 21), 2021.

[21] Grant Ho, Aashish Sharma, Mobin Javed, Vern Paxson, and David Wagner. Detecting credential spearphishing in enterprise settings. In 26th USENIX Security Symposium (USENIX Security 17), pages 469–485, 2017.

[22] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W Craig. Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays. PLoS genetics, 4(8), 2008.

[23] Robert A Jacobs. Increased rates of convergence through learning rate adaptation. Neural networks, 1(4):295–307, 1988.

[24] Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? arXiv preprint arXiv:2006.07709, 2020.

[25] Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Revisiting membership inference under realistic assumptions. In Proceedings on Privacy Enhancing Technologies (PoPETs), 2021.

[26] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong. Memguard: Defending against black-box membership inference attacks via adversarial examples. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 259–274, 2019.

[27] Alex Kantchelian, Michael Carl Tschantz, Sadia Afroz, Brad Miller, Vaishaal Shankar, Rekha Bachwani, Anthony D Joseph, and J Doug Tygar. Better malware ground truth: Techniques for weighting anti-virus vendor labels. In Proceedings of the 8th ACM Workshop on Artificial Intelligence and Security, pages 45–56, 2015.

[28] Zico Kolter and Marcus A Maloof. Learning to detect and classify malicious executables in the wild. Journal of Machine Learning Research, 7(12), 2006.

[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.

[30] Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pages 950–957, 1992.

[31] Aleksandar Lazarevic, Levent Ertoz, Vipin Kumar, Aysel Ozgur, and Jaideep Srivastava. A comparative study of anomaly detection schemes in network intrusion detection. In Proceedings of the 2003 SIAM international conference on data mining, pages 25–36. SIAM, 2003.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[33] Klas Leino and Matt Fredrikson. Stolen memories: Leveraging model memorization for calibrated white-box membership inference. arXiv preprint arXiv:1906.11798, 2019.

[34] Zheng Li and Yang Zhang. Membership leakage in label-only exposures. arXiv preprint arXiv:2007.15528, 2020.

[35] Yugeng Liu, Rui Wen, Xinlei He, Ahmed Salem, Zhikun Zhang,