### 3.4 Maintenance & Limitations

Thanks to the small size and minimal invasiveness of VV8’s patches, maintenance has been relatively inexpensive. Development began on Chrome 63 and was easily transitioned to Chrome 64, which was used for primary data collection. We have since ported our patches through Chrome 72, encountering only minor issues in the process (e.g., whitespace changes disrupting patch merges, capitalization changes in internal API names).

Our trace logs must be created dynamically as new threads are encountered. Since the Chrome sandbox prevents file creation, we currently run VV8 with the sandbox disabled for convenience. In production, we run VV8 inside isolated Linux containers, which mitigates the loss of the sandbox to some extent. Future development will include sandbox integration if necessary.

Past research [39, 40] on fingerprinting JavaScript engines suggests that sophisticated adversaries could use relative scores across micro-benchmarks as a side-channel to identify VV8. However, such benchmarks and evasions would be detectable in VV8’s trace logs, and JavaScript timing side-channel attacks can be disrupted [17, 47]. It is unlikely that an adversary sophisticated enough to fingerprint VV8 in the wild would not also be able to fingerprint in-band instrumentation, which also shows measurable deviation from baseline performance.

Furthermore, we expect to improve VV8 performance in future iterations by exploring asynchronous log flushing, placing log-filtering tests in the injected bytecode (where they can be JIT-optimized), and using more efficient forms of context tracking.

### 3.5 Collection System

To collect data at large scale using VV8, we built an automated crawling and post-processing system, as shown in Figure 4. Worker nodes (for collection, post-processing, and work queues) are deployed across a Kubernetes cluster backed by 80 physical CPU cores and 512GiB of RAM distributed across 4 physical servers. Initial jobs (i.e., URLs to visit) are placed in a Redis-based work queue to be consumed by collection worker nodes. Post-processing jobs (i.e., archived logs to parse and aggregate) are placed in another work queue to be consumed by post-processing worker nodes. Collection metadata and trace logs are archived to a MongoDB document store. Aggregate feature usage data is stored in a PostgreSQL RDBMS for analytic queries.

The collection worker node Docker image contains the VV8 binary itself and two accompanying programs written in Python 3: Carburetor and Manifold. Carburetor is responsible for fueling VV8, while Manifold handles the byproducts of execution, compressing and archiving the trace log files emitted during automated browsing.

The post-processor worker node Docker image contains a work queue dispatcher and the main post-processor engine. The dispatcher interfaces with our existing work queue infrastructure and is written in Python 3. The post-processor engine is written in Go, which provides ease-of-use comparable to Python but with significantly higher performance.

### 4 Data Collection

#### 4.1 Methodology

**Overview.** We collected native feature usage traces and related data by automating VV8 via the Chrome DevTools interface to visit the Alexa top 50k web domains. Each visit to a domain used the simple URL template `http://DOMAIN/`. We visited each domain in our target list 5 times; each planned visit constituted a job. We recorded headers and bodies of all HTTP requests and responses along with the VV8 trace logs. Trace log files were compressed and archived immediately during jobs, then queued for post-processing. Post-processing associated logs with the originating job/domain and produced our analytic data set.

**User Input Simulation.** Simply visiting a page may result in significant JavaScript activity, but this activity may not be representative. To address this, we borrowed a solution from Snyder et al. [49]: random “monkey testing” of the UI using the open-source gremlins.js library [5]. To preserve some degree of reproducibility, we used a deterministic, per-job seed for gremlins.js’s random number generator.

Once a page’s DOM was interaction-ready, we unleashed our gremlins.js interaction for 30 seconds. We blocked all main-frame navigations if they led to different domains (e.g., from `example.com` to `bogus.com`). When allowing intra-domain navigation (e.g., from `example.com` to `www.example.com`), we stopped counting time until the new destination was loaded and resumed the monkey testing. We immediately closed any dialog boxes (e.g., `alert()`) opened during the monkey testing to keep JavaScript execution from blocking. This 30-second mock-interaction procedure was performed 5 times, independently, per visited domain. (Snyder et al. [49] arrived at these parameters experimentally.)

#### 4.2 Data Post-Processing

We parsed the trace logs to reconstruct the execution context of each recorded event and to aggregate results by that context. The resulting output included all the distinct scripts encountered and aggregate feature usage tuples.

**Script Harvesting.** VV8 records the full JavaScript source of every script it encounters in its trace log (exactly once per log). We extracted and archived all such scripts, identifying them by script hash and lexical hash. Script hashes are simply the SHA256 hash of the full script source (encoded as UTF-8); they served as the script’s unique ID. Lexical hashes were computed by tokenizing the script and SHA256-hashing the sequence of JavaScript token type names that result. These are useful because many sites generate “unique” JavaScript scripts that differ only by timestamps embedded in comments or unique identifiers in string literals. Such variants produced identical lexical hashes, allowing us to group variant families.

**Feature Usage Tuples.** We recorded a feature usage tuple for each distinct combination of log file, visit domain, security origin, active script, access site, access mode, and feature name. The log file component let us distinguish collection runs. The visit domain is the Alexa domain originally queued for processing. The security origin was the value of `window.origin` in the active execution context, which may be completely different from the visit domain in the context of an `<iframe>`. The active script is identified by script hash. The access site is the character offset within the script that triggered this usage event. The access mode is how the feature was used (get, set, or call). The feature name is a name synthesized from the name of the receiver object’s constructor function (effectively its type name) and the name of the accessed member of that object (i.e., the property or method name).

#### 4.3 Results

**Success and Failure Rates.** Our methodology called for 5 visits to the Alexa 50k, so the whole experiment consisted of 250,000 distinct jobs. Successful jobs visited the constructed URL, launched gremlins.js, and recorded at least 30 seconds of pseudo-interaction time. Jobs resulting in immediate redirects (by HTTP or JavaScript) to a different domain before any interaction began were deemed “dead ends.” From job status, we extract the per-domain coverage listed in Table 2. For “active” domains, all 5 jobs succeeded, and we observed native JavaScript API usage. For “silent” domains, all 5 jobs succeeded, but we observed no native JavaScript API usage. For “facade” domains, all 5 jobs were “dead ends” (i.e., the domain is an alias). All of the above are considered “successful” domains. Some domains were “broken,” with all 5 jobs failing; a tiny number were “inconsistent,” with a mix of failed/succeeded jobs. This failure rate is consistent with prior results crawling web sites. Snyder et al. [49] reported a lower per-domain failure rate (2.7%), but this was over the Alexa top 10,000 only. On the other extreme, a recent measurement study by Merzdovnik et al. [36] reported successful visits to only about 100k out of the top Alexa 200k web domains.

**Aggregate Feature Usage.** Over the entire Alexa 50k, we observed 53% of Chrome-IDL-defined standard JavaScript API features used at least once. Note that our observations comprise a lower bound on usage, since we did not crawl applications requiring authentication (e.g., Google Documents), which we intuitively anticipate may use a wider range of APIs than generic, public-facing content. Most modern sites use JavaScript heavily, but no site uses all available features. The plot in Figure 5 climbs steeply before leveling out into a gentle upward slope. The small but distinctive “cliffs” observed at `rocket-league.com` (Alexa 16,495) and `noa.al` (Alexa 22,184) are caused by large clumps of SVG-related features being used for the first time.

### 5 Bot Detection Artifacts

Modern websites adapt their behavior based on the capabilities of the browser that is visiting them. The identification of a specific browser implementation is called user-agent fingerprinting and is often used for compatibility purposes. To provide a case study of VV8’s unique abilities, we use it to automatically discover artifacts employed by a form of user-agent fingerprinting used by some websites in the wild to detect automated browser platforms.

The technique we study exploits the presence of distinctive, non-standard features on API objects like `Window` (which doubles as the JavaScript global object) and `Navigator` as provided by automated browsers and browser simulacra. (Since even modern search engine indexers need some degree of JavaScript support [1], we do not consider mechanisms used to identify “dumb,” non-JavaScript-executing crawlers like wget.) Here, VV8’s ability to trace native property accesses without a priori knowledge of the properties to instrument sets it apart from in-band instrumentation, which cannot wrap a proxy around the global object or the unforgeable `window.document` property. Note that “native API property access” here means a property access on an object that crosses the JavaScript/native API boundary, regardless of whether or not that specific property is standardized or even implemented.

Bot detection is a special case of user-agent fingerprinting, where “bots” are automated web clients not under the direct control of a human user (e.g., headless browsers used as JavaScript-supporting web crawlers). Bots may be a nuisance or even a threat to websites [25], and they may cause financial loss to advertisers via accidental (or intentional) impression and/or click fraud. If the visitor’s user-agent fingerprint matches a known bot, a site can choose to “defend” itself against undesired bot access by taking evasive action (e.g., redirecting to an error page) [24]. Non-standard features distinctive to known bot platforms constitute bot artifacts. We exploit VV8’s comprehensive API-property-access tracing to systematically discover novel artifacts.

#### 5.1 Artifact Discovery Methodology

We discover previously unknown bot artifacts by clustering the access sites (i.e., script offsets of feature accesses) for candidate features near those of known “seed” artifacts. The key insight underlying our approach is code locality: in our experience, artifact tests tend to be clustered near each other in user-agent fingerprinting code encountered across the web. We exploit this locality effect to automate the process of eliminating noise and identifying a small set of candidates for manual analysis.

**Candidate Feature Pool.** Before searching for artifacts, we prune our search space to eliminate impossible candidates. We eliminate features defined in the Chrome IDL files, as these are standard-derived features unlikely to be distinctive to a bot platform. We also eliminate features seen set or called, as these are likely distinctive to JavaScript libraries, not the browser environment itself. This second round of pruning is especially important because JavaScript notoriously conflates its global namespace with its “global object.” Thus, in web browsers, global JavaScript variables are accessible as properties of the `window` object along with all the official members of the `Window` interface. Retaining only features we never see set or called eliminates significant noise (e.g., references to the `Window.jQuery` feature) from our pool of candidate features: from 7,928,522 distinct names to 1,907,499.

**Seed Artifact Selection.** We further narrow our candidate pool using access site locality clustering around “seed” artifacts (Table 3). These features are among the most commonly listed in anecdotal bot detection checklists found in developer hubs like Stack Exchange [2], reflecting the popularity of Selenium’s browser automation suite and the lighter-weight PhantomJS headless browser.

**Candidate Artifact Discovery.** With a pruned candidate feature pool and a set of seed artifacts in hand, we can automatically discover new artifacts.