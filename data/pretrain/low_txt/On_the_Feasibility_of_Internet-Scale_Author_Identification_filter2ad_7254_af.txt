### Features and Performance

The features utilized in conjunction with those from [23] enhance performance by 2%.

### Implementation and Performance

We implemented our classifiers using a variety of programming languages and libraries, including MATLAB and Python/Numpy. MATLAB proved to be the fastest due to its optimized linear algebra routines. Among the classifiers we tested, the nearest-neighbor (NN) classifier is the simplest and therefore the fastest to train and test. Computing the class centroids (i.e., collapsing each class) takes a negligible amount of time. Classifying 150,000 points from 50,000 distinct classes takes approximately 4.5 minutes. Regularized Least Squares Classifier (RLSC), on the other hand, requires substantially more processing time for training, taking about 12 minutes to train on all the data, minus the three points removed for testing. After training, RLSC behaves similarly to NN in making predictions.

### Optimization Techniques

We present several optimization techniques that lead to a significant speedup in running time. The most significant factor during the prediction phase is expressing computations as matrix multiplications. For example, RLSC predictions are obtained by computing the inner product of all one-versus-all classifiers with each testing sample. The Euclidean distance used in nearest-neighbors can also be expanded into a series of inner products. Once our computations are represented as matrix multiplications, we can compute everything in large batches, fully utilizing the computer’s hardware. This optimization alone reduced prediction time from 10 hours to 4.5 minutes. Interestingly, we found that avoiding sparse format representations of our data was several times faster because they are not as optimized. Additionally, we employed a form of dynamic programming for RLSC, which dramatically decreased the number of computations, reducing training time from two days to 12 minutes.

### Limitations, Future Work, and Conclusions

Our work has several important limitations. First, the attack is unlikely to succeed if the victim intentionally obfuscates their writing style. Second, while we have validated our attack in a cross-context setting (e.g., two different blogs), we have not tested it in a cross-domain setting (e.g., labeled text from a blog and anonymous text from an email). These limitations are discussed in detail in Section III. Finally, our method might not meet the requirements of forensic applications where the number of authors is very small (the classification task might even be binary) but the amount of text is limited.

We outline three main avenues for future research to refine our techniques:
1. **Studying More Classifiers:** Investigate additional classifiers such as regularized discriminant analysis and fully understand their limitations and advantages.
2. **Extending Binary Classifiers to Multi-Class Settings:** Explore more principled and robust approaches for extending binary classifiers to highly multi-class settings. One promising approach is the "all pairs" method, where every class is compared against every other class. While this method yields good results, it is computationally prohibitive. A "tournament" between classes using a binary classifier is another promising approach, and we have some early results in this area.
3. **Cross-Context Investigation:** Conduct a more thorough investigation of different classifiers and analyze the impact of training and test set sizes. Understanding and modeling how writing style is affected by context has the potential to significantly improve accuracy.

In conclusion, we have conducted the first investigation into the possibility that stylometry techniques might pose a widespread privacy risk by identifying authors of anonymously published content based on their writing style. Previous work has applied similar techniques to distinguish among up to 300 authors; we consider the scenario of 100,000. Our findings indicate that these techniques remain surprisingly effective: in about 20% of trials in each experiment, the author of the sample is ranked first among all 100,000. Authors with large amounts of text already online are even more vulnerable. Even in cases where the author of a text is not the one estimated to be most likely, some risk of identification remains. Specifically, in the median case, the likely authors of a sample can be narrowed down to a set of 100–200, a reduction by a factor of 500–1000. While this alone is unlikely to produce a match with certainty, if combined with another source of information, it may be enough to identify the author.

Importantly, our findings represent a lower bound on the severity of this type of risk, and we can expect the development of more sophisticated techniques to worsen the situation. Future work to address the privacy threat should include further characterizations of the most vulnerable authors and improved writing-style obfuscation techniques.

### Acknowledgements

We would like to thank the authors of [28] for sharing the notes of their reimplementation of the Writeprints algorithm from [23] and the authors of [46] for sharing their Google profiles dataset. We are also grateful to Roberto Perdisci, Mario Frank, Ling Huang, and anonymous reviewers for their helpful comments. This material is supported by the National Science Foundation under Grants No. CCF-0424422, 0842695, by the MURI program under Air Force Office of Scientific Research Grant No. FA9550-08-1-0352, the National Science Foundation Graduate Research Fellowship under Grant No. DGE-0946797, the Department of Defense through the National Defense Science and Engineering Graduate Fellowship Program, by the Intel Science and Technology Center for Secure Computing, and by a grant from the Amazon Web Services in Education program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.

### References

[1] A. Furtwangler, *The Authority of Publius: A Reading of the Federalist Papers*. Cornell University Press, 1984.
[2] Supreme Court of the United States, “McIntyre vs. Ohio Elections Commission, 514 U.S. 334,” 1995.
[3] M. L. Sifry, *WikiLeaks and the age of transparency*. Counterpoint, 2011.
[4] Mike Giglio, “The Facebook Freedom Fighter,” Newsweek, Feb. 2011.
[5] J. Bone, “Vogue model Liskula Cohen wins right to unmask offensive blogger,” The Times, Aug. 2009.
[6] J. Brumley, “Unmasked blogger blames First Baptist, sheriff’s office,” The Florida Times-Union, Apr. 2009.
[7] M. E. Larios, “ePublius: Anonymous Speech Rights Online,” Rutgers Law Record, Vol. 37, p. 36, 2010.
[8] G. H. Pike, “The Right to Remain Anonymous,” Information Today, Vol. 25, No. 4, p. 17, April 2008.
[9] J. Mayer, “Any person... a pamphleteer: Internet Anonymity in the Age of Web 2.0,” Undergraduate Senior Thesis, Princeton University, 2009.
[10] R. Dingledine, N. Mathewson, and P. Syverson, “Tor: The second-generation onion router,” in USENIX Security Symposium, 2004.
[11] M. Koppel, J. Schler, and S. Argamon, “Authorship attribution in the wild,” Language Resources and Evaluation, vol. 45, no. 1, pp. 83–94, 2011.
[12] K. Luyckx and W. Daelemans, “The effect of author set size and data size in authorship attribution,” Literary and Linguistic Computing, vol. 26, no. 1, pp. 35–55, 2011.
[13] H. Paskov, “A regularization framework for active learning from imbalanced data,” Master’s thesis, MIT, 2008.
[14] C. Rubin, “Surprised Employer Fires Sex Blogger,” Inc. Magazine, May 2010.
[15] M. Schwartz, “The Troubles of Korea’s Influential Economic Pundit,” WIRED, Oct. 2009.
[16] M. Brennan and R. Greenstadt, “Practical attacks against authorship recognition techniques,” in IAAI, 2009.
[17] G. Kacmarcik and M. Gamon, “Obfuscating document stylometry to preserve author anonymity,” in The Association for Computer Linguistics, 2006.
[18] T. C. Mendenhall, “The characteristic curves of composition,” Science, vol. ns-9, no. 214S, pp. 237–246, 1887.
[19] ——, “A mechanical solution of a literary problem,” Popular Science Monthly, vol. 60, no. 2, pp. 97–105, 1901.
[20] F. Mosteller and D. L. Wallace, *Inference and Disputed Authorship: The Federalist*. Addison-Wesley, 1964.
[21] E. Stamatatos, “A survey of modern authorship attribution methods,” J. Am. Soc. Inf. Sci. Technol., vol. 60, pp. 538–556, March 2009.
[22] D. Madigan, A. Genkin, D. D. Lewis, S. Argamon, D. Fradkin, and L. Ye, “Author identification on the large scale,” in Joint Meeting of the Interface and Classification Society of North America, 2005.
[23] A. Abbasi and H. Chen, “Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace,” ACM Transactions on Information Systems, vol. 26, no. 2, Mar. 2008.
[24] O. Y. de Vel, A. Anderson, M. Corney, and G. M. Mohay, “Mining email content for author identification forensics,” SIGMOD Record, vol. 30, no. 4, pp. 55–64, 2001.
[25] C. E. Chaski, “Who’s at the keyboard? authorship attribution in digital evidence investigations,” IJDE, vol. 4, no. 1, 2005.
[26] ——, “The Keyboard Dilemma and Authorship Identification,” in IFIP Int. Conf. Digital Forensics, 2007, pp. 133–146.
[27] D. I. Holmes, “A stylometric analysis of mormon scripture and related texts,” Journal of the Royal Statistical Society Series A Statistics in Society, vol. 155, no. 1, pp. 91–120, 1992.
[28] S. Afroz, M. Brennan, and R. Greenstadt, “Detecting Hoaxes, Frauds, and Deception in Writing Style Online,” in IEEE Symposium on Security and Privacy, 2012.
[29] E. Stamatatos, N. Fakotakis, and G. Kokkinakis, “Text genre detection using common word frequencies,” in Proceedings of the 18th conference on Computational linguistics - Volume 2, ser. COLING ’00. Stroudsburg, PA, USA: Association for Computational Linguistics, 2000, pp. 808–814.
[30] M. Koppel, J. Schler, and S. Argamon, “Computational methods in authorship attribution,” JASIST, vol. 60, no. 1, pp. 9–26, 2009.
[31] E. Backer and P. van Kranenburg, “On musical stylometry–a pattern recognition approach,” pp. 299 – 309, 2005, in Memoriam: Azriel Rosenfeld.
[32] M. Shevertalov, J. Kothari, E. Stehle, and S. Mancoridis, “On the use of discretized source code metrics for author identification,” in SSBSE, 2009.
[33] W.-J. Li, K. Wang, S. J. Stolfo, and B. Herzog, “Fileprints: identifying file types by n-gram analysis,” in IEEE Systems, Man, and Cybernetics Information Assurance Workshop, 2005.
[34] L. Zhuang, F. Zhou, and J. D. Tygar, “Keyboard acoustic emanations revisited,” in ACM conference on Computer and communications security, 2005.
[35] P. Juola, “Authorship attribution,” Foundations and Trends in Information Retrieval, vol. 1, pp. 233–334, December 2006.
[36] C. E. Chaski, “Empirical evaluations of language-based author identification techniques,” Forensic Linguistics, vol. 8, no. 1, pp. 1–65, 2001.
[37] H. Maurer, F. Kappe, and B. Zaka, “Plagiarism - a survey,” Journal of Universal Computer Science, vol. 12, no. 8, pp. 1050–1084, 2006.
[38] J. R. Rao and P. Rohatgi, “Can pseudonymity really guarantee privacy?” in Proceedings of the 9th conference on USENIX Security Symposium - Volume 9. Berkeley, CA, USA: USENIX Association, 2000, pp. 7–7.
[39] M. Koppel, J. Schler, S. Argamon, and E. Messeri, “Authorship attribution with thousands of candidate authors,” in SIGIR, 2006, pp. 659–660.
[40] M. Nanavati, N. Taylor, W. Aiello, and A. Warfield, “Herbert west: deanonymizer,” in Proceedings of the 6th USENIX conference on Hot topics in security, ser. HotSec’11. Berkeley, CA, USA: USENIX Association, 2011, pp. 6–6.
[41] S. Hill and F. J. Provost, “The myth of the double-blind review? Author identification using only citations,” SIGKDD Explorations, vol. 5, no. 2, pp. 179–184, 2003.
[42] J. K. Bradley, P. G. Kelley, and A. Roth, “Author identification from citations,” Tech. Rep., Dec. 03 2008.
[43] P. Eckersley, “How unique is your web browser?” Electronic Frontier Foundation, Tech. Rep., 2009. [Online]. Available: https://panopticlick.eff.org/browser-uniqueness.pdf
[44] G. Wondracek, T. Holz, E. Kirda, and C. Kruegel, “A practical attack to de-anonymize social network users,” in IEEE Symposium on Security and Privacy, 2010.
[45] D. Irani, S. Webb, K. Li, and C. Pu, “Large online social footprints–an emerging threat,” in International Conference on Computational Science and Engineering, 2009.
[46] D. Perito, C. Castelluccia, M. A. Kˆaafar, and P. Manils, “How unique and traceable are usernames?” in PETS, 2011, pp. 1–17.
[47] A. K. Jain, A. Ross, and S. Prabhakar, “An introduction to biometric recognition,” IEEE Trans. on Circuits and Systems for Video Technology, vol. 14, pp. 4–20, 2004.
[48] L. Wang, T. Tan, S. Member, H. Ning, and W. Hu, “Silhouette analysis-based gait recognition for human identification,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 12, pp. 1505–1518, 2003.
[49] R. Plamondon and S. N. Srihari, “On-line and off-line handwriting recognition: A comprehensive survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, pp. 63–84, January 2000.
[50] F. Monrose and A. D. Rubin, “Keystroke dynamics as a biometric for authentication,” Future Generation Computer Systems, vol. 16, no. 4, pp. 351–359, 2000.
[51] L. Sweeney, “k-anonymity: A model for protecting privacy,” IEEE Security And Privacy, vol. 10, no. 5, pp. 557–570, 2002.
[52] A. Narayanan and V. Shmatikov, “Robust de-anonymization of large sparse datasets,” in IEEE Symposium on Security and Privacy, 2008, pp. 111–125.
[53] R. Leiby, “The Hill’s sex diarist reveals all (well, some),” The Washington Post, May 2004.
[54] K. Burton, A. Java, and I. Soboroff, “The ICWSM 2009 Spinn3r dataset,” in International AAAI conference on weblogs and social media, 2009.
[55] D. Klein and C. D. Manning, “Accurate unlexicalized parsing,” in Proceedings of the 41st Meeting of the Association for Computational Linguistics, 2003, pp. 423–430.
[56] H. Baayen, H. van Halteren, and F. Tweedie, “Outside the cave of shadows: using syntactic annotation to enhance authorship attribution,” Lit Linguist Computing, vol. 11, no. 3, pp. 121–132, Sep. 1996.
[57] M. Gamon, “Linguistic correlates of style: authorship classification with deep linguistic analysis features,” in Proceedings of the 20th international conference on Computational Linguistics, ser. COLING ’04. Stroudsburg, PA, USA: Association for Computational Linguistics, 2004.
[58] G. Forman, “An extensive empirical study of feature selection metrics for text classification,” Hewlett-Packard Labs, Tech. Rep., 2002.
[59] I. G. Bernhard E. Boser and V. Vapnik, “A training algorithm for optimal margin classifiers,” in COLT, 1992, pp. 144–152.
[60] C. Cortes and V. Vapnik, “Support-vector networks,” in Machine Learning, vol. 20, no. 3, 1995, pp. 273–297.
[61] T. Joachims, “Making large-scale SVM learning practical,” in Advances in Kernel Methods - Support Vector Learning, 1999.