### Communication and Annotation of Shared Objects

Communication between players is facilitated through a shared object known as the chain. Any modifications to this shared object are annotated with an output annotation. We also annotate the public and private keys of each player:

```java
KeyPair pair = keyGenerator.generateKeyPair();
this.keyPair = new KeyPair(
    @input "pubKey" pair.getPublic(),
    @input "privKey" pair.getPrivate()
);
```

The use of the private key for signatures is re-labeled to indicate that it is a permitted use of the private key:

```java
dsa.initSign(@relabel "privKeyForSigning" privKey);
```

### Inferred Policy

The inferred policy for the program indicates that the only sensitive information being communicated to the other player is the use of the private key for signature creation. The private key itself does not get revealed:

```plaintext
output chain (cid:55)→Reveal(privKeyForSigning[0+])
```

### Cryptographic Classes

We provided signatures for the cryptographic classes used in the program, such as `java.security.KeyPair` and `java.security.Signature`. These signatures indicated that there was no information flow from, for example, a signing key to the message digest. This lack of information flow is correct under symbolic models of cryptography and corresponds to negligible information flows in computational models.

However, in an implementation, the ciphertext is clearly computed from the plaintext, and our analysis conservatively concludes that the ciphertext reveals (non-negligible) information about the plaintext. Thus, our analysis would not be able to infer the signatures of the cryptographic library from the library implementation.

### Comparison with Jif Policies

Two of the case studies, Battleship and Mental Poker, have equivalent Jif versions with security annotations from the decentralized label model (DLM) [21]. The security guarantees offered by DLM policies differ significantly from those offered by our security policies. Our security policies focus on what information can be revealed by program execution, whereas DLM policies focus on who may learn and release information.

For example, in the Jif Battleship program, Player 1’s board is annotated as readable only by Player 1, and releasing information to Player 2 requires an explicit declassification annotation in a code context with Player 1’s authority. The Jif annotations do not directly describe what information is declassified.

### Related Work

In this work, we use static analysis to infer declassification policies that a program satisfies. Our declassification policies specify strong information-flow security while remaining simple and intuitive. Sabelfeld and Myers [7] survey language-based techniques for static reasoning about information-flow security. We focus on two areas of related work: specification of declassification policies and inference of security policies.

#### Declassification Policies

Declassification, or information release, occurs when sensitive information is made more public. It violates the semantic security condition of noninterference [10], yet commonly occurs in systems that handle sensitive information. Sabelfeld and Sands [9] survey semantic security conditions for declassification, categorizing them based on what information may be revealed, when the information may be revealed, who controls the release, and where in the system or program the release occurs.

Our policies specify what information may be released, characterized as expressions whose evaluation an observer may learn. Through conditional policies (if `d` then `p1` else `p2`), we can express certain conditions under which release may or may not occur, a form of when-declassification. Our policies can also express what code must be executed for the release to occur, a limited form of where-declassification.

The most closely related security condition is localized delimited release [22, 23], which uses escape hatch expressions to specify what secret information may be released and requires release to occur only at declassify commands. The security condition requires that when an output occurs, the observer can only learn the valuation of escape hatch expressions for which an appropriate declassify command has been executed. This is similar to satisfying the security policy if-executed `release-e` then `Reveal(e)`, where `release-e` is a mark command that occurs immediately before any declassification of `e`.

Rocha et al. [24] use graphs to describe how output values are allowed to depend on inputs. Like us, they seek to ease the annotation burden on the programmer; whereas we focus on policy inference, they focus on allowing the user to specify the graph policy separately from the code and then verify whether unannotated code satisfies the policy.

Sabelfeld and Sands [9] present prudent principles for declassification security conditions. We satisfy semantic consistency, conservativity, and non-occlusion; the principle of monotonicity of release is not applicable, as our programs have no declassification annotations.

#### Inference of Security Policies

Backes et al. [25] present an analysis that automatically discovers an equivalence relation characterizing the secret information a program may reveal. They quantify a program’s information flow by computing sizes of the equivalence classes. While potentially very precise, such quantitative policies may be difficult to interpret; in contrast, our policies are qualitative, and we have emphasized readability and intuition in their design.

Banerjee et al. [26] suggest model checking to determine if programs satisfy declassification policies expressed as abstraction functions; counter-examples produced by the model checker allow the declassification policy to be refined, and the process repeated, to determine the least amount of information that a program declassifies. As with Backes et al., we believe the key difference is that our declassification policies represent a better trade-off between precision, intuitiveness, and ease of inference.

King et al. [16] statically infer information flows to investigate the precision of security-type checking with respect to implicit information flows. Although they perform a context-sensitive analysis, they infer flow-insensitive types: within a given context, a variable is assumed to always contain information at the same security level. By contrast, we infer finer-grained policies, similar to flow-sensitive security types [27].

Pottier and Conchon [28] describe how to extend existing type systems with information security, enabling standard type inference algorithms to infer security types. However, such algorithms are unable to take full advantage of our precise security policies. For example, conditional policies incorporate path-sensitive information that is unavailable in standard typing disciplines.

Liu and Milanova [29, 30] and Livshits et al. [31] present static analyses to infer explicit information flows. Although the analyses are efficient and practical, they do not track implicit flows, and so it is unclear what security guarantees they provide. For example, an analysis of the password checking program from the Introduction that ignores implicit flows may conclude that the attacker learns nothing about the secret password.

Smith and Thober [32] perform type-inference for a highly polymorphic object-oriented security-type system. Top-level security policies are specified independently of code, and inferred types are used to determine whether the program is secure, given the policy. Like us, they seek to reduce the programmer burden. Although they simplify policy enforcement, the programmer must explicitly state security policies. By contrast, we aim to infer policies, which further reduces programmer burden.

King et al. [33] present a model for information-flow blame that aids identification of code that violates security. Information-flow blame helps the programmer understand information flows in a program. However, it requires that program types are annotated with security policies, and it cannot infer policies or provide security guarantees if the program fails to type check.

Tschantz and Wing [34] develop a tool that analyzes C-language programs and infers incident-insensitive non-interference policies, which allow the existence of high-security data, but not the data itself, to be revealed. Their tool discovers a set of program traces that violate incident-insensitive noninterference and works over a subset of C.

### Conclusion and Future Work

This work demonstrates that it is possible to infer precise and expressive information-flow policies for Java programs. Key contributions include defining an expressive policy language suited for precise inference, defining a dataflow inference algorithm, and implementing the analysis for Java.

Several avenues for further research remain open. It can be difficult to understand how security annotations affect inferred policies and how non-local control flow (e.g., exceptions) lead to implicit flows. One possible way to mitigate these difficulties would be to build more sophisticated user interfaces for the analysis tools.

Policy inference as described in this paper relies on expensive program analyses. Scalability may be improved by moving to a flow-insensitive analysis. Fortunately, artifacts developed in this work provide a promising testbed for evaluating scalable techniques.

### Acknowledgments

We thank Aslan Askarov and Andrei Sabelfeld for sharing the mental poker source code. We thank our reviewers and shepherd for useful feedback. This research is sponsored by the Air Force Research Laboratory and supported by the National Science Foundation under Grant No. 1054172.

### References

[1] A. Askarov and A. Sabelfeld, “Localized delimited release: combining the what and where dimensions of information release,” in Proc. 2007 Workshop on Programming Languages and Analysis for Security. New York, NY, USA: ACM Press, 2007.
[2] K. R. O’Neill, M. R. Clarkson, and S. Chong, “Information-flow security for interactive programs,” in CSFW ’06. IEEE, 2006.
[3] A. C. Myers, A. Sabelfeld, and S. Zdancewic, “Enforcing robust declassification,” in Proc. Computer Security Foundations Workshop, 2004.
[4] R. Giacobazzi and I. Mastroeni, “Abstract non-interference: parameterizing non-interference by abstract interpretation,” in POPL ’04. New York, NY, USA: ACM, 2004.
[5] D. Clark, S. Hunt, and P. Malacaria, “Quantified interference for a while language,” Electronic Notes in Theoretical Computer Science, vol. 112, Jan. 2005.
[6] S. Chong and A. C. Myers, “End-to-end enforcement of erasure and declassification,” in CSF ’08. IEEE, 2008.
[7] A. Sabelfeld and A. Myers, “Language-based information-flow security,” IEEE Journal on Selected Areas in Communications, vol. 21, no. 1, Jan. 2003.
[8] A. Askarov and A. Sabelfeld, “Security-typed languages for implementation of cryptographic protocols: A case study,” in ESORICS ’05, 2005.
[9] A. Sabelfeld and D. Sands, “Dimensions and principles of declassification,” in CSFW ’05. IEEE, 2005.
[10] J. A. Goguen and J. Meseguer, “Security policies and security models,” in Proc. IEEE Symposium on Security and Privacy. IEEE Computer Society, Apr. 1982.
[11] M. R. Clarkson and F. B. Schneider, “Hyperproperties,” in Proc. 21st IEEE Computer Security Foundations Symposium. IEEE Computer Society, Jul. 2008.
[12] D. E. Denning and P. J. Denning, “Certification of programs for secure information flow,” Communications of the ACM, vol. 20, no. 7, Jul. 1977.
[13] A. C. Myers, L. Zheng, S. Zdancewic, S. Chong, and N. Nystrom, “Jif: Java information flow,” 2001–2009, software release. Located at http://www.cs.cornell.edu/jif.
[14] N. Nystrom, M. Clarkson, and A. C. Myers, “Polyglot: An extensible compiler framework for Java,” in Compiler Construction, 12th International Conference, CC 2003, ser. Lecture Notes in Computer Science, G. Hedin, Ed., no. 2622. Warsaw, Poland: Springer-Verlag, Apr. 2003.
[15] A. Milanova, A. Rountev, and B. G. Ryder, “Parameterized object sensitivity for points-to analysis for Java,” ACM Transactions on Software Engineering and Methodology, vol. 14, no. 1, 2005.
[16] D. King, B. Hicks, M. Hicks, and T. Jaeger, “Implicit flows: Can’t live with 'em, can’t live without 'em,” in Proc. International Conference on Information Systems Security (ICISS), ser. LNCS, vol. 5352, Dec. 2008.
[17] K. D. Cooper, T. J. Harvey, and K. Kennedy, “A simple, fast dominance algorithm,” Software Practice & Experience, vol. 4, 2001.
[18] B. De Sutter, L. Van Put, and K. De Bosschere, “A practical interprocedural dominance algorithm,” ACM Transactions on Programming Languages and Systems, vol. 29, no. 4, 2007.
[19] A. C. Myers, “JFlow: Practical mostly-static information flow control,” in POPL ’99. New York, NY, USA: ACM Press, 1999.
[20] Y. Smaragdakis, M. Bravenboer, and O. Lhoták, “Pick your contexts well: understanding object-sensitivity,” in Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. New York, NY, USA: ACM, 2011, pp. 17–30.
[21] A. C. Myers and B. Liskov, “Complete, safe information flow with decentralized labels,” in Proc. IEEE Symposium on Security and Privacy. IEEE Computer Society, May 1998.
[22] A. Askarov and A. Sabelfeld, “Gradual release: Unifying declassification, encryption and key release policies,” in Proc. IEEE Symposium on Security and Privacy. IEEE Computer Society, 2007.
[23] ——, “Tight enforcement of information-release policies for dynamic languages,” in Proc. 22nd IEEE Computer Security Foundations Workshop, 2009.
[24] B. P. S. Rocha, S. Bandhakavi, J. d. Hartog, W. H. Winsborough, and S. Etalle, “Towards static flow-based declassification for legacy and untrusted programs,” in Proceedings of the 2010 IEEE Symposium on Security and Privacy. Washington, DC, USA: IEEE Computer Society, 2010.
[25] M. Backes, B. Köpf, and A. Rybalchenko, “Automatic discovery and quantification of information leaks,” in Proc. 2009 30th IEEE Symposium on Security and Privacy. Washington, DC, USA: IEEE Computer Society, 2009.
[26] A. Banerjee, R. Giacobazzi, and I. Mastroeni, “What you lose is what you leak: Information leakage in declassification policies,” Electronic Notes in Theoretical Computer Science, vol. 173, 2007.
[27] S. Hunt and D. Sands, “On flow-sensitive security types,” in Conference Record of the Thirty-Third Annual ACM Symposium on Principles of Programming Languages. New York, NY, USA: ACM Press, Jan. 2006.
[28] F. Pottier and S. Conchon, “Information flow inference for free,” in Proc. 5th ACM SIGPLAN International Conference on Functional Programming. New York, NY, USA: ACM Press, 2000.
[29] Y. Liu and A. Milanova, “Static analysis for inference of explicit information flow,” in Proc. 8th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering. New York, NY, USA: ACM, 2008.
[30] ——, “Practical static analysis for inference of security-related program properties,” in Proc. IEEE 17th International Conference on Program Comprehension, May 2009.
[31] B. Livshits, A. V. Nori, S. K. Rajamani, and A. Banerjee, “Merlin: Specification inference for explicit information flow problems,” in PLDI ’09, 2009.
[32] S. F. Smith and M. Thober, “Improving usability of information flow security in Java,” in Proc. 2007 Workshop on Programming Languages and Analysis for Security. New York, NY, USA: ACM Press, 2007.
[33] D. King, T. Jaeger, S. Jha, and S. A. Seshia, “Effective blame for information-flow violations,” in Proc. 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering, 2008.
[34] M. C. Tschantz and J. M. Wing, “Extracting conditional confidentiality policies,” in SEFM ’08: Proc. 2008 Sixth IEEE International Conference on Software Engineering and Formal Methods. Washington, DC, USA: IEEE Computer Society, 2008.