以下是优化后的文本，使其更加清晰、连贯和专业：

---

**参考文献**

[17] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv preprint arXiv:1708.06733 (2017).

[18] Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality Reduction by Learning an Invariant Mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), Vol. 2. IEEE, 1735–1742.

[19] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. 2008. Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Technical Report 07–49. University of Massachusetts, Amherst.

[20] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. 2020. Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 301–310.

[21] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning Multiple Layers of Features from Tiny Images. (2009).

[22] Wenshuo Li, Jincheng Yu, Xuefei Ning, Pengjun Wang, Qi Wei, Yu Wang, and Huazhong Yang. 2018. Hu-Fu: Hardware and Software Collaborative Attack Framework against Neural Networks. In 2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI). IEEE, 482–487.

[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In European Conference on Computer Vision. Springer, 740–755.

[24] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-Pruning: Defending against Backdooring Attacks on Deep Neural Networks. In International Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273–294.

[25] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. 2019. ABS: Scanning Neural Networks for Backdoors by Artificial Brain Stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 1265–1282.

[26] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In 25th Annual Network and Distributed System Security Symposium (NDSS). 18–221.

[27] Yuntao Liu, Yang Xie, and Ankur Srivastava. 2017. Neural Trojans. In 2017 IEEE International Conference on Computer Design (ICCD). IEEE, 45–48.

[28] Jiajun Lu, Theerasit Issaranon, and David Forsyth. 2017. SafeNet: Detecting and Rejecting Adversarial Examples Robustly. In Proceedings of the IEEE International Conference on Computer Vision. 446–454.

[29] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning Generic Context Embedding with Bidirectional LSTM. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. 51–61.

[30] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and Their Compositionality. In Advances in Neural Information Processing Systems. 3111–3119.

[31] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. Universal Adversarial Perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1765–1773.

[32] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. 2017. Towards Poisoning of Deep Learning Algorithms with Back-Gradient Optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 27–38.

[33] Sinno Jialin Pan and Qiang Yang. 2009. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering 22, 10 (2009), 1345–1359.

[34] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks against Machine Learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. 506–519.

[35] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. In 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 582–597.

[36] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. 2015. Deep Face Recognition. (2015).

[37] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 1532–1543.

[38] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 779–788.

[39] Joseph Redmon and Ali Farhadi. 2018. YOLOv3: An Incremental Improvement. arXiv preprint arXiv:1804.02767 (2018).

[40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In Advances in Neural Information Processing Systems. 91–99.

[41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision 115, 3 (2015), 211–252.

[42] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: A Unified Embedding for Face Recognition and Clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 815–823.

[43] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. 2018. Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks. In Advances in Neural Information Processing Systems. 6103–6113.

[44] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2019. A General Framework for Adversarial Examples with Objectives. ACM Transactions on Privacy and Security (TOPS) 22, 3 (2019), 1–30.

[45] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs. Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition. Neural Networks 32 (2012), 323–332.

[46] Jacob Steinhardt, Pang Wei Koh, and Percy S Liang. 2017. Certified Defenses for Data Poisoning Attacks. In Advances in Neural Information Processing Systems. 3517–3529.

[47] Yi Sun, Xiaogang Wang, and Xiaoou Tang. 2014. Deep Learning Face Representation from Predicting 10,000 Classes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1891–1898.

[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing Properties of Neural Networks. arXiv preprint arXiv:1312.6199 (2013).

[49] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. 2014. DeepFace: Closing the Gap to Human-Level Performance in Face Verification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1701–1708.

[50] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. 2016. Stealing Machine Learning Models via Prediction APIs. In 25th USENIX Security Symposium (USENIX Security 16). 601–618.

[51] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. 2019. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. In 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 707–723.

[52] Mei Wang and Weihong Deng. 2018. Deep Visual Domain Adaptation: A Survey. Neurocomputing 312 (2018), 135–153.

[53] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. 2016. A Survey of Transfer Learning. Journal of Big Data 3, 1 (2016), 9.

[54] Lior Wolf, Tal Hassner, and Itay Maoz. 2011. Face Recognition in Unconstrained Videos with Matched Background Similarity. In CVPR 2011. IEEE, 529–534.

[55] Mike Wu, Michael C Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. 2018. Beyond Sparsity: Tree Regularization of Deep Models for Interpretability. In Thirty-Second AAAI Conference on Artificial Intelligence. Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA.

[56] Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. 2016. A Methodology for Formalizing Model-Inversion Attacks. In 2016 IEEE 29th Computer Security Foundations Symposium (CSF). IEEE, 355–370.

[57] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. arXiv preprint arXiv:1704.01155 (2017).

[58] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. 2019. Latent Backdoor Attacks on Deep Neural Networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 2041–2055.

[59] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and Understanding Convolutional Networks. In European Conference on Computer Vision. Springer, 818–833.

---

**附录**

**A. 不成功的替代设计**

在我们的研究过程中，我们尝试了一些其他的设计方案，但这些方案未能成功。在本节中，我们将讨论其中的一些，并解释它们失败的原因。

我们的攻击模型类似于BadNets [17]，假设攻击者可以访问完整的训练集。因此，特洛伊训练是通过毒化训练数据的一个随机子集来实现的，即在这些数据上打上触发器并修改其标签为目标标签。最初，我们探索了一种要求较低的攻击模型，即攻击者只能访问模型而不能访问训练集。因此，我们尝试通过逆向工程 [50, 56] 生成训练数据，这种方法使用基于优化的方法从给定的输出标签生成输入样本。生成的样本然后经过混合器处理，并用于特洛伊训练。

我们在CIFAR-10数据集上尝试了两种逆向工程方法来生成有毒样本：(i) 分别为每个触发器标签单独逆向工程输入，然后使用混合器将生成的输入混合；(ii) 直接逆向工程一个复合输入，该输入包含触发器标签的特征，即通过一起反转标签来搜索能够最大化触发器标签logits的输入。图4(d)显示了第一种方法的一个示例，我们在其中分别为飞机和汽车标签逆向工程了两个样本。请注意，逆向工程生成的图像在大多数情况下可能不看起来像真实的物体，但它具有与真实图像相同的功能。然后，我们将它们提供给混合器以生成如图4(d)右侧所示的有毒样本。图4(e)显示了第二种方法的一个示例，我们在其中直接从两个标签逆向工程生成有毒样本。观察到生成的图像包含来自飞机和汽车的特征。

然而，我们的经验表明，通过上述两种方法训练的特洛伊模型在有毒数据上的性能较差，甚至对正常输入的分类准确性也有所下降。这些替代方案失败的主要原因是逆向工程生成的样本缺乏多样性（特征组合），因为基于优化的方法倾向于生成相同或非常有限的特征组合（触发器标签）。这是可以理解的，因为原始模型没有经过学习过程来强制模型学习各种特征组合。因此，如果模型没有学习到这些信息，就无法从中逆向工程出信息。因此，在我们的当前设计中，访问训练集和随机混合方法对于攻击的成功至关重要。需要注意的是，尽管输入逆向工程在 [26] 中成功用于生成训练输入，但他们的触发器只是简单的补丁，不需要模型学习大量信息。相比之下，我们的攻击利用了各种现有特征的组合。

**B. 我们的实验任务**

表9列出了我们的实验任务及其相关信息。第1列显示了任务名称。第2列显示了数据集。第3和第4列显示了数据集的统计信息。第5列显示了模型的输入大小。第6列显示了模型架构。

- **对象识别（CIFAR-10）**：此任务主要涉及计算机视觉模型。CIFAR-10数据集是一个轻量级且广泛使用的机器学习研究数据集。任务是识别10个不同类别的图像（例如，飞机和汽车）。数据集包含60K个样本。我们测试的模型是一个包含4个卷积层和3个全连接层的CNN。
- **交通标志识别（GTSRB）**：此任务常用于评估深度神经网络的攻击。任务是识别43种不同的交通标志，模拟自动驾驶汽车的应用场景。它使用德国交通标志基准数据集（GTSRB），包含39K个标记训练图像和13K个测试图像。CNN模型包含6个卷积层和3个全连接层。
- **人脸识别（YouTube Face）**：此任务模拟通过人脸识别进行安全筛查的情景，试图识别1,595个不同人的面孔。数据集的大规模增加了我们方法的计算成本，因此它是评估我们攻击的良好候选。它使用从不同人的YouTube视频中提取的YouTube Face数据集。我们使用对齐版本并过滤掉少于100个输入图像的较少见标签。这导致了1,283个不同的标签和约600K张图像。我们遵循先前的工作 [36] 使用VGG-Face架构，该架构包含13个卷积层和3个全连接层。
- **主题分类（AG's News）**：此任务是对输入文本的主题进行分类。AG's News数据集包含来自AG语料库的新闻文章，涵盖四个最大的类别。目标是识别四个不同主题的句子（例如，世界和体育）。数据集包含120K个训练样本。我们使用的模型是双向LSTM，每个方向有两层。我们使用GloVe [37] 模型进行词表示。
- **对象检测（COCO2014）**：此任务训练一个模型来检测图像中的对象，并通过边界框返回其类别和空间位置。COCO是最广泛使用的对象检测数据集之一，包含各种尺度的对象。COCO的样本包括多样化对象，具有不同的大小和不同程度的遮挡甚至视觉杂乱。我们在流行的YOLOv3 [39] 检测框架上应用了我们的攻击，该框架采用了一个包含76个卷积层的新骨干网络。
- **对象检测（VOC07+12）**：此数据集包含2007年和2012年的PASCAL视觉对象类挑战赛的数据，这是两个知名的对象检测竞赛。数据集中的每个图像包含一组对象，共20个不同类别。我们使用常见的07+12组合，即结合2007年和2012年的数据。

---

**表9：任务和数据集**

| 任务 | 数据集 | 训练样本数量 | 标签数量 | 输入大小 | 模型架构 |
| --- | --- | --- | --- | --- | --- |
| 对象识别 (OR) | CIFAR-10 | 50,000 | 10 | 32x32x3 | CNN (4卷积层, 3全连接层) |
| 交通标志识别 (SR) | GTSRB | 35,288 | 43 | 32x32x3 | CNN (6卷积层, 3全连接层) |
| 人脸识别 (FR) | YouTube Face | 599,967 | 1,283 | 224x224x3 | VGG-Face (13卷积层, 3全连接层) |
| 主题分类 (TC) | AG's News | 120,000 | 4 | 无限制 | 双向LSTM (2层) |
| 对象检测 (OD) | COCO2014 | 117,263 | 80 | 无限制 | YOLOv3 (76卷积层) |
| 对象检测 (OD) | VOC07+12 | 16,551 | 20 | 无限制 | YOLOv3 (76卷积层) |
| 对象检测 (OD) | ILSVRC2015 | 456,567 | 200 | 无限制 | YOLOv3 (76卷积层) |

---