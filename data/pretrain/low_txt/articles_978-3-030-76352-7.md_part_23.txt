### Intercepted by Istio and Fault Injection

When intercepted by Istio, the system returns a 500 error status. Users can specify the percentage of requests that should be failed. In our experiment, we injected faults into 17 services that cover the main flow of the train ticket application. We generated data by running a scenario where 100% of incoming requests are failed. To measure the observability of these faults, we executed the user flow 20 times after injecting the fault. For each service, the average running time for 20 iterations was 19 minutes, and the average number of log lines was 164,740. We found that the average number of error messages was 270, with a minimum of 40 and a maximum of 1,436. The ts-station-service fault generated the maximum number of errors, likely because it is one of the farthest nodes from the gateway node (ts-ui-dashboard) and all nodes in the path emit errors.

### Experiments

In this section, we present an empirical study conducted to demonstrate the effectiveness of our proposed approach for fault localization. The experiments are designed to show that a causal graph, in conjunction with golden signals, delivers superior results compared to using only a dependency graph (static topology) among microservices emitting error templates. Localizing operational faults involves two aspects: 1) localizing the faulty microservice and 2) identifying which error message (template) within the faulty microservice is responsible.

#### Evaluating Fault Localization at Microservice Level

To evaluate the first aspect, we divided the experiments into two groups, G1 and G2. In Group G1, the topology is static, while in Group G2, the topology is dynamic and computed using causality-based techniques. We used three approaches to generate the causal graph: PC, Blasso, and Blinear. Our results show that the dynamic topology, called a causal graph, outperforms the static topology in all variations. Finally, we perform last-mile fault localization to pinpoint the faulty error template.

**Group 1:**
- **G1A:** We use error signals emitted by each microservice and traverse the dependency graph to the leaf nodes, picking the leaf nodes as the faulty nodes.
- **G1B:** Instead of traversing to the leaf nodes, we run PageRank over the subgraph consisting of all microservices emitting error signals to get a ranked list of faulty nodes.
- **G1C:** We consider only those microservices that cause golden signal errors and run the Personalized PageRank algorithm on the static topology to get a ranked list of faulty nodes.

**Group 2:**
- **G2A:** The nodes causing golden signal errors are considered potential faulty nodes.
- **G2B:** Causal microservice nodes to the golden signal node are identified and assigned weights based on causality scores. Then, Personalized PageRank is used on the causal graph, factoring in weights on the causal nodes to do the first level of fault localization.

### Results

We used a time bin size of 10 ms as the inter-arrival time between error logs in this dataset. The threshold for the number of golden signal errors was set to 15. To calculate precision and recall, we used a graph-based approach. If the localized node does not exactly match the ground truth node, we calculate the match based on the distance (in number of hops) of the returned node \( n \) according to the following equation:

\[ S = 1 - \frac{h_n}{H + 1} \]

Here, \( S \) is the final match score for the returned node \( n \), \( h_n \) is the distance (in hops) of this node from the ground truth node, and \( H \) is a pre-configured threshold for the maximum number of hops allowed. For our experiments, we used \( H = 3 \). In all PageRank-based methods, we measured precision and recall for the top 3 results.

Figure 4 shows the results of experiments conducted to evaluate the accuracy of our approach at the microservice level. We observed low precision and recall for G1A, indicating that errors emitted by the leaf nodes are not the potential source of the fault. In G1B, there was a 7% increase in F1 score as PageRank helps in identifying the most impactful node. However, the performance was low because all nodes emitting error signals were considered. In G2A, where golden signal errors are factored in, we observed a 42% increase in F1 score (with the Blinear causal technique), signifying the usefulness of golden signals for fault localization.

The error signals often start from the faulty microservices and then propagate to other non-faulty microservices via inter-microservice interactions. Due to this, sometimes non-faulty microservices might show a causal relationship with golden signal errors, resulting in low precision for G2A. Out of the 17 faults, six were injected into services that directly interact with the ts-ui-dashboard service. When a fault is ingested in any of these six services, only the error signals emitted by the ts-ui-dashboard service have evidence of failure. In the G2A approach, we consider only those microservices that have a causal relationship with the ts-ui-dashboard, so the failure of these six services is not captured, leading to low recall.

To further improve performance, we used Personalized PageRank. The results clearly indicate that our approach, G2C, using the Blinear causal technique, outperforms all other approaches with an F1 score of 0.88. With the Blasso and PC causal techniques, we achieved F1 scores of 0.83 and 0.87, respectively. The PC-based technique had the highest recall of 0.96. We observed that with the dependency graph, G2B, we get low F1-scores: 0.49 (Blasso), 0.51 (PC), and 0.58 (Blinear). This could be because the dependency graph is not a true indication of runtime behavior. A microservice might interact with multiple services, but in a user scenario, not all flows need to execute. The Personalized PageRank algorithm on the static dependency graph does not assign high centrality scores to the faulty nodes, as non-faulty nodes have many incoming and outgoing edges, resulting in high centrality scores. In contrast, the causal graph indicates runtime interactions among various microservices, capturing error propagation across the application well.

### Last Mile Fault Localization

We applied our Last Mile Fault Localization (LMFL) technique to analyze the error templates emitted by the top 3 potential faulty microservices. We observed that usually, there are 3-4 unique error templates emitted by each microservice. Instead of analyzing all error templates of the top 3 microservices, we narrowed down to the error templates with a high causal relationship with golden signal errors and high centrality scores. This reduced the number of potential root cause error templates by 70%. On these error templates, we performed further analysis to determine whether the service emitting the error template is at fault or if one of its child nodes is. Figure 5 shows the improvement in precision and F1 for the G1C, G2A, and G2B approaches. The recall remains the same as LMFL is applied to the output of the previous step.

### Conclusion

In this paper, we presented a golden signal-based fault localization approach that infers the causal relationship among services emitting error signals and those emitting golden signal errors. We used a PageRank-based graph centrality approach to efficiently localize faults. The proposed approach improves state-of-the-art techniques by:
1. Using golden signal error rate to localize operational faults.
2. Time series modeling of the error rate from log data.
3. Using only positive samples for last-mile fault localization.
4. Using regression and conditional independence-based causal techniques.

Our experimental results demonstrate the effectiveness of this approach. This technique can be easily extended to use other golden signals such as latency, saturation, and traffic. In the future, we plan to conduct more experiments with real-world datasets and explore other types of golden signals.

### References

1. Istio service mesh. https://istio.io/. Accessed 16 Aug 2020
2. TrainTicket: A benchmark microservice system. https://github.com/FudanSELab/train-ticket/. Accessed 16 Aug 2020
3. Aggarwal, P., Atreja, S., Dasgupta, G., Mandal, A.: System anomaly detection using parameter flows, December 2019
4. Beyer, B., Jones, C., Petoff, J., Murphy, N.R.: Site Reliability Engineering: How Google Runs Production Systems. O’Reilly Media Inc., Newton (2016)
5. Chow, M., Meisner, D., Flinn, J., Peek, D., Wenisch, T.F.: The mystery machine: end-to-end performance analysis of large-scale internet services. In: 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 2014), pp. 217–231 (2014)
6. Geweke, J.F.: Measures of conditional linear dependence and feedback between time series. J. Am. Stat. Assoc. 79(388), 907–915 (1984)
7. Granger, C.W.J.: Investigating causal relations by econometric models and cross-spectral methods. Econometrica 37(3), 424–438 (1969)
8. Gupta, M., Mandal, A., Dasgupta, G., Serebrenik, A.: Runtime monitoring in continuous deployment by differencing execution behavior model. In: Pahl, C., Vukovic, M., Yin, J., Yu, Q. (eds.) ICSOC 2018. LNCS, vol. 11236, pp. 812–827. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-03596-9_58
9. Jia, T., Chen, P., Yang, L., Li, Y., Meng, F., Xu, J.: An approach for anomaly diagnosis based on hybrid graph model with logs for distributed services. In: 2017 IEEE International Conference on Web Services (ICWS), pp. 25–32. IEEE (2017)
10. Kalisch, M., Bühlmann, P.: Estimating high-dimensional directed acyclic graphs with the PC-algorithm. J. Mach. Learn. Res. 8, 613–636 (2007)
11. Kim, M., Sumbaly, R., Shah, S.: Root cause detection in a service-oriented architecture. ACM SIGMETRICS Perform. Eval. Rev. 41(1), 93–104 (2013)
12. Kobayashi, S., Otomo, K., Fukuda, K.: Causal analysis of network logs with layered protocols and topology knowledge. In: 2019 15th International Conference on Network and Service Management (CNSM), pp. 1–9 (2019)
13. Kobayashi, S., Otomo, K., Fukuda, K., Esaki, H.: Mining causality of network events in log data. IEEE Trans. Netw. Serv. Manag. 15(1), 53–67 (2018)
14. Lin, J., Chen, P., Zheng, Z.: Microscope: pinpoint performance issues with causal graphs in micro-service environments. In: Pahl, C., Vukovic, M., Yin, J., Yu, Q. (eds.) ICSOC 2018. LNCS, vol. 11236, pp. 3–20. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-03596-9_1
15. Mariani, L., Monni, C., Pezzé, M., Riganelli, O., Xin, R.: Localizing faults in cloud systems. In: 2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST), pp. 262–273 (2018)
16. Mi, H., Wang, H., Zhou, Y., Lyu, M.R.T., Cai, H.: Toward fine-grained, unsupervised, scalable performance diagnosis for production cloud computing systems. IEEE Trans. Parallel Distrib. Syst. 24(6), 1245–1255 (2013)
17. Nandi, A., Mandal, A., Atreja, S., Dasgupta, G.B., Bhattacharya, S.: Anomaly detection using program control flow graph mining from execution logs. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 215–224 (2016)
18. Nguyen, H., Shen, Z., Tan, Y., Gu, X.: Fchain: toward black-box online fault localization for cloud systems. In: 2013 IEEE 33rd International Conference on Distributed Computing Systems, pp. 21–30 (2013)
19. Park, T., Casella, G.: The Bayesian lasso. J. Am. Stat. Assoc. 103(482), 681–686 (2008)
20. Spirtes, P., Glymour, C.: An algorithm for fast recovery of sparse causal graphs. Soc. Sci. Comput. Rev. 9(1), 62–72 (1991)
21. Tan, J., Pan, X., Marinelli, E., Kavulya, S., Gandhi, R., Narasimhan, P.: Kahuna: problem diagnosis for mapreduce-based cloud computing environments. In: 2010 IEEE Network Operations and Management Symposium-NOMS 2010, pp. 112–119. IEEE (2010)
22. Valdés-Sosa, P., et al.: Estimating brain functional connectivity with sparse multivariate autoregression. Philos. Trans. R. Soc. Lond. Ser. B Biol. Sci. 360, 969–81 (2005)
23. Voas, J.M.: PIE: a dynamic failure-based technique. IEEE Trans. Software Eng. 18(8), 717 (1992)
24. Wang, P., et al.: CloudRanger: root cause identification for cloud native systems. In: 2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), pp. 492–502. IEEE (2018)
25. Weber, I., Li, C., Bass, L., Xu, X., Zhu, L.: Discovering and visualizing operations processes with POD-Discovery and POD-Viz. In: 2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, pp. 537–544. IEEE (2015)
26. Wu, L., Tordsson, J., Elmroth, E., Kao, O.: MicroRCA: root cause localization of performance issues in microservices. In: NOMS 2020–2020 IEEE/IFIP Network Operations and Management Symposium, pp. 1–9. IEEE (2020)
27. Xu, J., Chen, P., Yang, L., Meng, F., Wang, P.: LogDC: problem diagnosis for declaratively-deployed cloud applications with log. In: 2017 IEEE 14th International Conference on e-Business Engineering (ICEBE), pp. 282–287. IEEE (2017)
28. Zeng, C., Wang, Q., Wang, W., Li, T., Shwartz, L.: Online inference for time-varying temporal dependency discovery from time series. In: 2016 IEEE International Conference on Big Data (Big Data), pp. 1281–1290. IEEE (2016)

### Using Language Models to Pre-train Features for Optimizing Information Technology Operations Management Tasks

**Authors:**
- Xiaotong Liu
- Yingbei Tong
- Anbang Xu
- Rama Akkiraju

**Affiliation:**
IBM Research Almaden, San Jose, USA

**Emails:**
- xiaotong.liu@ibm.com
- yingbei.tong@ibm.com
- anbangxu@us.ibm.com
- akkiraju@us.ibm.com

**Abstract:**
Information Technology (IT) Operations management is a significant challenge for companies that rely on IT systems for mission-critical business applications. While IT operators increasingly leverage analytical tools powered by artificial intelligence (AI), the volume, variety, and complexity of data generated in the IT Operations domain pose substantial challenges in managing these applications. In this work, we present an approach to leveraging language models to pre-train features for optimizing IT Operations management tasks, such as anomaly prediction from logs. Specifically, using log-based anomaly prediction as the task, we show that machine learning models built using language models (embeddings) trained with IT Operations domain data as features outperform AI models built using language models with general-purpose data as features. Furthermore, we present our empirical results outlining the influence of factors such as the type of language models, the type of input data, and the diversity of input data, on the prediction accuracy of our log anomaly prediction model when language models trained from IT Operations domain data are used as features. We also present the run-time inference performance of log anomaly prediction models built using language models as features in an IT Operations production environment.

**Keywords:**
- AI for IT operations
- Language modeling
- Anomaly detection

### 1 Introduction

Information Technology (IT) Operations management is a significant challenge for most companies that rely on IT systems for mission-critical business applications. Despite best intentions, designs, and development practices, software and hardware systems are susceptible to outages, resulting in millions of dollars in labor, revenue loss, and customer satisfaction issues. IT downtime costs an estimated $26.5 billion in lost revenue each year based on a survey of companies.

(c) Springer Nature Switzerland AG 2021
H. Hacid et al. (Eds.): ICSOC 2020 Workshops, LNCS 12632, pp. 150–161, 2021.
https://doi.org/10.1007/978-3-030-76352-7_18