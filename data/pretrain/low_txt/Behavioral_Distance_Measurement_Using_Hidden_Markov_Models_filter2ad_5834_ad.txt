### Performance Evaluation

#### Test Configuration
The performance tests were conducted using the static test suite included with WebBench 5.0, with a setting of 10 concurrent client threads. Each test ran for 1600 seconds, with statistics calculated at 100-second intervals. Both L and W used Apache web server version 2.2.2.

**Table 2: Configurations of Computers in the Performance Overhead Evaluation**

| Machine Name | Operating System         | CPU               | Memory | Remarks   |
|--------------|--------------------------|-------------------|--------|-----------|
| L            | Linux kernel 2.6.8       | Pentium IV 2.0 GHz | 512 MB | Replica   |
| W            | Windows XP Pro SP2       | Pentium IV 2.0 GHz | 512 MB | Replica   |
| P            | Windows XP Pro SP2       | Pentium IV 2.2 GHz | 512 MB | Proxy     |
| C            | Linux kernel 2.6.11      | Pentium IV 3.0 GHz | 1 GB   | Client    |

#### Test Scenarios
We are primarily interested in the request throughput and latency as observed by C in five different tests:

1. **Baseline (Output Voting Only):**
   - L and W send their responses to the proxy P, which performs output voting.
   - No system call traces are collected on L or W, and no behavioral distance is calculated.
   - This serves as the baseline for our tests.

2. **System Call Tracing:**
   - L and W capture the system calls made by the web server processes/threads and send this information to another machine (not P) for logging and potential offline behavioral-distance calculations.
   - This test includes the costs of collecting and sending the system call information but not the cost of calculating behavioral distances.

3. **Online Behavioral Distance Calculation:**
   - The system call information is sent to proxy P for online behavioral distance calculation.
   - P computes the behavioral distance (in addition to performing output voting) before responding to the client.

4. **Caching Behavioral Distance Results:**
   - The results of each behavioral distance calculation are cached at P so that they do not need to be recomputed if the same system call sequences are received from L and W in the future.
   - This reduces the overhead of behavioral distance calculations over time.

5. **Single Server Performance:**
   - Only W and C are used to evaluate the performance of an individual server, with neither output voting nor behavioral distance calculations.

#### Test Results
- **First Test (Baseline):**
  - P only performs output voting.
  - This serves as the reference point for comparison.

- **Second Test (System Call Tracing):**
  - Shows the performance overhead of capturing and transporting system call information off L and W.
  - The overhead is minimal: approximately 1% in throughput and 0.03 milliseconds in latency on average.

- **Third Test (Online Behavioral Distance Calculation):**
  - Demonstrates the overhead of capturing system call information and performing HMM-based behavioral distance calculation.
  - This adds substantial overhead to the request processing time.

- **Fourth Test (Caching Behavioral Distance Results):**
  - Reduces the overhead of behavioral distance calculations through caching.
  - By the end of the test, there is less than a 20% throughput loss and 0.59 milliseconds of additional latency on average.
  - The cache takes some time to warm up, but the performance improves significantly over time.

- **Fifth Test (Single Server Performance):**
  - Compares the performance of a single server (W) without output voting or behavioral distance calculations.
  - Indicates that L and W are roughly 25% underutilized in the fourth test due to the bottleneck at the proxy.

**Figure 1: Performance Overhead of the HMM-based Behavioral Distance**

[Insert Figure 1 here]

### Conclusion
In this paper, we presented a new algorithm for computing behavioral distance between processes. Our approach addresses shortcomings in prior techniques, particularly in better accounting for system-call orderings while maintaining comparable performance. Empirical tests suggest that our algorithm offers strong defense against mimicry attacks and provides substantial improvement in the false-alarm rate over previous proposals. We believe this algorithm is a significant step toward the practical use of behavioral distance as an anomaly detection technique, especially in fault- and intrusion-tolerant architectures that redundantly execute requests on multiple diverse platforms.

### References
1. M. Abd-El-Malek, G. R. Ganger, G. R. Goodson, M. K. Reiter, and J. J. Wylie. Fault-scalable Byzantine fault-tolerant services. In Proceedings of the 20th ACM Symposium on Operating Systems Principles, pages 59–74, October 2005.
2. L. Alvisi, D. Malkhi, E. Pierce, and M. K. Reiter. Fault detection for Byzantine quorum systems. IEEE Transactions on Parallel Distributed Systems, 12(9), September 2001.
3. L. E. Baum and T. Petrie. Statistical inference for probabilistic functions of finite state Markov chains. Ann. Math. Statist., 37:1554–1563, 1966.
4. S. Bhatkar, A. Chaturvedi, and R. Sekar. Dataflow anomaly detection. In Proceedings of the 2006 IEEE Symposium on Security and Privacy, 2006.
5. R. W. Buskens and R. P. Bianchini, Jr. Distributed on-line diagnosis in the presence of arbitrary faults. In Proceedings of the 23rd International Symposium on Fault-Tolerant Computing, pages 470–479, June 1993.
6. C. Cachin and J. A. Poritz. Secure intrusion-tolerant replication on the Internet. In Proceedings of the 2002 International Conference on Dependable Systems and Networks, 2002.
7. M. Castro and B. Liskov. Practical Byzantine fault tolerance and proactive recovery. ACM Transactions on Computer Systems, 20(4), November 2002.
8. M. Castro, R. Rodrigues, and B. Liskov. BASE: Using abstraction to improve fault tolerance. ACM Transactions on Computer Systems, 21(3), August 2003.
9. L. Chen and A. Avizienis. N-version programming: A fault-tolerance approach to reliability of software operation. In Proceedings of the 8th International Symposium on Fault-Tolerant Computing, pages 3–9, 1978.
10. S. Cho and S. Han. Two sophisticated techniques to improve HMM-based intrusion detection systems. In Proceedings of the 6th International Symposium on Recent Advances in Intrusion Detection (RAID 2003), 2003.
11. B. Cox, D. Evans, A. Filipi, J. Rowanhill, W. Hu, J. Davidson, J. Knight, A. Nguyen-Tuong, and J. Hiser. N-variant systems – A secretless framework for security through diversity. In Proceedings of the 15th USENIX Security Symposium, August 2006.
12. R. I. A. Davis, B. C. Lovell, and T. Caelli. Improved estimation of Hidden Markov Model parameters from multiple observation sequences. In Proceedings of the 16th International Conference on Pattern Recognition (ICPR 2002), 2002.
13. H. H. Feng, J. T. Griffin, Y. Huang, S. Jha, W. Lee, and B. P. Miller. Formalizing sensitivity in static analysis for intrusion detection. In Proceedings of the 2004 IEEE Symposium on Security and Privacy, 2004.
14. H. H. Feng, O. M. Kolesnikov, P. Fogla, W. Lee, and W. Gong. Anomaly detection using call stack information. In Proceedings of the 2003 IEEE Symposium on Security and Privacy, 2003.
15. S. Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaff. A sense of self for Unix processes. In Proceedings of the 1996 IEEE Symposium on Security and Privacy, 1996.
16. D. Gao, M. K. Reiter, and D. Song. Gray-box extraction of execution graph for anomaly detection. In Proceedings of the 11th ACM Conference on Computer & Communication Security, 2004.
17. D. Gao, M. K. Reiter, and D. Song. On gray-box program tracking for anomaly detection. In Proceedings of the 13th USENIX Security Symposium, 2004.
18. D. Gao, M. K. Reiter, and D. Song. Behavioral distance for intrusion detection. In Proceedings of the 8th International Symposium on Recent Advances in Intrusion Detection (RAID 2005), 2005.
19. J. T. Griffin, S. Jha, and B. P. Miller. Detecting manipulated remote call streams. In Proceedings of the 11th USENIX Security Symposium, 2002.
20. J. T. Griffin, S. Jha, and B. P. Miller. Efficient context-sensitive intrusion detection. In Proceedings of Symposium on Network and Distributed System Security, 2004.
21. C. Kruegel, D. Mutz, F. Valeur, and G. Vigna. On the detection of anomalous system call arguments. In Proceedings of the 8th European Symposium on Research in Computer Security (ESORICS 2003), 2003.
22. L. Lamport. The implementation of reliable distributed multiprocess systems. Computer Networks, 2:95–114, 1978.
23. I. M. Meyer and R. Durbin. Comparative ab initio prediction of gene structures using pair HMMs. Oxford University Press, 2002.
24. L. Pachter, M. Alexandersson, and S. Cawley. Applications of generalized pair Hidden Markov Models to alignment and gene finding problems. Computational Biology, 9(2), 2002.
25. L. R. Rabiner. A tutorial on Hidden Markov Models and selected applications in speech recognition. In Proceedings of IEEE, February 1989.
26. M. K. Reiter. Secure agreement protocols: Reliable and atomic group multicast in Rampart. In Proceedings of the 2nd ACM Conference on Computer and Communication Security, pages 68–80, November 1994.
27. F. B. Schneider. Implementing fault-tolerant services using the state machine approach: A tutorial. ACM Computing Surveys, 22(4):299–319, December 1990.
28. R. Sekar, M. Bendre, D. Dhurjati, and P. Bollineni. A fast automaton-based method for detecting anomalous program behaviors. In Proceedings of the 2001 IEEE Symposium on Security and Privacy, 2001.
29. P. H. Sellers. On the theory and computation of evolutionary distances. SIAM J. Appl. Math., 26:787–793, 1974.
30. K. Shin and P. Ramanathan. Diagnosis of processors with Byzantine faults in a distributed computing system. In Proceedings of the 17th International Symposium on Fault-Tolerant Computing, pages 55–60, 1987.
31. K. Tan, J. McHugh, and K. Killourhy. Hiding intrusions: From the abnormal to the normal and beyond. In Proceedings of the 5th International Workshop on Information Hiding, October 2002.
32. D. Wagner and D. Dean. Intrusion detection via static analysis. In Proceedings of the 2001 IEEE Symposium on Security and Privacy, 2001.
33. D. Wagner and P. Soto. Mimicry attacks on host-based intrusion detection systems. In Proceedings of the 9th ACM Conference on Computer and Communications Security, 2002.
34. C. Warrender, S. Forrest, and B. Pearlmutter. Detecting intrusions using system calls: alternative data models. In Proceedings of the 1999 IEEE Symposium on Security and Privacy, 1999.
35. A. Wespi, M. Dacier, and H. Debar. Intrusion detection using variable-length audit trail patterns. In Proceedings of the 2000 Recent Advances in Intrusion Detection, 2000.
36. J. Yin, J. Martin, A. Venkataramani, L. Alvisi, and M. Dahlin. Separating agreement from execution for Byzantine fault tolerant services. In Proceedings of the 19th ACM Symposium on Operating System Principles, October 2003.

### Appendix A: Updating the bi Parameters of λ

The idea of updating the \( b_i \) parameters of \( \lambda \) is similar to updating the \( a_i \) parameters (see Section 4.3). To compute the expected number of times \( \lambda \) emits observable symbol \([x, y]\) at state \( q_i \) when generating \([S_1, S_2]\), we define a conditional probability, \( \zeta([x, y], u, v, i) \), as follows:

\[
\zeta([x, y], u, v, i) = \Pr_{\lambda} \left( \text{State}_t = q_i \land \text{Out}_{t,1} = \text{Seq}(x) \land \text{Out}_{t,2} = \text{Seq}(y) \land \text{Out}_{\leq t,1} = \text{Pre}(S_1, u) \land \text{Out}_{\leq t,2} = \text{Pre}(S_2, v) \mid \text{Out}_{>0,1} = S_1 \land \text{Out}_{>0,2} = S_2 \right)
\]

where

\[
\text{Seq}(x) = 
\begin{cases} 
x & \text{if } x \neq \sigma \\
\epsilon & \text{if } x = \sigma 
\end{cases}
\]

and \( \text{Out}_{t,1} \) is the sequence of system calls from \( C_1 \) in the first component of the emitted symbol in iteration \( t \), with either one (if the component of the emitted symbol is not \( \sigma \)) or zero (if the component of the emitted symbol is \( \sigma \)) system call in the sequence. \( \text{Out}_{t,2} \) is defined similarly.

\[
\gamma(u, v, i) = \sum_{t \geq 0} \Pr_{\lambda} \left( \text{State}_t = q_i \land \text{Out}_{\leq t,1} = \text{Pre}(S_1, u) \land \text{Out}_{\leq t,2} = \text{Pre}(S_2, v) \mid \text{Out}_{>0,1} = S_1 \land \text{Out}_{>0,2} = S_2 \right)
\]

This represents the probability of \( \lambda \) being in state \( q_i \) after emitting \( u \) system calls for process 1 and \( v \) system calls for process 2, and the last observable symbol emitted by state \( q_i \) is \([x, y]\), given that the system call sequences for process 1 and process 2 are \( S_1 \) and \( S_2 \), respectively.