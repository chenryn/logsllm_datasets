### Trends in Section IV

In Section IV, we observed several trends, such as the increasing failure rates with higher chip density and a greater number of CPUs. Interestingly, our model can provide insights into the relative changes in error rates for different system design choices. For example, using lower-density DIMMs can reduce the failure rate of high-end servers by 57.7%, while halving the number of cores can reduce it by 34.6%. This suggests that designing systems with lower-density DIMMs can offer a more significant reliability benefit compared to reducing the number of CPUs. Our model allows system architects to explore a wide range of design options for memory reliability quickly and easily. We hope that by using this model, architects can evaluate the reliability trade-offs of their system configurations to achieve better memory reliability. The model is also available online at [1].

### Effect of Page Offlining at Scale

Next, we discuss the results of a study examining ways to reduce memory errors using page offlining [49, 16]. Page offlining removes a physical page of memory containing an error from the set of pages the operating system can allocate, thereby reducing the likelihood of a more severe uncorrectable error on that page. While previous work evaluated page offlining using simulations on memory traces [16], we deployed page offlining on a subset of the machines (12,276 servers) and observed the results. We describe the system design decisions required to make page offlining effective at scale and analyze its effectiveness.

#### A. Design Decisions and Implementation

The three main design decisions we explored for utilizing page offlining in practice are: (1) when to take a page offline, (2) for how long to take a page offline, and (3) how many pages to take offline (the first and last of which were also identified in [16]).

1. **When to Take a Page Offline?**
   ECC DIMMs provide flexibility for tolerating correctable errors for a certain period. In some settings, it may be sensible to wait until a specific number of memory errors occur on a page within a given time before taking the page offline. We adopted a conservative approach and took any page with a memory error offline immediately (similar to the most aggressive policy examined in prior work [16]). The rationale is that leaving a page with an error in use increases the risk of an uncorrectable error. Another option could be to leave pages with errors in use for longer, especially if applications are designed to be less affected by memory errors. This approach is taken by Flikker [31], which developed a programming model for reasoning about data reliability, and by heterogeneous-reliability memory systems where parts of memory can be less reliable, and application data that is less vulnerable to errors can be allocated there [33].

2. **For How Long to Take a Page Offline?**
   A key question in large-scale page offlining is how to persist an offlined page across machine reboots (both planned and unplanned) and hardware changes (e.g., disk replacement). Existing techniques do not handle these cases. Allowing an offlined page with a permanent error to come back online can increase the window of vulnerability for uncorrectable errors. We examined a policy that takes pages offline permanently. To track offlined pages across machine reboots, we store them by host name in a distributed database queried when the OS kernel loads. This allows known-bad pages to be taken offline before the kernel allocates them to applications. Entries in this database need to be updated as DRAM parts are replaced in a system.

3. **How Many Pages to Take Offline?**
   Taking a page offline reduces the size of physical memory, potentially causing increased swapping of pages to storage. To limit the negative performance impact, we place a cap on the number of physical pages that can be taken offline. Unlike prior work, as shown in Section III-B, socket and channel failures can cause page offlining to remove large portions of the physical address space, leading to significant swapping and performance degradation. Logs are routinely inspected on each machine to check how many pages have been taken offline. When the amount of physical memory taken offline exceeds 5% of a server’s physical memory capacity, a repair ticket is generated for the server.

#### B. Effectiveness

Figure 18 shows a timeline of the normalized number of errors in the 12,276 servers we examined. The experiment was conducted over 86 days, and we measured the number of errors as a moving average over 30 days. As it was a production environment, page offlining was deployed gradually over several days. We divide the graph into three regions based on the different phases of our experiment:
- **Region a:** State of the servers before page offlining was deployed.
- **Region b:** State of the servers while page offlining was being deployed gradually to 100% of the servers.
- **Region c:** State of the servers after page offlining was fully deployed.

The initial hump in Region a from days 0 to 7 was due to a bank failure on one server that generated a large number of errors. By day 8, its effects were no longer noticeable in the moving average, and we compare the effectiveness of page offlining to the error rate after day 8.

Three observations can be made from Figure 18:
1. After deploying page offlining to 100% of the fleet at day 25, the error rate continued to decrease until day 50. This is likely because some pages that contained errors but were not accessed immediately after deployment were eventually accessed, triggering an error and taking the page offline. Additionally, some pages cannot be taken offline immediately due to OS restrictions, which we will describe in the next section.
2. Comparing the error rate at day 18 (right before initial testing) to the error rate at day 50 (after deploying page offlining and letting the servers run for a couple of weeks), the error rate decreased by around 67%. This is smaller than the 86% to 94% error rate reduction reported in Hwang et al.’s study [16]. One reason for this could be that the prior study included socket and channel errors in their simulation, increasing the number of errors that could be avoided by page offlining.
3. We observe a relatively high error occurrence rate (e.g., at day 57, the error rate is still around 18% of the maximum amount), even after page offlining. This suggests that it is important to design devices and other techniques to reduce the error rate that is not affected by aggressive page offlining.

#### C. Limitations

While page offlining is relatively effective at reducing the number of reported errors, we found two main limitations that were not addressed in prior work. First, it reduces memory capacity, requiring the repair of a machine after a certain fraction of its pages have been taken offline. Second, it may not always succeed in real systems. We logged the failure rate of page offlining and found that around 6% of the attempts to take a page offline initially failed. One example of why a page may fail to be taken offline in the Linux kernel is if its contents cannot be locked for exclusive access. For instance, if the data is being prefetched into the page cache at the time of offlining, locking the page could result in a deadlock, and the Linux kernel does not allow this. However, this issue could be fixed by retrying page offlining at a later time, albeit with added complexity to the system software.

Despite these limitations, we find that page offlining, when adapted to function at scale, provides reasonable memory error tolerance benefits, as demonstrated. We look forward to future works that analyze the interaction of page offlining with other error correction methods.

### Related Work

To the best of our knowledge, we have performed the first analysis of DRAM failure trends (on modern DRAM devices using modern data-intensive workloads) that have not been identified in prior work (e.g., chip density, transfer width, workload type), presented the first regression-based model for examining the memory failure rate of systems, and performed the first analysis of page offlining in the field. Prior large-scale empirical studies of memory errors analyzed various aspects of memory errors in different systems. We have already presented extensive comparisons to the most prevalent of them [44, 16, 47, 48, 10] throughout the paper. We will briefly discuss these and others here.

Schroeder et al. conducted the first study of memory errors in the field on a majority of Google’s servers in 2009 [44]. Their study showed the relatively high rate of memory errors across Google’s server population, provided evidence that errors are dominated by device failures (versus alpha particles), and noted that they did not observe any indication that newer generations of DRAM devices have worse error behavior, that CPU and memory utilization are correlated with error rate, and that the average server error rate is very high—findings clarified by our study five years later, as explained in Section III. Their work formed the basis for what is known about DRAM errors in the field.

Hwang et al. performed an analysis on a trace of memory errors from a sample of Google servers and IBM supercomputers, showing how errors are distributed across various DRAM components [16], but without controlling for the effect of socket and channel failures. The high number of repeat address errors led them to simulate the effectiveness of page offlining (proposed in [49]) on the memory error traces, which they found to reduce the error rate by 86% to 94%. Note that their study of page offlining, unlike ours (presented in Section VI), was done purely in simulation, not in a large-scale system.

Sridharan et al. examined memory errors in a supercomputing environment [47, 48, 10]. Similar to Hwang et al., they found that most memory errors are permanent and additionally identified occurrences of multi-DIMM errors, speculating as to their origin. They also found that DRAM vendor and age are correlated with error rate. Concurrently with our work, Sridharan et al. observed that average server errors are much larger than median server errors [46], though we quantify and provide a model for the full distribution of errors per server. Siddiqua et al. provided an error classification methodology for the memory controller and memory bus but did not classify memory errors at a finer DRAM chip-level granularity as we do [45]. They found that a small number of faults generate a large number of errors and that faults are predominantly permanent.

Nightingale et al. examined the failure rate of consumer PCs and showed that increased CPU frequency is correlated with increased DRAM error rates [40]. A pair of works by Li et al. analyzed memory errors on 212 Ask.com servers and evaluated their application-level impact [27, 28]. They found that most memory errors are permanent and that memory errors affected applications in noticeable ways, and proposed a technique to monitor memory for errors to reduce application impact.

### Conclusions

We performed a comprehensive analysis of memory errors across all of Facebook’s servers over fourteen months. We analyzed a variety of factors and how they affect server failure rates, observing several new reliability trends for memory systems that have not been discussed before in the literature. We find that (1) memory errors follow a power-law distribution, specifically a Pareto distribution with a decreasing hazard rate, with the average error rate exceeding the median error rate by around 55%; (2) non-DRAM memory failures from the memory controller and memory channel contribute the majority of errors and create a kind of denial of service attack in servers; (3) more recent DRAM cell fabrication technologies (as indicated by chip density) show higher failure rates (prior work that measured DRAM capacity, which is not closely related to fabrication technology, observed inconclusive trends); (4) DIMM architecture decisions affect memory reliability: DIMMs with fewer chips and lower transfer widths have the lowest error rates, likely due to electrical noise reduction; and (5) while CPU and memory utilization do not show clear trends with respect to failure rates, workload type can influence server failure rate by up to 6.5%.

We developed a model for memory failures and showed how system design choices, such as using lower-density DIMMs and fewer processors, can reduce failure rates of baseline servers by up to 57.7%. We also performed the first analysis of page offlining in a real-world environment, showing that the error rate can be reduced by around 67% and identifying and fixing several real-world challenges to the technique.

We hope that the data and analyses presented in our work can aid in (1) clearing up potential inaccuracies and limitations in past studies’ conclusions, (2) understanding the effects of different factors on memory reliability, (3) the design of more reliable DIMM and memory system architectures, and (4) improving evaluation methodologies for future memory reliability studies.

### Acknowledgments

We thank Konrad Lai, Manish Modi, and Jon Brauer for their contributions to the work. We also thank the students who attended the Fall 2014 lectures of 18-742 at CMU, who provided feedback on earlier drafts. We thank the anonymous reviewers from ISCA 2014, OSDI 2015, and DSN 2015 and the members of the SAFARI research group for their comments and suggestions. This work is supported in part by Samsung and the Intel Science and Technology Center for Cloud Computing, as well as NSF grants 0953246, 1065112, 1212962, and 1320531.

### References

[1] “DRAM Failure Model,” http://www.ece.cmu.edu/~safari/tools.html.
[2] “Open Compute Project,” http://www.opencompute.org/.
[3] “The R Project for Statistical Computing,” http://www.r-project.org/.
[4] D. Borthakur et al., “Apache Hadoop goes realtime at Facebook,” SIGMOD, 2011.
[5] P. Chakka et al., “Hive: A Warehousing Solution Over a Map-Reduce Framework,” VLDB, 2009.
[6] P.-F. Chia, S.-J. Wen, and S. Baeg, “New DRAM HCI Qualification Method Emphasizing on Repeated Memory Access,” IRW, 2010.
[7] C. Constantinescu, “Trends and Challenges in VLSI Circuit Reliability,” IEEE Micro, 2003.
[8] M. E. Crovella and A. Bestavros, “Self-similarity in world wide web traffic: Evidence and possible causes,” IEEE/ACM TON, 1997.
[9] M. E. Crovella, M. S. Taqqu, and A. Bestavros, A Practical Guide to Heavy Tails. Chapman & Hall, 1998.
[10] N. DeBardeleben et al., “Extra Bits on SRAM and DRAM Errors – More Data from the Field,” SELSE, 2014.
[11] T. J. Dell, “A White Paper on the Benefits of Chipkill-Correct ECC for PC Server Main Memory,” IBM Microelectronics Division, Nov. 1997.
[12] R. W. Hamming, “Error Correcting and Error Detecting Codes,” Bell System Technical Journal, Apr. 1950.
[13] M. Harchol-Balter, “Task assignment with unknown duration,” J. ACM, 2002.
[14] M. Harchol-Balter and A. Downey, “Exploiting Process Lifetime Distributions for Dynamic Load Balancing,” SIGMETRICS, 1996.
[15] D. Hosmer and S. Lemeshow, Applied Logistic Regression (Second Edition). John Wiley and Sons, Inc., 2000.
[16] A. Hwang, I. Stefanovici, and B. Schroeder, “Cosmic Rays Don’t Strike Twice: Understanding the Characteristics of DRAM Errors and the Implications for System Design,” ASPLOS, 2012.