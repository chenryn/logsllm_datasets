在本章中，我们深入探讨了批处理的主题。首先介绍了诸如 `awk`、`grep` 和 `sort` 等 Unix 工具，并进一步分析了这些工具的设计理念如何被应用于 MapReduce 以及更现代的数据流引擎中。其中一些关键设计原则包括：输入数据是不可变的，输出则作为另一个（可能是未知的）程序的输入，而复杂问题通过编写“只做一件事”的小工具来解决。

在 Unix 世界里，文件和管道构成了允许不同程序间组合的统一接口；而在 MapReduce 中，这一角色由分布式文件系统承担。我们还观察到，数据流引擎引入了自己的管道式数据传输机制，以避免将中间状态物化到分布式文件系统中，但作业的初始输入和最终输出通常仍使用 HDFS。

分散式批处理框架需要解决两个主要问题：
- **分割槽**：在 MapReduce 中，Mapper 根据输入文件块进行分割。Mapper 的输出随后被重新分配、排序并合并至可配置数量的 Reducer 分割槽中，目的是确保所有相关数据（例如具有相同键的所有记录）聚集在同一位置。后 MapReduce 时代的流处理引擎虽然尽量避免不必要的排序操作，但在数据分区方面采取了类似的方法。
- **容错性**：MapReduce 频繁地写入磁盘，这使得从单个失败任务恢复变得容易，无需重启整个作业，但在无故障情况下会降低执行速度。相比之下，数据流引擎更多地依赖内存存储中间状态，减少了对磁盘的依赖，这意味着如果某个节点出现故障，则需重算更多数据。确定性运算符可以减少所需重算的数据量。

接着，我们讨论了几种用于 MapReduce 的连接算法，它们同样适用于 MPP 数据库及数据流引擎内部，并很好地展示了分片算法的工作原理：
- **排序合并连接**：每个参与连接的输入都通过一个提取连接键的 Mapper 进行处理。经过分片、排序和合并后，具有相同键的所有记录最终会被发送给同一个 Reducer 函数，该函数负责输出已连接好的记录。
- **广播哈希连接**：当其中一个连接输入相对较小，可以完全加载到哈希表中时，可以为大端的每个分片启动一个 Mapper，在每个 Mapper 中加载小端的哈希表，然后扫描大端，逐条记录查询哈希表。
- **分片哈希连接**：如果两个连接输入采用相同的分片方式（即使用相同的键、哈希函数及分片数量），则可以在每个分片上独立应用哈希表方法。

分布式批处理引擎遵循一种有意限制的编程模型：回调函数（如 Mapper 和 Reducer）被假定为无状态，并且除了指定的输出外，不应有任何外部可见的副作用。这种限制使框架能够隐藏某些复杂的分布式系统问题，比如在遇到崩溃或网络故障时安全地重试任务，任何失败任务的输出都将被丢弃。如果多个任务成功完成同一分片，则只有一个的结果实际可见。

得益于这样的框架结构，开发者在编写批处理作业代码时无需担心实现容错机制——框架保证了即使发生错误，作业的最终输出也将与没有错误时一致。与在线服务相比，后者在处理用户请求的同时还需要考虑将写入数据库作为请求处理的一部分，批处理提供了更强的可靠性语义保障。

批处理作业的一个显著特征在于它读取特定输入数据集并生成输出数据，而不修改原始输入——换句话说，输出是从输入派生出来的。至关重要的是，输入数据是有界的：其大小是已知且固定的（例如，包含某时间点的日志文件或数据库快照）。因此，作业清楚何时完成了全部输入数据的读取，从而确保作业最终总是能够完成。

在接下来的一章中，我们将转向流处理领域，在那里输入是无界的——意味着有源源不断的实时数据流入系统。在这种场景下，作业永远不会真正结束，因为任何时候都可能接收到新的工作。尽管如此，在某些方面，流处理与批处理存在相似之处。然而，关于无限数据流的假设也对我们构建系统的方式带来了许多变化。

## 参考文献
1. Jeffrey Dean and Sanjay Ghemawat: “MapReduce: Simplified Data Processing on Large Clusters,” at *6th USENIX Symposium on Operating System Design and Implementation* (OSDI), December 2004.
...
[其余参考文献保持不变]