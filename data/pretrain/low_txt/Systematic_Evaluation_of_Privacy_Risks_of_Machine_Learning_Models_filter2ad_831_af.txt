### References

1. Michael Backes, Pascal Berrang, Mathias Humbert, and Praveen Manoharan. "Membership Privacy in MicroRNA-Based Studies." In *ACM Conference on Computer and Communications Security*, pages 319–330, 2016.

2. Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks." In *USENIX Security Symposium*, pages 267–284, 2019.

3. Nicholas Carlini and David Wagner. "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods." In *ACM Workshop on Artificial Intelligence and Security*, pages 3–14. ACM, 2017.

4. Rich Caruana, Steve Lawrence, and Lee Giles. "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping." In *Advances in Neural Information Processing Systems*, pages 402–408, 2001.

5. Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. "GAN-Leaks: A Taxonomy of Membership Inference Attacks Against GANs." In *NeurIPS Workshop on Privacy in Machine Learning*, 2019.

6. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "ImageNet: A Large-Scale Hierarchical Image Database." In *2009 IEEE Conference on Computer Vision and Pattern Recognition*, pages 248–255, 2009.

7. Cynthia Dwork. "Differential Privacy." In *33rd International Colloquium on Automata, Languages and Programming, Part II*. Springer Verlag, July 2006.

8. Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. "Calibrating Noise to Sensitivity in Private Data Analysis." In *Theory of Cryptography* (edited by Shai Halevi and Tal Rabin), pages 265–284, 2006.

9. Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. "Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures." In *ACM Conference on Computer and Communications Security*, 2015.

10. Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov. "Property Inference Attacks on Fully Connected Neural Networks Using Permutation Invariant Representations." In *ACM Conference on Computer and Communications Security*, pages 619–633, 2018.

11. Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. "LOGAN: Membership Inference Attacks Against Generative Models." In *Proceedings on Privacy Enhancing Technologies*, number 1, 2019.

12. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep Residual Learning for Image Recognition." In *IEEE Conference on Computer Vision and Pattern Recognition*, pages 770–778, 2016.

13. Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. "Adversarial Example Defense: Ensembles of Weak Defenses Are Not Strong." In *USENIX Workshop on Offensive Technologies*, 2017.

14. Benjamin Hilprecht, Martin Härterich, and Daniel Bernau. "Monte Carlo and Reconstruction Membership Inference Attacks Against Generative Models." In *Proceedings on Privacy Enhancing Technologies*, 2019.

15. Bargav Jayaraman and David Evans. "Evaluating Differentially Private Machine Learning in Practice." In *USENIX Security Symposium*, pages 1895–1912, 2019.

16. Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong. "MemGuard: Defending Against Black-Box Membership Inference Attacks via Adversarial Examples." In *ACM Conference on Computer and Communications Security*, 2019.

17. Alex Krizhevsky. "Learning Multiple Layers of Features from Tiny Images." 2009.

18. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. "ImageNet Classification with Deep Convolutional Neural Networks." In *Advances in Neural Information Processing Systems*, pages 1097–1105, 2012.

19. Anders Krogh and John A. Hertz. "A Simple Weight Decay Can Improve Generalization." In *Advances in Neural Information Processing Systems*, pages 950–957, 1992.

20. Klas Leino and Matt Fredrikson. "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference." *arXiv preprint arXiv:1906.11798*, 2019.

21. Changchang Liu, Xi He, Thee Chanyaswad, Shiqiang Wang, and Prateek Mittal. "Investigating Statistical Privacy Frameworks from the Perspective of Hypothesis Testing." In *Proceedings on Privacy Enhancing Technologies*, pages 233–254, 2019.

22. Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, and Kai Chen. "Understanding Membership Inferences on Well-Generalized Learning Models." *arXiv preprint arXiv:1802.04889*, 2018.

23. Laurens van der Maaten and Geoffrey Hinton. "Visualizing Data Using t-SNE." *Journal of Machine Learning Research*, 9(Nov):2579–2605, 2008.

24. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. "Towards Deep Learning Models Resistant to Adversarial Attacks." In *International Conference on Learning Representations*, 2018.

25. Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. "Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning." In *ACM Conference on Computer and Communications Security*, 2017.

26. H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. "Learning Differentially Private Recurrent Language Models." In *International Conference on Learning Representations*, 2018.

27. Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. "Densely Connected Convolutional Networks." In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 2017.

28. Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. "Exploiting Unintended Feature Leakage in Collaborative Learning." In *IEEE Symposium on Security and Privacy*, 2019.

29. Milad Nasr, Reza Shokri, and Amir Houmansadr. "Machine Learning with Membership Privacy Using Adversarial Regularization." In *ACM Conference on Computer and Communications Security*, 2018.

30. Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. "Machine Learning Models That Remember Too Much." In *ACM Conference on Computer and Communications Security*, pages 587–601, 2017.

31. Milad Nasr, Reza Shokri, and Amir Houmansadr. "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-Box Inference Attacks Against Centralized and Federated Learning." In *IEEE Symposium on Security and Privacy*, 2019.

32. Nicolas Papernot, Martín Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. "Semi-Supervised Knowledge Transfer for Deep Learning from Private Training Data." In *International Conference on Learning Representations*, 2017.

33. Lutz Prechelt. "Early Stopping—But When?" In *Neural Networks: Tricks of the Trade*. Springer, 1998.

34. Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. "Knock Knock, Who's There? Membership Inference on Aggregate Location Data." In *Network and Distributed Systems Security Symposium*, 2018.

35. Md Atiqur Rahman, Tanzila Rahman, Robert Laganière, Noman Mohammed, and Yang Wang. "Membership Inference Attack Against Differentially Private Deep Learning Model." *Transactions on Data Privacy*, 2018.

36. Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario Fritz, and Yang Zhang. "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning." In *USENIX Security Symposium*, 2020.

37. Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes. "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models." In *Network and Distributed Systems Security Symposium*, 2019.

38. Virat Shejwalkar and Amir Houmansadr. "Reconciling Utility and Membership Privacy via Knowledge Distillation." *arXiv preprint arXiv:1906.06589*, 2019.

39. Reza Shokri and Vitaly Shmatikov. "Privacy-Preserving Deep Learning." In *ACM Conference on Computer and Communications Security*, 2015.

40. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. "Membership Inference Attacks Against Machine Learning Models." In *IEEE Symposium on Security and Privacy*, pages 3–18, 2017.

41. Karen Simonyan and Andrew Zisserman. "Very Deep Convolutional Networks for Large-Scale Image Recognition." In *International Conference on Learning Representations*, 2015.

42. Liwei Song, Reza Shokri, and Prateek Mittal. "Privacy Risks of Securing Machine Learning Models Against Adversarial Examples." In *ACM Conference on Computer and Communications Security*, 2019.

43. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. "Dropout: A Simple Way to Prevent Neural Networks from Overfitting." *The Journal of Machine Learning Research*, 15(1):1929–1958, 2014.

44. Bingzhe Wu, Shiwan Zhao, ChaoChao Chen, Haoyang Xu, Li Wang, Xiaolu Zhang, Guangyu Sun, and Jun Zhou. "Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection." In *Advances in Neural Information Processing Systems*, 2019.

45. Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. "On Early Stopping in Gradient Descent Learning." *Constructive Approximation*, 26(2):289–315, 2007.

46. Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting." In *IEEE Computer Security Foundations Symposium*, 2018.

47. Sergey Zagoruyko and Nikos Komodakis. "Wide Residual Networks." In *Proceedings of the British Machine Vision Conference*, 2016.

### A. Membership Inference Attacks on Other Datasets

In this section, we perform membership inference attacks on two additional image datasets: CH-MNIST and Car196.

**CH-MNIST Dataset:**
- **Description:** The CH-MNIST dataset contains histology tiles from patients with colorectal cancer. It consists of 64×64 black-and-white images from 8 different classes of tissue, with a total of 5,000 samples.
- **Training Setup:** We use 2,000 data samples to train a convolutional neural network (CNN) with 2 convolution blocks, each with 32 and 64 output channels, respectively.
- **Performance:** The model achieves 99.0% training accuracy and 71.7% test accuracy.
- **Source:** [Kaggle](https://www.kaggle.com/kmader/colorectal-histology-mnist)

**Car196 Dataset:**
- **Description:** The Car196 dataset contains colored images of 196 classes of cars, split into 8,144 training images and 8,041 testing images.
- **Training Setup:** To achieve good accuracy, we use a pre-trained ResNet50 classifier on ImageNet and fine-tune it on the Car196 training set.
- **Performance:** The model achieves 99.3% training accuracy and 87.5% test accuracy.
- **Source:** [Stanford AI Lab](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)

### Table 7: Membership Inference Attacks on Image Datasets

| Dataset | Attack Accuracy (NN-based) | Attack Accuracy (Icorr) | Attack Accuracy (Iconf) | Attack Accuracy (Ientr) | Attack Accuracy (IMentr) |
|---------|----------------------------|-------------------------|-------------------------|-------------------------|--------------------------|
| CH-MNIST | 70.5%                      | -                       | -                       | -                       | -                        |
| Car196   | 63.1%                      | 63.7%                   | 55.9%                   | 72.6%                   | 63.7%                    |

### B. Privacy Risk Score with Different Training/Test Selection Probabilities

We present the distribution of privacy risk scores for the undefended Purchase100 classifier, considering different probabilities of selecting a sample from the training or test set. The computation of the privacy risk score \( r(z) \) is as described in Section 4.1, with the addition of prior distributions \( P(z \in D_{tr}) \) and \( P(z \in D_{te}) = 1 - P(z \in D_{tr}) \).

**Figure 10:**
- **(a) \( P(z \in D_{tr}) = 0.1 \)**
- **(b) \( P(z \in D_{tr}) = 0.3 \)**
- **(c) \( P(z \in D_{tr}) = 0.7 \)**
- **(d) \( P(z \in D_{tr}) = 0.9 \)**

In all cases, most training samples have privacy risk scores larger than the prior training probability. We compute a distance value between the prior distribution and the privacy risk score (posterior) distribution as:

\[ \text{Distance} = \frac{1}{|D_{tr}|} \sum_{z \in D_{tr}} (r(z) - P(z \in D_{tr})) \]

The distance values are:
- 0.05 when \( P(z \in D_{tr}) = 0.1 \)
- 0.09 when \( P(z \in D_{tr}) = 0.3 \)
- 0.07 when \( P(z \in D_{tr}) = 0.7 \)
- 0.02 when \( P(z \in D_{tr}) = 0.9 \)

For comparison, the distance value is 0.1 when \( P(z \in D_{tr}) = 0.5 \). As \( P(z \in D_{tr}) \) approaches 0.5, the uncertainty of membership inference increases.

### C. Validation of Privacy Risk Score on Texas100 Classifiers

**Figure 11:**
- **Left:** Estimate the real probability of being a member using our proposed privacy risk score.
- **Right:** Estimate the real probability of being a member using the output of the NN attack classifier.

The root-mean-square errors (RMSE) values of our privacy risk score are 0.08 and 0.05, while the RMSE values of the NN attack classifier’s output are 0.13 and 0.21 for the undefended and defended Texas100 classifiers, respectively.

### D. Validation of Privacy Risk Scores on Different Model Architectures

**Figure 12:**
- **Left:** Validation of privacy risk score with varied model architectures on defended Purchase100 classifiers [31].
- **Right:** Validation of privacy risk score with varied model architectures on defended Texas100 classifiers [20].

The legend is expressed as (activation function, width, depth). The RMSE values between the privacy risk score (x-axis) and the probability of being a member (y-axis) for all lines are smaller than 0.10.

This indicates that the privacy risk score is a more meaningful indicator of the real probability of being a member compared to the output of the NN attack classifier.