### Impact of the First Heuristic Employed by CRASHFINDER DYNAMIC

The heuristic used for selecting bit positions for selective fault injection in CRASHFINDER DYNAMIC involves picking two random positions within a word: one from the high-level bits and one from the low-level bits. This approach, however, may overlook other bit positions that could lead to long-latency crashes (LLCs). We evaluated the effect of increasing the number of sampled bits to 3 and 5, but even this did not significantly increase the number of LLCs detected by CRASHFINDER. The primary reason is that many of the missed errors can only be reproduced by injecting faults into very specific bit positions, which would require nearly exhaustive fault injections on the words identified by CRASHFINDER DYNAMIC. Such an exhaustive approach would prohibitively increase the time required for the selective fault injection phase. Therefore, we decided to retain the current heuristic, especially since the difference in performance between CRASHFINDER STATIC and CRASHFINDER DYNAMIC is only 2.33%.

It is important to note that the heuristic-based approach used here is an approximation, and there may be multiple sources of inaccuracy. We plan to further quantify the limitations of this heuristic-based approach in future work.

### IX. Discussion

#### A. Implications for Selective Protection

One of the key findings from evaluating CRASHFINDER is that a very small number of instructions are responsible for most of the LLCs in a program. According to Table II, only 0.89% of static instructions account for more than 90% of the LLC-causing errors (based on the recall of CRASHFINDER). Furthermore, CRASHFINDER can precisely identify these instructions, allowing them to be selectively protected.

An example of a selective protection technique is value range checking in software [10]. A range check is typically inserted after the instruction that produces the data item to be checked. For instance, the assertion `ptr_address < 0x001b` inserted after the static instruction producing `ptr_address` will check the value of the variable each time the instruction is executed. Given that the total number of executions of all such LLC-causing instructions is only 0.385% (Table II), the overhead of these checks is likely to be extremely low. We will explore this direction in future work.

#### B. Implications for Checkpointing Techniques

Our study also establishes the feasibility of fine-grained checkpointing techniques for programs, as such techniques would otherwise incur frequent state corruptions in the presence of LLCs. For example, Chandra et al. [6] found that the frequency of checkpoint corruption when using a fine-grained checkpointing technique ranges between 25% and 40% due to LLCs. They concluded that one should avoid using such fine-grained checkpointing techniques and instead use application-specific coarse-grained checkpointing, where the probability of checkpoint corruption is 1% to 19%. However, by deploying our technique and selectively protecting the LLC-causing locations in the program, we can minimize the chances of checkpoint corruption. Based on the 90% recall of CRASHFINDER, we can achieve a 10-fold reduction in the number of LLC-causing locations, thereby reducing the checkpoint corruption probability of fine-grained checkpointing. This would make fine-grained checkpointing feasible, allowing for faster recovery from errors. This is another direction we plan to explore in the future.

#### C. Limitations and Improvements

One of the main limitations of CRASHFINDER is its long execution time (on average 4 days) to find LLC-causing errors in a program. The bulk of this time is spent on the selective fault injection phase, which injects faults into thousands of dynamic instances found by CRASHFINDER DYNAMIC to determine if they are LLCs. While this is still orders of magnitude faster than performing exhaustive fault injections, it remains a relatively high one-time cost to protect the program. One way to speed this up would be to parallelize the process, but this comes at the cost of increased computational resources.

Another way to improve the technique is to enhance the precision of CRASHFINDER STATIC. Currently, CRASHFINDER STATIC takes only a few seconds to analyze even large programs and find LLC-causing locations. However, its precision is quite low (25.4%). In some cases, this may be acceptable, as we can protect a few more locations and incur higher overheads. Even with this over-protection, we still only protect less than 6% of the program's dynamic instructions (Table II). The precision can be further improved by identifying all possible aliases and control flow paths at compile time [22], and filtering out patterns unlikely to cause LLCs.

Another limitation is that the recall of CRASHFINDER is only about 90%. Although this is a significant recall, it can be improved by (1) building a more comprehensive static analyzer to cover uncovered cases that do not belong to the dominant LLC-causing patterns, and (2) improving the heuristic used in the selective fault injection phase by increasing the number of fault injections, albeit at the cost of increased performance overheads (as we found in RQ4, this heuristic was responsible for most of the difference in recall between CRASHFINDER and CRASHFINDER STATIC).

Finally, while the benchmark applications are chosen from a variety of domains such as scientific computing, multimedia, statistics, and games, other domains like database programs or system software applications are not covered. Additionally, all the benchmarks are single-node applications. We defer the extension of CRASHFINDER for distributed applications to future work.

### X. Conclusion and Future Work

In this paper, we address an important but often neglected problem in the design of dependable software systems: identifying faults that propagate for a long time before causing crashes, or LLCs. Unlike prior work, which has only performed a coarse-grained analysis of such faults, we conduct a fine-grained characterization of LLCs. We find that there are only three code patterns in the program responsible for almost all LLCs, and these patterns can be efficiently identified through static analysis. We build a static analysis technique to find these patterns and augment it with a dynamic analysis and selective fault-injection based technique to filter out false positives. We implement our technique in a fully automated tool called CRASHFINDER. Our results show that CRASHFINDER achieves a 9-order-of-magnitude speedup over exhaustive fault injections to identify LLCs, has no false positives, and successfully identifies over 90% of the LLC-causing locations in ten benchmark programs.

For future work, we plan to (1) apply the results of CRASHFINDER to selectively protect LLC-causing locations and measure the overhead, (2) combine CRASHFINDER with fine-grained checkpointing methods to achieve fast recovery in the case of crashes, and (3) reduce the performance overhead of CRASHFINDER and improve its accuracy with more sophisticated static analysis.

### XI. Acknowledgment

We thank the anonymous reviewers of DSN’15 and Keun Soo Yim for their comments that helped improve the paper. This work was supported in part by a Discovery Grant from the Natural Science and Engineering Research Council (NSERC), Canada, and an equipment grant from the Canada Foundation for Innovation (CFI). We also thank the Institute of Computing, Information and Cognitive Systems (ICICS) at the University of British Columbia for travel support.

### References

[1] C. Basile, L. Wang, Z. Kalbarczyk, and R. Iyer. Group communication protocols under errors. In Reliable Distributed Systems. Proceedings. 22nd International Symposium on, pages 35–44, Oct 2003.

[2] C. Bienia, S. Kumar, J. P. Singh, and K. Li. The PARSEC benchmark suite: Characterization and architectural implications. In Proceedings of the 17th international conference on Parallel architectures and compilation techniques, pages 72–81. ACM, 2008.

[3] S. Borkar. Designing reliable systems from unreliable components: the challenges of transistor variability and degradation. Micro, IEEE, 25(6):10–16, 2005.

[4] S. Borkar. Electronics beyond nano-scale CMOS. In Proceedings of the 43rd annual Design Automation Conference, pages 807–808. ACM, 2006.

[5] S. Chandra and P. M. Chen. How fail-stop are faulty programs? In Fault-Tolerant Computing. Digest of Papers. Twenty-Eighth Annual International Symposium on, pages 240–249. IEEE, 1998.

[6] S. Chandra and P. M. Chen. The impact of recovery mechanisms on the likelihood of saving corrupted state. In Proceedings of the 13th International Symposium on Software Reliability Engineering, ISSRE., pages 91–101. IEEE, 2002.

[7] C. Constantinescu. Intermittent faults and effects on reliability of integrated circuits. In Reliability and Maintainability Symposium, pages 370–374. IEEE, 2008.

[8] S. Feng, S. Gupta, A. Ansari, and S. Mahlke. Shoestring: probabilistic soft error reliability on the cheap. In ACM SIGARCH Computer Architecture News, volume 38, pages 385–396. ACM, 2010.

[9] W. Gu, Z. Kalbarczyk, R. K. Iyer, and Z. Yang. Characterization of Linux kernel behavior under errors. In International Conference on Dependable Systems and Networks. IEEE Computer Society, 2003.

[10] S. K. S. Hari, S. V. Adve, and H. Naeimi. Low-cost program-level detectors for reducing silent data corruptions. In Dependable Systems and Networks (DSN), 42nd Annual IEEE/IFIP International Conference on, pages 1–12. IEEE, 2012.

[11] S. K. S. Hari, S. V. Adve, H. Naeimi, and P. Ramachandran. Relyzer: Exploiting application-level fault equivalence to analyze application resiliency to transient faults. In ACM SIGARCH Computer Architecture News, volume 40, pages 123–134. ACM, 2012.

[12] S. K. S. Hari, R. Venkatagiri, S. V. Adve, and H. Naeimi. Ganges: Gang error simulation for hardware resiliency evaluation. In Dependable Systems and Networks (DSN), 42nd Annual IEEE/IFIP International Conference on, pages 1–12. IEEE, 2012.

[13] J. L. Henning. SPEC CPU2000: Measuring CPU performance in the new millennium. Computer, 33(7):28–35, 2000.

[14] A. Lanzaro, R. Natella, S. Winter, D. Cotroneo, and N. Suri. An empirical study of injected versus actual interface errors. In Proceedings of the International Symposium on Software Testing and Analysis, pages 397–408. ACM, 2014.

[15] C. Lattner and V. Adve. LLVM: A compilation framework for lifelong program analysis & transformation. In Code Generation and Optimization. International Symposium on, pages 75–86. IEEE, 2004.

[16] S. Liu, K. Pattabiraman, T. Moscibroda, and B. G. Zorn. Flikker: Saving DRAM refresh-power through critical data partitioning. In Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems, pages 213–224. ACM, 2011.

[17] Q. Lu, K. Pattabiraman, M. S. Gupta, and J. A. Rivers. SDCTune: A model for predicting the SDC proneness of an application for configurable protection. In Proceedings of the International Conference on Compilers, Architecture and Synthesis for Embedded Systems, page 23. ACM, 2014.

[18] S. Narayanan, J. Sartori, R. Kumar, and D. L. Jones. Scalable stochastic processors. In Proceedings of the Conference on Design, Automation and Test in Europe, pages 335–338. European Design and Automation Association, 2010.

[19] N. Oh, P. P. Shirvani, and E. J. McCluskey. Error detection by duplicated instructions in super-scalar processors. Reliability, IEEE Transactions on, 51(1):63–75, 2002.

[20] L. Rashid, K. Pattabiraman, and S. Gopalakrishnan. Modeling the propagation of intermittent hardware faults in programs. In Dependable Computing (PRDC), IEEE 16th Pacific Rim International Symposium on, pages 19–26. IEEE, 2010.

[21] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. I. August. SWIFT: Software implemented fault tolerance. In Proceedings of the international symposium on Code generation and optimization, pages 243–254. IEEE Computer Society, 2005.

[22] V. Robert and X. Leroy. A formally-verified alias analysis. In Certified Programs and Proofs, pages 11–26. Springer, 2012.

[23] A. Sampson, W. Dietl, E. Fortuna, D. Gnanapragasam, L. Ceze, and D. Grossman. EnerJ: Approximate data types for safe and general low-power computation. In ACM SIGPLAN Notices, volume 46, pages 164–174. ACM, 2011.

[24] S. K. Sastry Hari, M.-L. Li, P. Ramachandran, B. Choi, and S. V. Adve. MSWAT: Low-cost hardware fault detection and diagnosis for multicore systems. In Proceedings of the 42nd Annual IEEE/ACM International Symposium on Microarchitecture, pages 122–132. ACM, 2009.

[25] J. A. Stratton, C. Rodrigues, I.-J. Sung, N. Obeid, L.-W. Chang, N. Anssari, G. D. Liu, and W.-m. W. Hwu. Parboil: A revised benchmark suite for scientific and commercial throughput computing. Center for Reliable and High-Performance Computing, 2012.

[26] A. Thomas and K. Pattabiraman. Error detector placement for soft computation. In Dependable Systems and Networks (DSN), 43rd Annual IEEE/IFIP International Conference on, pages 1–12. IEEE, 2013.

[27] N. J. Wang and S. J. Patel. RESTORE: Symptom-based soft error detection in microprocessors. Dependable and Secure Computing, IEEE Transactions on, 3(3):188–201, 2006.

[28] J. Wei, A. Thomas, G. Li, and K. Pattabiraman. Quantifying the accuracy of high-level fault injection techniques for hardware faults. In Dependable Systems and Networks (DSN), 44rd Annual IEEE/IFIP International Conference on, 2014.

[29] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. The SPLASH-2 programs: Characterization and methodological considerations. In ACM SIGARCH Computer Architecture News, volume 23, pages 24–36. ACM, 1995.

[30] K. S. Yim, Z. T. Kalbarczyk, and R. K. Iyer. Quantitative analysis of long-latency failures in system software. In Dependable Computing, PRDC’09. 15th IEEE Pacific Rim International Symposium on, pages 23–30. IEEE, 2009.