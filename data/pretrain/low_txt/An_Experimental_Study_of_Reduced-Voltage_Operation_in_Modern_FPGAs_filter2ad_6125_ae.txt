以下是优化后的参考文献列表，使其更加清晰、连贯和专业：

1. ZedBoard. (2020). Retrieved from http://zedboard.org/product/zedboard

2. Bacha, A., et al. (2013). Dynamic reduction of voltage margins by leveraging on-chip ECC in Itanium II processors. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

3. Bacha, A., et al. (2014). Using ECC feedback to guide voltage speculation in low-voltage processors. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

4. Bertran, R., et al. (2014). Voltage noise in multi-core processors: Empirical characterization and optimization opportunities. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

5. Borkar, S., et al. (1999). Design challenges of technology scaling. *IEEE Micro*.

6. Boutros, A., et al. (2018). You cannot improve what you do not measure: FPGA vs. ASIC efficiency gaps for convolutional neural network inference. *ACM Transactions on Reconfigurable Technology and Systems (TRETS)*.

7. Brewer, R., et al. (2019). The impact of proton-induced single events on image classification in a neuromorphic computing architecture. *Transactions on Nuclear Science (TNS)*.

8. Cai, Y., et al. (2013). Threshold voltage distribution in MLC NAND flash memory: Characterization, analysis, and modeling. In *Design, Automation & Test in Europe Conference & Exhibition (DATE)*.

9. Cai, Y., et al. (2015). Read disturb errors in MLC NAND flash memory: Characterization, mitigation, and recovery. In *Dependable Systems and Networks (DSN)*.

10. Cai, Y., et al. (2017). Error characterization, mitigation, and recovery in flash-memory-based solid-state drives. *Proceedings of the IEEE*.

11. Chandramoorthy, N., et al. (2019). Resilient low voltage accelerators for high energy efficiency. In *High-Performance Computer Architecture (HPCA)*.

12. Chang, K., et al. (2017). Understanding reduced-voltage operation in modern DRAM devices: Experimental characterization, analysis, and mechanisms. *SIGMETRICS Performance Evaluation Review*.

13. Chang, K., et al. (2018). Voltron: Understanding and exploiting the voltage-latency-reliability trade-offs in modern DRAM chips to improve energy efficiency. arXiv:1805.03175.

14. Chatzidimitriou, A., et al. (2019). Assessing the effects of low voltage in branch prediction units. In *International Symposium on Performance Analysis of Systems and Software (ISPASS)*.

15. Chen, T., et al. (2014). DianNao: A small-footprint high-throughput accelerator for ubiquitous machine learning. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

16. Chen, Y., et al. (2016). Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

17. Chishti, Z., et al. (2009). Improving cache lifetime reliability at ultra-low voltages. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

18. Cristal, A., et al. (2018). LEGaTO: First steps towards an energy-efficient toolset for heterogeneous computing. In *Samos Conference on Embedded Systems (SAMOS)*.

19. Cristal, A., et al. (2018). LEGaTO: Towards energy-efficient, secure, fault-tolerant toolset for heterogeneous computing. In *Conference on Fault-Tolerant Computing (CF)*.

20. Deng, C., et al. (2018). PermDNN: Efficient compressed DNN architecture with permuted diagonal matrices. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

21. Ernst, D., et al. (2003). Razor: A low-power pipeline based on circuit-level timing speculation. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

22. Feldman, M. (2019). Good times for FPGA enthusiasts. Retrieved from https://www.top500.org/news/good-times-for-fpga-enthusiasts/

23. Givaki, K., et al. (2020). On the resilience of deep learning for reduced-voltage FPGAs. In *Parallel, Distributed and Network-Based Processing (PDP)*.

24. Gizopoulos, D., et al. (2019). Modern hardware margins: CPUs, GPUs, FPGAs recent system-level studies. In *IEEE International On-Line Testing Symposium (IOLTS)*.

25. Guo, K., et al. (2019). A survey of FPGA-based neural network accelerators. *Journal of Signal Processing Systems*.

26. Han, S., et al. (2015). Learning both weights and connections for efficient neural networks. In *Neural Information Processing Systems (NIPS)*.

27. Han, S., et al. (2016). EIE: Efficient inference engine on compressed deep neural networks. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

28. He, K., et al. (2016). Deep residual learning for image recognition. In *Computer Vision and Pattern Recognition (CVPR)*.

29. He, Y., et al. (2017). Channel pruning for accelerating very deep neural networks. In *International Conference on Computer Vision (ICCV)*.

30. Hill, P., et al. (2017). DeftNN: Addressing bottlenecks for DNN execution on GPUs via synapse vector elimination and near-compute data fission. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

31. Himanshu, K., et al. (2009). A 320 mV 56 μW 411 GOPs/W Ultra-Low Voltage Motion Estimation Accelerator in 65 nm CMOS. *IEEE Journal of Solid-State Circuits (JSSC)*.

32. Huang, W., et al. (2011). Temperature-aware architecture: Lessons and opportunities. *IEEE Micro*.

33. Jha, S., et al. (2019). Kayotee: A fault injection-based system to assess the safety and reliability of autonomous vehicles to faults and errors. arXiv:1907.01024.

34. Jha, S., et al. (2019). ML-based fault injection for autonomous vehicles: A case for Bayesian fault injection. In *Dependable Systems and Networks (DSN)*.

35. Jouppi, N., et al. (2017). In-datacenter performance analysis of a tensor processing unit. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

36. Kaliorakis, M., et al. (2018). Statistical analysis of multicore CPUs operation in scaled voltage conditions. *Calibration and Linearization Algorithms (CAL)*.

37. Karandikar, S., et al. (2018). FireSim: FPGA-accelerated cycle-exact scale-out system simulation in the public cloud. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

38. Khaleghi, B., et al. (2019). FPGA energy efficiency by leveraging thermal margin. arXiv:1911.07187.

39. Khorasani, F., et al. (2018). In-register parameter caching for dynamic neural nets with virtual persistent processor specialization. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

40. Kim, N. S., et al. (2003). Leakage current: Moore’s law meets static power. *IEEE Computer*.

41. Kim, S., et al. (2018). MATIC: Learning around errors for efficient low-voltage neural network accelerators. In *Design, Automation & Test in Europe Conference & Exhibition (DATE)*.

42. Kim, Y. D., et al. (2015). Compression of deep convolutional neural networks for fast and low power mobile applications. arXiv:1511.06530.

43. Koppula, S., et al. (2019). EDEN: Enabling energy-efficient, high-performance deep neural network inference using approximate DRAM. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

44. Krizhevsky, A., et al. (2012). ImageNet classification with deep convolutional neural networks. In *Neural Information Processing Systems (NIPS)*.

45. Lavin, A., et al. (2016). Fast algorithms for convolutional neural networks. In *Computer Vision and Pattern Recognition (CVPR)*.

46. LeCun, Y., et al. (2015). Deep learning. *Nature*.

47. Lee, S. K., et al. (2019). A 16-nm always-on DNN processor with adaptive clocking and multi-cycle banked SRAMs. *IEEE Journal of Solid-State Circuits (JSSC)*.

48. Leng, J., et al. (2015). GPU voltage noise: Characterization and hierarchical smoothing of spatial and temporal voltage noise interference in GPU architectures. In *High-Performance Computer Architecture (HPCA)*.

49. Leng, J., et al. (2015). Safe limits on voltage reduction efficiency in GPUs: A direct measurement approach. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

50. Leng, J., et al. (2020). Asymmetric resilience: Exploiting task-level idempotency for transient error recovery in accelerator-based systems. In *High-Performance Computer Architecture (HPCA)*.

51. Li, G., et al. (2017). Understanding error propagation in deep learning neural network (DNN) accelerators and applications. In *Supercomputing (SC)*.

52. Li, H., et al. (2019). On-chip memory technology design space explorations for mobile deep neural network accelerators. In *Design Automation Conference (DAC)*.

53. Li, Z., et al. (2019). E-RNN: Design optimization for efficient recurrent neural networks in FPGAs. In *High-Performance Computer Architecture (HPCA)*.

54. Libano, F., et al. (2018). Selective hardening for neural networks in FPGAs. *Transactions on Nuclear Science (TNS)*.

55. Libano, F., et al. (2020). Understanding the impact of quantization, accuracy, and radiation on the reliability of convolutional neural networks on FPGAs. *Transactions on Nuclear Science (TNS)*.

56. Liu, Y., et al. (2019). Fault injection attack on deep neural network. In *International Conference on Computer-Aided Design (ICCAD)*.

57. Ma, Y., et al. (2018). Optimizing the convolution operation to accelerate deep neural networks on FPGA. *IEEE Transactions on Very Large Scale Integration (VLSI) Systems (TVLSI)*.

58. MaxIntegrated. (2019). Retrieved from https://www.maximintegrated.com

59. Miller, T., et al. (2012). VRSync: Characterizing and eliminating synchronization-induced voltage emergencies in many-core processors. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

60. Molchanov, P., et al. (2016). Pruning convolutional neural networks for resource efficient inference. arXiv:1611.06440.

61. Moons, B., et al. (2017). A 0.26-to-10 TOPs/W subword-parallel dynamic-voltage-accuracy-frequency-scalable convolutional neural network processor in 28nm FDSOI. In *International Solid-State Circuits Conference (ISSCC)*.

62. Fowers, J., et al. (2018). A configurable cloud-scale DNN processor for real-time AI. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

63. Moradi, A., et al. (2014). Side-channel leakage through static power. In *Cryptographic Hardware and Embedded Systems (CHES)*.

64. Mottaghi, M. H., et al. (2019). Aging mitigation in FPGAs considering delay, power, and temperature. *Technical Report (TR)*.

65. Nakahara, H., et al. (2017). A batch normalization free binarized convolutional deep neural network on an FPGA. In *Field-Programmable Custom Computing Machines (FPGA)*.

66. Neshatpour, K., et al. (2018). Enhancing power, performance, and energy efficiency in chip multiprocessors exploiting inverse thermal dependence. *IEEE Transactions on Very Large Scale Integration (VLSI) Systems (TVLSI)*.

67. Nurvitadhi, E., et al. (2016). Accelerating binarized neural networks: Comparison of FPGA, CPU, GPU, and ASIC. In *Field-Programmable Technology (FPT)*.

68. Nurvitadhi, E., et al. (2019). Why compete when you can work together: FPGA-ASIC integration for persistent RNNs. In *Field-Programmable Custom Computing Machines (FCCM)*.

69. Pandey, P., et al. (2019). GreenTPU: Improving timing error resilience of a near-threshold tensor processing unit. In *Design Automation Conference (DAC)*.

70. Papadimitriou, G., et al. (2017). Harnessing voltage margins for energy efficiency in multicore CPUs. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

71. Papadimitriou, G., et al. (2017). Voltage margins identification on commercial x86-64 multicore microprocessors. In *IEEE International On-Line Testing Symposium (IOLTS)*.

72. Papadimitriou, G., et al. (2019). Adaptive voltage/frequency scaling and core allocation for balanced energy and performance on multicore CPUs. In *High-Performance Computer Architecture (HPCA)*.

73. Papadimitriou, G., et al. (2020). Exceeding conservative limits: A consolidated analysis on modern hardware margins. *IEEE Transactions on Device and Materials Reliability (TDMR)*.

74. Parashar, A., et al. (2017). SCNN: An accelerator for compressed-sparse convolutional neural networks. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

75. Parasyris, K., et al. (2018). A framework for evaluating software on reduced margins hardware. In *Dependable Systems and Networks (DSN)*.

76. Park, J., et al. (2017). Scale-out acceleration for machine learning. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

77. Power Management Bus (PMBus). (2020). Retrieved from https://pmbus.org/

78. Putnam, A., et al. (2014). A reconfigurable fabric for accelerating large-scale datacenter services. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

79. Qiu, J., et al. (2016). Going deeper with embedded FPGA platform for convolutional neural network. In *Field-Programmable Custom Computing Machines (FPGA)*.

80. Reagen, B., et al. (2016). Minerva: Enabling low-power, highly-accurate deep neural network accelerators. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

81. Reagen, B., et al. (2018). Ares: A framework for quantifying the resilience of deep neural networks. In *Design Automation Conference (DAC)*.

82. Riera, M., et al. (2018). Computation reuse in DNNs by exploiting input similarity. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

83. Roelke, A., et al. (2017). Pre-RTL voltage and power optimization for low-cost, thermally challenged multicore chips. In *International Conference on Computer Design (ICCD)*.

84. Salamat, S., et al. (2019). Workload-aware opportunistic energy efficiency in multi-FPGA platforms. arXiv:1908.06519.

85. Salami, B. (2018). Aggressive undervolting of FPGAs: Power & reliability trade-offs. Ph.D. Dissertation, Universitat Politècnica de Catalunya (UPC).

86. Salami, B., et al. (2015). HATCH: Hash table caching in hardware for efficient relational join on FPGA. In *Field-Programmable Custom Computing Machines (FCCM)*.

87. Salami, B., et al. (2016). Accelerating hash-based query processing operations on FPGAs by a hash table caching technique. In *Conference on Architectures and Mechanisms for Language Processing (CARLA)*.

88. Salami, B., et al. (2017). AxleDB: A novel programmable query processing platform on FPGA. *Microprocessors and Microsystems (MICPRO)*.

89. Salami, B., et al. (2018). A demo of FPGA aggressive voltage downscaling: Power and reliability tradeoffs. In *Field-Programmable Logic and Applications (FPL)*.

90. Salami, B., et al. (2018). Comprehensive evaluation of supply voltage under-scaling in FPGA on-chip memories. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

91. Salami, B., et al. (2018). Fault characterization through FPGA undervolting. In *Field-Programmable Logic and Applications (FPL)*.

92. Salami, B., et al. (2018). On the resilience of RTL NN accelerators: Fault characterization and mitigation. In *Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)*.

93. Salami, B., et al. (2019). Evaluating built-in ECC of FPGA on-chip memories for the mitigation of undervolting faults. In *Parallel, Distributed and Network-Based Processing (PDP)*.

94. Salami, B., et al. (2020). LEGaTO: Low-energy, secure, and resilient toolset for heterogeneous computing. In *Design, Automation & Test in Europe Conference & Exhibition (DATE)*.

95. Sharma, H., et al. (2016). From high-level deep neural models to FPGAs. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

96. Sharma, H., et al. (2018). Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

97. Shen, L., et al. (2019). Fast voltage transients on FPGAs: Impact and mitigation strategies. In *Field-Programmable Custom Computing Machines (FCCM)*.

98. Shen, Y., et al. (2017). Escher: A CNN accelerator with flexible buffering to minimize off-chip transfer. In *Field-Programmable Custom Computing Machines (FCCM)*.

99. Shen, Y., et al. (2019). Maximizing CNN accelerator efficiency through re-source partitioning. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

100. Simonyan, K., et al. (2014). Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556.

101. Suda, N., et al. (2016). Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks. In *Field-Programmable Custom Computing Machines (FPGA)*.

102. Swaminathan, K., et al. (2017). Bravo: Balanced reliability-aware voltage optimization. In *High-Performance Computer Architecture (HPCA)*.

103. Sze, V., et al. (2017). Efficient processing of deep neural networks: A tutorial and survey. *Proceedings of the IEEE*.

104. Szegedy, C., et al. (2015). Going deeper with convolutions. In *Computer Vision and Pattern Recognition (CVPR)*.

105. Tang, Z., et al. (2019). The impact of GPU DVFS on the energy and performance of deep learning: An empirical study. In *ACM e-Energy*.

106. Trindade, M. G., et al. (2019). Assessment of a hardware-implemented machine learning technique under neutron irradiation. *Transactions on Nuclear Science (TNS)*.

107. Uht, A., et al. (2004). Going beyond worst-case specs with TEAtime. *IEEE Computer*.

108. Vaishnav, A., et al. (2018). A survey on FPGA virtualization. In *Field-Programmable Logic and Applications (FPL)*.

109. Wang, X., et al. (2019). Bit prudent in-cache acceleration of deep convolutional neural networks. In *High-Performance Computer Architecture (HPCA)*.

110. Whatmough, P., et al. (2017). 14.3 A 28nm SoC with a 1.2 GHz 568nJ/prediction sparse deep-neural-network engine with > 0.1 timing error rate tolerance for IoT applications. In *International Solid-State Circuits Conference (ISSCC)*.

111. Whatmough, P., et al. (2018). DNN Engine: A 28-nm timing-error tolerant sparse deep neural network processor for IoT applications. *IEEE Journal of Solid-State Circuits (JSSC)*.

112. Wilkerson, C., et al. (2008). Trading off cache capacity for reliability to enable low voltage operation. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

113. Wilkerson, C., et al. (2010). Reducing cache power with low-cost, multi-bit error-correcting codes. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

114. Xiao, Q., et al. (2017). Exploring heterogeneous algorithms for accelerating deep convolutional neural networks on FPGAs. In *Design Automation Conference (DAC)*.

115. Xilinx. (2019). 7 Series FPGAs Memory Resources. Retrieved from https://www.xilinx.com/support/documentation/user_guides/ug473_7Series_Memory_Resources.pdf

116. Xilinx. (2019). DNNDK Guide. Retrieved from https://www.xilinx.com/support/documentation/user_guides/ug1327-dnndk-user-guide.pdf

117. Xilinx. (2019). DPU User Guide. Retrieved from https://www.xilinx.com/support/documentation/ip_documentation/dpu/v3_1/pg338-dpu.pdf

118. Xilinx. (2019). UltraScale Architecture Memory Resources. Retrieved from https://www.xilinx.com/support/documentation/user_guides/ug573-ultrascale-memory-resources.pdf

119. Xilinx. (2019). Zynq UltraScale+ MPSoC ZCU102 Evaluation Kit. Retrieved from https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html

120. Yalcin, G., et al. (2014). Exploiting a fast and simple ECC for scaling supply voltage in Level-1 caches. In *IEEE International On-Line Testing Symposium (IOLTS)*.

121. Yalcin, G., et al. (2016). Exploring energy reduction in future technology nodes via voltage scaling with application to 10nm. In *Parallel, Distributed and Network-Based Processing (PDP)*.

122. Yang, L., et al. (2017). SRAM voltage scaling for energy-efficient convolutional neural networks. In *International Symposium on Quality Electronic Design (ISQED)*.

123. Yazdani, R., et al. (2018). The dark side of DNN pruning. In *Proceedings of the International Symposium on Computer Architecture (ISCA)*.

124. Yufei, M., et al. (2017). Optimizing loop operation and dataflow in FPGA acceleration of deep convolutional neural networks. In *Field-Programmable Custom Computing Machines (FPGA)*.

125. Zhang, C., et al. (2015). Optimizing FPGA-based accelerator design for deep convolutional neural networks. In *Field-Programmable Custom Computing Machines (FPGA)*.

126. Zhang, J., et al. (2018). Thundervolt: Enabling aggressive voltage underscaling and timing error resilience for energy efficient deep learning accelerators. In *Design Automation Conference (DAC)*.

127. Zhang, J. J., et al. (2018). Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator. In *VLSI Test Symposium (VTS)*.

128. Zhang, S., et al. (2016). Cambricon-X: An accelerator for sparse neural networks. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

129. Zhang, X., et al. (2018). Shufflenet: An extremely efficient convolutional neural network for mobile devices. In *Computer Vision and Pattern Recognition (CVPR)*.

130. Zhou, A., et al. (2017). Incremental network quantization: Towards lossless CNNs with low-precision weights. arXiv:1702.03044.

131. Zhu, Z., et al. (2019). A configurable multi-precision CNN computing framework based on single bit RRAM. In *Design Automation Conference (DAC)*.

132. Zou, A., et al. (2018). Voltage-stacked GPUs: A control theory driven cross-layer solution for practical voltage stacking in GPUs. In *Proceedings of the International Symposium on Microarchitecture (MICRO)*.

---

这个优化后的参考文献列表更加清晰、连贯，并且符合学术写作的标准。希望这对你有帮助！