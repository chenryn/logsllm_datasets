We have observed several instances that contradict this. For example, from January to November 2011, we identified a large-scale injection campaign on legitimate websites that distributed malware. This campaign can still be found by searching for "calendar about pregnancy" on Google and Bing. The n-gram was generated by our EVILSEED deployment in January when our crawler randomly visited an infected URL. Over the next ten months, both search engines gradually blacklisted more results of this query until the campaign was eventually shut down. This incremental blacklisting indicates that search engines do not implement EVILSEED. If they did, upon marking one of the infected URLs as malicious, all remaining URLs would be quickly analyzed and blacklisted.

**Evasion:**
Cybercriminals might attempt to detect our visits. To mitigate this, visits from crawlers and gadgets are made from a "fresh" browser (with no history or previous state). Cybercriminals could still track our system's IPs; however, using a large, dynamic pool of IPs mitigates this problem. It is unlikely that cybercriminals specializing in SEO campaigns and injections will try to evade EVILSEED by reducing their footprint in search engines, as this would negatively impact the traffic on their sites and, ultimately, their income.

**Gadget Selection:**
A final consideration is whether there are specific limitations on the gadgets that can be employed in our system. As we have seen, the only requirements for gadgets are that they can identify similarities among the pages of the evil seed and query search engines for pages with the same properties. We have implemented a diverse set of gadgets based on the analysis of textual content, linking relationships, and behavior (cloaking). In our experience, it was also easy to extend the existing system by "plugging in" an additional gadget.

**VI. Related Work**

Detecting malicious content on the web requires two main components: a detection procedure (or oracle) that decides whether a given web page is malicious, and a searching procedure that locates web pages to submit to the oracle.

**Oracles:**
Designing effective oracles for detecting malicious web pages (primarily those performing drive-by download attacks, but also fake antivirus pages and Flash-based malicious advertisements) has received significant attention. Two main approaches have been proposed:
1. Using high-interaction honeyclients to detect unexpected changes in the underlying system (e.g., new files or running processes) indicating a successful compromise.
2. Using low-interaction honeyclients or regular browsers instrumented with specialized, lightweight detectors. Additional efforts have focused on preventing unconsented content execution and identifying common infrastructure (central servers in malware distribution networks) used by large numbers of attacks. In this work, we use Wepawet (a low-interaction honeyclient) and the Google Safe Browsing blacklist (produced using high-interaction honeyclients) as our oracles. These tools are used as black boxes, and any other available tool capable of evaluating the malicious nature of a web page could be used in EVILSEED. Since honeyclients require significant resources to analyze a web page, a prefiltering step is often applied to quickly discard likely-benign pages. Our work is orthogonal to prefilters: we focus on building datasets more likely to contain malicious web pages rather than filtering out uninteresting ones.

**Searching for Candidate Pages:**
The second component of a system that detects malicious web content is one that gathers the web pages to be passed to the oracle. Previous work has focused on web crawling, identifying common features of malicious pages, and monitoring and learning from attackers' behavior. Traditionally, researchers have relied on large web crawls to collect web pages, which are effective because they provide a "complete" view of the web. However, these massive crawls require extensive infrastructure, which is only available to a few organizations. Alternatively, web crawls can be guided to favor parts of the web that, according to heuristics, are more likely to contain malicious content. These smaller, targeted crawls are feasible for smaller players but yield lower detection rates. Our approach strikes a balance between these two alternatives. EVILSEED does not require the extensive infrastructure used for large crawls (the entire system runs on one off-the-shelf server-class machine). Instead, it mines malicious samples to generate search engine queries that point to URLs with high toxicity, leveraging the work that search engines have already performed.

Another technique for searching for malicious content is based on the observation that malicious resources are often similar, as they are created using the same attack toolkits. Previous efforts have focused on domain names, identifying features characteristic of malicious domains controlled by cybercriminals. These characteristics can be used to identify additional, previously uncategorized domains that are likely to be malicious. One important difference in our approach is that we aim to identify legitimate but compromised pages rather than domains directly under the control of cybercriminals. In this context, the involved domain names are not decided by the cybercriminals.

A final approach to identifying potentially malicious sites involves replicating the techniques cybercriminals use to search for vulnerable web sites. The insight here is that vulnerable web sites searched by cybercriminals are likely to get compromised and become malicious. We approach the problem differently. Instead of identifying actual (manually-crafted) queries from existing search engine logs, we analyze known, malicious pages to automatically extract searches that can be used to discover both compromised landing pages and malware distribution sites.

**VII. Conclusions**

As malicious activity on the web continues to increase, it is critical to improve our defenses. An important component of this defense is the ability to identify as many malicious web pages as possible. This is a daunting task that requires substantial resources. In this paper, we propose a novel approach to improve the effectiveness of the search process for malicious web pages. We leverage a seed of known, malicious web pages and extract characterizing similarities. Then, we use the infrastructure of search engines and the data they have collected to quickly identify other pages with the same characteristics, which are also likely to be malicious. We have implemented this approach in a tool called EVILSEED and validated it on large-scale datasets. Our results show that EVILSEED can retrieve a set of candidate web pages with a much higher percentage of malicious web pages compared to random crawling (and even to results returned for manually-crafted, malicious queries). Therefore, using EVILSEED can significantly improve the effectiveness of the malicious page discovery process.

**Acknowledgments**

This research draws on data provided by the University Research Program for Google Search, a service provided by Google to promote a greater common understanding of the web. This work was supported by the ONR under grant N000140911042 and by the National Science Foundation (NSF) under grants CNS-0845559 and CNS-0905537, and by the European Commission through project 257007 (SysSec).

**References**

[1] N. Provos, D. McNamee, P. Mavrommatis, K. Wang, and N. Modadugu, “The Ghost in the Browser: Analysis of Web-based Malware,” in USENIX Workshop on Hot Topics in Understanding Botnet, 2007.

[2] N. Provos, P. Mavrommatis, M. A. Rajab, and F. Monrose, “All Your iFrames Point to Us,” in USENIX Security Symposium, 2008.

[3] M. Polychronakis, P. Mavrommatis, and N. Provos, “Ghost Turns Zombie: Exploring the Life Cycle of Web-based Malware,” in USENIX Workshop on Large-Scale Exploits and Emergent Threats, 2008.

[4] M. A. Rajab, L. Ballard, P. Mavrommatis, N. Provos, and X. Zhao, “The Nocebo Effect on the Web: An Analysis of Fake Anti-Virus Distribution,” in USENIX Workshop on Large-Scale Exploits and Emergent Threats, 2010.

[5] M. Cova, C. Leita, O. Thonnard, A. Keromytis, and M. Dacier, “An Analysis of Rogue AV Campaigns,” in Symposium on Recent Advances in Intrusion Detection (RAID), 2010.

[6] M. Cova, C. Kruegel, and G. Vigna, “Detection and Analysis of Drive-by-Download Attacks and Malicious JavaScript Code,” in International World Wide Web Conference (WWW), 2010.

[7] J. Nazario, “PhoneyC: A Virtual Client Honeypot,” in USENIX Workshop on Large-Scale Exploits and Emergent Threats, 2009.

[8] K. Rieck, T. Krueger, and A. Dewald, “Cujo: Efficient Detection and Prevention of Drive-by-Download Attacks,” in Annual Computer Security Applications Conference (ACSAC), 2010.

[9] Y.-M. Wang, D. Beck, X. Jiang, R. Roussev, C. Verbowski, S. Chen, and S. King, “Automated Web Patrol with Strider HoneyMonkeys: Finding Web Sites that Exploit Browser Vulnerabilities,” in Symposium on Network and Distributed System Security (NDSS), 2006.

[10] C. Seifert and R. Steenson, “Capture-HPC,” http://goo.gl/vSdds.

[11] N. Provos, J. McClain, and K. Wang, “Search Worms,” in ACM Workshop on Recurring Malcode (WORM), 2006.

[12] J. John, F. Yu, Y. Xie, M. Abadi, and A. Krishnamurthy, “Searching the Searchers with SearchAudit,” in USENIX Security Symposium, 2010.

[13] IBM, “Mid-Year Trend and Risk Report,” Tech. Rep., 2010.

[14] T. Moore and R. Clayton, “Evil Searching: Compromise and Recompromise of Internet Hosts for Phishing,” in International Conference on Financial Cryptography and Data Security, 2008.

[15] S. Small, J. Mason, F. Monrose, N. Provos, and A. Stubblefield, “To Catch A Predator: A Natural Language Approach for Eliciting Malicious Payloads,” in USENIX Security Symposium, 2008.

[16] Google, “Safe Browsing API,” http://goo.gl/vIYfk, 2011.

[17] J. Stokes, R. Andersen, C. Seifert, and K. Chellapilla, “WebCop: Locating Neighborhoods of Malware on the Web,” in USENIX Workshop on Large-Scale Exploits and Emergent Threats, 2010.

[18] J. Long, E. Skoudis, and A. v. Eijkelenborg, Google Hacking for Penetration Testers, 2004.

[19] J. Long, “Google Hacking Database,” http://goo.gl/qmPA8.

[20] Cult of the Dead Cow, “Goolag Scanner,” http://goo.gl/lBK1o.

[21] D. Canali, M. Cova, C. Kruegel, and G. Vigna, “A Fast Filter for the Large-Scale Detection of Malicious Web Pages,” in International World Wide Web Conference (WWW), 2011.

[22] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, “Zozzle: Low-Overhead Mostly Static JavaScript Malware Detection,” Microsoft Research, Tech. Rep., 2010.

[23] Yahoo, “Yahoo Term Extraction API,” http://goo.gl/wGacG.

[24] R. Flores, “How Blackhat SEO Became Big,” Trend Micro, Tech. Rep., 2010.

[25] F. Howard and O. Komili, “Poisoned Search Results: How Hackers Have Automated Search Engine Poisoning Attacks to Distribute Malware.” SophosLabs, Tech. Rep., 2010.

[26] B. Googins, “Black Hat SEO Demystified: Abusing Google Trends to Serve Malware,” 2010.

[27] B. Zdrnja, “Down the RogueAV and Blackhat SEO Rabbit Hole,” http://goo.gl/5nDuK, 2010.

[28] D. Wang, S. Savage, and G. Voelker, “Cloak and Dagger: Dynamics of Web Search Cloaking,” in 18th ACM Conference on Computer and Communications Security. ACM, 2011.

[29] B. Wu and B. D. Davison, “Detecting Semantic Cloaking on the Web,” 2006.

[30] K. Chellapilla and D. M. Chickering, “Improving Cloaking Detection Using Search Query Popularity and Monetizability,” in 2nd International Workshop on Adversarial Information Retrieval on the Web (AIRWeb), 2006.

[31] B. Wu and B. D. Davison, “Cloaking and Redirection: A Preliminary Study,” in Workshop on Adversarial Information Retrieval on the Web (AIRWeb), 2005.

[32] M. Cutts and J. Shellen, “Preventing Comment Spam,” http://goo.gl/rA9mZ, 2005.

[33] S. Ford, M. Cova, C. Kruegel, and G. Vigna, “Analyzing and Detecting Malicious Flash Advertisements,” in Annual Computer Security Applications Conference (ACSAC), 2009.

[34] A. Moshchuk, T. Bragin, D. Deville, S. Gribble, and H. Levy, “SpyProxy: Execution-Based Detection of Malicious Web Content,” in USENIX Security Symposium, 2007.

[35] A. Moshchuk, T. Bragin, S. Gribble, and H. Levy, “A Crawler-Based Study of Spyware in the Web,” in Symposium on Network and Distributed System Security (NDSS), 2006.

[36] B. Feinstein and D. Peck, “Caffeine Monkey: Automated Collection, Detection and Analysis of Malicious JavaScript,” in Black Hat Security Conference, 2007.

[37] P. Likarish, E. Jung, and I. Jo, “Obfuscated Malicious JavaScript Detection Using Classification Techniques,” in Conference on Malicious and Unwanted Software (Malware), 2009.

[38] P. Ratanaworabhan, B. Livshits, and B. Zorn, “NOZZLE: A Defense Against Heap-Spraying Code Injection Attacks,” in USENIX Security Symposium, 2009.

[39] M. Egele, P. Wurzinger, C. Kruegel, and E. Kirda, “Defending Browsers Against Drive-by Downloads: Mitigating Heap-Spraying Code Injection Attacks,” in Conference on Detection of Intrusions and Malware & Vulnerability Assessment (DIMVA), 2009.

[40] B. Hartstein, “Jsunpack: A Solution to Decode JavaScript Exploits as They Rapidly Evolve,” in ShmooCon Conference, 2009.

[41] S. Chenette, “The Ultimate Deobfuscator,” in ToorCon, 2008.

[42] L. Lu, V. Yegneswaran, P. Porras, and W. Lee, “BLADE: An Attack-Agnostic Approach for Preventing Drive-By Malware Infections,” in ACM Conference on Computer and Communications Security (CCS), 2010.

[43] J. Zhang, C. Seifert, J. Stokes, and W. Lee, “ARROW: Generating Signatures to Detect Drive-By Downloads,” in International World Wide Web Conference (WWW), 2011.

[44] C. Seifert, P. Komisarczuk, and I. Welch, “Identification of Malicious Web Pages with Static Heuristics,” in Austalasian Telecommunication Networks and Applications Conference, 2008.

[45] S. Yadav, A. K. Reddy, A. Reddy, and S. Ranjan, “Detecting Algorithmically Generated Malicious Domain Names,” in Conference on Internet Measurement (IMC), 2010.

[46] L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi, “EXPOSURE: Finding Malicious Domains Using Passive DNS Analysis,” in Symposium on Network and Distributed System Security (NDSS), 2011.

[47] M. Felegyhazi, C. Kreibich, and V. Paxson, “On the Potential of Proactive Domain Blacklisting,” in USENIX Workshop on Large-Scale Exploits and Emergent Threats, 2010.