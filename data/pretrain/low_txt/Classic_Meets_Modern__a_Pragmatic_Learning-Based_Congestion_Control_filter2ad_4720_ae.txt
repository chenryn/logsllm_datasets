### 5.4 Discussion of Efficient Value Estimation in QTCP-Generalization

In this section, we analyze the efficiency of value approximation for the proposed generalization-based Kanerva coding method in QTCP-Generalization. Our experimental results demonstrate that even with a relatively small set of prototypes (300 prototypes, representing 0.0269% of the possible state space), the generalization-based Kanerva coding can provide effective state abstraction. This approach has consistently shown improvements in learning performance across all experimental evaluations.

We argue that using a small set of prototypes is feasible with the generalization-based Kanerva coding approach because the key factor affecting the performance of function approximation is not the number of prototypes but rather their reasonable layout. As long as the number of prototypes is not excessively low, varying the number of prototypes does not result in statistically significant differences in learning outcomes [16]. Therefore, the superior performance of our proposed generalization-based Kanerva coding, as evidenced by our experimental results, is primarily due to the fine-grained levels of generalization for all prototypes, which are dynamically adjusted based on the visited state samples by the RL learner.

### 6 Related Work

#### 6.1 Congestion Control Protocols

TCP is a well-studied topic in both wired and wireless networking. Over the years, numerous end-to-end congestion control mechanisms have been proposed. For instance, Cubic uses a cubic function to adjust the congestion window (cwnd) and is known for its aggressive search for spare bandwidth. Vegas [17] uses delay as a congestion control indicator and starts to decrease cwnd when the measured round-trip time (RTT) exceeds the expected value. Other notable end-to-end congestion control protocols include Compound [18], Fast [19], and BBR [20].

While these protocols each have unique properties, they share the common approach of using fixed functions or rules to change cwnd in response to network conditions. The limitation of this fixed-rule strategy is that it cannot adapt to the complexity and rapid evolution of modern data networks. These protocols do not learn from experience or history and are unable to predict the consequences of their actions. Even if an action reduces performance, the algorithm would still mechanically and repeatedly select the same action.

Several techniques have been explored to address the limitations of traditional TCP protocols. For example, Remy [12] uses offline training to find the optimal mapping from every possible network condition to the sender's behavior. Remy performs well when prior assumptions about the network at design time are consistent with the network situations in experiments. However, performance may degrade when real networks violate these prior assumptions [21]. The mappings stored in the lookup table are pre-calculated, which, like other traditional TCP variants, cannot adapt to continuously varying network environments. In Remy’s approach, the lookup table must be recomputed (which may take days to train the model) when new network conditions arise.

PCC [22] is a recently proposed protocol that can rapidly adapt to changing network conditions. PCC works by aggressively searching for better actions to change the sending rate. However, its performance may diminish in some cases because its greedy exploration can get trapped in local optima, requiring specific strategies to reach the globally optimal solution. Both Remy and PCC treat the network as a black box, focusing on finding the change in the sending rate that leads to the best performance without directly interpreting the environment or leveraging previous experience.

#### 6.2 Reinforcement Learning and Its Applications

Reinforcement Learning (RL) has achieved significant success in solving sequential decision problems and has been effectively applied to a variety of applications. The advantage of RL is its ability to learn to interact with the surrounding environment based on its own experience. For example, [23] proposed using a linear bandit model to minimize the total switching cost in a multichannel wireless network. [24] used deep reinforcement learning (DRL) to handle the large, complex state space in cloud resource allocation and power management. [25] proposed a DRL-based framework for power-efficient resource allocation in cloud RANs.

Additionally, many RL-based schemes have been proposed to improve the quality of service (QoS) for network applications. For example, [26] proposed an RL-based algorithm to generate congestion control rules specifically for multimedia applications. [27] formulated a network resource allocation problem in a multi-user video streaming domain as a decentralized partially observable Markov decision process (DEC-POMDP) and applied a distributed RL algorithm to solve the problem. However, the work in [27] did not provide practical techniques to help the RL algorithm adapt to complex network topologies with continuous state spaces. [28] used an RL algorithm to adaptively change parameter configurations and thus improve the QoE of video streaming. Its limitation arises from its use of a tabular-based algorithm, which directly stores and updates value functions as entries in a table, confining its application to large, continuous domains.

All the above-mentioned schemes are task-driven and designed for specific applications, making them unsuitable for direct application to the congestion control problem. To the best of our knowledge, QTCP is the first proposed solution that applies RL directly to TCP congestion control protocol design.

#### 6.3 Related Function Approximation Techniques

Many approximation approaches have been developed to abstract and compress full state spaces in RL tasks with a large number of states. One effective approach is function approximation [6], which reduces the size of the state space by representing it with an abstracted and parameterized function. The explicit table that stores value functions is replaced by an approximate and compact parameterized version. Various function approximation techniques have been developed, including tile coding (also known as CMAC) and its variants, such as adaptive tile coding [29], and tree-based state partitions [30]. However, when solving practical real-world problems, limitations arise from the coding schemes used in these approaches, such as requiring task-dependent criteria-based heuristics, spending impractical computational resources to explore each dimension for partitions, and the exponential increase in state space and function approximation complexity with the number of dimensions. These limitations prevent the application of these approaches to very large, high-dimensional, or continuous state spaces.

However, Kanerva coding technique scales well with high-dimensional, continuous problem domains [7]. Our proposed generalization-based Kanerva coding approach further improves its approximation ability and reduces convergence time by dynamically optimizing the layout of the prototype set and providing finer-grained discrimination of the explored state areas.

### 7 Conclusion

Our work describes QTCP, an effective RL-based approach that derives high-quality decision policies and successfully handles highly complex network domains with a broad set of characteristics. Unlike preprogrammed rule-based TCP, QTCP uses reinforcement signals (rewards) to learn congestion control rules from experience, requiring no prior knowledge or model of the network dynamics. This makes our approach widely applicable to various network settings. Moreover, our learning agent applies a novel generalization-based Kanerva coding approach to reduce training time and the necessary state space to search. This approach reformulates the original function approximator with adjustable generalization granularity across states, enabling the abstraction of sufficient information from a wide range of environmental signals and even using a very small subset of the state space to accurately approximate the whole state space.

Our QTCP-Generalization achieved better throughput and delay performance than both NewReno and QTCP-Baseline (a learning-based TCP with the best currently-existing Kanerva-based function approximator) in our evaluations. Specifically, the average throughput of QTCP-Generalization outperformed QTCP-Baseline by 35.2% and NewReno by 59.5%. Our approach also demonstrated slightly better RTT performance than NewReno. We conclude that QTCP with generalization-based Kanerva coding can be used to manage congestion in a wide range of network conditions and enables quick online policy development with minimal computational and memory expenses.

### References

[1] T. Yilmaz and O. B. Akan, “State-of-the-art and research challenges for consumer wireless communications at 60 GHz,” IEEE Transactions on Consumer Electronics, vol. 62, no. 3, pp. 216–225, 2016.
[2] O. B. Akan, O. B. Karli, and O. Ergul, “Cognitive radio sensor networks,” IEEE Network, vol. 23, no. 4, 2009.
[3] Y. Li, Y. Li, B. Cao, M. Daneshmand, and W. Zhang, “Cooperative spectrum sharing with energy-save in cognitive radio networks,” in Global Communications Conference (GLOBECOM), 2015 IEEE. IEEE, 2015, pp. 1–6.
[4] E. S. Hosseini, V. Esmaeelzadeh, R. Berangi, and O. B. Akan, “A correlation-based and spectrum-aware admission control mechanism for multimedia streaming in cognitive radio sensor networks,” International Journal of Communication Systems, vol. 30, no. 3, 2017.
[5] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan, “Data center TCP (DCTCP),” in ACM SIGCOMM Computer Communication Review, vol. 40, no. 4. ACM, 2010, pp. 63–74.
[6] L. Frommberger, Qualitative Spatial Abstraction in Reinforcement Learning. Springer Science & Business Media, 2010.
[7] R. Sutton and A. Barto, Reinforcement Learning: An Introduction. Bradford Books, 1998.
[8] S. Floyd, A. Gurtov, and T. Henderson, “The NewReno modification to TCP’s fast recovery algorithm,” 2004.
[9] D.-M. Chiu and R. Jain, “Analysis of the increase and decrease algorithms for congestion avoidance in computer networks,” Computer Networks and ISDN systems, vol. 17, no. 1, pp. 1–14, 1989.
[10] B. D. F. Zhou, M. DiFelice, and K. R. Chowdhury, “Towards fast flow convergence in cognitive radio cellular networks,” IEEE Globecom. IEEE, 2017.
[11] K. R. Chowdhury, M. Di Felice, and I. F. Akyildiz, “TCP CRAHN: A transport control protocol for cognitive radio ad hoc networks,” Mobile Computing, IEEE Transactions on, vol. 12, no. 4, pp. 790–803, 2013.
[12] K. Winstein and H. Balakrishnan, “TCP ex machina: Computer-generated congestion control,” in ACM SIGCOMM Computer Communication Review, vol. 43, no. 4. ACM, 2013, pp. 123–134.
[13] C. Wu and W. Meleis, “Function approximation using tile and Kanerva coding for multi-agent systems,” in Proc. Of Adaptive Learning Agents Workshop (ALA) in AAMAS, 2009.
[14] P. W. Keller, S. Mannor, and D. Precup, “Automatic basis function construction for approximate dynamic programming and reinforcement learning,” in Proc. of Intl. Conf. on Machine Learning, 2006.
[15] C. Wu and W. M. Meleis, “Adaptive Kanerva-based function approximation for multi-agent systems,” in Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 3. International Foundation for Autonomous Agents and Multiagent Systems, 2008, pp. 1361–1364.
[16] M. Allen and P. Fritzsche, “Reinforcement learning with adaptive Kanerva coding for Xpilot game AI,” in 2011 IEEE Congress of Evolutionary Computation (CEC). IEEE, 2011, pp. 1521–1528.
[17] L. S. Brakmo and L. L. Peterson, “TCP Vegas: End to end congestion avoidance on a global internet,” Selected Areas in Communications, IEEE Journal on, vol. 13, no. 8, pp. 1465–1480, 1995.
[18] K. Tan, J. Song, Q. Zhang, and M. Sridharan, “A compound TCP approach for high-speed and long distance networks,” in Proceedings-IEEE INFOCOM, 2006.
[19] D. X. Wei, C. Jin, S. H. Low, and S. Hegde, “Fast TCP: Motivation, architecture, algorithms, performance,” IEEE/ACM Transactions on Networking (ToN), vol. 14, no. 6, pp. 1246–1259, 2006.
[20] N. Cardwell, Y. Cheng, C. S. Gunn, S. H. Yeganeh, and V. Jacobson, “BBR: Congestion-based congestion control,” Queue, vol. 14, no. 5, p. 50, 2016.
[21] A. Sivaraman, K. Winstein, P. Thaker, and H. Balakrishnan, “An experimental study of the learnability of congestion control,” in Proceedings of the 2014 ACM conference on SIGCOMM. ACM, 2014, pp. 479–490.
[22] M. Dong, Q. Li, D. Zarchy, P. B. Godfrey, and M. Schapira, “PCC: Re-architecting congestion control for consistent high performance,” NSDI’15 Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation, 2015.
[23] T. Le, C. Szepesvari, and R. Zheng, “Sequential learning for multi-channel wireless network monitoring with channel switching costs,” IEEE Transactions on Signal Processing, vol. 62, no. 22, pp. 5919–5929, 2014.
[24] N. Liu, Z. Li, J. Xu, Z. Xu, S. Lin, Q. Qiu, J. Tang, and Y. Wang, “A hierarchical framework of cloud resource allocation and power management using deep reinforcement learning,” in Distributed Computing Systems (ICDCS), 2017 IEEE 37th International Conference on. IEEE, 2017, pp. 372–382.
[25] Z. Xu, Y. Wang, J. Tang, J. Wang, and M. C. Gursoy, “A deep reinforcement learning based framework for power-efficient resource allocation in cloud RANs,” in Communications (ICC), 2017 IEEE International Conference on. IEEE, 2017, pp. 1–6.
[26] O. Habachi, H.-P. Shiang, M. van der Schaar, and Y. Hayel, “Online learning based congestion control for adaptive multimedia transmission,” IEEE Transactions on Signal Processing, vol. 61, no. 6, pp. 1460–1469, 2013.
[27] M. Hemmati, A. Yassine, and S. Shirmohammadi, “An online learning approach to QoE-fair distributed rate allocation in multi-user video streaming,” in Signal Processing and Communication Systems (ICSPCS), 2014 8th International Conference on. IEEE, 2014, pp. 1–6.
[28] J. van der Hooft, S. Petrangeli, M. Claeys, J. Famaey, and F. De Turck, “A learning-based algorithm for improved bandwidth-awareness of adaptive streaming clients,” in Integrated Network Management (IM), 2015 IFIP/IEEE International Symposium on. IEEE, 2015, pp. 131–138.
[29] S. Whiteson, M. E. Taylor, P. Stone et al., Adaptive tile coding for value function approximation. Computer Science Department, University of Texas at Austin, 2007.
[30] S. Chernova and M. Veloso, “Tree-based policy learning in continuous domains through teaching by demonstration,” in Proceedings of Workshop on Modeling Others from Observations (MOO 2006), 2006.

### Author Biographies

**Wei Li** is a PhD candidate in the ECE department of Northeastern University. He is conducting his research under the guidance of Prof. Waleed Meleis. He graduated from Northeastern University with an M.S. in Computer Engineering in 2012 and obtained his B.S. in Control Engineering from the University of Electronic Science and Technology of China. His current research focuses on reinforcement learning algorithms, function approximation techniques, and learning-based network applications.

**Fan Zhou** is a PhD student in the ECE Department at Northeastern University. He is currently working in the Next Generation Networks and Systems Lab under the supervision of Prof. Kaushik Chowdhury. He received his B.S. and M.S. degrees from Hohai University (2011) and Beijing University of Posts and Telecommunications (2014). His research interests span different layers of wireless networking, with a specific focus on the design and implementation of high-performance data transfer architectures.

**Kaushik Roy Chowdhury** (M’09-SM’15) received his M.S. degree from the University of Cincinnati in 2006 and his Ph.D. degree from the Georgia Institute of Technology in 2009. He was an Assistant Professor from 2009 to 2015 at Northeastern University, where he is now an Associate Professor in the Electrical and Computer Engineering Department. He was a winner of the Presidential Early Career Award for Scientists and Engineers (PECASE) in 2017, ONR Director of Research Early Career Award in 2016, and the NSF CAREER Award in 2015. His current research interests include dynamic spectrum access, wireless RF energy harvesting, and IoT, particularly in the area of intra/on-body communication.

**Waleed Meleis** is an Associate Professor and Associate Chair in the Department of Electrical and Computer Engineering at Northeastern University. He received his BSE degree in Electrical Engineering from Princeton University and his MS and PhD degrees in Computer Science and Engineering from the University of Michigan. His research focuses on the application of algorithm design, combinatorial optimization, and machine learning to diverse engineering problems, including cloud computing, spectrum management, high-performance compilers, instruction scheduling, parallel programming, and network management.