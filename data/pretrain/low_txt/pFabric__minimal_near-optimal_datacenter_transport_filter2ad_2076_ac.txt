**Figure 4: Empirical Traffic Distributions for Benchmarking**

Empirical traffic distributions used for benchmarks are based on real production datacenter measurements [3, 12]. These distributions include two flow size distributions:

- **Web Search Workload**: This distribution is derived from a datacenter supporting web search [3].
- **Data Mining Workload**: This distribution is from a cluster running large data mining jobs [12].

Flows arrive according to a Poisson process, with source and destination chosen uniformly at random. The flow arrival rate is adjusted to achieve the desired level of load in the network fabric. Both workloads exhibit a mix of small and large flows with heavy-tailed characteristics.

- **Web Search Workload**: Over 95% of all bytes come from the 30% of flows that are 1–20 MB.
- **Data Mining Workload**: More than 80% of the flows are less than 10 KB, and 95% of all bytes are in the ∼3.6% of flows that are larger than 35 MB. This extreme skew makes the data mining workload easier to handle, as it is less likely that multiple large flows will be concurrently active from/to one fabric port, reducing network contention.

**Performance Metrics**

We use two main performance metrics, similar to prior work [14, 21, 3]:

- **Deadline-Constrained Traffic**: Application throughput, defined as the fraction of flows that meet their deadline.
- **Non-Deadline Traffic**: Flow completion time (FCT), including the average FCT across all flows, separately for small and large flows, and the 99th percentile FCT for small flows. All FCTs are normalized to the best possible completion time for each flow, assuming no interference from competing traffic.

**Schemes Compared**

- **TCP-DropTail**: Standard TCP-New Reno with SACK and DropTail queues.
- **DCTCP**: Congestion control algorithm with ECN marking at the fabric queues [3].
- **pFabric**: Design described in this paper, including both switch and minimal rate control. By default, the remaining flow size is used as the priority for each packet.
- **PDQ**: Prior approach for minimizing flow completion times or missed deadlines, implemented with Early Start and Early Termination enhancements [14].
- **Ideal**: Central scheduler with a complete view of all flows, scheduling them in nondecreasing order of size and in a maximal manner (Algorithm 1). Simulated in MATLAB using the same sequence of flow arrivals as in ns2 benchmarks.

**Parameter Settings**

| Scheme        | Queue Size | Initial CWND | Minimum RTO | Marking Threshold | Probing Interval | K (for Early Start) |
|---------------|------------|--------------|--------------|-------------------|------------------|---------------------|
| TCP-DropTail  | 225KB      | 12 pkts      | 200µs        | -                 | -                | -                   |
| DCTCP         | 225KB      | 12 pkts      | 200µs        | 22.5KB            | -                | -                   |
| pFabric       | 36KB       | 12 pkts      | 45µs         | -                 | -                | -                   |
| PDQ           | 225KB      | 12 pkts      | 200µs        | -                 | 15µs             | 2                   |

**Basic Performance Measures**

- **Seamless Switching Between Flows**: We test pFabric's ability to seamlessly switch between flows by generating 5 large transfers of 20 MB to a single destination host. Figure 5(a) shows that pFabric schedules flows one-by-one, with each flow grabbing the full 10 Gbps bandwidth. The last of the 5 flows completes after ∼80.15 ms, only 150 µs more than the best possible completion time of 80 ms for a 100 MB transfer at 10 Gbps.

- **Loss Rate**: We measure the loss rate under stress conditions with up to 50 concurrent large flows to a single destination port. Without probe mode, the loss rate rises sharply from ∼4.8% to ∼38.5% as the number of flows increases. With probe mode, the loss rate is significantly lower, under 5.5% with 50 flows (Figure 5(b)).

- **Incast Performance**: Incast traffic patterns, common in large-scale web applications and storage systems, can cause throughput degradation for TCP. We simulate Incast by having a receiver node request a 100 MB file striped across N sender nodes. Figure 6 shows that pFabric handles Incast well, achieving a total request completion time of 81.1 ms at 50 senders, slightly higher than DCTCP's near-ideal 80 ms due to the overhead of serially scheduling flows.

**Overall Performance**

- **Deadline-Unconstrained Traffic**: pFabric achieves near-optimal flow completion times for all flow sizes and loads. Figure 7 shows that pFabric's average FCT is very close to the Ideal scheme and significantly better than other schemes. For the web search workload, pFabric's performance is within ∼0.7-17.8% of the Ideal scheme, and for the data mining workload, it is within ∼1.7-10.6%. Compared to PDQ, pFabric's FCT is ∼19-39% lower for the web search workload and ∼40-50% lower for the data mining workload.

The data mining workload generally performs better, especially at high loads, because the largest ∼3.6% of flows contribute over 95% of all bytes, and these large flows arrive infrequently, reducing sustained congestion. PDQ's normalized FCT is at least two for very small flows due to the extra RTT overhead for flow initiation.