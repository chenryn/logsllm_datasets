# Coverage Histogram and Test Configuration Analysis

## Coverage Histogram
- **Handles, with Specials**
- **Handles, without Specials**
- **with Specials**
- **without Specials**

| Coverage | Handles, w/ Specials | Handles, w/o Specials | w/ Specials | w/o Specials |
|----------|---------------------|-----------------------|-------------|--------------|
| 1        | 1                   | 0                     | e-1         | 5            |
| 1        | 1                   | 0                     | e-1         | 4            |
| 1        | 1                   | 0                     | e-1         | 3            |
| 1        | 1                   | 0                     | e-1         | 2            |
| 1        | 1                   | 0                     | e-1         | 1            |
| 0        | 0                   | e-0                   | 1           | 0            |
| 0        | 0                   | e-9                   | 1           | 0            |
| 0        | 0                   | e-8                   | 1           | 0            |
| 0        | 0                   | e-7                   | 1           | 0            |
| 0        | 0                   | e-6                   | 1           | 0            |
| 0        | 0                   | e-5                   | 1           | 0            |
| 0        | 0                   | e-4                   | 1           | 0            |
| 0        | 0                   | e-3                   | 1           | 0            |
| 0        | 0                   | e-2                   | 1           | 0            |
| 0        | 0                   | e-1                   | 1           | 0            |

### Figure 5: Test Case Coverage
- **Coverage Classes**: The coverage of executed test cases compared to the set of input vectors in different test configurations.

## Table 2: Average Coverage per Test Configuration
- **Test Configurations**:
  - **w/o Sp. Han.**
  - **Han., w/ Sp.**
  - **Han., w/o Sp.**
  - **w/ Sp.**
- **Average Coverage**:
  - **w/o Sp. Han.**: 4.422906 · 10^-1
  - **Han., w/ Sp.**: 2.763518 · 10^-1
  - **Han., w/o Sp.**: 4.389137 · 10^-1
  - **w/ Sp.**: 2.763809 · 10^-1
- **Average Incomplete Coverage**:
  - **w/o Sp. Han.**: 7.086199 · 10^-2
  - **Han., w/ Sp.**: 2.272866 · 10^-2
  - **Han., w/o Sp.**: 6.748508 · 10^-2
  - **w/ Sp.**: 2.275769 · 10^-2

### Notes:
- The Y-axis depicts the number of functions per class.
- The handle test types have no visible impact. However, the average coverage for configurations with handles differs slightly from those without handles.
- Excluding some test cases via static analysis has a visible impact. Configurations without special test cases show more functions with lower coverage.
- Some functions do not benefit from this exclusion due to indirect function calls via function pointers. These functions were excluded from the static analysis.
- For 76 out of 148 functions, the number of test cases could be reduced.

## Autocannon Benchmark Results
- **Figure 6**: Comparison of the number of crashed and robust test cases.
- **Figure 7**: Variation in the percentage of unrobust test cases per function.
- **Robustness Distribution**:
  - 67 functions crashed in at least 90% of the test cases.
  - 66 functions crashed in at most 10% of the test cases.
  - 40 functions fall between these two extremes.
  - Functions without arguments were not tested.

## Protection Hypotheses Evaluation
- **Apache Execution with Protection Wrapper**:
  - **W**: Generated from fault injection experiments with lower test coverage.
  - **WSA**: Generated from fault injection experiments with higher test coverage using static analysis.
  - **Results**:
    - **W**: No false positives.
    - **WSA**: 10 functions leading to false positives (6.17% of all function calls). 3 of these functions were also wrapped by W.

### Microbenchmark with Fault Injection
- **Results**:
  - **W**: Predicted 56.81% crashes, 1.7% false positives.
  - **WSA**: Predicted 51.39% crashes, 0.6% false positives.

## Related Work
- **Dependability Benchmarking and Automatic Patch Generation**:
  - Combines dependability benchmarking [10, 11] and automatic patch generation [8, 15, 14].
  - HEALERS [8]: Limited by an inflexible type system, only tested 4 APR functions.
  - AutoPatch [15]: Addresses bad error handling but may introduce unexpected errors.
  - Stelios et al.: Patches buffer overrun bugs in applications, requires exploiting code.
  - Ballista [10, 11]: Dependability benchmark for POSIX implementations, limited to a specific API.

## Conclusion
- **Contributions**:
  - A flexible approach to hardening arbitrary libraries for robustness and security.
  - A new dependability benchmark that can measure the robustness of any library using static analysis.
  - A table-based approach to derive protection hypotheses from benchmark results.
- **Evaluation**:
  - Protection hypotheses predicted up to 56.85% of crashes.
  - Low number of misclassified robust argument values as unrobust.
  - Potential improvements through adding more appropriate checks and test types.

## Acknowledgements
- Thanks to Martin Kretzschmar for introducing us to LLVM.

## References
- [1] Apache Software Foundation. Apache Portable Runtime Project. http://apr.apache.org.
- [2] D. Box and C. Sells. Essential .NET 1. The Common Language Runtime, volume 1. Addison-Wesley Longman, November 2002.
- [3] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox. Microreboot – a technique for cheap recovery. In 6th Symposium on Operating Systems Design and Implementation (OSDI), pages 31–44, December 2004.
- [4] U. Drepper. How to Write Shared Libraries. Technical Report, Red Hat, January 2005. http://people.redhat.com/drepper/dsohowto.pdf.
- [5] D. Engler. Weird Things That Surprise Academics Trying to Commercialize a Static Checking Tool. Part of an invited talk at SPIN05 and CONCUR05, 2005. http://www.stanford.edu/engler/spin05-coverity.pdf.
- [6] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and B. Chelf. Bugs as Deviant Behavior: A General Approach to Inferring Errors in Systems Code. In SOSP ’01: Proceedings of the Eighteenth ACM Symposium on Operating Systems Principles, pages 57–72, New York, NY, USA, 2001. ACM Press.
- [7] C. Fetzer and Z. Xiao. A Flexible Generator Architecture for Improving Software Dependability. In Proceedings of the Thirteenth International Symposium on Software Reliability Engineering (ISSRE), pages 155–164, Annapolis, MD, Nov 2002.
- [8] C. Fetzer and Z. Xiao. HEALERS: A Toolkit for Enhancing the Robustness and Security of Existing Applications. In International Conference on Dependable Systems and Networks (DSN2003 demonstration paper), San Francisco, CA, USA, June 2003.
- [9] J. Arlat and Y. Crouzet. Faultload Representativeness for Dependability Benchmarking. In Workshop on Dependability Benchmarking, pages 29–30, June 2002.
- [10] P. Koopman and J. DeVale. Comparing the Robustness of POSIX Operating Systems. In FTCS ’99: Proceedings of the Twenty-Ninth Annual International Symposium on Fault-Tolerant Computing, page 30, Washington, DC, USA, 1999. IEEE Computer Society.
- [11] P. Koopman and J. DeVale. The Exception Handling Effectiveness of POSIX Operating Systems. IEEE Trans. Softw. Eng., 26(9):837–848, 2000.
- [12] C. Lattner and V. Adve. LLVM: A Compilation Framework for Lifelong Program Analysis & Transformation. In Proceedings of the 2004 International Symposium on Code Generation and Optimization (CGO’04), Palo Alto, California, Mar 2004.
- [13] K. Pattabiraman, G. P. Saggese, D. Chen, Z. Kalbarczyk, and R. K. Iyer. Dynamic Derivation of Application-Specific Error Detectors and Their Implementation in Hardware. In Proceedings of the Sixth European Dependable Computing Conference (EDCC 2006), October 2006.
- [14] S. Sidiroglou and A. D. Keromytis. Countering Network Worms Through Automatic Patch Generation. Technical Report, Columbia University Computer Science Department, 2003.
- [15] M. Süsskraut and C. Fetzer. Automatically Finding and Patching Bad Error Handling. In Proceedings of the Sixth European Dependable Computing Conference (EDCC 2006), October 2006.
- [16] M. Süsskraut and C. Fetzer. Learning Library-Level Error Return Values from Syscall Error Injection. In Proceedings of the Sixth European Dependable Computing Conference (EDCC 2006) [Fast Abstract], volume Proceedings Supplemental, 2006.
- [17] D. van Heesch. Doxygen. http://www.doxygen.org.
- [18] J. Yang, D. Evans, D. Bhardwaj, T. Bhat, and M. Das. Terracotta: Mining Temporal API Rules from Imperfect Traces. In 28th International Conference on Software Engineering, May 2006. http://www.cs.virginia.edu/terracotta/.