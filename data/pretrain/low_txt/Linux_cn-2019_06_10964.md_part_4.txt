### 优化后的文本

我从存储基准测试入手，开始了初步的开发。该测试基于从现实世界数据集中提取的大约440万个序列描述符，并生成合成数据点以输入到这些序列中。这个阶段的开发仅针对单独的存储系统进行，对于快速识别性能瓶颈和在高并发负载场景下触发死锁至关重要。

完成概念性开发后，该基准测试在我的Macbook Pro上达到了每秒2000万次吞吐量——这还是在我同时打开十几个Chrome页面和Slack的情况下。尽管这一结果令人印象深刻，但它也表明了进一步推动这项测试的价值有限（或者在高随机环境下运行的意义不大）。毕竟，这是合成的数据，除了提供一个良好的第一印象外，其实际价值有限。然而，这一吞吐量比最初的设计目标高出20倍，是时候将其部署到真实的Prometheus服务器上，以增加更多现实环境中的开销和场景。

实际上，我们没有可重现的Prometheus基准测试配置，特别是缺乏不同版本的A/B测试。不过，现在有了！[这里](https://github.com/prometheus/prombench) 是我们的新基准测试工具。它允许我们声明性地定义基准测试场景，并将其部署到AWS的Kubernetes集群上。虽然这不是全面基准测试的最佳环境，但与64核128GB内存的专用裸机服务器相比，它更能反映我们用户群体的实际使用情况。

我们部署了两个Prometheus 1.5.2服务器（V2存储系统）和两个来自2.0开发分支的Prometheus（V3存储系统）。每个Prometheus实例都运行在配备SSD的专用服务器上。我们在工作节点上部署了横向扩展的应用程序，并让其暴露典型的微服务度量。此外，Kubernetes集群本身及其节点也被监控。整个系统由另一个Meta-Prometheus监督，用于监控每个Prometheus实例的健康状况和性能。

为了模拟序列分流，微服务定期扩展和收缩，移除旧Pod并生成新的Pod，从而创建新的序列。通过选择“典型”的查询来模拟查询负载，对每个Prometheus版本执行一次。

总体而言，伸缩和查询负载以及采样频率远远超过了Prometheus的生产部署。例如，我们每隔15分钟更换60%的微服务实例以产生序列分流。在现代基础设施上，这种情况一天仅发生1-5次。这确保了我们的V3设计能够应对未来几年的工作负载。结果显示，在极端情况下，Prometheus 1.5.2和2.0之间的性能差异更加明显。

总的来说，我们每秒从850个目标收集大约11万个样本，每次暴露50万个序列。

系统运行一段时间后，我们可以评估一些关键指标。我们评估了两个版本在12小时后达到稳定状态时的几个指标。

> 注意：从Prometheus图形界面截图中Y轴轻微截断。

![堆内存使用（GB）](/data/attachment/album/201906/11/180818v78vvozde7kpg2w7.png)

*堆内存使用（GB）*

内存资源的使用对用户来说是一个主要问题，因为它相对不可预测且可能导致进程崩溃。显然，查询服务器正在消耗大量内存，这主要是由于查询引擎的开销。总体来看，Prometheus 2.0的内存消耗减少了3-4倍。大约6小时后，Prometheus 1.5出现了一个明显的峰值，对应于我们设置的6小时保留边界。删除操作的成本非常高，导致资源消耗急剧增加。这一点在下面几张图中均有体现。

![CPU使用（核心/秒）](/data/attachment/album/201906/11/180819b2jxxdln8p8u9pb2.png)

*CPU使用（核心/秒）*

类似的模式也体现在CPU使用上，但查询服务器与非查询服务器之间的差异尤为明显。每秒获取大约11万个数据需要0.5核心/秒的CPU资源，而新存储系统的CPU消耗几乎可以忽略不计。总体来看，新存储系统所需的CPU资源减少了3到10倍。

![磁盘写入（MB/秒）](/data/attachment/album/201906/11/180820veq9tj2eedwv24ll.png)

*磁盘写入（MB/秒）*

最引人注目和意想不到的改进表现在磁盘写入利用率上。这清楚地说明了为什么Prometheus 1.5容易导致SSD损耗。我们看到初始上升发生在第一个块被持久化到序列文件中的时期，随后删除操作引发重写导致第二个上升。令人惊讶的是，查询服务器与非查询服务器显示出了非常不同的利用率。

相比之下，Prometheus 2.0每秒仅向其预写日志写入约1兆字节。当块被压缩到磁盘时，写入会定期出现峰值。总体上，这节省了惊人的97-99%。

![磁盘大小（GB）](/data/attachment/album/201906/11/180821s8amft225g81gzf5.png)

*磁盘大小（GB）*

与磁盘写入密切相关的是总磁盘空间占用量。由于我们对样本（占大部分数据）使用了相同的压缩算法，因此磁盘占用量应当相同。在更稳定的系统中，这通常是正确的，但由于我们需要处理高序列分流，还需要考虑每个序列的开销。

如我们所见，Prometheus 1.5在两个版本达到稳定状态之前，由于保留操作导致存储空间急速上升。Prometheus 2.0似乎在每个序列上的开销显著降低。我们可以清楚地看到预写日志线性地充满整个存储空间，然后在压缩完成后瞬间下降。事实上，对于两个Prometheus 2.0服务器，它们的曲线并不完全匹配，这一点需要进一步调查。

前景大好。剩下最重要的部分是查询延迟。新的索引应该优化了查找复杂度。处理数据的过程，例如`rate()`函数或聚合，没有实质性的改变。这些都是查询引擎要做的事情。

![第99个百分位查询延迟（秒）](/data/attachment/album/201906/11/180823fwyw0w81w4dn8waw.png)

*第99个百分位查询延迟（秒）*

数据完全符合预期。在Prometheus 1.5上，查询延迟随着存储的序列而增加。只有在保留操作开始且旧序列被删除后才会趋于稳定。相比之下，Prometheus 2.0从一开始就保持在合适的水平。

我们需要花一些心思在数据采集方式上，通过对范围查询和即时查询的组合、进行轻或重的计算、访问更多或更少的文件等方面的选择来估计对服务器发出的查询请求。这并不需要代表真实世界查询的分布，也不能代表冷数据的查询性能。我们可以假设所有样本数据都是保存在内存中的热数据。

尽管如此，我们可以相当自信地说，整体查询效果对序列分流变得非常有弹性，并且在高压基准测试场景下提升了4倍的性能。在更为静态的环境下，我们可以假设查询时间大多数花费在查询引擎上，改善程度明显较低。

![摄入的样本/秒](/data/attachment/album/201906/11/180824kzjqgj5gjm3gyfqe.png)

*摄入的样本/秒*

最后，快速看一下不同Prometheus服务器的摄入率。我们可以看到搭载V3存储系统的两个服务器具有相同的摄入速率。几小时后变得不稳定，这是因为不同的基准测试集群节点由于高负载变得无响应，与Prometheus实例无关。（两个2.0的曲线完全匹配这一事实希望足够具有说服力）

尽管有更多的CPU和内存资源，两个Prometheus 1.5.2服务器的摄入率大大降低。高序列分流的压力导致无法采集更多的数据。

那么现在每秒可以摄入的绝对最大样本数是多少？我不知道——虽然这是一个相当容易优化的指标，但除了稳固的基线性能之外，它并不是特别有意义。

有许多因素会影响Prometheus的数据流量，没有一个单一数字能够描述捕获质量。最大摄入率在历史上是一个导致基准偏差的度量，并且忽视了更重要的方面，例如查询性能和对序列分流的弹性。关于资源使用线性增长的大致猜想通过一些基本测试得到了证实。很容易推断出其中的原因。

我们的基准测试模拟了高动态环境下Prometheus的压力，比真实世界中的更大。结果显示，即使在未优化的云服务器上运行，效果也超出了预期。最终，成功将取决于用户反馈而不是基准数字。

> 注意：在撰写本文的同时，Prometheus 1.6正在开发中，它允许更可靠地配置最大内存使用量，并可能显著减少整体消耗，有利于稍微提高CPU使用率。我没有重复对此进行测试，因为整体结果变化不大，尤其是在高序列分流的情况下。

### 总结

Prometheus开始应对高基数序列与单独样本的吞吐量。这仍然是一项富有挑战性的任务，但新的存储系统似乎为我们展示了未来的美好前景。第一个配备V3存储系统的[alpha版本Prometheus 2.0](https://prometheus.io/blog/2017/04/10/promehteus-20-sneak-peak/)已经可以用来测试了。在早期阶段预计还会出现崩溃、死锁和其他bug。

存储系统的代码可以在[这个单独的项目中找到](https://github.com/prometheus/tsdb)。Prometheus对于寻找高效本地存储时间序列数据库的应用来说非常有用，这一点令人非常惊讶。

> 这里需要感谢很多人作出的贡献，以下排名不分先后：
>
> Bjoern Rabenstein 和 Julius Volz 在 V2 存储引擎上的打磨工作以及 V3 存储系统的反馈，为新一代设计奠定了基础。
>
> Wilhelm Bierbaum 对新设计不断的建议与见解作出了很大的贡献。Brian Brazil 不断的反馈确保了我们最终得到的是语义上合理的方法。与 Peter Bourgon 深刻的讨论验证了设计并形成了这篇文章。
>
> 别忘了我们整个 CoreOS 团队与公司对于这项工作的赞助与支持。感谢所有那些听我一遍遍唠叨 SSD、浮点数、序列化格式的同学。

---
作者：[Fabian Reinartz](https://twitter.com/fabxc)  
译者：[LuuMing](https://github.com/LuuMing)  
校对：[wxy](https://github.com/wxy)  
本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创编译，[Linux中国](https://linux.cn/) 荣誉推出