### Optimized Text

#### Dataset Collection
A service provider can collect a clean dataset at a relatively low cost, for example, by having its employees generate and manually label a high-quality root dataset.

#### Adaptive Attacks and Hierarchical Root of Trust
We considered an adaptive attack by extending the state-of-the-art framework for local model poisoning attacks to our FLTrust. We acknowledge that there may be stronger local model poisoning attacks against FLTrust, which is an interesting area for future research. Additionally, it would be valuable to explore a hierarchical root of trust. For instance, the root dataset could contain multiple subsets with varying levels of trust. Subsets with higher trust could have a greater impact on the aggregation process.

#### Conclusion and Future Work
We proposed and evaluated a new federated learning method called FLTrust, designed to achieve Byzantine robustness against malicious clients. The key innovation in FLTrust is that the server itself collects a small, clean training dataset (the root dataset) to establish initial trust. Our extensive evaluations across six datasets show that FLTrust, even with a small root dataset, can effectively resist a large fraction of malicious clients. Specifically, under adaptive attacks with a significant number of malicious clients, FLTrust can still train global models that are as accurate as those trained by FedAvg in a benign environment. Future work includes:
1. Designing more sophisticated local model poisoning attacks against FLTrust.
2. Exploring a hierarchical root of trust system.

#### Acknowledgement
We thank the anonymous reviewers for their constructive feedback. This work was supported in part by NSF grants No. 1937786, 1943226, and 2110252, an IBM Faculty Award, and a Google Faculty Research Award.

#### References
[1] Federated Learning: Collaborative Machine Learning without Centralized Training Data. [Online]. Available: https://ai.googleblog.com/2017/04/federated-learning-collaborative.html

[2] Machine Learning Ledger Orchestration for Drug Discovery (MELLODDY). [Online]. Available: https://www.melloddy.eu/

[3] Utilization of FATE in Risk Management of Credit in Small and Micro Enterprises. [Online]. Available: https://www.fedai.org/cases/utilization-of-fate-in-risk-management-of-credit-in-small-and-micro-enterprises/

[4] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A Public Domain Dataset for Human Activity Recognition Using Smartphones,” in ESANN, 2013.

[5] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to Backdoor Federated Learning,” in AISTATS, 2020, pp. 2938–2948.

[6] M. Barborak, A. Dahbura, and M. Malek, “The Consensus Problem in Fault-Tolerant Computing,” ACM Computing Surveys (CSur), vol. 25, no. 2, pp. 171–220, 1993.

[7] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing Federated Learning through an Adversarial Lens,” in ICML, 2019, pp. 634–643.

[8] B. Biggio, B. Nelson, and P. Laskov, “Poisoning Attacks against Support Vector Machines,” in ICML, 2012.

[9] P. Blanchard, E. M. E. Mhamdi, R. Guerraoui, and J. Stainer, “Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent,” in NIPS, 2017.

[10] X. Cao, J. Jia, and N. Z. Gong, “Data Poisoning Attacks to Local Differential Privacy Protocols,” arXiv preprint arXiv:1911.02046, 2019.

[11] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning,” in arxiv, 2017.

[12] Y. Chen, L. Su, and J. Xu, “Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent,” in POMACS, 2017.

[13] M. Cheng, T. Le, P.-Y. Chen, J. Yi, H. Zhang, and C.-J. Hsieh, “Query-Efficient Hard-Label Black-Box Attack: An Optimization-Based Approach,” in ICLR, 2019.

[14] A. Cheu, A. Smith, and J. Ullman, “Manipulation Attacks in Local Differential Privacy,” arXiv preprint arXiv:1909.09630, 2019.

[15] M. Fang, X. Cao, J. Jia, and N. Z. Gong, “Local Model Poisoning Attacks to Byzantine-Robust Federated Learning,” in USENIX Security Symposium, 2020.

[16] M. Fang, N. Z. Gong, and J. Liu, “Influence Function Based Data Poisoning Attacks to Top-N Recommender Systems,” in Proceedings of The Web Conference 2020, 2020, pp. 3019–3025.

[17] M. Fang, G. Yang, N. Z. Gong, and J. Liu, “Poisoning Attacks to Graph-Based Recommender Systems,” in Proceedings of the 34th Annual Computer Security Applications Conference, 2018, pp. 381–392.

[18] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain,” in Machine Learning and Computer Security Workshop, 2017.

[19] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” in CVPR, 2016, pp. 770–778.

[20] J. Jia, B. Wang, X. Cao, and N. Z. Gong, “Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing,” in Proceedings of The Web Conference 2020, 2020, pp. 2718–2724.

[21] J. N. Kather, C.-A. Weis, F. Bianconi, S. M. Melchers, L. R. Schad, T. Gaiser, A. Marx, and F. G. Zöllner, “Multi-Class Texture Analysis in Colorectal Cancer Histology,” Scientific Reports, vol. 6, p. 27988, 2016.

[22] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon, “Federated Learning: Strategies for Improving Communication Efficiency,” in NIPS Workshop on Private Multi-Party Machine Learning, 2016.

[23] A. Krizhevsky, G. Hinton et al., “Learning Multiple Layers of Features from Tiny Images,” 2009.

[24] Y. LeCun, C. Cortes, and C. Burges, “MNIST Handwritten Digit Database,” Available: http://yann.lecun.com/exdb/mnist, 1998.

[25] B. Li, Y. Wang, A. Singh, and Y. Vorobeychik, “Data Poisoning Attacks on Factorization-Based Collaborative Filtering,” in NIPS, 2016.

[26] L. Li, W. Xu, T. Chen, G. B. Giannakis, and Q. Ling, “RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets,” in AAAI, vol. 33, 2019, pp. 1544–1551.

[27] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang, “Trojaning Attack on Neural Networks,” in NDSS, 2018.

[28] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-Efficient Learning of Deep Networks from Decentralized Data,” in AISTATS, 2017.

[29] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault, “The Hidden Vulnerability of Distributed Learning in Byzantium,” in ICML, 2018.

[30] L. Muñoz-González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee, E. C. Lupu, and F. Roli, “Towards Poisoning of Deep Learning Algorithms with Back-Gradient Optimization,” in AISec, 2017.

[31] L. Muñoz-González, K. T. Co, and E. C. Lupu, “Byzantine-Robust Federated Machine Learning through Adaptive Model Averaging,” arXiv preprint arXiv:1909.05125, 2019.

[32] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein, U. Saini, C. Sutton, J. D. Tygar, and K. Xia, “Exploiting Machine Learning to Subvert Your Spam Filter,” in LEET, 2008.

[33] Y. Nesterov and V. Spokoiny, “Random Gradient-Free Minimization of Convex Functions,” vol. 17, no. 2. Springer, 2017, pp. 527–566.

[34] S. Rajput, H. Wang, Z. Charles, and D. Papailiopoulos, “Detox: A Redundancy-Based Framework for Faster and More Robust Gradient Aggregation,” in NIPS, 2019, pp. 10 320–10 330.

[35] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. Tygar, “Antidote: Understanding and Defending Against Poisoning of Anomaly Detectors,” in ACM IMC, 2009.

[36] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, “Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,” in NIPS, 2018.

[37] O. Suciu, R. Marginean, Y. Kaya, H. D. III, and T. Dumitras, “When Does Machine Learning Fail? Generalized Transferability for Evasion and Poisoning Attacks,” in USENIX Security Symposium, 2018.

[38] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data Poisoning Attacks against Federated Learning Systems,” arXiv preprint arXiv:2007.08432, 2020.

[39] R. Vershynin, “Introduction to the Non-Asymptotic Analysis of Random Matrices,” arXiv preprint arXiv:1011.3027, 2010.

[40] M. J. Wainwright, “High-Dimensional Statistics: A Non-Asymptotic Viewpoint,” vol. 48. Cambridge University Press, 2019.

[41] B. Wang and N. Z. Gong, “Attacking Graph-Based Classification via Manipulating the Graph Structure,” in CCS, 2019.

[42] H. Xiao, K. Rasul, and R. Vollgraf. (2017) Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms.

[43] C. Xie, K. Huang, P.-Y. Chen, and B. Li, “DBA: Distributed Backdoor Attacks against Federated Learning,” in ICLR, 2020.

[44] C. Xie, S. Koyejo, and I. Gupta, “Zeno: Distributed Stochastic Gradient Descent with Suspicion-Based Fault-Tolerance,” in ICML, 2019, pp. 6893–6901.

[45] G. Yang, N. Z. Gong, and Y. Cai, “Fake Co-Visitation Injection Attacks to Recommender Systems,” in NDSS, 2017.

[46] H. Yang, X. Zhang, M. Fang, and J. Liu, “Byzantine-Resilient Stochastic Gradient Descent: A Lipschitz-Inspired Coordinate-Wise Median Approach,” in CDC, IEEE, 2019, pp. 5832–5837.

[47] Z. Yang and W. U. Bajwa, “Byrdie: Byzantine-Resilient Distributed Coordinate Descent for Decentralized Learning,” IEEE Transactions on Signal and Information Processing over Networks, vol. 5, no. 4, pp. 611–627, 2019.

[48] D. Yin, Y. Chen, K. Ramchandran, and P. Bartlett, “Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates,” in ICML, 2018.

[49] Z. Zhang, J. Jia, B. Wang, and N. Z. Gong, “Backdoor Attacks to Graph Neural Networks,” arXiv preprint arXiv:2006.11165, 2020.

### Appendix

#### Proof of Theorem 1
Before proving Theorem 1, we first restate our FLTrust algorithm and prove some lemmas. In our setting where \( R_l = 1 \), only the combined learning rate \( \alpha \cdot \beta \) influences FLTrust. Therefore, given a combined learning rate, we can always set \( \beta = 1 \) and let \( \alpha \) be the combined learning rate. In this case, the local model update \( g_i \) and server update \( g_0 \) are equivalent to the gradients of the \( i \)-th client and the server, respectively. We denote by \( S \) the set of clients whose cosine similarity \( c_i \) is positive in the \( t \)-th global iteration. Let \( \bar{g}_i = \frac{\|g_0\|}{\|g_i\|} \cdot g_i \) and \( \phi_i = \text{ReLU}(c_i) \). Then, we can rewrite Equation (4) as:

\[
g = \sum_{i \in S} \phi_i \bar{g}_i, \quad \text{s.t.} \quad \sum_{i \in S} \phi_i = 1, \quad \phi_i > 0 \quad \text{for} \quad i \in S.
\]

**Proof:**
Since \( \nabla F(w^*) = 0 \), we have the following:

\[
\|w_{t-1} - w^* - \alpha \nabla F(w_{t-1})\|^2 = \|w_{t-1} - w^* - \alpha (\nabla F(w_{t-1}) - \nabla F(w^*))\|^2
\]

\[
= \|w_{t-1} - w^*\|^2 + \alpha^2 \|\nabla F(w_{t-1}) - \nabla F(w^*)\|^2 - 2\alpha \langle w_{t-1} - w^*, \nabla F(w_{t-1}) - \nabla F(w^*) \rangle.
\]

By Assumption 1, we have:

\[
\|\nabla F(w_{t-1}) - \nabla F(w^*)\| \leq L \|w_{t-1} - w^*\|,
\]

\[
F(w^*) + \langle \nabla F(w^*), w_{t-1} - w^* \rangle \leq F(w_{t-1}),
\]

\[
F(w_{t-1}) + \langle \nabla F(w_{t-1}), w^* - w_{t-1} \rangle \leq F(w^*).
\]

Summing up inequalities (29) and (30), we have:

\[
\langle w^* - w_{t-1}, \nabla F(w_{t-1}) - \nabla F(w^*) \rangle \geq -\frac{\mu}{2} \|w_{t-1} - w^*\|^2.
\]

Substituting inequalities (28) and (31) into (27), we have:

\[
\|w_{t-1} - w^* - \alpha \nabla F(w_{t-1})\|^2 \leq \left(1 + \alpha^2 L^2 - \alpha \mu \right) \|w_{t-1} - w^*\|^2.
\]

By choosing \( \alpha = \frac{\mu}{2L^2} \), we have:

\[
\|w_{t-1} - w^* - \alpha \nabla F(w_{t-1})\|^2 \leq \left(1 - \frac{\mu^2}{4L^2}\right) \|w_{t-1} - w^*\|^2,
\]

which concludes the proof.

**Lemma 3:**
Suppose \( \Delta_1 = \sqrt{\frac{2\sigma_1^2 (d \log 6 + \log(3/\delta))}{|D_0|}} \) and \( \Delta_3 = \sqrt{\frac{2\sigma_2^2 (d \log 6 + \log(3/\delta))}{|D_0|}} \). If \( \Delta_1 \leq \frac{\sigma_1^2}{\gamma^2} \) and \( \Delta_3 \leq \frac{\sigma_2^2}{\gamma^2} \), then we have:

\[
\Pr \left( \left\| \frac{1}{|D_0|} \sum_{X_i \in D_0} \nabla h(X_i, w) - \mathbb{E}[\nabla h(X, w)] \right\| \geq 2\Delta_1 \right) \leq \delta,
\]

\[
\Pr \left( \left\| \frac{1}{|D_0|} \sum_{X_i \in D_0} \nabla f(X_i, w^*) - \nabla F(w^*) \right\| \geq 2\Delta_3 \|w - w^*\| \right) \leq \delta.
\]