### References

1. N. J. Hopper, J. Langford, and L. von Ahn, “Provably secure steganography,” in *CRYPTO 2002* (M. Yung, ed.), vol. 2442 of LNCS, pp. 77–92, Springer, Heidelberg, Aug. 2002.
2. L. von Ahn and N. J. Hopper, “Public-key steganography,” in *EUROCRYPT 2004* (C. Cachin and J. Camenisch, eds.), vol. 3027 of LNCS, pp. 323–341, Springer, Heidelberg, May 2004.
3. M. Backes and C. Cachin, “Public-key steganography with active attacks,” in *TCC 2005* (J. Kilian, ed.), vol. 3378 of LNCS, pp. 210–226, Springer, Heidelberg, Feb. 2005.
4. N. Dedic, G. Itkis, L. Reyzin, and S. Russell, “Upper and lower bounds on black-box steganography,” in *TCC 2005* (J. Kilian, ed.), vol. 3378 of LNCS, pp. 227–244, Springer, Heidelberg, Feb. 2005.
5. C. Grothoff, K. Grothoff, L. Alkhutova, R. Stutsman, and M. Atallah, “Translation-based steganography,” in *International Workshop on Information Hiding*, pp. 219–233, Springer, 2005.
6. M. Shirali-Shahreza and M. H. Shirali-Shahreza, “Text steganography in SMS,” *2007 International Conference on Convergence Information Technology (ICCIT 2007)*, pp. 2260–2265, 2007.
7. Z. Yu, L. Huang, Z. Chen, L. Li, X. Zhao, and Y. Zhu, “Steganalysis of synonym-substitution based natural language watermarking,” 2009.
8. C.-Y. Chang and S. Clark, “Linguistic steganography using automatically generated paraphrases,” in *Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10*, (Stroudsburg, PA, USA), pp. 591–599, Association for Computational Linguistics, 2010.
9. C.-Y. Chang and S. Clark, “Practical linguistic steganography using contextual synonym substitution and a novel vertex coding method,” *Computational Linguistics*, vol. 40, pp. 403–448, Jun 2014.
10. T. Fang, M. Jaggi, and K. Argyraki, “Generating steganographic text with LSTMs,” *Proceedings of ACL 2017, Student Research Workshop*, 2017.
11. D. Volkhonskiy, I. Nazarov, B. Borisenko, and E. Burnaev, “Steganographic generative adversarial networks,” 2017.
12. Z. Yang, S. Jin, Y. Huang, Y. Zhang, and H. Li, “Automatically generate steganographic text based on Markov model and Huffman coding,” 2018.
13. L. Xiang, “Reversible natural language watermarking using synonym substitution and arithmetic coding,” 2018.
14. Z. Yang, X. Guo, Z. Chen, Y. Huang, and Y. Zhang, “RNN-Stega: Linguistic steganography based on recurrent neural networks,” *IEEE Transactions on Information Forensics and Security*, vol. 14, pp. 1280–1295, May 2019.
15. S.-Y. Huang and P.-S. Huang, “A homophone-based Chinese text steganography scheme for chatting applications,” *Journal of Information Science & Engineering*, vol. 35, no. 4, 2019.
16. F. Dai and Z. Cai, “Towards near-imperceptible steganographic text,” *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, 2019.
17. Z. M. Ziegler, Y. Deng, and A. M. Rush, “Neural linguistic steganography,” 2019.
18. Z. Yang, Y. Huang, and Y.-J. Zhang, “A fast and efficient text steganalysis method,” *IEEE Signal Processing Letters*, vol. 26, pp. 627–631, 2019.
19. Z. Yang, K. Wang, J. Li, Y. Huang, and Y. Zhang, “TS-RNN: Text steganalysis based on recurrent neural networks,” *IEEE Signal Processing Letters*, p. 1–1, 2019.
20. Z. Yang, N. Wei, J. Sheng, Y. Huang, and Y.-J. Zhang, “TS-CNN: Text steganalysis from semantic space based on convolutional neural network,” 2018.
21. A. Wilson, P. Blunsom, and A. Ker, “Detection of steganographic techniques on Twitter,” *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, 2015.
22. J. Kodovsky, J. Fridrich, and V. Holub, “Ensemble classifiers for steganalysis of digital media,” *IEEE Transactions on Information Forensics and Security*, vol. 7, pp. 432–444, April 2012.
23. P. Meng, L. Huang, Z. Chen, W. Yang, and D. Li, “Linguistic steganography detection based on perplexity,” in *2008 International Conference on MultiMedia and Information Technology*, pp. 217–220, Dec 2008.
24. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” *OpenAI Blog*, vol. 1, no. 8, 2019.
25. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” 2020.
26. O. Blog, “Better language models and their implications.” Available at https://openai.com/blog/better-language-models/, February 2019.
27. N. J. Hopper, “Toward a theory of steganography,” tech. rep., CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE, 2004.
28. A. Karpathy, “The unreasonable effectiveness of recurrent neural networks.” https://karpathy.github.io/2015/05/21/rnn-effectiveness/, May 2015.
29. A. van Dalen, “The algorithms behind the headlines,” *Journalism Practice*, vol. 6, no. 5-6, pp. 648–658, 2012.
30. A. Sen, S. Alfeld, X. Zhang, A. Vartanian, Y. Ma, and X. Zhu, “Training set camouflage,” *Decision and Game Theory for Security*, p. 59–79, 2018.
31. M. Chaumont, “Deep learning in steganography and steganalysis from 2015 to 2019,” 2019.
32. Z. Durumeric, E. Wustrow, and J. A. Halderman, “ZMap: Fast internet-wide scanning and its security applications,” in *Presented as part of the 22nd USENIX Security Symposium (USENIX Security 13)*, pp. 605–620, 2013.
33. S. Cutler, “Project 25499 IPv4 HTTP scans.” https://scans.io/study/mi.
34. HuggingFace, “huggingface/swift-coreml-transformers.” https://github.com/huggingface/swift-coreml-transformers, Oct 2019.
35. T. Simonite, “Apple’s latest iPhones are packed with AI smarts.” https://www.wired.com/story/apples-latest-iphones-packed-with-ai-smarts/.
36. S. J. Oh, B. Schiele, and M. Fritz, “Towards reverse-engineering black-box neural networks,” in *Explainable AI: Interpreting, Explaining and Visualizing Deep Learning*, pp. 121–144, Springer, 2019.
37. A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes, “ML-Leaks: Model and data independent membership inference attacks and defenses on machine learning models,” arXiv preprint arXiv:1806.01246, 2018.
38. M. Juuti, S. Szyller, S. Marchal, and N. Asokan, “PRADA: Protecting against DNN model stealing attacks,” in *2019 IEEE European Symposium on Security and Privacy (EuroS&P)*, pp. 512–527, IEEE, 2019.

### Efficiency of Meteor

We now show that the asymptotic expected throughput of Meteor is proportional to the entropy in the communication channel. Recall that the entropy in a distribution \( P \) is computed as:
\[
H(P) = -\sum_{i} p_i \log_2(p_i)
\]
where \( p_i \) is the probability of the \( i \)-th possible outcome of \( P \). Similarly, the expected throughput of Meteor can be computed as:
\[
\text{Expected Throughput} = \sum_{i} p_i \cdot \text{Exp}(p_i)
\]
where \(\text{Exp}(\cdot)\) is the expected number of shared prefix bits for some continuous interval of size \( p_i \). Thus, the remaining task is to compute a concrete bound on \(\text{Exp}(\cdot)\).

We will make the simplifying assumption that the start of an interval \( p_i \) is placed randomly between \([0, 2^{\beta+1})\). Note that interval \( i \) will never start after \( 2^{\beta+1} - p_i \) in practice, so we assume the number of prefix bits in this case to be 0. This simplification will lead to an expected throughput strictly less than the true value. Additionally, the starting locations for each interval are not independent in practice, as they each depend on \( p_j \neq i \). However, this independence assumption also leads to equal or lower expected throughput, as the starting point for larger intervals will actually be more biased towards the middle of the distribution, where \(\text{Exp}(\cdot)\) will be lower, and smaller distributions will be biased to start near the edges of the distribution, where \(\text{Exp}(\cdot)\) will be higher.

For a given small \(\epsilon\), if \( i \) starts between \([0, \epsilon)\), then it is contained completely before the prefix 01 begins, and thus would transmit 2 bits. The following \( p_i \) starting points all transmit only 1 bit.