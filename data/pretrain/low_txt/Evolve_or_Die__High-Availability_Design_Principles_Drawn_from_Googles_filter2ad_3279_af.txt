### Lessons and Avenues for Future Research in High-Availability Design

Our lessons highlight several avenues for research in high-availability design, an area that has received less attention compared to high-performance design. Each lesson embodies a broad research area, where our deployed solution represents one point in a large design space that future research can explore. Some potential directions for future research include:

1. **Optimal Topology Partitioning and Control Domains**: Investigating the best ways to partition topologies and control domains to contain the impact of failures.
2. **Dynamic Fallback Mechanisms**: Developing dynamic methods to quickly assess when to fall back and which traffic to divert to ensure a smooth, almost transparent fallback.
3. **Static Consistency Reasoning and State Provenance**: Creating methods to statically reason about the consistency of control plane state and track state provenance to ensure consistency.
4. **Scalable In-Band Measurement Methods**: Designing scalable in-band measurement methods that permit fast and accurate failure localization while being robust to failures and attacks.
5. **Fail-Open Detection and State Reconciliation**: Techniques to robustly detect fail-open conditions and correctly reconcile control and data plane states in a failed-open system upon recovery.

### Management Plane Research

A significant, yet underexplored area in high-availability design is the management plane. Future research in this area could focus on:

1. **Intent Specification and Network Configuration**: Exploring how to specify "intent" (i.e., what the network should look like) and how to configure and provision the network based on this intent.
2. **Ground Truth Collection and Reconciliation**: Methods to collect a snapshot of the network's "ground truth" (i.e., what the network actually looks like) and reconcile it with the intended configuration, as there is likely to be divergence between the two even with automated configuration and provisioning.
3. **Automated and Accurate Risk Assessment**: Developing mechanisms for automated and accurate risk assessment and enabling safe, yet frequent, network evolution using upgrade-in-place techniques.

### Overarching Challenges

We also identify several overarching challenges in high-availability design:

1. **Defining and Measuring SLOs**: How to define and measure Service Level Objectives (SLOs) for a network in a way that services or applications can use for their design.
2. **Network Unavailability Criteria**: Determining when a network is truly unavailable (e.g., when it drops a few packets, when its throughput falls below a certain threshold).
3. **Quantifying Availability Improvements**: Techniques to quantify improvements in availability.

### Related Work

#### Failures in Engineered Systems

Generic reasons for failures in engineered systems [7, 32] include heterogeneity and the impact of interactions and coupling between components. Our work focuses on a modern large-scale content provider, identifying specific classes of reasons why our networks fail, many of which can be attributed to the rapid evolution of our networks.

#### Network Failures in Distributed Systems

Network failures and their impact have been studied extensively in distributed systems. From impossibility results [1] to techniques for designing failure-tolerant distributed systems [9], to experience reports on individual failures in practical, large distributed systems [28, 36], these studies provide insights into real-world failures and their consequences. Bailis and Kingsbury [30] discuss publicly disclosed failures in deployed distributed systems caused by network partitions. Unlike this body of work, our focus is on failures in the network control, data, and management planes. While some of our failures are similar to those in distributed systems (e.g., failure cascades and split-brain failures), others, such as control plane network failures and management plane operations, do not arise or have less impact in distributed systems.

#### Network Failure Assessment

The networking literature has explored various ways to assess and quantify failures in networks over more than a decade. Early work examined link failure characteristics in medium to large-scale ISPs [33, 26, 39] and EGPs [22]. Our work explores a broader class of root causes for availability failures, ranging from device resource limitations to control plane bugs and management plane errors. More recent work has explored link, device, and component failures in enterprises [37], academic networks [34], and data centers [13], using sources like syslog errors, trouble tickets, and customer complaints. Our data source, post-mortem reports, are carefully curated and include root-cause assessments confirmed through lab or limited field settings, allowing us to broadly and accurately assess root causes across different types of networks.

Other work has focused on the role of misconfiguration in network failures [29, 31] and methods to reduce misconfiguration errors with shadow network configurations [2]. Recent work has also explored the impact of management plane operations on network health [12], identifying these operations by changes in device configurations or network topology, and measuring network health by the number of device alerts. Our work differs in that we use human-curated post-mortems, fully documented operations, and focus on availability impacts rather than general network health.

### Conclusions

By analyzing post-mortem reports at Google, we show that failures occur in all of our networks and across all planes. Many failures occur during management operations, which are fundamental given the rapid evolution of our networks. These failures have prompted us to adopt several high-availability design principles and associated mechanisms, including preserving the data plane upon failure, containing the failure radius, and designing fallbacks for systemic failure, as well as automated risk assessment and management plane automation. Our experience suggests that future networks must account for continuous evolution and upgrade as a key part of their availability architecture and design.

### Acknowledgements

We gratefully acknowledge the excellent feedback we have received from the reviewers and from our shepherd, Vyas Sekar. Discussions and feedback from Paulie Germano, Hossein Lotfi, Subhasree Mandal, Joon Ong, Arjun Singh, and David Wetherall significantly improved the paper. Finally, this work would not have been possible without the painstaking efforts of Google’s network operations, SRE, and development teams, who design, manage, and run our global network and carefully document and root-cause each significant failure.

### Bibliography

[1] Daniel Abadi. “Consistency Tradeoffs in Modern Distributed Database Design: CAP is Only Part of the Story”. In: IEEE Computer (2012).

[2] Richard Alimi, Ye Wang, and Yang Richard Yang. “Shadow configuration as a network management primitive”. In: Proc. ACM SIGCOMM. 2008.

[3] C. Ashton. What is the Real Cost of Network Downtime? http://www.lightreading.com/data-center/data-center-infrastructure/whats-the-real-cost-of-network-downtime/a/d-id/710595. 2014.

[4] B. Schneier. Security in the Cloud. https://www.schneier.com/blog/archives/2006/02/security_in_the.html. 2006.

[5] Betsy Beyer and Niall Richard Murphy. “Site Reliability Engineering: How Google Runs its Production Clusters”. In: O’Reilly, 2016. Chap. 1.

[6] Matt Calder, Xun Fan, Zi Hu, Ethan Katz-Bassett, John Heidemann, and Ramesh Govindan. “Mapping the Expansion of Google’s Serving Infrastructure”. In: Proc. of the ACM Internet Measurement Conference (IMC ’13). 2013.

[7] Carlson, J. M. and Doyle, John. “Highly Optimized Tolerance: Robustness and Design in Complex Systems”. In: Phys. Rev. Lett. 84 (Nov 2000), pp. 2529–2532.

[8] Cisco Visual Networking Index: The Zettabyte Era – Trends and Analysis. http://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/VNI_Hyperconnectivity_WP.html. 2014.

[9] Jeff Dean. Designs, Lessons and Advice from Building Large Distributed Systems. Keynote at LADIS 2009.

[10] E. Dubrova. “Fault-Tolerant Design”. In: Springer, 2013. Chap. 2.

[11] Tobias Flach et al. “Reducing Web Latency: the Virtue of Gentle Aggression”. In: Proc. ACM SIGCOMM. 2013.

[12] Aaron Gember-Jacobson, Wenfei Wu, Xiujun Li, Aditya Akella, and Ratul Mahajan. “Management Plane Analytics”. In: Proceedings of ACM IMC. IMC ’15. Tokyo, Japan: ACM, 2015, pp. 395–408. ISBN: 978-1-4503-3848-6.

[13] P. Gill, N. Jain, and N. Nagappan. “Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications”. In: Proc. ACM SIGCOMM. 2011.

[14] Chuanxiong Guo et al. “Pingmesh: A Large-Scale System for Data Center Network Latency Measurement and Analysis”. In: SIGCOMM Comput. Commun. Rev. 45.5 (Aug. 2015), pp. 139–152. ISSN: 0146-4833.

[15] R. Hinden. Virtual Router Redundancy Protocol. Internet Engineering Task Force, RFC 3768. 2004.

[16] Internet hiccups today? You’re not alone. Here’s why. http://www.zdnet.com/article/internet-hiccups-today-youre-not-alone-heres-why/.

[17] Y. Israelevtsky and A. Tseitlin. The Netflix Simian Army. http://techblog.netflix.com/2011/07/netflix-simian-army.html. 2011.

[18] Sushant Jain et al. “B4: Experience with a Globally-deployed Software Defined WAN”. In: Proceedings of the ACM SIGCOMM 2013. SIGCOMM ’13. Hong Kong, China: ACM, 2013, pp. 3–14. ISBN: 978-1-4503-2056-6.

[19] Juniper Networks MX 2020. http://www.juniper.net/elqNow/elqRedir.htm?ref=http://www.juniper.net/assets/us/en/local/pdf/datasheets/1000417-en.pdf.

[20] K. Krishnan. “Weathering the Unexpected”. In: ACM Queue (2012).

[21] Alok Kumar et al. “BwE: Flexible, Hierarchical Bandwidth Allocation for WAN Distributed Computing”. In: Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication. SIGCOMM ’15. London, United Kingdom: ACM, 2015, pp. 1–14. ISBN: 978-1-4503-3542-3.

[22] Craig Labovitz, Abha Ahuja, and Farnam Jahanian. “Experimental Study of Internet Stability and Wide-Area Network Failures”. In: Proc. International Symposium on Fault-Tolerant Computing. 1999.

[23] G. Linden. Make Data Useful. http://sites.google.com/site/glinden/Home/StanfordDataMining.2006-11-28.ppt. 2006.

[24] M. Canini, D. Venzano, P. Peresini, D. Kostic, and J. Rexford. “A NICE Way to Test OpenFlow Applications”. In: Presented as part of the 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI 12). San Jose, CA: USENIX, 2012, pp. 127–140. ISBN: 978-931971-92-8.

[25] M. Kuzniar, P. Peresini, M. Canini, D. Venzano, and D. Kostic. “A SOFT Way for OpenFlow Switch Interoperability Testing”. In: Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies. CoNEXT ’12. Nice, France: ACM, 2012, pp. 265–276. ISBN: 978-1-4503-1775-7.

[26] A. Markopoulou, G. Iannaccone, S. Bhattacharyya, C.-N. Chuah, Y. Ganjali, and C. Diot. “Characterization of Failures in an Operational IP Backbone Network”. In: IEEE/ACM Transactions on Networking (2008).

[27] I. Minei and J. Lucek. MPLS-Enabled Applications: Emerging Developments and New Technologies. 3rd ed. Wiley Inc., 2015.

[28] Andrew Montalenti. Kafkapocalypse: A Post-Mortem on our Service Outage. Parse.ly Tech Blog post. 2015.

[29] N. Feamster and H. Balakrishnan. “Detecting BGP Configuration Faults with Static Analysis”. In: Proceedings of the 2nd Symposium on Networked Systems Design and Implementation. USENIX Association. 2005, pp. 43–56.

[30] P. Bailis and K. Kingsbury. “An Informal Survey of Real-World Communications Failures”. In: Communications of the ACM (2014).

[31] R. Mahajan, D. Wetherall, and T. Anderson. “Understanding BGP Misconfiguration”. In: Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications. SIGCOMM ’02. Pittsburgh, Pennsylvania, USA: ACM, 2002, pp. 3–16. ISBN: 1-58113-570-X.

[32] John Rushby. “Critical System Properties: Survey and Taxonomy”. In: Reliability Engineering and System Safety 43.2 (1994), pp. 189–219.

[33] A. Shaikh, C. Isett, A. Greenberg, M. Roughan, and J. Gottlieb. “A Case Study of OSPF Behavior in a Large Enterprise Network”. In: Proc. ACM Internet Measurement Workshop. 2002.

[34] A. Shaikh, C. Isett, A. Greenberg, M. Roughan, and J. Gottlieb. “California Fault Lines: Understanding the Causes and Impact of Network Failures”. In: Proc. ACM SIGCOMM. 2010.

[35] Arjun Singh et al. “Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network”. In: SIGCOMM Comput. Commun. Rev. 45.5 (Aug. 2015), pp. 183–197. ISSN: 0146-4833.

[36] Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region. http://aws.amazon.com/message/65648/. Amazon Web Services. 2011.

[37] D. Turner, K. Levchenko, J. C. Mogul, S. Savage, and A. C. Snoeren. On Failure in Managed Enterprise Networks. Tech. rep. HPL-2012-101. HP Labs, 2012.

[38] Amin Vahdat et al. “Scalability and Accuracy in a Large-scale Network Emulator”. In: SIGOPS Oper. Syst. Rev. 36.SI (Dec. 2002), pp. 271–284. ISSN: 0163-5980.

[39] D. Watson, F. Jahanian, and C. Labovitz. “Experiences With Monitoring OSPF on a Regional Service Provider Network”. In: Proc. IEEE ICDCS. 2003.