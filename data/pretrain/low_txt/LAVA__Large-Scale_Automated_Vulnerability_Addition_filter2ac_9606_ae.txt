### Evaluation of FUZZER and SES on LAVA-M Programs

We executed both FUZZER and SES against each program in the LAVA-M suite, with a runtime limit of 5 hours for each program. For `md5sum`, we used the `-c` argument to check digests in a file, while for `base64`, we used the `-d` argument to decode base64-encoded data.

#### Results from SES
- **Uniq and md5sum**: SES did not detect any bugs in `uniq` or `md5sum`. We believe this is due to the unconstrained control flow in `uniq` and the failure of SES to execute code beyond the first instance of the hash function in `md5sum`.
- **Base64 and who**: SES was more successful with `base64` and `who`. It identified 9 out of 44 inserted bugs in `base64`, including both deep and shallow bugs. For `who`, all detected bugs were associated with one of two Data Usage Artifacts (DUAs) and occurred early in the trace. This is partly because multiple bugs share the same attack point, which limits SES's ability to find additional bugs at the same location without covering new code.

#### Results from FUZZER
- **General Performance**: FUZZER found bugs in all utilities except `who`. The bugs were distributed fairly uniformly throughout the programs, as they depend on guessing the correct 4-byte trigger at the right position in the input file.
- **Failure with who**: FUZZER's inability to find bugs in `who` is surprising. We speculate that the size of the seed file (the first 768 bytes of a utmp file) may have been too large for effective random mutation. Further investigation is needed to determine the true cause.

#### Comparison of Bug Detection
- **Overlap**: There was minimal overlap between the bugs found by FUZZER and SES, with only 2 bugs detected by both tools. This is a promising result for LAVA, as it indicates that the bugs created are not tailored to a specific bug-finding strategy.

### Related Work

The design of LAVA is driven by the need for dynamic, realistic, and large-scale bug corpora. Here, we compare LAVA with existing bug corpora:

- **Realistic Corpora**: Researchers have proposed creating bug corpora from student code, existing bug report databases, and public bug registries. However, these corpora remain static and relatively small.
- **Synthetic Testbeds**: Wilander and Kamkar created synthetic test cases for buffer overflow attacks, but these may not reflect real-world bugs. Zitser et al. evaluated static buffer overflow detectors using a corpus of 14 annotated buffer overflows, but this corpus is limited in size and complexity.
- **NIST SAMATE Project**: The NIST SAMATE project includes Juliet, a collection of 86,864 synthetic C and Java programs, and the IARPA STONESOUP dataset, which injects bugs into small code snippets. These corpora, however, do not use the original program inputs and have short data flows.
- **Commercial Tool Analysis**: Shiraishi et al. constructed 400 pairs of C functions to evaluate commercial static analysis tools. While this approach provides a measure of tool effectiveness, the functions are relatively short and may be easier to detect than bugs in larger code bases.
- **Automatic Program Transformation**: Rinard et al. introduced off-by-one errors in the Pine email client to test software usability in the presence of errors.

### Limitations and Future Work

- **Bug Injection**: LAVA currently injects only buffer overflows but can potentially inject other types of bugs, such as temporal safety and meta-character bugs, using taint-based analysis. However, logic errors, cryptographic flaws, and side-channel vulnerabilities are likely beyond LAVA's capabilities.
- **Unintended Bugs**: LAVA sometimes introduces unintended bugs, such as use-after-free and dereference of uninitialized pointers. These artifacts can be reduced but may require intractable whole-program static analysis.
- **Language Support**: LAVA is currently limited to C source code, but the approach could be extended to other languages with suitable source-to-source rewriting frameworks.
- **Evaluation Metrics**: Future work includes evaluating real, named tools and measuring false alarm rates. Tools that generate triggering inputs, like SES and FUZZER, can easily have their false alarm rates measured, while static analyzers and abstract interpretation tools require more manual effort.

### Conclusion

LAVA is a fully automated system that can inject large numbers of realistic bugs into C programs. It has already been used to introduce over 4,000 buffer overflows into open-source Linux C programs. LAVA is ready for immediate use as an on-demand source of realistic ground truth vulnerabilities, driving the development and evaluation of advanced vulnerability discovery tools.

### Acknowledgements

We thank Graham Baker, Chris Connelly, Jannick Pewny, and Stelios Sidiroglou-Douskos for valuable discussions and suggestions. Special thanks to Amy Jiang for critical initial Clang development and debugging.

### References

[References listed here as per the original text]

---

This revised version aims to provide a clearer, more coherent, and professional presentation of the content.