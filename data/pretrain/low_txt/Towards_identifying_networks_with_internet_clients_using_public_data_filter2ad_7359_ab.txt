### Global Anycast and PoP Analysis

Anycast routes to different Points of Presence (PoPs) globally. The `dig @8.8.8.8 o-o.myaddr.l.google.com -t TXT` command returns the specific PoP that is reached. Our measurements were conducted from AWS and Vultr cloud VMs worldwide, covering 22 out of the 45 Google Public DNS PoPs (indicated by red dots in Figure 1). We tested all AWS regions and reached 16 PoPs, with an additional 6 PoPs covered by Vultr. 

Our measurements encompass PoPs in:
- United States (seven states)
- Canada (two provinces)
- Asia (five countries/regions)
- Europe (five countries)
- South America (two countries)
- Australia

These 22 PoPs account for 95% of Google Public DNS queries to Microsoft services. We verified with Google that some PoPs we did not reach are inactive, and 18 of the 23 PoPs we do not cover do not issue any queries to Microsoft, suggesting they may be inactive. Appendix A.1 (Figure 5) shows the PoPs we did not measure.

### Measurement Optimization

To reduce the number of measurements, we do not query each prefix at every Google Public DNS PoP. Instead, we query a prefix only at its most likely PoPs. Since anycast typically routes clients to nearby PoPs, we use MaxMind to map each /24 prefix to a geolocation. We first queried each PoP with 78,637 prefixes randomly selected from the public IPv4 address space, where MaxMind indicates an error radius smaller than 200 km. For each PoP, we determined the radius that contains 90% of the prefixes that returned a cache hit for at least one of the four most popular domains in the Alexa top global sites list, which support ECS and have TTLs greater than one minute. This radius is considered the likely service radius for that PoP. In subsequent measurements, we queried a PoP only for prefixes that MaxMind places within the PoP’s service radius (combining the MaxMind location and error radius for the prefix).

Figure 2 justifies this approach, showing that for three PoPs with diverse geographies, the service radius ranges from 478 km to 3,273 km. Using per-PoP service radii results in an average of 2.4 million /24 prefixes to probe at each PoP, compared to an average of 4.4 million if we used the maximum service radius of 5,524 km (used for Zurich) for all PoPs.

### Probing Details

We selected the four most popular domains from the Alexa top sites global list that both support ECS and have TTLs greater than one minute (as of September 22, 2021):
- www.google.com (rank 1)
- www.youtube.com (rank 2)
- facebook.com (rank 7)
- www.wikipedia.org (rank 13)

We also included one popular domain hosted by Microsoft Azure Traffic Manager that supports ECS and has a TTL of 5 minutes, which we used to validate our methodology. We issued cache probes for 120 hours at a rate of 50 prefixes per second per domain at each PoP, continuously looping over the list of assigned prefixes. Since Google Public DNS employs multiple independent cache pools at each PoP, we issued 5 redundant queries for each ⟨PoP, prefix, domain⟩ combination to increase the likelihood of covering multiple caches. We used DNS over TCP instead of UDP to avoid triggering the lower rate limit for repeated queries. A prefix was considered active if Google Public DNS returned a cache hit for any domain indicating the prefix with a return scope > 0 for an ECS query.

### Strengths and Limitations

**Strengths:**
- Google Public DNS cache probing can be replicated without requiring privileged access or data.
- It directly measures (likely) active client prefixes rather than activity from recursive resolvers or another proxy of client activity.
- It allows for developing rich signals by combining observations across time and domains.

**Limitations:**
- It measures active use of Google Public DNS, which is popular but may have skewed adoption along various dimensions.
- DNS's use of recursive resolvers and caching introduces complexities in comparing activity levels across ECS prefixes.
- Quantifying how much a DNS record was used from cache within its TTL and cross-prefix comparisons are challenging due to differences in addressing (including NAT) within an ECS prefix.

### Chromium DNS Queries

We call our second approach "DNS logs." We look for DNS queries matching a signature of the Chromium web browser codebase, which is part of browsers including Chrome, Microsoft Edge, Brave, and Opera. The number of Chromium DNS queries from a prefix is an indicator of the level of client activity.

**Methodology:**
Chromium detects DNS interception by querying for random strings of 7-15 lowercase letters. These queries are sent when the browser starts and when the device’s IP address or DNS configuration changes. Because these queries often have no valid TLD appended, they should not result in cache hits at recursive resolvers and go to a DNS root server. To separate Chromium queries from others, we use the heuristic that randomly generated strings likely have few collisions. Empirical simulations show that Chromium queries would collide fewer than 7 times per day across all roots with 99% probability.

We looked for queries matching this pattern queried less often than our daily threshold in the DITL traces, which contain 2 days of queries to most root DNS servers. The queries in the traces include the IP address of the querier, generally the recursive resolver used by the Chromium client. We processed J, H, M, A, K, and D root, the roots that offer un-anonymized, complete traces in the most recent available (2020) DITL.

**Strengths and Limitations:**

**Strengths:**
- Provides a direct, precise signal with global coverage.
- Counting Chromium queries offers truly global coverage.
- Can be done by many researchers through access to DNS-OARC or via collaboration with a root deployment.

**Limitations:**
- IP addresses seen in root DNS packet traces are of recursive resolvers, providing a signal of client activity at the recursive resolver level rather than at the prefix or AS level.
- User activity and the presence of Chromium queries are not perfectly correlated.
- DITL traces are only available yearly and do not contain all root letters, limiting time-based analysis.
- The implementation of this feature is subject to change, and the Chromium team has shown interest in reducing the number of DNS queries going to the root DNS.

### Validation and Cross-Comparison

**Datasets:**
- **APNIC User Estimates (APNIC):** Uses a heuristic based on Google Ad volumes to generate user population estimates by AS.
- **Private Datasets:** Contain measures of client activity for two popular Microsoft Azure services: CDN and DNS Traffic Manager. These services are used by billions of users in tens of thousands of ASes and hundreds of countries/regions daily.

**Comparisons:**
- **Microsoft Clients:** Proportional to the number of times clients access the CDN, aggregated by client IP address.
- **Microsoft Resolvers:** Count of client IP addresses that the CDN observes using each recursive resolver, aggregated by recursive resolver IP address.
- **Cloud ECS Prefixes:** Set of ECS prefixes observed in DNS queries for authoritative records of Traffic Manager, Azure’s DNS-based load balancing for cloud tenants.

**Results:**
- DNS activity is a good proxy for web client activity. Comparing /24 prefixes from Microsoft clients (no ECS) with ECS prefixes from cloud ECS prefixes, the CDN sees HTTP requests from prefixes responsible for 97.2% of the DNS queries. Prefixes seen in ECS queries are responsible for 92% of the HTTP requests to the CDN.
- Cache probing recovers most DNS activity. Our cache probing includes 91% of the ground truth ECS/24 prefixes, showing that our approach can uncover the vast majority of a service’s client population using Google Public DNS.
- **AS-Level Results:** Pairwise comparisons show that 66,804 ASes were in at least one dataset, with 64,766 (97%) being in the Microsoft clients. APNIC misses 64% of the ASes observed as hosting Microsoft clients. Our techniques miss only 40.1% (DNS logs) and 44.5% (cache probing), recovering 74.2% and 81.9% of the ASes observed by APNIC. Combined, they detect 51,859 ASes. The low overlap means combining our datasets yields more overlap with others. For example, cache probing ∪ DNS logs observes 21,866 ASes in APNIC (93.8%) and 50,006 ASes in Microsoft clients (77.2%).

**Analysis:**
- **AS Types:** Of all 29,973 ASes detected by our methods but absent in APNIC, ASdb categorizes 27,773 (92.7%). Of these, 10,998 (39.5%) are Internet Service Providers (ISPs), 4,823 (17.4%) are hosting/cloud providers, and 1,723 (6.2%) are schools.
- **Missed ASes:** Generally small. ASes that at least one of our techniques identifies as hosting clients account for 98.8% of the Microsoft clients queries (compared to 92% for APNIC).
- **Country Analysis:** In most countries, cache probing uncovers client activity in ASes that APNIC identifies as hosting all or almost all of the Internet users. For example, in the U.S., 100%, in India, 99%, and in China, 98%.

**Global Coverage:**
- Figure 1 plots the MaxMind geolocations of prefixes where cache probing detects activity. For each prefix with a return scope larger than /24, we assume all its /24 subprefixes are active. For return scopes smaller than /24, we assume the entire /24 prefix it belongs to is active. Our measurements infer more activity in some regions than others, e.g., Europe is more active than China.

**Prefix-Level Analysis:**
- Figure 4 depicts the fraction of /24 prefixes announced by each AS that our cache probing technique detects as active. When Google Public DNS returns a cache hit for a prefix with a scope bigger than /24, we know at least one /24 in the prefix has client activity, but we cannot infer exactly which or how many. We estimate upper and lower bounds.