# Schedule and Assessment Overview

## Skill Assessment
- **Targets Prepared**
- **Observers Ready**
- **15-Minute Overview**
- **60-Minute Test**

## Introductions
- **Sprint Hours:**
  - Apply targeting strategy
  - Hourly survey
  - Lunch

## Discussion
- **Team Interview:**
  - Utility
  - Interaction
  - Success
  - Failure

## Team Synchronization
- Repeated sessions for team synchronization

## Timeline
| Time       | Activity                       |
|------------|--------------------------------|
| 8:00 am    |                                |
| 8:30 am    |                                |
| 9:00 am    |                                |
| 9:30 am    |                                |
| 10:00 am   |                                |
| 10:30 am   |                                |
| 11:00 am   |                                |
| 11:30 am   |                                |
| 12:00 pm   |                                |
| 12:30 pm   |                                |
| 1:00 pm    |                                |
| 1:30 pm    |                                |
| 2:00 pm    |                                |
| 2:30 pm    |                                |
| 3:00 pm    |                                |
| 3:30 pm    |                                |
| 4:00 pm    |                                |

## USENIX Association
- **29th USENIX Security Symposium** (Page 1143)

## C. Hourly Questions
- What is your pseudonym?
- How many minutes were spent interacting with tools?
- How many minutes were spent harnessing?
- How much time was spent on research?
- Are you feeling productive, surprised, frustrated, doubtful, or confused?

## D. End-of-Day Questions
- What is your pseudonym?
- I learned something today.
- I felt frustrated today.
- I worked with another team member today (team lead excluded).
- I accomplished something today.
- I feel exhausted today.
- I enjoyed my work today.
- I learned a new skill today.
- I was bored today.

## E. End-of-Experiment Questions
1. Which vulnerability-discovery method do you feel was more effective?
2. Which method made you feel like you were part of a team?
3. Which method made the best use of your personal skills?
4. Which method made the best use of your team’s skills?
5. Which method did you think was easier to get started with?
6. Which method do you think is easier for a novice to contribute to?
7. Did you learn any valuable skills during the experiment?
8. Which method did you learn more from?
9. Which method did you enjoy more?
10. Which method frustrated you the most?
11. If you were asked to lead a vulnerability-discovery project, which method would you choose?
12. How prepared do you think you were for the vulnerability-discovery work before initial training?
13. How prepared do you think you were after initial training?
14. How prepared do you think you were after the experiment?
15. What was your interest in doing vulnerability-discovery work before the experiment?
16. What was your interest after the experiment?
17. How many unique bugs did you find during the experiment?
18. Which method of learning was best for you during the experiment? (Instructor-led training, hands-on experience, other)
19. Were there any external factors that affected your or your team’s performance during the experiment? (e.g., network outages, room temperature, experiment hours)
20. Do you have any thoughts or comments you would like us to consider?

## F. Skill Assessment Binaries
- **Collection:**
  - Cyber Grand Challenge
  - Rode0day (binary only)
  - Rode0day (with source)
  - OSS-Fuzz (with source)
- **Binaries:**
  - Childs_Game
  - Game_Night
  - Casino_Games
  - tcpdumpB
  - fileB
  - audiofileB
  - bzipS
  - jqS
  - jpegS
  - vorbis
  - libarchive
  - libxml2
  - c-ares
  - freetype2
  - openssl

## References
[1] Thanassis Avgerinos, Alexandre Rebert, Sang Kil Cha, and David Brumley. Enhancing symbolic execution with veritesting. Communications of the ACM, 59(6):93–100, May 2016.
[2] Domagoj Babic, Stefan Bucur, Yaohui Chen, Franjo Ivancic, Tim King, Markus Kusano, Caroline Lemieux, László Szekeres, and Wei Wang. FUDGE: Fuzz driver generation at scale. In Proceedings of the 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2019, page 975–985, New York, New York, USA, 2019. ACM.
[3] Carl Boettiger. An introduction to Docker for reproducible research. Operating Systems Review, 49(1):71–79, January 2015.
[4] Norbou Buchler, Prashanth Rajivan, Laura R Marusich, Lewis Lightner, and Cleotilde Gonzalez. Sociometrics and observational assessment of teaming and leadership in a cyber security defense competition. Computers & Security, 73:114–136, 2018.
[5] Sang Kil Cha, Thanassis Avgerinos, Alexandre Rebert, and David Brumley. Unleashing Mayhem on binary code. In Proceedings of the 2012 IEEE Symposium on Security and Privacy, SP ’12, pages 380–394, Washington, DC, USA, 2012. IEEE Computer Society.
[6] Gary Charness, Uri Gneezy, and Michael A Kuhn. Experimental methods: Between-subject and within-subject design. Journal of Economic Behavior & Organization, 81(1):1–8, 2012.
[7] Christian Cioce, Daniel Loffredo, and Nasser Salim. Program fuzzing on high performance computing resources. Technical Report SAND2019-0674, Sandia National Laboratories, Albuquerque, New Mexico, USA, January 2019. https://www.osti.gov/servlets/purl/1492735 [Accessed January 23, 2020].
[8] Elena F. Corriero. Counterbalancing. In Mike Allen, editor, The SAGE Encyclopedia of Communication Research Methods, volume 1. SAGE Publications, Thousand Oaks, California, USA, 2017.
[9] Richard Draeger. Within-subjects design. In Mike Allen, editor, The SAGE Encyclopedia of Communication Research Methods, volume 4. SAGE Publications, Thousand Oaks, California, USA, 2017.
[10] Florian Fainelli. The OpenWrt embedded development framework, February 2008. Invited talk at the 2008 Free and Open Source Software Developers European Meeting.
[11] Ming Fang and Munawar Haﬁz. Discovering buffer overflow vulnerabilities in the wild: An empirical study. In Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM ’14, New York, New York, USA, 2014. ACM.
[12] Andrew Fasano, Tim Leek, Brendan Dolan-Gavitt, and Josh Bundt. The rode0day to less-buggy programs. IEEE Security & Privacy, 17(6):84–88, November 2019.
[13] Ivan Fratric. 365 days later: Finding and exploiting Safari bugs using publicly available tools, October 2018. https://googleprojectzero.blogspot.com/2018/10/365-days-later-finding-and-exploiting.html [Accessed March 30, 2019].
[14] GitLab. Issue boards. https://about.gitlab.com/product/issueboard/ [Accessed December 17, 2019].
[15] Google. Fuzzer test suite. https://github.com/google/fuzzer-test-suite [Accessed December 18, 2019].
[16] Allen D. Householder, Garret Wassermann, Art Manion, and Chris King. The CERT(C) guide to coordinated vulnerability disclosure. https://resources.sei.cmu.edu/asset_files/SpecialReport/2017_003_001_503340.pdf [Accessed March 31, 2019].
[17] Schuyler W. Huck and Robert A. McLean. Using a repeated measures ANOVA to analyze the data from a pretest-posttest design: a potentially confusing task. Psychological Bulletin, 82(4):511–518, 1975.
[18] Robert Joyce. Come get your free NSA reverse engineering tool!, March 2019. Presentation at the 2019 RSA Conference.
[19] Brian Kernighan and Rob Pike. The UNIX Programming Environment. Prentice-Hall, Inc., Englewood Cliffs, New Jersey, USA, 1984.
[20] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. Evaluating fuzz testing. CoRR, abs/1808.09700, 2018.
[21] Valentin J. M. Manès, HyungSeok Han, Choong-woo Han, Sang Kil Cha, Manuel Egele, Edward J. Schwartz, and Maverick Woo. The art, science, and engineering of fuzzing: A survey. IEEE Transactions on Software Engineering, October 2019. Early Access.
[22] MITRE. CWE-125: Out-of-bounds read. https://cwe.mitre.org/data/definitions/125.html [Accessed February 3, 2020].
[23] MITRE. CWE-787: Out-of-bounds write. https://cwe.mitre.org/data/definitions/787.html [Accessed February 3, 2020].
[24] Nadim Nachar. The Mann-Whitney U: A test for assessing whether two independent samples come from the same distribution. Tutorials in Quantitative Methods for Psychology, 4(1):13–20, 2008.
[25] Cal Newport. Deep Work: Rules for Focused Success in a Distracted World. Grand Central Publishing, New York; Boston, 1st ed. edition, January 2016.
[26] Vegard Nossum. Fuzzing the OpenSSH daemon using AFL. http://www.vegardno.net/2017/03/fuzzing-openssh-daemon-using-afl.html [Accessed March 30, 2019].
[27] Anne Oeldorf-Hirsch. Between-subjects design. In Mike Allen, editor, The SAGE Encyclopedia of Communication Research Methods, volume 4. SAGE Publications, Thousand Oaks, California, USA, 2017.
[28] Hacker One. The 2019 hacker report. https://www.hackerone.com/resources/reporting/the-2019-hacker-report [Accessed December 4, 2019].
[29] OpenWrt. ubus (OpenWrt micro bus architecture). https://openwrt.org/docs/techref/ubus [Accessed January 22, 2020].
[30] Athanasios Papoulis and S. Unnikrishna Pillai. Probability, Random Variables, and Stochastic Processes. Tata McGraw-Hill Education, New York, New York, 2nd edition, 2002.
[31] Reginald E. Sawilla and Xinming Ou. Identifying critical attack assets in dependency attack graphs. In Sushil Jajodia and Javier López, editors, 13th European Symposium on Research in Computer Security, volume 5283 of Lecture Notes in Computer Science, pages 18–34. Springer, 2008.
[32] Kostya Serebryany. OSS-Fuzz - Google’s continuous fuzzing service for open source software, August 2017. Invited talk at the 26th USENIX Security Symposium.
[33] Michael R. Sheldon, Michael J. Fillyaw, and W. Douglas Thompson. The use and interpretation of the Friedman test in the analysis of ordinal-scale data in repeated measures designs. Physiotherapy Research International, 1(4):221–228, 1996.
[34] R. Shirrey. RFC 4949: Internet Security Glossary, version 2. https://tools.ietf.org/rfc/rfc4949.txt [Accessed March 31, 2019], August 2007.
[35] Paul Spooren. Running OpenWrt inside Docker. https://forum.openwrt.org/t/running-openwrt-inside-docker-sbin-init-stuck/13774/8 [Accessed December 17, 2019].
[36] Nick Stephens, John Grosen, Christopher Salls, Audrey Dutcher, Ruoyu Wang, Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna. Driller: Augmenting fuzzing through selective symbolic execution. In Proceedings of the 23rd Annual Network and Distributed System Security Symposium, volume 16, pages 1–16, 2016.
[37] Robert Swiecki. Honggfuzz. http://honggfuzz.com [Accessed December 18, 2019].
[38] Trail of Bits. Challenge sets. https://www.trailofbits.com/research-and-development/challenge-sets/ [Accessed December 17, 2019].
[39] Sai Vamsi, Venkata Balamurali, K. Surya Teja, and Praveen Mallela. Classifying difficulty levels of programming questions on HackerRank. In International Conference on E-Business and Telecommunications, volume 3, pages 301–308. Springer, 2019.
[40] Daniel Votipka, Rock Stevens, Elissa Redmiles, Jeremy Hu, and Michelle Mazurek. Hackers vs. testers: A comparison of software vulnerability discovery processes. In 2018 IEEE Symposium on Security and Privacy, pages 374–391, May 2018.
[41] R. F. Woolson. Wilcoxon Signed-Rank Test, pages 1–3. American Cancer Society, 2008.
[42] Michal Zalewski. American Fuzzy Lop. http://lcamtuf.coredump.cx/afl/ [Accessed March 30, 2019].

---

**USENIX Association**
- **29th USENIX Security Symposium** (Page 1146)