### 40.40%
- **N/A**

### Precision and Coverage
- **Precision: 0.8**
- **Average Improvement: 0.84**
- **Coverage: 0.71**
- **Average Improvement: 0.7275**
- **N/A**

### Ensemble Approach
- **Ball Agglomerative**
- **Hypergraph Non-Ensemble**

### 7.4 Improvement with Cluster-Quality Metrics
We first examine the effectiveness of cohesion and separation as measures of cluster quality. We selected four best-coverage cases, one from each clustering (see Table 7), and computed cohesion and separation for each constituent cluster. We categorized clusters into "clean" (those consisting of malware from only one family) and "mixed" (those consisting of malware samples from multiple families). The cumulative distribution functions (CDFs) of these clusters are plotted in Figure 5. These figures show that cohesion (Co) and separation (Cs) effectively distinguish between good and bad clusters, with clean clusters typically having lower cohesion and higher separation compared to mixed clusters.

Next, we integrated the cluster-quality measures into DUET as described in Section 6 and applied the same ensemble algorithms on the augmented connectivity matrices for both best-case and random scenarios (Table 7). Figure 6 compares the precision and coverage results of the agglomerative ensemble algorithm with and without quality measures. The results for other algorithms follow the same trends. A smaller cohesion indicates better cluster quality (Section 6).

**Figure 5: CDF for Cohesion (Co) and Separation (Cs)**

- **Clean Cluster**
- **Mixed Cluster**
  - **1**
  - **0.8**
  - **0.6**
  - **0.4**
  - **0.2**
  - **F D C 0**
  - **0 0.2 0.4 0.6 0.8 1**
  - **1 0.8 0.6 0.4 0.2 F D C 0 0.9 0.92 0.94 0.96 0.98 1**
  - **Cluster Cohesion**
  - **Cluster Separation**

**Figure 6: Agglomerative Ensemble Algorithm with Quality Measures**
- **(B) Best Scenario**
- **(R) Random Scenario**
- **Quality measures (B)**
- **No quality measures (B)**
- **Quality measures (R)**
- **No quality measures (R)**
- **1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0**
- **0.2 0.4 0.6 0.8 1 Threshold**
- **Precision**
- **Coverage**
- **Avg Recall: 0.27; Stdev: 0.07**

From the figure, we observe that ensemble algorithms with quality measures outperform their counterparts by 5–10% in terms of precision. However, this improvement comes at a cost, as there is a decrease in malware coverage after incorporating the quality measures. This is because the quality measures weaken the connectivity between samples in low-quality clusters, making them more likely to be excluded from the final clusters. The coverage reduction ranges from 3–30%, with the largest drop often occurring when the threshold is around 0.5. This is due to the composition of member clusterings (half from dynamic approaches and half from static approaches). Without quality measures, a threshold of 0.5 (majority voting) can result in a false consensus among approaches of the same type. Using quality measures, the reduced connectivity between weakly-related samples drops below the threshold, improving precision but lowering sample coverage (Figure 6). However, the decrease in coverage is not necessarily a disadvantage, as the remaining clusters are often of better quality, allowing higher confidence when using these clusters. Incorporating diversified sets of clustering techniques could potentially mitigate this problem.

### 7.5 Run Time Performance of DUET
DUET consists of two main components: trace collector and clustering system. The most time-consuming step is the trace collection. For all 5,647 malware programs, it took approximately 12 hours to extract static features and 7 days to collect dynamic traces on a single machine with an Intel Core i7 3.0G CPU and 16 GB of RAM. Although the trace collection step is expensive, it is required for any malware analysis task and has already been efficiently conducted as a daily process in the antivirus industry. It only needs to be done once and can be parallelized due to the independence between malware samples. For example, Anubis, an online service for executing and dynamically analyzing malware, can process more than 1 million samples on the fly as they are submitted during a one-week period [21]. Thus, scalability is becoming less of a concern for dynamic malware analysis, making its integration with static analysis very practical and beneficial.

The second component of DUET is the clustering system, which includes individual static/dynamic clusterings and cluster ensembles. Their running times on the entire dataset are summarized in Figure 7. We observe that the clustering runs much faster for dynamic features than for static features. This is because the space of static features is orders of magnitude larger than that of dynamic features. As a result, DUET-S takes longer to compare similarity between longer static feature vectors, which also helps DUET-S achieve better precision than DUET-D (as shown in Section 7.2). The running times of different ensemble algorithms are usually in the range of 70–90 seconds, except for hypergraph partitioning, which is the most complex and time-consuming (about 200 seconds). To better understand the scalability, we measured the running time of DUET with different numbers of input malware samples, as shown in Figure 8. The figure confirms that the runtime complexity of DUET’s ensemble component is O(n^2), where n is the number of malware samples. Due to this quadratic complexity, DUET’s performance may suffer when the number of malware samples grows significantly. A potential solution is to employ the principle of ProtoCluster, where ensemble algorithms can first be applied to a relatively small set of prototypes and then propagated to associated samples, avoiding expensive computation over the entire dataset.

**Figure 7: Running Times of Different Components**

**Figure 8: Scalability of DUET’s Components**

### 8. Discussions
Here, we discuss several limitations of the current DUET prototype and possible improvements to alleviate them. DUET’s performance depends on the successful extraction of useful features from malware binaries and run-time behaviors. First, like any other static-analysis approach, DUET-S is vulnerable to binary/instruction-level obfuscation and advanced run-time packers. For instance, anti-disassembly techniques, such as the mixture of code and data and indirect control flow, can confuse disassemblers, preventing DUET-S from extracting features. Instruction-level polymorphism can create syntactically distinct but semantically similar variants, bypassing DUET’s similarity comparison. Although the current DUET prototype does not handle these types of obfuscation for simplicity, advanced de-obfuscation and normalization [22, 30] can be used to mitigate the problems. For packers that cannot be handled by generic unpacking algorithms, specialized unpacking tools such as Armadillo Killer [1] can be employed.

Similarly, DUET-D, as a dynamic analysis system, can be circumvented by specifically crafted evasion techniques. Since any dynamic analysis system typically can only afford to execute a malware program for a short period, it can be evaded by inserting stalling codes before the real malicious code. Systems such as HASEN [18] have been proposed to detect and automatically skip such stalling code. Another limitation of DUET-D is its reliance on virtual machines to provide a controlled environment for malware execution. Unfortunately, malware programs can check for virtual environments [25] and behave differently from what they do in a real system. Countermeasures include using more transparent environments [8, 9], detecting split identities of malware samples [5], and forcing multiple path exploration [26]. Finally, both DUET-S and DUET-D cannot handle file infectors or parasitic malware, which inject themselves into host binaries (e.g., Sality virus), because the majority of extracted features may belong to the host binaries. In practice, techniques for detecting parasitic malware [29] should be used to pre-filter these samples.

Malware analysis is an ongoing battle between adversaries and defenders. None of the aforementioned countermeasures are perfect or long-lasting. Nevertheless, new emerging techniques can be leveraged to raise the bar and handle common malware types, which, in practice, prove to be beneficial. Furthermore, the respective limitations of static and dynamic analysis reinforce the necessity and importance of integrating these two approaches to deal with advanced malware samples.

### 9. Concluding Remarks
In this paper, we designed, developed, and evaluated an automatic malware-clustering system called DUET, which exploits cluster ensembles as a principled method to effectively integrate static and dynamic analyses. Using a number of real-life malware samples, we evaluated the performance of ensemble methods for both best-case and random scenarios, demonstrating that DUET can improve coverage by 20–40% while achieving nearly the highest precision of the individual clustering algorithms. We also made further improvements to existing cluster ensemble algorithms by leveraging cluster-quality measures. Overall, the evaluation results show DUET’s ability to combine multiple malware clustering techniques to create more effective and accurate clusters.

### 10. References
[1] Unpackers. http://www.exetools.com/unpackers.htm.
[2] C. C. Alexander Strehl, Joydeep Ghosh. Cluster ensembles - a knowledge reuse framework for combining multiple partitions. Journal of Machine Learning Research, 2002.
[3] B. Anderson, C. Storlie, and T. Lane. Improving malware classification: bridging the static/dynamic gap. In Proceedings of the 5th ACM workshop on Security and artificial intelligence, AISec ’12, 2012.
[4] M. Bailey, J. Andersen, Z. M. mao, and F. Jahanian. Automated classification and analysis of internet malware. In In Proceedings of RAID, 2007.
[5] D. Balzarotti, M. Cova, C. Karlberger, C. Kruegel, E. Kirda, and G. Vigna. Efficient detection of split personalities in malware. In NDSS, 2010.
[6] U. Bayer, P. Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda. Scalable, behavior-based malware clustering. In Proc. of the 16th NDSS, 2009.
[7] M. Christodorescu and S. Jha. Static analysis of executables to detect malicious patterns. In Proc. of the 12th USENIX Security Symposium, 2003.
[8] A. Dinaburg, P. Royal, M. Sharif, and W. Lee. Ether: malware analysis via hardware virtualization extensions. In Proceedings of CCS’08, 2008.
[9] A. Fattori, R. Paleari, L. Martignoni, and M. Monga. Dynamic and transparent analysis of commodity production systems. In Proceedings of ASE’10, 2010.
[10] A. Fred. Finding consistent clusters in data partitions. In Proc. 3d Int. Workshop on Multiple Classifier, 2001.
[11] A. L. N. Fred and A. K. Jain. Data clustering using evidence accumulation. In Proceedings of the 16th International Conf. on Pattern Recognition, 2002.
[12] A. Gionis, H. Mannila, and P. Tsaparas. Clustering aggregation. ACM Trans. Knowl. Discov. Data, 1, March 2007.
[13] F. Guo, P. Ferrie, and T.-C. Chiueh. A study of the packer problem and its solutions. In RAID ’08, 2008.
[14] Y. Hong, S. Kwong, Y. Chang, and Q. Ren. Unsupervised feature selection using clustering ensembles and population based incremental learning algorithm. Pattern Recognition, 41(9):2742–2756, 2008.
[15] J. Jang, D. Brumley, and S. Venkataraman. Bitshred: feature hashing malware for scalable triage and semantic analysis. In Proceedings of CCS’11, 2011.
[16] M. E. Karim, A. Walenstein, A. Lakhotia, and L. Parida. Malware phylogeny generation using permutations of code. J. in Computer Virology, 2005.
[17] Karypis Lab. Hypergraph partitioning software. http://glaros.dtc.umn.edu/gkhome/views/metis, 2010.
[18] C. Kolbitsch, E. Kirda, and C. Kruegel. The power of procrastination: detection and mitigation of execution-stalling malicious code. CCS ’11, 2011.
[19] J. Z. Kolter and M. A. Maloof. Learning to detect and classify malicious executables in the wild. Journal of Machine Learning Research, 7:2006, 2006.
[20] C. W. Konrad Rieck, Philipp Trinius, and T. Holz. Automatic analysis of malware behavior using machine learning. Technical report, 2011.
[21] C. Kruegel, E. Kirda, U. Bayer, D. Balzarotti, and I. Habibi. A view on current malware behaviors. In 2nd USENIX Workshop on LEET, 2009.
[22] C. Kruegel, W. Robertson, F. Valeur, and G. Vigna. Static disassembly of obfuscated binaries. In USENIX Security’04, 2004.
[23] T. Lee and J. J.Mody. Behavioral classification. In Proceedings of EICAR Conference, 2006.
[24] C. Leita, U. Bayer, and E. Kirda. Exploiting diverse observation perspectives to get insights on the malware landscape. In DSN, 1 2010.
[25] T. Liston. On the cutting edge: Thwarting virtual machine detection. http://handlers.sans.org/tliston/ThwartingVMDetection Liston Skoudis.pdf.
[26] A. Moser, C. Kruegel, and E. Kirda. Exploring multiple execution paths for malware analysis. In Proceedings of Oakland’07, 2007.
[27] K. Rieck, T. Holz, C. Willems, P. Düssel, and P. Laskov. Learning and classification of malware behavior. In Proceedings of DIMVA’08, 2008.
[28] P. Security. Bredolab.aw. http://www.pandasecurity.com/homeusers/security-info/220087/Bredolab.AW.
[29] A. Srivastava and J. Griffin. RAID. In Recent Advances in Intrusion Detection. 2010.
[30] S. K. Udupa, S. K. Debray, and M. Madou. Deobfuscation: Reverse engineering obfuscated code. Reverse Engineering, Working Conference on, 2005.
[31] G. Wicherski. pehash: A novel approach to fast malware clustering. In LEET’2009.
[32] Y. Ye, T. Li, Y. Chen, and Q. Jiang. Automatic malware categorization using cluster ensemble. In Proceedings of the 16th ACM SIGKDD, 2010.