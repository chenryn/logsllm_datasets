### Observations and Vulnerabilities in Access Patterns

These subjects exhibit normal and justified access patterns, represented as grey edges with right-side objects. However, they also have the ability to access critical system files, which is unjustified and overly permissive. Attackers can exploit vulnerabilities in these subjects to compromise critical files via these access patterns.

Specifically, without prior knowledge of any vulnerabilities or attacks, SPOKE identifies `mediaserver`, a subject previously found to have the notorious `libstagefright` vulnerability (CVE-2015-1538). Attackers can first compromise `mediaserver` using this vulnerability as a stepping stone, and then use the over-permissive access patterns defined by the rule:

```plaintext
allow mediaserver ANONYMIZED_LABEL : file {ioctl read write create getattr setattr append unlink link rename open}
```

to modify critical system files and ultimately gain control over the enterprise device. This risky rule, along with others, has been confirmed and removed by policy engineers.

### Discussion

#### Native Functional Tests and Other Knowledge Inputs

Currently, SPOKE primarily focuses on Android functional tests for applications and framework. However, functional tests for native executable binaries can also be used to extract domain knowledge for pure native functionality in an Android system. SPOKE can be enhanced with techniques such as `ptrace/ltrace` and native library hooking to achieve this feature, which we leave as future work. Other dynamic analysis techniques, such as dynamic taint analysis [18], can provide detailed information flow of a series of access patterns. Static analysis methods like symbolic execution [33, 45] can identify code-level functionality and access patterns, providing additional insights into how access patterns and control flow are affected by specific inputs.

#### Data Mining and Machine Learning Possibilities

We designed an analysis engine in SPOKE to leverage the knowledge base for policy rule justification and attack surface analysis. Beyond this, other data mining and machine learning techniques can be applied within the analysis engine. For example, outlier/anomaly detection [22] can identify suspicious or mistakenly defined access patterns from certain subjects or objects that differ from the majority of the access patterns in the knowledge base. Bayesian networks [19, 20] can also be used to learn the relationships between access patterns and infer whether a new access pattern defined by a new rule is likely to be justified or over-permissive.

#### User-Based Access Pattern Collection

As the SEAndroid policy is eventually deployed to user devices for access control enforcement, human users can be involved in the testing and refinement process of SEAndroid policy development. With user agreement for data collection during testing (e.g., private data anonymization and no deliberate malicious usage), access patterns representing daily device use can be collected to help synthesize and refine policy rules. Existing user-based testing is already available for pre-released Android applications (e.g., Google Play Store Beta Testing [4]). We envision that SEAndroid policy development can also benefit from similar user beta testings.

### Related Work

In general, SPOKE’s knowledge extraction platform is a dynamic analysis system for Android. Numerous research efforts have been made in this field. DroidScope [44] proposed an emulation-based inspection to analyze both Java and native components of Android applications. CopperDroid [39] used QEMU and focused on system call analysis of Android malware. TaintDroid [18] provided a dynamic taint tracking system for information flow analysis in Android. In our case, we require domain knowledge from real devices since some security functionalities require hardware features, making virtualization-based approaches insufficient. Additionally, it is non-trivial and insufficient to port previous techniques, as we focus on the fundamental new problem of collecting domain knowledge for SEAndroid policy, which requires new techniques specific for knowledge extraction.

Although SEAndroid is relatively new, SELinux has been researched for years, including SELinux policy analysis and verification [12, 21, 26, 36], policy comparison [15], policy visualization [43], and policy information flow integrity measurement [24, 25, 40]. These works mainly analyzed the SELinux reference policy itself, which has been refined by the community over the years. In contrast, SEAndroid policy is fairly new and under active development by vendors. It is necessary to analyze SEAndroid policy together with the original domain knowledge to ensure the labels and rules defined in the policy are consistent with the real case. By collecting and leveraging domain knowledge, SPOKE creates a new dimension to policy development and analysis.

EASEAndroid [42] is a recent work that applied machine learning to analyze large-volume access events collected from user device logs to refine SEAndroid policy. SPOKE is orthogonal to EASEAndroid. EASEAndroid focuses on post-deployment policy analysis to refine the policy against attacks in the wild, while SPOKE focuses on pre-deployment analysis to bridge the knowledge gap for policy engineers during policy development and analysis. SPOKE can help policy engineers better understand and analyze the developed policy before it is deployed to user devices. Nevertheless, the knowledge from both SPOKE and EASEAndroid can be shared to provide better analysis results.

### Conclusion

SEAndroid policy development and analysis require domain knowledge. In this paper, we presented SPOKE, a knowledge engine that collects domain knowledge from functional tests and provides attack surface analysis through policy rule justification. We evaluated SPOKE using real-world functional tests. SPOKE successfully collected detailed domain knowledge and revealed over-permissive rules, helping policy engineers analyze and revise the policy.

### Acknowledgements

We would like to thank colleagues at Samsung Research America for their valuable input and resources. We also thank anonymous reviewers for their support in publishing this paper. William Enck’s work in this paper was supported in part by NSF grant CNS-1253346 and ARO grant W911NF-16-1-0299. Ninghui Li’s work was supported by NSF grant No. 1314688 and ARO grant W911NF-16-1-0127. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of Samsung or the funding agencies.

### References

[1] Android Testing. http://developer.android.com/tools/testing/index.html.
[2] AWS Device Farm of Mobile App Testing. https://aws.amazon.com/device-farm/.
[3] EMMA: a free Java code coverage tool. http://emma.sourceforge.net.
[4] Google Play Store Beta Testing. http://developer.android.com/distribute/googleplay/developer-console.html.
[5] Joshua Drake, Stagefright: Scary Code in the Heart of Android. https://www.blackhat.com/us-15/briefings.
[6] Profiling with Traceview. http://developer.android.com/tools/debugging/debugging-tracing.html.
[7] Security-Enhanced Linux in Android. https://source.android.com/security/selinux.
[8] SELinux Access Vector Rules. http://selinuxproject.org/page/AVCRules.
[9] SELinux Policy Analysis Tools. https://github.com/TresysTechnology/setools.
[10] SELinux Type Statements. http://selinuxproject.org/page/TypeStatements.
[11] Testdroid. http://testdroid.com/.
[12] M. Alam, J.-P. Seifert, Q. Li, and X. Zhang. Usage Control Platformization via Trustworthy SELinux. In ASIACCS ’08, pages 245–248. ACM, 2008.
[13] K. Beck. Test-driven development: by example. Addison-Wesley Professional, 2003.
[14] K. Burr and W. Young. Combinatorial test techniques: Table-based automation, test generation, and code coverage. In Proc. of the Intl. Conf. on Software Testing Analysis & Review. San Diego, 1998.
[15] H. Chen, N. Li, and Z. Mao. Analyzing and Comparing the Protection Quality of Security Enhanced Operating Systems. In NDSS ’09, 2009.
[16] W. Choi, G. Necula, and K. Sen. Guided GUI Testing of Android Apps with Minimal Restart and Approximate Learning. In OOPSLA ’13, pages 623–640, New York, NY, USA, 2013. ACM.
[17] R. DeMilli and A. J. Offutt. Constraint-based automatic test data generation. Software Engineering, IEEE Transactions on, 17(9):900–910, 1991.
[18] W. Enck, P. Gilbert, S. Han, V. Tendulkar, B.-G. Chun, L. P. Cox, J. Jung, P. McDaniel, and A. N. Sheth. TaintDroid: An Information-Flow Tracking System for Realtime Privacy Monitoring on Smartphones. ACM Trans. Comput. Syst., 32(2):5:1–5:29, June 2014.
[19] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifiers. Machine Learning, 29(2):131–163.
[20] N. Friedman, I. Nachman, and D. Pe'er. Learning Bayesian Network Structure from Massive Datasets: The Sparse Candidate Algorithm. In UAI’99, pages 206–215. Morgan Kaufmann Publishers Inc., 1999.
[21] B. Hicks, S. Rueda, and L. S. Clair. A Logical Specification and Analysis for SELinux MLS Policy. ACM Transactions on Information and System Security (TISSEC), 13(3):1–31, 2010.
[22] V. J. Hodge and J. Austin. A Survey of Outlier Detection Methodologies. Artificial Intelligence Review, 22(2):85–126.
[23] M. Howard, J. Pincus, and J. M. Wing. Measuring Relative Attack Surfaces. Springer, 2005.
[24] T. Jaeger, R. Sailer, and U. Shankar. PRIMA: Policy-Reduced Integrity Measurement Architecture. In SACMAT ’06, pages 19–28, 2006.
[25] T. Jaeger, R. Sailer, and X. Zhang. Analyzing Integrity Protection in the SELinux Example Policy. In USENIX Security ’03, 2003.
[26] T. Jaeger, R. Sailer, and X. Zhang. Resolving Constraint Conflicts. In SACMAT ’04, pages 105–114, New York, New York, USA, 2004. ACM Press.
[27] C. S. Jensen, M. R. Prasad, and A. Møller. Automated Testing with Targeted Event Sequence Generation. In ISSTA ’13, pages 67–77. ACM, 2013.
[28] P. Loscocco and S. Smalley. Integrating Flexible Support for Security Policies into the Linux Operating System. In USENIX Annual Technical Conference ’01, number February, pages 29–42, 2001.
[29] A. Machiry, R. Tahiliani, and M. Naik. Dynodroid: An Input Generation System for Android Apps. In ESEC/FSE ’13, pages 224–234, 2013.
[30] P. K. Manadhata and J. M. Wing. An Attack Surface Metric. IEEE Transactions on Software Engineering, 37(3):371–386, 2011.
[31] D. McCullough. Specifications for Multi-Level Security and a Hook-Up. In Security and Privacy, 1987 IEEE Symposium on, pages 161–161. IEEE, 1987.
[32] P. McMinn. Search-Based Software Test Data Generation: A Survey. Software Testing Verification and Reliability, 14(2):105–156, 2004.
[33] N. Mirzaei, S. Malek, C. S. Păsăreanu, N. Esfahani, and R. Mahmood. Testing Android Apps Through Symbolic Execution. SIGSOFT Softw. Eng. Notes, 37(6):1–5, Nov. 2012.
[34] E. Reshetova, F. Bonazzi, T. Nyman, R. Borgaonkar, and N. Asokan. Characterizing SEAndroid Policies in the Wild. ArXiv e-prints arXiv:1510.05497, Oct. 2015.
[35] J. Saltzer and M. Schroeder. The Protection of Information in Computer Systems. Proceedings of the IEEE, 63(9), Sept. 1975.
[36] A. Sasturkar, S. D. Stoller, C. R. Ramakrishnan, C. Science, and S. Brook. Policy Analysis for Administrative Role Based Access Control. In CSFW ’06, 2006.
[37] S. Smalley and R. Craig. Security Enhanced (SE) Android: Bringing Flexible MAC to Android. In NDSS ’13, 2013.
[38] S. Smalley, C. Vance, and W. Salamon. Implementing SELinux as a Linux Security Module. NAI Labs Report, 1(43):139, 2001.
[39] K. Tam, S. J. Khan, A. Fattori, and L. Cavallaro. CopperDroid: Automatic Reconstruction of Android Malware Behaviors. In NDSS ’15, 2015.
[40] H. Vijayakumar, G. Jakka, S. Rueda, J. Schiffman, and T. Jaeger. Integrity Walls: Finding Attack Surfaces from Mandatory Access Control Policies. In ASIACCS ’12, pages 75–76, 2012.
[41] W. Visser, S. Corina, and S. Khurshid. Test Input Generation with Java Pathfinder. ACM SIGSOFT Software Engineering Notes, 29(4):97–107, 2004.
[42] R. Wang, W. Enck, D. Reeves, X. Zhang, P. Ning, D. Xu, W. Zhou, and A. M. Azab. EASEAndroid: Automatic Policy Analysis and Refinement for Security Enhanced Android via Large-Scale Semi-Supervised Learning. In USENIX Security ’15, pages 351–366, Aug. 2015.
[43] W. Xu, M. Shehab, and G.-J. J. Ahn. Visualization Based Policy Analysis: Case Study in SELinux. In Proceedings of the 13th ACM Symposium on Access Control Models and Technologies, pages 165–174, 2008.
[44] L. K. Yan and H. Yin. DroidScope: Seamlessly Reconstructing the OS and Dalvik Semantic Views for Dynamic Android Malware Analysis. In USENIX Security ’12, pages 29–29, 2012.
[45] Z. Yang, M. Yang, Y. Zhang, G. Gu, P. Ning, and X. S. Wang. AppIntent: Analyzing Sensitive Data Transmission in Android for Privacy Leakage Detection. In CCS ’13, pages 1043–1054, 2013.

### Appendix

**Figure 5: Representative Bipartite Result of Attack Surface Analysis**

- **Subjects**: Red nodes on the left.
- **Objects**: Blue nodes on the right.
- **Justified Access Patterns**: Grey edges.
- **Over-Permissive Access Patterns**: Red edges, allowing unjustified subjects to access anonymized critical files, which have been confirmed and revoked by policy engineers.

With the help of SPOKE, policy rules in new releases are more stringent than the old ones.