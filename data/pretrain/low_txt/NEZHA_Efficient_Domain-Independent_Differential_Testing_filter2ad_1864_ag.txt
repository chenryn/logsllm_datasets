### 参考文献

[33] M. Georgiev, S. Iyengar, S. Jana, R. Anubhai, D. Boneh, and V. Shmatikov, “The most dangerous code in the world: validating SSL certificates in non-browser software,” in *Proceedings of the 2012 ACM Conference on Computer and Communications Security (CCS)*. ACM, 2012, pp. 38–49.

[34] P. Godefroid, A. Kiezun, and M. Y. Levin, “Grammar-based whitebox fuzzing,” in *Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)*, 2008, pp. 206–215.

[35] P. Godefroid, N. Klarlund, and K. Sen, “Dart: directed automated random testing,” in *Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)*, vol. 40, no. 6. ACM, 2005, pp. 213–223.

[36] P. Godefroid, M. Y. Levin, D. A. Molnar et al., “Automated whitebox fuzz testing,” in *Proceedings of the 2008 Network and Distributed Systems Symposium (NDSS)*, vol. 8, 2008, pp. 151–166.

[37] I. Haller, A. Slowinska, M. Neugschwandtner, and H. Bos, “Dowsing for overflows: A guided fuzzer to find buffer boundary violations,” in *22nd USENIX Security Symposium (USENIX Security ’13)*. Washington, D.C.: USENIX, 2013, pp. 49–64.

[38] C. Holler, K. Herzig, and A. Zeller, “Fuzzing with code fragments,” in *21st USENIX Security Symposium (USENIX Security ’12)*, 2012, pp. 445–458.

[39] A. D. Householder and J. M. Foote, “Probability-based parameter selection for black-box fuzz testing,” in *CMU/SEI Technical Report - CMU/SEI-2012-TN-019*, 2012.

[40] S. Jana, Y. Kang, S. Roth, and B. Ray, “Automatically Detecting Error Handling Bugs using Error Specifications,” in *25th USENIX Security Symposium (USENIX Security)*, Austin, August 2016.

[41] S. Jana and V. Shmatikov, “Abusing file processing in malware detectors for fun and profit,” in *Proceedings of the 2012 IEEE Symposium on Security and Privacy (S&P)*. IEEE Computer Society, 2012, pp. 80–94.

[42] Y. Kang, B. Ray, and S. Jana, “APEx: Automated Inference of Error Specifications for C APIs,” in *31st IEEE/ACM International Conference on Automated Software Engineering (ASE)*, Singapore, September 2016.

[43] J. C. King, “Symbolic execution and program testing,” *Communications of the ACM*, vol. 19, no. 7, pp. 385–394, 1976.

[44] J. C. Knight and N. G. Leveson, “An experimental evaluation of the assumption of independence in multiversion programming,” *IEEE Transactions on Software Engineering*, no. 1, pp. 96–109, 1986.

[45] J. Kornblum, “Identifying almost identical files using context triggered piecewise hashing,” *Digital Investigation*, vol. 3, pp. 91–97, 2006.

[46] P. Laskov et al., “Practical evasion of a learning-based classifier: A case study,” in *2014 IEEE Symposium on Security and Privacy (S&P)*. IEEE, 2014, pp. 197–211.

[47] V. Le, C. Sun, and Z. Su, “Finding deep compiler bugs via guided stochastic program mutation,” in *Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA)*, vol. 50, no. 10. ACM, 2015, pp. 386–399.

[48] B. A. Malloy and J. F. Power, “An interpretation of Purdom’s algorithm for automatic generation of test cases,” in *International Conference on Computer and Information Science*, 2001.

[49] D. Marinov and S. Khurshid, “Testera: A novel framework for automated testing of Java programs,” in *Proceedings of the 16th IEEE International Conference on Automated Software Engineering (ASE)*. Washington, DC, USA: IEEE Computer Society, 2001, pp. 22–.

[50] P. M. Maurer, “Generating test data with enhanced context-free grammars,” *IEEE Software*, vol. 7, no. 4, pp. 50–55, 1990.

[51] W. M. McKeeman, “Differential testing for software,” *Digital Technical Journal*, vol. 10, no. 1, pp. 100–107, 1998.

[52] B. P. Miller, L. Fredriksen, and B. So, “An empirical study of the reliability of Unix utilities,” *Communications of the ACM*, vol. 33, no. 12, pp. 32–44, 1990.

[53] R. P. Pargas, M. J. Harrold, and R. R. Peck, “Test-data generation using genetic algorithms,” *Software Testing Verification and Reliability*, vol. 9, no. 4, pp. 263–282, 1999.

[54] D. A. Ramos and D. R. Engler, “Practical, low-effort equivalence verification of real code,” in *International Conference on Computer Aided Verification*. Springer, 2011, pp. 669–685.

[55] S. Rawat, V. Jain, A. Kumar, L. Cojocar, C. Giuffrida, and H. Bos, “Vuzzer: Application-aware evolutionary fuzzing,” in *Proceedings of the Network and Distributed System Security Symposium (NDSS)*, 2017.

[56] J. Ruderman, “Introducing jsfunfuzz,” https://www.squarefree.com/2007/08/02/introducing-jsfunfuzz/.

[57] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “AddressSanitizer: A fast address sanity checker,” in *2012 USENIX Annual Technical Conference (USENIX ATC 2012)*, 2012, pp. 309–318.

[58] E. G. Sirer and B. N. Bershad, “Using production grammars in software testing,” in *Proceedings of the 2nd Conference on Domain-Specific Languages (DSL)*, vol. 35, no. 1. ACM, 1999, pp. 1–13.

[59] V. Srivastava, M. D. Bond, K. S. McKinley, and V. Shmatikov, “A security policy oracle: Detecting security holes using multiple API implementations,” *ACM SIGPLAN Notices*, vol. 46, no. 6, pp. 343–354, 2011.

[60] E. Stepanov and K. Serebryany, “MemorySanitizer: Fast detector of uninitialized memory use in C++,” in *Proceedings of the 13th Annual IEEE/ACM International Symposium on Code Generation and Optimization (CGO)*. IEEE Computer Society, 2015, pp. 46–55.

[61] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta, Y. Shoshitaishvili, C. Kruegel, and G. Vigna, “Driller: Augmenting fuzzing through selective symbolic execution,” in *Proceedings of the Network and Distributed System Security Symposium (NDSS)*, 2016.

[62] Tool Interface Standard, “The .xz File Format,” http://tukaani.org/xz/xz-file-format.txt, August 2009.

[63] Tool Interface Standard (TIS), “Executable and Linking Format (ELF) specification,” https://refspecs.linuxfoundation.org/elf/elf.pdf, May 1995.

[64] W. Xu, Y. Qi, and D. Evans, “Automatically evading classifiers: A case study on PDF malware classifiers,” in *Proceedings of the 2016 Network and Distributed Systems Symposium (NDSS)*, 2016.

[65] X. Yang, Y. Chen, E. Eide, and J. Regehr, “Finding and understanding bugs in C compilers,” in *Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)*. ACM, 2011, pp. 283–294.

[66] M. Zalewski, “American Fuzzy Lop,” http://lcamtuf.coredump.cx/afl/.

[67] A. Zeller, “Yesterday, my program worked. Today, it does not. Why?” in *Proceedings of the Joint Meeting on Foundations of Software Engineering (ESEC/FSE)*. Springer, 1999, pp. 253–267.

### 附录

#### A. NEZHA 报告的内存损坏漏洞

1. **ClamAV 使用后释放漏洞**：
   NEZHA 发现了 ClamAV 中的一个使用后释放堆漏洞，该漏洞在解析畸形的 XZ 归档文件时被触发。当 ClamAV 解析归档中的多个压缩块时，它会对一个内存缓冲区进行一系列的分配和释放操作。ClamAV 的内存分配例程仅在给定的内存指针为 NULL 时才会执行分配操作。然而，内存释放例程在释放缓冲区后未能将内存指针置为 NULL。因此，在一系列的分配-释放-分配操作之后，该漏洞会被触发。攻击者可以通过发送一个畸形的 XZ 归档文件来利用此漏洞，从而导致 ClamAV 在尝试扫描归档文件时崩溃。

2. **wolfSSL 内存错误**：
   NEZHA 发现了 wolfSSL 中的四个内存损坏漏洞，这些漏洞均被 wolfSSL 开发人员标记为严重，并在我们报告后的六天内进行了修补。其中两个漏洞是由于在 PemToDer 函数中缺少对畸形 PEM 证书头的检查引起的，这两个漏洞导致了越界内存读取。第三个漏洞是由于在 wolfSSL_CertManagerVerifyBuffer 例程中未检查 PemToDer 调用的返回值，导致了段错误。在这种情况下，保存 DER 格式证书的结构被破坏。第四个漏洞也发生在 Pem2Der 中，由于缺少对要转换的 PEM 证书大小的检查，导致了越界读取。这种情况可以由链中的中间证书触发，该证书具有正确的 PEM 头但为空体：缺少的检查会导致 Pem2Der 不返回任何错误，从而在后续验证过程中导致越界内存访问。

3. **GnuTLS 空指针解引用**：
   NEZHA 在 GnuTLS 的 gnutls_oid_to_ecc_curve 例程中发现了一个缺失的检查，该检查未能确保解引用的指针不为 NULL。这个漏洞在解析精心构造的证书时导致了段错误。

#### B. NEZHA 不同引导引擎的覆盖率和种群规模

在图 11 和图 12 中，我们展示了 NEZHA 各个引擎在第五部分 A 节实验设置下的覆盖率和种群增长情况。

![Coverage Increase](https://example.com/coverage_increase.png)
*图 11：NEZHA 各个引擎每代的覆盖率增加（基于 100 次运行的平均值，初始种子语料库包含 1000 个证书）。*

![Population Size Increase](https://example.com/population_size_increase.png)
*图 12：NEZHA 各个引擎每代的种群规模增加（基于 100 次运行的平均值，每次从 1000 个证书的种子语料库开始）。*

#### C. BoringSSL - 错误的 KeyUsage 表示

根据 RFC 标准，KeyUsage 扩展定义了证书密钥的目的，并使用位字符串表示密钥的各种用途。有效的 CA 证书必须包含此扩展，并且 keyCertSign 位应被设置。

BoringSSL 和 LibreSSL 在解析用于存储 X.509 证书中的 KeyUsage 扩展的 ASN.1 位字符串的方式上有所不同。每个位字符串都编码有一个“填充”字节，指示位表示结构中最低有效位的未使用位数。此字节永远不应超过 7。但如果该字节设置为大于 7 的值，BoringSSL 将无法解析位字符串并抛出错误（如清单 7 所示），而 LibreSSL 则会将该字节与 0x07 进行掩码处理并继续解析位字符串（如清单 8 所示）。

```c
// 清单 7：BoringSSL 代码用于验证位字符串
ASN1_BIT_STRING *c2i_ASN1_BIT_STRING(..., char **pp) {
    ...
    p = *pp;
    padding = *(p++);
    // 如果填充字节无效，则返回错误
    if (padding > 7) {
        OPENSSL_PUT_ERROR(ASN1, ASN1_R_INVALID_BIT_STRING_BITS_LEFT);
        goto err;
    }
    ret->flags &= ~(ASN1_STRING_FLAG_BITS_LEFT | 0x07);
    ret->flags |= (ASN1_STRING_FLAG_BITS_LEFT | i);
}
```

```c
// 清单 8：LibreSSL 代码用于验证位字符串
ASN1_BIT_STRING *c2i_ASN1_BIT_STRING(..., char **pp) {
    ...
    p = *pp;
    i = *(p++);
    // 对填充字节进行掩码处理，而不是进行检查
    ret->flags &= ~(ASN1_STRING_FLAG_BITS_LEFT | 0x07);
    ret->flags |= (ASN1_STRING_FLAG_BITS_LEFT | (i & 0x07));
}
```

这种细微的差异导致了对同一位字符串的不同解释。BoringSSL 无法解析位字符串，结果为空的 KeyUsage 扩展。LibreSSL 通过掩码处理填充字节成功解析了扩展。我们还发现这些库在解析证书签名请求 (CSR) 时表现出这种差异。这可能会产生严重的安全影响。例如，如果使用 BoringSSL 的 CA 解析攻击者提供的 CSR 并且没有正确解释扩展，CA 可能会误解密钥用途并且未检测到某些黑名单中的用途。在这种情况下，CA 可能会将畸形扩展复制到签发的证书中。随后，当使用 LibreSSL 的客户端解析该证书时，它将被视为具有有效的 KeyUsage 扩展，从而使攻击者能够以 CA 未曾意图的方式使用该证书。