# Title: Challenges in Inferring Internet Congestion Using Throughput Measurements

## Authors:
- Srikanth Sundaresan, Princeton University
- Xiaohong Deng, University of New South Wales
- Yun Feng, University of New South Wales
- Danny Lee, Georgia Tech
- Amogh Dhamdhere, CAIDA, Univ. of California, San Diego

### Abstract
We revisit the use of crowdsourced throughput measurements to infer and localize congestion on end-to-end paths, with a particular focus on points of interconnection between ISPs. We analyze three key challenges associated with this approach. First, accurately identifying which link on the path is congested requires fine-grained network tomography techniques, which are not supported by existing throughput measurement platforms. Coarse-grained network tomography can perform this link identification under certain topological conditions, but these conditions do not always hold on the global Internet. Second, existing measurement platforms provide limited visibility of paths to popular web content sources and only capture a small fraction of interconnections between ISPs. Third, crowdsourcing measurements inherently risks sample bias, as using measurements from volunteers across the Internet leads to uneven distribution of samples across time of day, access link speeds, and home network conditions. Finally, it is unclear how large a drop in throughput should be interpreted as evidence of congestion. We investigate these challenges in detail and offer guidelines for deploying measurement infrastructure, strategies, and technologies that can address empirical gaps in our understanding of congestion on the Internet.

### CCS Concepts
- Networks → Network Measurement

### Keywords
- Internet congestion, Internet topology, Throughput

### ACM Reference Format
Srikanth Sundaresan, Danny Lee, Xiaohong Deng, Yun Feng, and Amogh Dhamdhere. 2017. Challenges in Inferring Internet Congestion Using Throughput Measurements. In Proceedings of IMC '17, London, United Kingdom, November 1–3, 2017, 14 pages. https://doi.org/10.1145/3131365.3131382

### Introduction
The relentless growth of Internet traffic demands and the growing concentration of content across a few providers and distribution networks have led to capacity constraints, particularly at points of interconnection between content providers, transit providers, and access ISPs. Interconnection bandwidth contention has led to high-profile disputes over who should pay for additional interconnection capacity. These disputes have implications for network stability and performance, leaving the congested link as an externality for all users until the dispute is resolved. This situation has led to recent interest in techniques to detect and localize persistent inter-domain congestion. One approach that has received significant attention is the use of crowdsourced throughput measurements, such as those offered by the Measurement-Lab (M-Lab) platform or Ookla’s Speedtest.net.

We present a systematic analysis of the inference techniques and challenges associated with using throughput measurements to infer the presence, location, and characteristics of congestion. We use NDT tests collected by the M-Lab platform in May 2015, along with M-Lab’s analysis of this data, as a case study of throughput-based inferences, and explore three main challenges:

1. **Link Identification**: Accurately identifying which link on the path is congested requires fine-grained network tomography techniques, which are not supported by existing throughput measurement platforms. Coarse-grained network tomography can perform this link identification under certain topological conditions, but these conditions do not always hold on the global Internet.
2. **Limited Visibility**: Existing measurement platforms provide limited visibility of paths to popular web content sources and only capture a small fraction of interconnections between ISPs.
3. **Sample Bias**: Crowdsourcing measurements inherently risks sample bias, as using measurements from volunteers across the Internet leads to uneven distribution of samples across time of day, access link speeds, and home network conditions.
4. **Interpreting Throughput Drops**: It is unclear how large a drop in throughput should be interpreted as evidence of congestion.

After a systematic analysis of available data, we offer several suggestions for improving the utility of collected throughput measurements to characterize congestion upstream of a client’s access link. These suggestions include improved topology measurement and analysis techniques, more careful stratification of test results based on topological information, strategic deployment of server infrastructure to maximize coverage, and cross-validating crowdsourced measurements using more systematically collected data from other measurement platforms. Our analysis informs our own planning of existing and future measurement infrastructure, not just for throughput measurement, but for any generalized framework that aims to measure performance on an Internet-wide scale.

### Existing Throughput Measurement Platforms
Multiple platforms solicit crowdsourced throughput measurements from users using web-based speed tests, including Ookla’s Speedtest.net, DSLreports, and M-Lab. Additionally, clients that are part of the FCC’s Measuring Broadband America platform or the Bismark platform also perform periodic throughput measurements from home routers. Throughput measurements typically run simple bulk data transfers over TCP over a short period, aiming to measure the bottleneck link by saturating it with one or more TCP flows. The bottleneck link is commonly the “last mile”: the access link between the client and the Internet. In this scenario, the ideal location of the server is as close as possible to the client to minimize latency. TCP throughput has a well-understood inverse relationship with latency: the longer the latency across a path, the lower the throughput, all other factors being equal. Therefore, as broadband access speeds increase, low latencies from test servers to clients ensure that throughput measurements can saturate the bottleneck link on the path to the client.

#### Throughput Measurements on M-Lab
M-Lab is a distributed platform with hundreds of well-provisioned machines around the world that serve as destinations for a set of freely available performance measurement and diagnostic tools. Users can download and run any of the supported software tools that estimate performance characteristics of paths between the client and M-Lab servers. One of these tools, the Network Diagnostic Test (NDT), is a web-based tool that runs a throughput measurement in each direction: from client to server (upstream) and server to client (downstream). A client initiates an NDT measurement to M-Lab, and the M-Lab backend uses IP geolocation to select a server close to the client. Each test estimates the downstream throughput from the server to the client and the upstream throughput from the client to the server. The server logs statistics, including round-trip time, bytes sent, received, and acknowledged, congestion window size, and the number of congestion signals (multiplicative downward congestion window adjustments) received by the TCP sender. The server also stores the raw packet captures for the test, which are publicly available through Google’s BigQuery and Cloud Storage. Unlike Ookla and DSLreports, which only make aggregated stats available publicly, M-Lab makes all data available, including packet traces and supplementary path data.

#### Inferring Congestion Using M-Lab Data
The high density of U.S. server deployments on M-Lab facilitates the crowdsourcing of a rich set of measurements that include different combinations of transit provider and access ISPs. In 2014 and 2015, two well-publicized measurement reports—one from M-Lab itself and another from the “Battle for the Net” advocacy group—used NDT measurements on M-Lab to infer congestion in peering and transit networks in the U.S.

In 2014, a team from M-Lab analyzed 18 months of NDT data collected by the M-Lab platform to infer interdomain congestion between large transit ISPs and large access ISPs. The report aggregated results from clients of access ISPs to servers located in various transit providers and grouped the tests by source AS, destination AS, and server location. They analyzed metrics such as download throughput, flow round-trip time, and packet retransmission rates. They found diurnal patterns in the median values of these metrics, from which they inferred persistent congestion on paths between several U.S. access ISPs (including many large ISPs such as Comcast, AT&T, Verizon, and Time Warner) and transit providers (such as Cogent, Verizon, and XO). This report generated discussion in the academic community regarding the technical soundness of the measurement and analysis methods, and requests for revisions to the report to address its flaws.

Starting early 2015, a net neutrality advocacy group called “Battle for the Net” hosted a modified version of the NDT client on their website. Their client performed back-to-back tests with up to five M-Lab servers in the same region rather than just the closest one, in an attempt to observe more paths. This group released and then significantly modified a report claiming evidence of congestion in more networks than the original M-Lab report. Media coverage generated a jump in the number of NDT tests across M-Lab in May 2015, which prompted an M-Lab researcher to issue an update to M-Lab’s congestion report in a blog post. The posting described how they applied the same inference method on the newer data set to infer evidence of congestion in more interconnection links (such as from Verizon, Comcast, and Time Warner to GTT, and TATA).

In the meantime, a public comment in the policy debate on the AT&T-DirectTV merger approved in 2015 cited the 2014 M-Lab report as justification to propose severe regulatory conditions on AT&T following the merger, including mandated settlement-free peering with some peers and mandated upgrades to interconnection links upon reaching 70% utilization. This public comment to the FCC illustrated the need for objective, peer-reviewed analysis of the state of interdomain congestion measurement methodology. In this paper, we offer an attempt at such an analysis.

In June 2016, Google incorporated speed tests (driven by NDT) into Google search, allowing a sample of users that visited the Google search page to execute NDT tests against M-Lab servers. By December 2016, the number of monthly NDT tests had increased more than fourfold compared to June 2016. After fixing some issues with the Paris traceroute data collection, the M-Lab platform now collects a large volume of NDT measurements along with Paris traceroutes from the server to the client. The widespread interest in the previous analyses of the M-Lab data in technical and policy circles, along with the increased volume of NDT data in recent months, represents a tempting opportunity to repeat the analysis of 2014-2015 in recent months.

We emphasize that our goal in this work is not to challenge the conclusions presented in the M-Lab reports but instead to highlight the pitfalls and challenges in inferring congestion using throughput-based tests and to illustrate the use of path measurements paired with throughput tests for more rigorous analysis. We hope that doing so will encourage improvements in the testing platforms to better support inference and localization of congestion, and analysis that more rigorously accounts for the complexity of interdomain interconnection. We use the M-Lab data and the previously mentioned analyses as case studies because they were the first to attempt to use throughput measurements to infer and localize interdomain congestion. As such, the presentation in this paper necessarily delves into the particulars of the M-Lab infrastructure and analysis. Nonetheless, we believe that the results and recommendations are applicable to other platforms that attempt such analysis.

### Using Network Tomography to Infer Congestion
Given a set of end-to-end measurements of some metric of interest (such as throughput, delay, loss rate, or reachability) and knowledge (or measurements) of topology, one can use network tomography to infer the properties of each link in the topology. Binary network tomography constrains the tomography problem by assuming that network-internal links can be in one of two states—“good” or “bad”—and then attempts to find the smallest set of “bad” links that are consistent with the end-to-end observations. In the context of congestion localization, the end-to-end metric is an estimate of whether the path is congested, and links in the network can either be “congested” or “not congested.”

Unfortunately, path information is not always available from large-scale throughput measurement platforms. M-Lab collects Paris traceroutes from M-Lab servers toward clients that run measurements against their infrastructure and releases this data publicly. Paris traceroute overcomes the problems with using the traditional traceroute tool for inferring router-level topology and paths in the presence of load-balancing by carefully controlling the header fields in sent packets. However, the path information from Paris traceroute was incomplete prior to 2015 and in the latter half of 2016. Other large-scale throughput measurement platforms, such as Ookla’s Speedtest.net, either do not collect path information at all or do not release this data. Furthermore, even if path information is available from traceroutes, using it as input to a tomography algorithm is challenging due to issues with measurement synchronization and traceroute artifacts. One alternative is to use a simplified form of tomography at the AS-level; M-Lab’s studies of interconnection congestion used this simplified approach. This section describes this method and its assumptions.

#### Applying Simplified AS-Level Tomography for Congestion Localization
Strong diurnal trends in achieved throughput in NDT measurements from servers in an AS S (source AS) to clients in an AS A (access AS) indicate the presence of link(s) along the path between S and A that are congested at peak times. However, other problems unrelated to congestion could result in such diurnal trends in NDT download throughput, e.g., problems in the user’s home network or the user’s access network. One way to mitigate the effects of access link issues is to compare diurnal congestion trends seen in NDT measurements from an access network A to servers in different ASes. If paths from a source AS S1 to access network A show diurnal patterns indicating peak-hour congestion, but paths from source AS S2 to access AS A do not, this difference suggests that access link/home network congestion was not a factor. The M-Lab reports went further and claimed that any perceived performance degradation on paths from S to A was on the interdomain link between S and A. This simplified AS-level tomography approach relies on three key assumptions for this inference to be correct:

1. **Assumption 1**: There is no congestion internal to ASes. Thus, inferred congestion on end-to-end paths is at an interdomain link between ASes. This assumption is crucial to tomography at the AS-level; because finer-grained path information is not available (or not used), the internal structure of ASes is unknown. This assumption relies on the internal networks of ASes being well-provisioned to handle all incoming or outgoing traffic.
2. **Assumption 2**: The server and client ASes directly interconnect. That is, no other ASes are on paths from the server AS to the client AS. If both Assumption 1 and 2 hold, the tomography problem has a straightforward solution: any observed congestion on paths from server AS S to access AS A must be on the AS-level interconnection between S and A.
3. **Assumption 3**: All router-level interconnections over which an inference is made for the AS interconnection behave similarly. If this assumption holds, then AS-level inference accurately reflects the performance of every link or router-level interconnection within it. For this assumption to hold, it is critical that throughput measurements from a server in AS S to clients in AS A are aggregated in a way that the paths to those clients traverse a single interconnection between S and A, or interconnections between S and A that behave similarly (e.g., parallel links between the same border routers). As Claffy et al. discuss, interdomain congestion often shows regional effects. Consequently, aggregation across interdomain links is particularly problematic if those links are in different geographical regions, as they could vary widely in terms of diurnal throughput patterns.

These assumptions bear careful consideration in today’s complex Internet ecosystem. The first assumption, that ASes do not experience internal congestion, derives from historical knowledge that networks generally are willing to invest significant capital in upgrading their own internal infrastructure, while disputes tend to arise at interconnection points where who pays for the upgrade depends on private negotiation between the interconnecting parties. The data at our disposal does not allow us to investigate this assumption; however, evidence from recent events suggests that it may be valid. In the next section, we use the public M-Lab path data (though limited) to test the validity of assumptions 2 and 3.

### Is the Topology Amenable to Simplified Tomography?
Assessing the validity of the two topological assumptions described in the previous section (2 and 3) will inform our judgment of the feasibility of applying simplified AS-level tomography without the use of finer-grained path information. A limited set of path measurements in the M-Lab dataset sheds doubt on whether these assumptions hold generally. A fundamental requirement in our analysis is to use traceroute measurements from M-Lab to reason about AS-level interconnections between networks. Luckie et al. describe in detail many things that can go wrong in the inference of boundaries between ASes. Two recent pieces of work, bdrmap and MAP-IT, have taken steps toward overcoming these challenges.

In this section, we use MAP-IT, the more general of the two interdomain link identification tools, which can use a set of traceroutes that has already been collected, and apply it to the set of traceroutes collected from the M-Lab platform. We first describe the constrained set of path information available from the M-Lab and use MAP-IT to analyze whether this limited data reveals the extent to which the M-Lab server and client are in adjacent ASes. We then apply MAP-IT to the collected traceroute data to analyze whether NDT measurements from an M-Lab server often reach a given access AS over the same physical interconnection link. For most of this analysis, we use M-Lab data from 2015 to align it with the time periods in which the M-Lab reports and “Battle for the Net” studies were released (Nov 2014 to May 2015).

#### Path Measurements in the M-Lab Dataset