### Model of the Relationship Between Coverage-Increasing Test Case Rate and UnTracer’s Overhead

**Figure 13.** The model in Figure 13 illustrates the relationship between the rate of coverage-increasing test cases and the overhead per test case for UnTracer, AFL-Clang, AFL-QEMU, and AFL-Dyninst. For all rates to the left of the leftmost dashed vertical line (2%), UnTracer’s overhead per test case is less than that of AFL-Clang. Similarly, for all rates to the left of the rightmost dashed vertical line (50%), UnTracer’s overhead is less than that of AFL-QEMU and AFL-Dyninst. The average rate of coverage-increasing test cases observed during our evaluations was 4.92E-3.

### VII. Hybrid Fuzzing Evaluation

State-of-the-art hybrid fuzzers, such as Driller [18] and QSYM [19], combine program-directed mutation (e.g., concolic execution) with traditional blind mutation (e.g., AFL [5]). While hybrid approaches significantly increase code coverage, they do so at the cost of a reduced test case execution rate. In this section, we compare UnTracer, Clang [5] (white-box tracing), and QEMU [5] (black-box dynamically-instrumented tracing) implementations of the state-of-the-art hybrid fuzzer QSYM on seven of our eight benchmarks.7

#### A. Implementing QSYM-UnTracer

We implemented QSYM-UnTracer in QSYM’s core AFL-based fuzzer, which tracks coverage in several contexts: test case trimming (trim_case()), test case calibration (calibrate_case()), test case saving (save_if_interesting()), hybrid fuzzing syncing (sync_fuzzers()), and the “common” context used for most test cases (common_fuzz_stuff()). Below, we briefly discuss design choices specific to each context.

1. **Trimming and Calibration**: Test case trimming and calibration must be able to identify changes in a priori coverage. Therefore, the interest oracle is unsuitable since it only identifies new coverage. Instead, we utilize only the tracer binary.
2. **Saving Timeouts**: A sub-procedure of test case saving involves identifying unique timeout-producing and unique hang-producing test cases by tracing and comparing their coverage to a global timeout coverage. Since AFL only tracks this information for reporting purposes, and using an interest oracle or tracer would add unwanted overhead for binaries with many timeouts (e.g., djpeg in Table III), we configure UnTracer-AFL, AFL-Clang, and AFL-QEMU to only track total timeouts.
3. **Other Coverage Contexts**: For all other coverage contexts, we implement the UnTracer interest oracle and tracer execution model as described in Section V.

#### B. Evaluation Overview

To identify the performance impact of using UnTracer in hybrid fuzzing, we incorporated it into the state-of-the-art hybrid fuzzer QSYM and evaluated it against existing Clang- and QEMU-based QSYM implementations. Our experiments compared the number of test cases executed for all three hybrid fuzzer variants for seven of the eight benchmarks from Section VI (Table III) with 100ms timeouts. To account for randomness, we averaged the number of test cases executed from 8, 24-hour trials for each variant/benchmark combination. To form an average result for each variant across all benchmarks, we computed a per-variant geometric mean.

We distributed all trials across eight virtual machines among four workstations. Each host is a six-core Intel Core i7-7800X CPU @ 3.50GHz with 64GB of RAM, running two, two-CPU 6GB virtual machines. All eight virtual machines run Ubuntu 16.04 x86 64 (as opposed to 18.04 for previous experiments due to QSYM requirements). Figure 14 presents the results for each benchmark and the geometric mean across all benchmarks scaled to our baseline of the number of test cases executed by QSYM-QEMU.

#### C. Performance of UnTracer-based Hybrid Fuzzing

As shown in Figure 14, on average, QSYM-UnTracer achieves 616% and 79% more test case executions than QSYM-QEMU and QSYM-Clang, respectively. We considered the potential problem of excessive test case trimming and calibration. Since our implementation of QSYM-UnTracer defaults to the slow tracer binary for these operations, we initially expected a performance deficit. However, our results show that the performance advantage of interest oracle-based execution (i.e., the “common case”) far outweighs any performance deficit from trimming and calibration tracing.

### VIII. Discussion

In this section, we discuss several topics related to our evaluation and implementation. First, we consider the emergence of hardware-assisted coverage tracing, offering a literature-based estimation of its performance with and without coverage-guided tracing. Second, we detail the modifications required to add basic block edge coverage support to UnTracer and the likely performance impact of moving to edge-based coverage. Lastly, we highlight the engineering needed to make UnTracer fully support black-box binaries.

#### A. UnTracer and Intel Processor Trace

Recent work proposes leveraging hardware support for more efficient coverage tracing. kAFL [11], PTfuzz [12], and honggFuzz [4] adapt Intel Processor Trace (IPT) [35] for black-box binary coverage tracing. IPT saves the control-flow behavior of a program to a reserved portion of memory as it executes. After execution, the log of control-flow information is used in conjunction with an abstract version of the program to generate coverage information. Because monitoring occurs at the hardware level, it is possible to completely capture a program’s dynamic coverage at the basic block, edge, or path level with modest runtime overheads. The three main limitations of IPT are its requirement of a supporting processor, time-consuming control-flow log decoding, and compatibility with only x86 binaries.

Despite these limitations, it is important to understand how IPT impacts coverage-guided tracing. From a high level, coverage-guided tracing works with IPT because it is orthogonal to the tracing mechanism. Thus, an IPT variant of UnTracer would approach 0% overhead sooner than our Dyninst-based implementation due to IPT’s much lower tracing overhead. From a lower level, the question arises as to the value of coverage-guided tracing with relatively cheap black-box binary coverage tracing. To estimate IPT’s overhead in the context of our evaluation, we look to previous work. Zhang et al. [12] present a fuzzing-oriented analysis of IPT that shows it averaging around 7% overhead relative to AFL-Clang-fast. Although we cannot use this overhead result directly as we compile all benchmarks with AFL-Clang, according to AFL’s author, AFL-Clang is 10–100% slower than AFL-Clang-fast [5]. By applying these overheads to the average overhead of 36% of AFL-Clang from our evaluation, AFL-Clang-fast’s projected overhead is between 18–32%, and IPT’s projected overhead is between 19–35%.

#### B. Incorporating Edge Coverage Tracking

As discussed in Section II-B, two coverage metrics dominate the fuzzing literature: basic blocks and basic block edges. UnTracer, our implementation of coverage-guided tracing, uses basic block coverage. Alternatively, many popular fuzzers (e.g., AFL [5], libFuzzer [6], honggFuzz [4]) use edge coverage. While the trade-offs between basic block and edge coverage metrics have yet to be studied with respect to fuzzing outcomes, we believe it is important to consider coverage-guided tracing’s applicability to edge coverage metrics.

The first point to understand is that most fuzzers that use edge coverage metrics actually rely on basic block-level tracing [63]. Key to enabling accurate edge coverage while only tracing basic blocks is the removal of critical edges. A critical edge is an edge in the control-flow graph whose starting/ending basic blocks have multiple outgoing/incoming edges, respectively [62]. Critical edges make it impossible to identify which edges are covered from knowing only the basic blocks seen during execution. This inflates coverage and causes the fuzzer to erroneously discard coverage-increasing inputs.

The solution to the critical edge problem is to split each by inserting an intermediate basic block, as shown in Figure 15. The inserted “dummy” basic block consists of a direct control-flow transfer to the original destination basic block. For white-box binaries, edge-tracking fuzzers honggFuzz [4] and libFuzzer [6] fix critical edges during compilation [63]. This approach works for white-box use cases of coverage-guided tracing as well. Unfortunately, how to adapt this approach to black-box binaries is an open technical challenge.

With respect to performance, the impact of moving from basic block coverage to edge coverage is less clear. It is clear that, given that edge coverage is a super-set of basic block coverage, the rate of coverage-increasing test cases will increase. To determine if the increase in the rate of coverage-increasing test cases is significant enough to disrupt the asymmetry that gives coverage-guided tracing its performance advantage, we reference the results in Figure 13 and Table II. Given that seven out of eight of our benchmarks have rates of coverage-increasing test cases below 1 in 100,000 and Figure 13 shows that UnTracer provides benefit for rates below 1 in 50, moving to edge-based coverage needs to induce a 4-orders-of-magnitude increase in the rate of coverage-increasing test cases to undermine UnTracer’s value. Such an increase is unlikely given Table II, which shows that even for fuzzers using edge coverage, the rate of coverage-increasing test cases is in line with the rates in our evaluation. Thus, given UnTracer’s near-0% overhead, we expect that any increase in the rate of coverage-increasing test cases due to moving to edge coverage will not change the high-level result of this paper.

#### C. Comprehensive Black-Box Binary Support

Niche fuzzing efforts desire support for black-box (source-unavailable) binary coverage tracing. Currently, UnTracer relies on a mix of black- and white-box binary instrumentation for constructing its two versions of the target binary. For tracer binaries, we use Dyninst-based black-box binary rewriting [25] to insert the forkserver and tracing infrastructure; for oracles, we repurpose AFL’s assembler front-end (afl-as) [5] to insert the forkserver. As discussed in Section V-B, our initial implementation used Dyninst to instrument the oracle binary, but we had to switch to afl-as due to unresolved performance issues. Though instrumenting the oracle’s forkserver at assembly-time requires assembly code access, we expect that inserting the forkserver is not a technical challenge for modern black-box binary rewriters [64], [65], [66], [67] or through function hooking (e.g., via LD_PRELOAD [68]).

### IX. Related Work

Two research areas orthogonal but closely related to coverage-guided tracing are improving test case generation and system optimizations, as both share the net outcome of improving overall fuzzer performance. We overview recent work in each area and relate those results back to coverage-guided tracing.

#### A. Improving Test Case Generation

Coverage-guided grey-box fuzzers like AFL [5] and libFuzzer [6] generally employ “blind” test case generation—relying on random mutation, prioritizing coverage-increasing test cases. A drawback of this strategy is stalled coverage, e.g., when mutation fails to produce test cases matching a target binary’s magic bytes (multi-byte strings or numbers) comparison operations. Research approaches this problem from several directions:

- **Driller [18]** and **QSYM [19]** use concolic execution (i.e., a mix of concrete and symbolic execution) to attempt to solve magic byte comparisons via symbolic path constraints. As is common with symbolic execution, exponential path growth becomes a limiting factor as target binary complexity increases.
- **honggFuzz [4]** and **VUzzer [7]** both leverage static and dynamic analysis to identify locations and values of magic bytes in target binaries.
- **Steelix [9]** improves coverage by inferring magic bytes from lighter-weight static analysis and static instrumentation.
- **Angora [39]** incorporates byte-level taint tracking, outperforming Steelix’s coverage on the synthetic LAVA datasets [69].

However, despite seeing higher rates of coverage-increasing test cases, these fuzzers still face the overhead of tracing all generated test cases.

Instead of attempting to focus mutation on matching magic byte comparisons all at once, an alternative set of approaches uses program transformation to make matching more tractable:

- **AFL-lafIntel [70]** unrolls magic bytes into single comparisons at compile-time, but currently only supports white-box binaries.
- **MutaGen [71]** utilizes mutated “input-producing” code from the target binary for test case generation, but it relies on input-producing code availability and faces slow execution speed due to dynamic instrumentation.
- **T-Fuzz [47]** attempts to strip target binaries of coverage-stalling code but suffers “transformational explosion” on complex binaries.

Changes in test case mutation schemes have also offered potential workarounds to stalled coverage. **FidgetyAFL [58]**, **AFLFast [8]**, and **VUzzer** all prioritize mutating test cases exercising rare basic blocks. Ultimately, coverage-guided fuzzers need to balance the benefits of improved test case generation with the overhead of tracing.