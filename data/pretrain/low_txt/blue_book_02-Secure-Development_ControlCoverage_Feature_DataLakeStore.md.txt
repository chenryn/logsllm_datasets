## Important Notice: DevOps Kit (AzSK) Sunset
The DevOps Kit (AzSK) will be sunset by the end of FY21. For more details, please refer to the [AzSK Sunset Notice](../../../ReleaseNotes/AzSKSunsetNotice.md).

---

### Data Lake Store: Description and Rationale

| Control | Severity | Automated Fix Script | Description and Rationale |
|---------|----------|----------------------|---------------------------|
| **Authentication with Azure Active Directory (AAD)** | High | No | All users and applications must authenticate using AAD-based credentials. Leveraging the native enterprise directory for authentication ensures a high level of assurance in user identity, which is essential for subsequent access control. All Enterprise subscriptions are automatically associated with their enterprise directory (e.g., `xxx.onmicrosoft.com`), and users in this directory are trusted for authentication to enterprise subscriptions. |
| **Role-Based Access Control (RBAC)** | Medium | Yes | All users and identities must be granted the minimum required permissions using RBAC. This ensures that users have just enough permissions to perform their tasks, minimizing the exposure of resources in case of a user or service account compromise. |
| **Access Control List (ACL) for Data Lake Store** | High | Yes | Access to the Data Lake Store file system must be limited using appropriate ACLs. The 'Other' group must not have any access. This ensures that data in ADLS is protected and accessible only to entities with a legitimate need to access it. |
| **Firewall on Data Lake Store** | Medium | Yes | The firewall should be enabled on Data Lake Store to restrict access to specific clients. While this may not be feasible in all scenarios, when used, it provides an extra layer of access control protection for critical assets. |
| **Secure Use of AdlCopy Tool** | High | No | The AdlCopy tool must be used securely when copying data from storage blobs to Data Lake Store. Using HTTPS ensures server/service authentication and protects data in transit from man-in-the-middle, eavesdropping, and session-hijacking attacks. Incautious use of storage keys in the AdlCopy command can result in key exposure to unauthorized users. |
| **Service Principal Identity for Clients** | High | No | Clients such as web jobs and standalone apps should use a service principal identity to access Data Lake Store. Using a 'user' account should be avoided, as it typically has broader privileges. A dedicated service principal ensures that the identity does not have permissions beyond those specifically granted for the given scenario. |
| **Encryption at Rest** | High | Yes | Sensitive data must be encrypted at rest. This minimizes the risk of data loss from physical theft and helps meet regulatory compliance requirements. |
| **Encryption in Transit** | High | No | Sensitive data must be encrypted in transit. Using HTTPS ensures server/service authentication and protects data in transit from man-in-the-middle, eavesdropping, and session-hijacking attacks. |
| **Diagnostics Logs Retention** | Medium | Yes | Diagnostics logs must be enabled with a retention period of at least 365 days. Retaining logs for a sufficient period allows activity trails to be recreated during investigations in the event of an incident or compromise. A one-year retention period is typical for many compliance requirements. |
| **Periodic Review of Diagnostic Logs** | Medium | No | Diagnostic logs for Data Lake Store should be reviewed periodically. Regular reviews of diagnostics, activity, and audit logs help identify anomalous activity early, rather than after a major compromise. |
| **Backup and Disaster Recovery Planning** | Medium | No | Backup and disaster recovery plans must be in place for Data Lake Store. Since Data Lake Analytics does not offer built-in backup and disaster recovery features, teams must ensure adequate backups of critical workloads. |
| **Data Retention and Cleanup** | Medium | No | Data in Data Lake Store should be cleaned up using file retention policies. Data should not be retained longer than necessary for business use cases. Periodic purging or cleanup minimizes the risk of compromise and helps limit the costs of maintaining the data. |

This table outlines the key controls, their severity, whether an automated fix script is available, and a detailed description and rationale for each control.