### Classical Stylometric Analysis and Future Research Directions

Classical stylometric analysis typically requires a minimum of 5,000 words. Our results raise several questions that motivate future research:

1. **Binary Code Analysis**:
   - Malicious code is often only available in binary format. It would be interesting to investigate whether syntactic features can be partially preserved in binaries. This may require enhancing our feature set to incorporate information obtained from control flow graphs.

2. **Classification Accuracy**:
   - We aim to explore methods to further increase classification accuracy. For example, we will investigate the use of features that have joint information gain alongside those that have individual information gain to see if this improves performance. Additionally, designing features that capture larger fragments of the abstract syntax tree (AST) could provide significant improvements. These changes, along with the addition of lexical and layout features, may significantly enhance the Python results and help generalize the approach.

3. **Code Normalization**:
   - Finally, we plan to investigate whether code can be automatically normalized to remove stylistic information while preserving functionality and readability.

### Acknowledgments

This material is based on work supported by the ARO (U.S. Army Research Office) Grant W911NF-14-1-0444, the DFG (German Research Foundation) under the project DEVIL (RI 2469/1-1), and AWS in Education Research Grant award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the ARO, DFG, and AWS.

### References

1. The tigress diversifying C virtualizer, <http://tigress.cs.arizona.edu>.
2. Google Code Jam, <https://code.google.com/codejam>, 2014.
3. Stunnix, <http://www.stunnix.com/prod/cxxo/>, November 2014.
4. Abbasi, A., & Chen, H. (2008). Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace. *ACM Transactions on Information Systems*, 26(2), 1–29.
5. Afroz, S., Brennan, M., & Greenstadt, R. (2012). Detecting hoaxes, frauds, and deception in writing style online. In *Security and Privacy (SP)*, IEEE Symposium on (pp. 461–475).
6. Aiken, A., et al. (2005). Moss: A system for detecting software plagiarism. University of California–Berkeley. See <http://www.cs.berkeley.edu/aiken/moss.html>.
7. Breiman, L. (2001). Random forests. *Machine Learning*, 45(1), 5–32.
8. Burrows, S., & Tahaghoghi, S. M. (2007). Source code authorship attribution using n-grams. In *Proceedings of the Australasian Document Computing Symposium*.
9. Burrows, S., Uitdenbogerd, A. L., & Turpin, A. (2009). Application of information retrieval techniques for source code authorship attribution. In *Database Systems for Advanced Applications* (pp. 699–713). Springer.
10. Ding, H., & Samadzadeh, M. H. (2004). Extraction of Java program fingerprints for software authorship identification. *Journal of Systems and Software*, 72(1), 49–57.
11. Elenbogen, B. S., & Seliya, N. (2008). Detecting outsourced student programming assignments. *Journal of Computing Sciences in Colleges*, 23(3), 50–57.
12. Frantzeskou, G., MacDonell, S., Stamatatos, E., & Gritzalis, S. (2008). Examining the significance of high-level programming features in source code author classification. *Journal of Systems and Software*, 81(3), 447–460.
13. Frantzeskou, G., Stamatatos, E., Gritzalis, S., Chaski, C. E., & Howald, B. S. (2007). Identifying authorship by byte-level n-grams: The source code author profile (SCAP) method. *International Journal of Digital Evidence*, 6(1), 1–18.
14. Frantzeskou, G., Stamatatos, E., Gritzalis, S., & Katsikas, S. (2006). Effective identification of source code authors using byte-level information. In *Proceedings of the 28th International Conference on Software Engineering* (pp. 893–896). ACM.
15. Gray, A., Sallis, P., & MacDonell, S. (n.d.). Software forensics: Extending authorship analysis techniques to computer programs.
16. Hayes, J. H., & Offutt, J. (2010). Recognizing authors: An examination of the consistent programmer hypothesis. *Software Testing, Verification and Reliability*, 20(4), 329–356.
17. Inocencio, R. (2013). U.S. programmer outsources own job to China, surfs cat videos.
18. Kothari, J., Shevchenko, M., Stehle, E., & Mancoridis, S. (2007). A probabilistic approach to source code authorship identification. In *Information Technology, ITNG’07. Fourth International Conference on* (pp. 243–248). IEEE.
19. Krsul, I., & Spafford, E. H. (1997). Authorship analysis: Identifying the author of a program. *Computers & Security*, 16(3), 233–257.
20. Lange, R. C., & Mancoridis, S. (2007). Using code metric histograms and genetic algorithms to perform author identification for software forensics. In *Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation* (pp. 2082–2089). ACM.
21. MacDonell, S. G., Gray, A. R., MacLennan, G., & Sallis, P. J. (1999). Software forensics for discriminating between program authors using case-based reasoning, feedforward neural networks, and multiple discriminant analysis. In *Neural Information Processing, ICONIP’99. 6th International Conference on* (Vol. 1, pp. 66–71). IEEE.
22. Narayanan, A., Paskov, H., Gong, N. Z., Bethencourt, J., Stefanov, E., Shin, E. C. R., & Song, D. (2012). On the feasibility of internet-scale author identification. In *Security and Privacy (SP)*, IEEE Symposium on (pp. 300–314). IEEE.
23. Pellin, B. N. (2000). Using classification techniques to determine source code authorship. White Paper: Department of Computer Science, University of Wisconsin.
24. Pike, R. (2011). The Sherlock plagiarism detector.
25. Prechelt, L., Malpohl, G., & Philippsen, M. (2002). Finding plagiarisms among a set of programs with JPlag. *J. UCS*, 8(11), 1016.
26. Quinlan, J. (1986). Induction of decision trees. *Machine Learning*, 1(1), 81–106.
27. Rosenblum, N., Zhu, X., & Miller, B. (2011). Who wrote this code? Identifying the authors of program binaries. *Computer Security–ESORICS 2011* (pp. 172–189).
28. Shevchenko, M., Kothari, J., Stehle, E., & Mancoridis, S. (2009). On the use of discretized source code metrics for author identification. In *Search Based Software Engineering, 2009 1st International Symposium on* (pp. 69–78). IEEE.
29. Spafford, E. H., & Weeber, S. A. (1993). Software forensics: Can we track code to its authors? *Computers & Security*, 12(6), 585–595.
30. Stoleru, A., Overdorf, R., Afroz, S., & Greenstadt, R. (2014). Classify, but verify: Breaking the closed-world assumption in stylometric authorship attribution. In *IFIP Working Group 11.9 on Digital Forensics*. IFIP.
31. Wikipedia. (2014). Saeed Malekpour. [Online; accessed 04-November-2014].
32. Yamaguchi, F., Golde, N., Arp, D., & Rieck, K. (2014). Modeling and discovering vulnerabilities with code property graphs. In *Proc. of IEEE Symposium on Security and Privacy (S&P)*.
33. Yamaguchi, F., Wressnegger, C., Gascon, H., & Rieck, K. (2013). Chucky: Exposing missing checks in source code for vulnerability discovery. In *Proceedings of the 2013 ACM SIGSAC Conference on Computer & Communications Security* (pp. 499–510).

### Appendix: Keywords and Node Types

#### Abstract Syntax Tree (AST) Node Types

Table 10 lists the AST node types generated by Joern that were incorporated into the feature set.

- AdditiveExpression
- ArgumentList
- BitAndExpression
- Callee
- CastTarget
- ConditionalExpression
- ElseStatement
- Expression
- ForStatement
- Identifier
- IdentifierDeclType
- IncDecOp
- Label
- OrExpression
- ParameterType
- RelationalExpression
- ShiftExpression
- SizeofOperand
- UnaryExpression
- WhileStatement
- AndExpression
- ArrayIndexing
- BlockStarter
- CallExpression
- CompoundStatement
- ContinueStatement
- EqualityExpression
- ExpressionStatement
- FunctionDef
- IdentifierDecl
- IfStatement
- InclusiveOrExpression
- MemberAccess
- Parameter
- PrimaryExpression
- ReturnStatement
- Sizeof
- Statement
- UnaryOp
- Argument
- AssignmentExpr
- BreakStatement
- CastExpression
- Condition
- DoStatement
- ExclusiveOrExpression
- ForInit
- GotoStatement
- IdentifierDeclStatement
- IncDec
- InitializerList
- MultiplicativeExpression
- ParameterList
- PtrMemberAccess
- ReturnType
- SizeofExpr
- SwitchStatement
- UnaryOperator

#### C++ Keywords

Table 11 shows the C++ keywords used in the feature set.

- alignas
- auto
- case
- class
- continue
- double
- export
- friend
- long
- not
- or_eq
- reinterpret_cast
- static
- template
- try
- unsigned
- wchar_t
- alignof
- bitand
- catch
- compl
- decltype
- dynamic_cast
- extern
- goto
- mutable
- not_eq
- private
- return
- static_assert
- this
- typedef
- using
- while
- and
- bitor
- char
- const
- default
- else
- false
- if
- namespace
- nullptr
- protected
- short
- static_cast
- thread_local
- typeid
- virtual
- xor
- and_eq
- bool
- char16_t
- constexpr
- delete
- enum
- float
- inline
- new
- operator
- public
- signed
- struct
- throw
- typename
- void
- xor_eq
- asm
- break
- char32_t
- const_cast
- do
- explicit
- for
- int
- noexcept
- or
- register
- sizeof
- switch
- true
- union
- volatile

### Appendix: Original vs. Obfuscated Code

**Figure 6: A code sample X**

Figure 6 shows a source code sample X from our dataset, which is 21 lines long. After obfuscation with Tigress, sample X became 537 lines long. Figure 7 shows the first 13 lines of the obfuscated sample X.

**Figure 7: Code sample X after obfuscation**

---

This optimized version aims to improve clarity, coherence, and professionalism.