### Rebuild Times for Single and Two Disk Failures

**Figure 20: Rebuild Times for Single and Two Disk Failures**

- **Y-Axis (Rebuild Time in Hours):**
  - 35
  - 30
  - 25
  - 20
  - 15
  - 10
  - 5
  - 0

- **X-Axis (Rebuild Time in Hours):**
  - 2.7
  - 2.8
  - 2.94
  - 3.66

- **Legend:**
  - Critical Rebuild
  - Degraded Rebuild

- **(a) Single Disk Failure:**
  - Trinity declustered parity and dRAID show the fastest rebuild times, requiring only 2.7 and 2.8 hours, respectively.
  - O-SODP exhibits similar rebuild performance to DP and dRAID.
  - G-SODP requires slightly more rebuild time.
  - Traditional RAID, lacking declustering, requires more than 24 hours to perform a rebuild.

- **(b) Two Disk Failures:**
  - The priority reconstruction algorithm is used to give vulnerable stripes higher rebuild priority to avoid data loss.
  - In our configuration (e.g., 8 + 2), the maximal tolerable failures within a single stripeset is 2.
  - Figure 20(b) compares the critical rebuild (e.g., stripes with 2 failures) and degraded rebuild (e.g., stripes with 1 failure) times during the reconstruction process.
  - DP, dRAID, and O-SODP have almost the same degraded rebuild time due to full declustering.
  - A drawback of the SODP schemes is the longer critical rebuild times.
  - RAID systems do not use priority reconstruction.

### Comparison of Storage Efficiency

- **Figure 21: Probability of Data Loss for Varying Parity Overheads**

- **Y-Axis (Probability of Data Loss):**
  - 100%
  - 80%
  - 60%
  - 40%
  - 20%
  - 0%

- **X-Axis (Parity Configurations):**
  - RS(2,2)
  - RS(4,2)
  - RS(6,2)
  - RS(8,2)

- **Legend:**
  - DP
  - dRAID
  - G-SODP
  - RAID

- **(a) Simultaneous Failures:**
  - Additional parity overhead is moderately effective at improving data loss probabilities during burst failures.

- **(b) Burst Failures Over 24h:**
  - G-SODP provides low probabilities of data loss while using low parity overheads.

### Conclusions

In our re-examination of declustered parity data placement schemes, it is evident that existing designs are not well-suited to handle correlated failure bursts common in cloud and HPC data centers. To address this, we propose two new criteria for declustered parity designs:

1. **Maximizing the number of simultaneous disk failures tolerated without increasing parity overhead.**
2. **Minimizing disk rebuild time by balancing parity stripes across all disks.**

To balance these criteria, we proposed Single-Overlap Declustered Parity (SODP), a data placement scheme that minimizes rebuild time and tolerates more failed disks than existing designs. However, to build a flexible algorithm, we relaxed the single-overlap requirement, allowing a small number of stripesets that do not overlap all other stripesets. This more flexible algorithm demonstrated the lowest data loss during failures occurring within small windows of time.

Our experiments showed that adding disks to a declustered placement group increases the probability of a data loss event by more than a linear amount when faced with correlated failures. The additional disk results in a less than linear improvement in rebuild time because the amount of data being rebuilt remains fixed even as the rebuild rate is proportionally increased. Thus, a data protection scheme like Greedy Single-Overlap Declustered Parity (G-SODP), which trades small amounts of rebuild performance to tolerate a large number of disk failures, can reduce the probability of data loss substantially (up to 30x in some test configurations).

### Future Work

- Detailed evaluation of the read and write performance of SODP-based data protection schemes.
- Further exploration of how to optimize the tradeoffs between rebuild performance and disk failure tolerance.
- Application of our design principles to distributed storage systems leveraging multiple levels of erasure coding and replication.

### Acknowledgements

We thank John Bent for helpful discussions on priority rebuild algorithms within declustered parity storage systems. We also thank Data Direct Networks for their support and co-funding this work as partners in the Efficient Mission Centric Computing Consortium (EMC3). The university authors were supported by funding from NSF (grant #CNS-1563956). This manuscript has been approved for unlimited release and has been assigned LA-UR-20-23191. This work has been co-authored by an employee of Triad National Security, LLC, which operates Los Alamos National Laboratory under Contract No. 89233218CNA000001 with the U.S. Department of Energy/National Nuclear Security Administration. The publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of the manuscript, or allow others to do so, for United States Government purposes.

### References

[1] Doug Beaver, Sanjeev Kumar, Harry C. Li, Jason Sobel, and Peter Vajgel. Finding a needle in haystack: Facebook’s photo storage. In Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation, OSDI’10, pages 47–60, Berkeley, CA, USA, 2010. USENIX Association.

[2] Subramanian Muralidhar, Wyatt Lloyd, Sabyasachi Roy, Cory Hill, Ernest Lin, Weiwen Liu, Satadru Pan, Shiva Shankar, Viswanath Sivakumar, Linpeng Tang, and Sanjeev Kumar. f4: Facebook’s warm BLOB storage system. In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), pages 383–398, Broomfield, CO, October 2014. USENIX Association.

[3] S. Oral, J. Simmons, J. Hill, D. Leverman, F. Wang, M. Ezell, R. Miller, D. Fuller, R. Gunasekaran, Y. Kim, S. Gupta, D. T. S. S. Vazhkudai, J. H. Rogers, D. Dillow, G. M. Shipman, and A. S. Bland. Best practices and lessons learned from deploying and operating large-scale data-centric parallel file systems. In SC ’14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 217–228, Nov 2014.

[4] Glenn K. Lockwood, Kirill Lozinskiy, Lisa Gerhardt, Ravi Cheema, Damian Hazen, and Nicholas J. Wright. Designing an all-flash Lustre file system for the 2020 NERSC Perlmutter system. In Proceedings of the 2019 Cray User Group (CUG), 2019.

[5] Ao Ma, Rachel Traylor, Fred Douglis, Mark Chamness, Guanlin Lu, Darren Sawyer, Surendar Chandra, and Windsor Hsu. Raidshield: characterizing, monitoring, and proactively protecting against disk failures. ACM Transactions on Storage (TOS), 11(4):17, 2015.

[6] Seagate Technology LLC. Seagate cloud storage array, January 2018.

[7] Seagate Technology LLC. Exos E 4U106, January 2018.

[8] Kevin M. Greenan, James S. Plank, and Jay J. Wylie. Mean time to meaningless: MTTF, Markov models, and storage system reliability. In Proceedings of the 2nd USENIX Conference on Hot Topics in Storage and File Systems, HotStorage’10, pages 5–5, Berkeley, CA, USA, 2010. USENIX Association.

[9] Saurabh Kadekodi, K. V. Rashmi, and Gregory R. Ganger. Cluster storage systems gotta have heart: Improving storage efficiency by exploiting disk-reliability heterogeneity. In 17th USENIX Conference on File and Storage Technologies (FAST 19), pages 345–358, Boston, MA, February 2019. USENIX Association.

[10] Asaf Cidon, Stephen Rumble, Ryan Stutsman, Sachin Katti, John Ousterhout, and Mendel Rosenblum. Copysets: Reducing the frequency of data loss in cloud storage. In Presented as part of the 2013 USENIX Annual Technical Conference (USENIX ATC 13), pages 37–48, 2013.

[11] Ramnatthan Alagappan, Aishwarya Ganesan, Yuvraj Patel, Thanumalayan Sankaranarayana Pillai, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. Correlated crash vulnerabilities. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI’16, pages 151–167, Berkeley, CA, USA, 2016. USENIX Association.

[12] Daniel Ford, François Labelle, Florentina I. Popovici, Murray Stokely, Van-Anh Truong, Luiz Barroso, Carrie Grimes, and Sean Quinlan. Availability in globally distributed storage systems. In Presented as part of the 9th USENIX Symposium on Operating Systems Design and Implementation, Vancouver, BC, 2010. USENIX.

[13] James Lujan, Manuel Vigil, Garrett Kenyon, Karissa Sanbonmatsu, and Brian Albright. Trinity supercomputer now fully operational.

[14] Mark Holland and Garth A. Gibson. Parity declustering for continuous operation in redundant disk arrays. In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS V, pages 23–35, New York, NY, USA, 1992. ACM.

[15] Guillermo A. Alvarez, Walter A. Burkhard, and Flaviu Cristian. Tolerating multiple failures in RAID architectures with optimal storage and uniform declustering. In ACM SIGARCH Computer Architecture News, volume 25, pages 62–72. ACM, 1997.

[16] John Fragalla. Improving Lustre OST performance with ClusterStor GridRAID. In 2014 HPCAC Stanford HPC Exascale Conference, 2014.

[17] GA Alvarez, Walter A. Burkhard, LL Stockmeyer, and Flaviu Cristian. Declustered disk array architectures with optimal and near-optimal parallelism. In Proceedings. 25th Annual International Symposium on Computer Architecture (Cat. No. 98CB36235), pages 109–120. IEEE, 1998.

[18] Thomas JE Schwarz, Jesse Steinberg, and Walter A. Burkhard. Permutation development data layout (PDDL). In Proceedings Fifth International Symposium on High-Performance Computer Architecture, pages 214–217. IEEE, 1999.

[19] ZFS dRAID. https://github.com/openzfs/zfs/wiki/dRAID-HOWTO, 2016.

[20] Guangyan Zhang, Zican Huang, Xiaosong Ma, Songlin Yang, Zhufan Wang, and Weimin Zheng. RAID+: Deterministic and balanced data distribution for large disk enclosures. In 16th USENIX Conference on File and Storage Technologies (FAST 18), pages 279–294, 2018.

[21] Neng Wang, Yinlong Xu, Yongkun Li, and Si Wu. OI-RAID: A two-layer RAID architecture towards fast recovery and high reliability. In 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), pages 61–72. IEEE, 2016.

[22] Zhipeng Li, Min Lv, Yinlong Xu, Yongkun Li, and Liangliang Xu. D3: Deterministic data distribution for efficient data reconstruction in erasure-coded distributed storage systems. In 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pages 545–556. IEEE, 2019.

[23] Richard R. Muntz and John CS Lui. Performance analysis of disk arrays under failure. Computer Science Department, University of California, 1990.

[24] Jeff Bonwick, Matt Ahrens, Val Henson, Mark Maybee, and Mark Shellenbaum. The zettabyte file system. Technical report, Sun Microsystems, 2003.

[25] Eric J. Schwabe and Ian M. Sutherland. Improved parity-declustered layouts for disk arrays. Journal of Computer and System Sciences, 53(3):328–343, 1996.

[26] Huan Ke. Optimal Single Overlap Declustered Parity (O-SODP) Feasible Solutions. shorturl.at/hqMR0.

[27] Mi Zhang, Shujie Han, and Patrick PC Lee. A simulation analysis of reliability in erasure-coded data centers. In 2017 IEEE 36th Symposium on Reliable Distributed Systems (SRDS), pages 144–153. IEEE, 2017.

[28] John P. Klein and Melvin L. Moeschberger. Survival analysis: Techniques for censored and truncated data. Springer Science & Business Media, 2006.

[29] Bianca Schroeder and Garth A. Gibson. Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you? In Proceedings of the 5th USENIX Conference on File and Storage Technologies, FAST '07, Berkeley, CA, USA, 2007. USENIX Association.

[30] Cameron Davidson-Pilon, Jonas Kalderstam, Paul Zivich, Ben Kuhn, Andrew Fiore-Gartland, Luis Moneda, Gabriel, Daniel Wilson, Alex Parij, Kyle Stark, Steven Anton, Lilian Besson, Jona, Harsh Gadgil, Dave Golland, Sean Hussey, Ravin Kumar, Javad Noorbakhsh, Andreas Klintberg, Eduardo Ochoa, Dylan Albrecht, dhuynh, Dmitry Medvinsky, Denis Zgonjanin, Daniel S. Katz, Daniel Chen, Christopher Ahern, Chris Fournier, Arturo, and André F. Rendeiro. Camdavidsonpilon/lifelines: v0.22.3 (late), August 2019.

[31] Mark Swan. Sonexion GridRAID characteristics. In Proceedings of the 2014 Cray User Group (CUG), 2014.