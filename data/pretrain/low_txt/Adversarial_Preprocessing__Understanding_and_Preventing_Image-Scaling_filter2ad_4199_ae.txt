### Majority of the Cases

In the majority of cases, participants recognized both the source and target classes. However, in a few instances, they only identified the source class. A closer analysis reveals that the distortion in these cases was so significant that detecting specific classes became challenging, leading to the participants' inability to specify the source class.

### Summary

We conclude that the two proposed defenses are robust against various adaptive attacks. These attacks are optimized with respect to the number of changes, providing strong empirical evidence for the robustness of the defenses. If a vulnerable scaling algorithm must be used in a machine-learning system or if the reconstruction of the original class is essential, we recommend using one of the defenses as a preprocessing step.

### User Study: Success Rate of Adaptive Attack Against Median Filter

**Figure 18: User study to determine the success rate of the adaptive attack against the median filter with respect to O2.**

- **Allowed Pixel Changes (δ [%]):** 0, 20, 40, 60, 80, 100
- **Success Rate (O1 [%]):** 
  - Median Filter: 0, 25, 50, 75, 100
  - CV—Nearest: 0, 25, 50, 75, 100
  - CV—Linear: 0, 25, 50, 75, 100
  - CV—Cubic: 0, 25, 50, 75, 100
  - TF—Nearest: 0, 25, 50, 75, 100
  - TF—Linear: 0, 25, 50, 75, 100
  - TF—Cubic: 0, 25, 50, 75, 100
- **Random Filter:** 0, 25, 50, 75, 100
- **Scaling Ratio (Specified [%]):** 2, 3, 4, 5
- **OpenCV—Nearest:** 0, 50, 100
- **OpenCV—Linear:** 0, 50, 100
- **OpenCV—Cubic:** 0, 50, 100

### Availability

Our dataset and code are publicly available at [http://scaling-attacks.net](http://scaling-attacks.net) to encourage further research on secure image scaling. Our defenses are implemented in C++ with Eigen, making them easily employable as plugins for TensorFlow.

### Acknowledgment

We would like to thank our shepherd Nicolas Papernot, the anonymous reviewers, and David Wagner for their suggestions and comments. We also acknowledge funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2092 CASA - 390781972 and the research grant RI 2469/3-1. Additionally, we received support from the German Ministry for Education and Research as BIFOLD - Berlin Institute for the Foundations of Learning and Data (ref. 01IS18025A and ref. 01IS18037A), and from the state of Lower Saxony under the project Mobilise.

### References

[1] M. Barni and F. Pérez-González. "Coping with the enemy: Advances in adversary-aware signal processing." In: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). 2013.

[2] B. Biggio, B. Nelson, and P. Laskov. "Support Vector Machines Under Adversarial Label Noise." In: Proc. of Asian Conference on Machine Learning (ACML). 2011.

[3] B. Biggio and F. Roli. "Wild patterns: Ten years after the rise of adversarial machine learning." In: Pattern Recognition 84 (2018).

[4] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, and F. Roli. "Evasion Attacks against Machine Learning at Test Time." In: Machine Learning and Knowledge Discovery in Databases. Springer, 2013.

[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2009.

[6] N. Carlini and D. A. Wagner. "Towards Evaluating the Robustness of Neural Networks." In: Proc. of IEEE Symposium on Security and Privacy (S&P). 2017.

[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Tech. rep. 2018. arXiv: 1810.04805.

[8] J. Fridrich. Steganography in Digital Media: Principles, Algorithms, and Applications. Cambridge University Press, 2010.

[9] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov. "Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations." In: Proc. of ACM Conference on Computer and Communications Security (CCS). 2018.

[10] T. Gu, B. Dolan-Gavitt, and S. Garg. BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. Tech. rep. 2017. arXiv: 1708.06733.

[11] K. He, X. Zhang, S. Ren, and J. Sun. "Deep Residual Learning for Image Recognition." In: Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016.

[12] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. "Densely Connected Convolutional Networks." In: Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017.

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. "ImageNet: Classification with Deep Convolutional Neural Networks." In: Advances in Neural Information Proccessing Systems (NIPS). 2012.

[14] J. Li, S. Ji, T. Du, B. Li, and T. Wang. "TextBugger: Generating Adversarial Text Against Real-world Applications." In: Proc. of Network and Distributed System Security Symposium (NDSS). 2019.

[15] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang. "Trojaning Attack on Neural Networks." In: Proc. of Network and Distributed System Security Symposium (NDSS). 2018.

[16] D. G. Lowe. "Distinctive Image Features from Scale-Invariant Keypoints." In: International Journal of Computer Vision 60.2 (2004).

[17] M. Lécuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. "Certified Robustness to Adversarial Examples with Differential Privacy." In: Proc. of IEEE Symposium on Security and Privacy (S&P). 2019.

[18] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. "Distributed Representations of Words and Phrases and their Compositionality." In: Advances in Neural Information Proccessing Systems (NIPS). 2013.

[19] A. V. Oppenheim, J. R. Buck, and R. W. Schafer. Discrete-Time Signal Processing; 2nd ed. Prentice-Hall, 1999.

[20] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Berkay Celik, and A. Swami. "Practical Black-Box Attacks against Machine Learning." In: Proc. of ACM Asia Conference on Computer and Communications Security (ASIA CCS). 2017.

[21] N. Papernot, P. McDaniel, A. Sinha, and M. P. Wellman. "SoK: Security and Privacy in Machine Learning." In: Proc. of IEEE European Symposium on Security and Privacy (EuroS&P). Apr. 2018.

[22] E. Quiring, D. Arp, and K. Rieck. "Forgotten Siblings: Unifying Attacks on Machine Learning and Digital Watermarking." In: IEEE European Symposium on Security and Privacy (EuroS&P). 2018.

[23] E. Quiring, A. Maier, and K. Rieck. "Misleading Authorship Attribution of Source Code using Adversarial Learning." In: Proc. of USENIX Security Symposium. 2019.

[24] E. Quiring and K. Rieck. "Backdooring and Poisoning Neural Networks with Image-Scaling Attacks." In: Deep Learning and Security Workshop (DLS). 2020.

[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. "ImageNet Large Scale Visual Recognition Challenge." In: International Journal of Computer Vision (IJCV) 115.3 (2015).

[26] S. Sardy, P. Tseng, and A. G. Bruce. "Robust Wavelet Denoising." In: IEEE Transactions on Signal Processing 49 (2001).

[27] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. "Membership Inference Attacks against Machine Learning Models." In: Proc. of IEEE Symposium on Security and Privacy (S&P). 2017.

[28] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. Tech. rep. 2014. arXiv: 1409.1556.

[29] S. W. Smith. The Scientist and Engineer’s Guide to Digital Signal Processing. California Technical Publishing, 1997.

[30] C. Sun, C. Tang, X. Zhu, X. Li, and L. Wang. "An efficient method for salt-and-pepper noise removal based on shearlet transform and noise detection." In: AEUE - International Journal of Electronics and Communications 69.12 (2015).

[31] I. Sutskever, O. Vinyals, and Q. V. Le. "Sequence to Sequence Learning with Neural Networks." In: Advances in Neural Information Proccessing Systems (NIPS). 2014.

[32] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing properties of neural networks. Tech. rep. 2013. arXiv: 1312.6199.

[33] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. "Stealing Machine Learning Models via Prediction APIs." In: Proc. of USENIX Security Symposium. 2016.

[34] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao. "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks." In: Proc. of IEEE Symposium on Security and Privacy (S&P). 2019.

[35] Q. Xiao, Y. Chen, C. Shen, Y. Chen, and K. Li. "Seeing is Not Believing: Camouflage Attacks on Image Scaling Algorithms." In: Proc. of USENIX Security Symposium. 2019.

### Downgrade Attack to Nearest Scaling

As part of our analysis, we identified a side effect in the implementation of \( g(p) \) (see Eq. (6)) in OpenCV and TensorFlow. An adversary can enforce the usage of nearest scaling by choosing a respective scaling factor, even though the library is supposed to use bilinear, bicubic, or Lanczos scaling. Specifically, if the scaling ratio is an uneven integer, \( \beta = 2z + 1, z \in \mathbb{N} \), OpenCV effectively uses nearest scaling. In TensorFlow, any integer \( \beta \in \mathbb{N} \) leads to the same effect. Thus, if the adversary can control the source image size, she can resize her image to obtain the respective scaling factor. This, in turn, allows her to perform a more powerful scaling attack by creating attack images with less distortion, as the ratio of considered pixels decreases (see Section 3.3). Note that we do not exploit this issue in our evaluation. We test over a variety of scaling factors to draw general conclusions on scaling attacks.

**Table 5: Implementation of \( g(p) \) in OpenCV, TensorFlow, and Pillow**

| Library | \( g(·) \) |
|---------|------------|
| OpenCV  | \( g(p) = (p + 0.5) \cdot \beta - 0.5 \) |
| TensorFlow | \( g(p) = p \cdot \beta \) (*) |
| Pillow  | \( g(p) = (p + 0.5) \cdot \beta \) |

(*) The scaling function in TensorFlow can be changed to the definition from OpenCV. However, this option is not exposed in `tf.image.resize_images`, the high-level resizing API.

To understand the reason, we need to consider the mapping \( g(p) \) and the kernel \( w \). Table 5 shows the slightly different implementations of \( g(p) \) in OpenCV, TensorFlow, and Pillow. For OpenCV, if \( \beta \) is an uneven integer, \( g(p) \) will always be an integer. Thus, only one pixel will be used for the convolution. A closer look at the definition of the kernels in Figure 6 reveals the underlying reason. Each kernel is zero for integer positions. Therefore, if \( g(p) \) is an integer and the kernel is exactly positioned here, each neighboring pixel obtains a weight of zero. Thus, only the pixel at position \( g(p) \) is used. This behavior corresponds to nearest scaling. We observe this effect for bilinear, bicubic, and Lanczos scaling in OpenCV and TensorFlow. On the contrary, Pillow uses a dynamic kernel width, so we do not observe this behavior in this case.

### Selective Random Filter

Our random filter is identical to the selective median filter, except that it takes a random point from each window instead of the median. Given a point \( p \in P \), we consider a window \( W_p \) around \( p \) of size \( 2\beta_h \times 2\beta_v \) and randomly select a point as a reconstruction of \( p \). Again, we exclude points \( p' \in P \) from this window to limit the attacker’s influence.

Randomly selecting a point for reconstruction comes with challenges. First, the reconstruction becomes non-deterministic. Second, the scaled image might suffer from poor quality. However, our evaluation shows that the loss due to random sampling is small and may be acceptable for the benefit of very efficient runtime performance. The filter reconstructs an image with a complexity of \( O(|P|) \), which is independent of the scaling ratio. Furthermore, the filter also provides strong protection from attacks. If an image contains \( |P| \) relevant points, there exist \( |P| \cdot 4^{\beta_h \beta_v} \) possible combinations for its reconstruction. For a scaling ratio of 5 and a target size of 200x200, this amounts to 4 million different combinations an attacker needs to guess from.

### Adaptive Attack Against Median Filter

In the following, we analyze our adaptive attack against the median-based defense. We demonstrate that the attack is optimal regarding the L0, L1, and L2 norms if each window \( W_p \) does not overlap with other windows. An adversary cannot make fewer changes to control the output of the median filter.

For a given attack image and window \( W_p \), the adversary seeks to manipulate the pixels in \( W_p \) such that the median \( m \) over \( W_p \) still corresponds to \( p \). In this way, the modifications from the image-scaling attack remain even after applying the median filter. Without loss of generilarity, we assume that \( m < p \) and unroll \( W_p \) to a one-dimensional signal. We consider a signal with an odd length \( k \) and denote the numerical order by brackets, so that the signal is given by:

\[ x(1), \ldots, x\left(\frac{k}{2}\right), m\left(\frac{k+1}{2}\right), x\left(\frac{k+2}{2}\right), \ldots, x(l), \ldots, x(k) \]

We denote by \( x(l) \) the largest pixel in the sorted signal that is smaller than \( p \). The objective is to change the signal with the fewest possible changes such that \( m = p \).

We start by observing that we need to change \( l - \frac{k+1}{2} + 1 \) pixels to move the median to \( p \). Fewer changes do not impact the numerical order sufficiently. We can thus conclude that the minimal L0 norm for an attack is given by:

\[ L_0 = l - \frac{k+1}{2} + 1 \]

Furthermore, we can derive a simple bound for the L2 norm:

\[ (L_2)^2 = \sum_{\frac{k+1}{2} \leq i \leq l} (x(i) - p)^2 \leq L_0 (m - p)^2 \]

Overall, we can exactly compute the number and amount of required changes for a successful attack. Our analysis also shows that the attack always depends on the concrete pair of a source and a target image, and there is no notion of a class boundary. Consequently, we cannot derive a general bound, as achieved with certifiable defenses against adversarial examples. However, our empirical results in Section 5.5 demonstrate that the necessary changes are very large if the target and source images show realistic content, so that the median \( m \) and the target value \( p \) are not close to each other.

### Additional Figures

**Figures 19 to 23** provide further information and examples from our evaluation. They include visual examples of successful and failed attacks, highlighting the working principle of image-scaling attacks.

**Figure 19: Success rate of attack regarding objective O2: the similarity between source image and attack image, measured by the PSNR value.**

**Figure 20: Best images of the L0 version of our adaptive attack against area scaling. The attack fails in all cases with respect to objective O2, as each attack image is not similar to the source image anymore.**

**Figure 21: Selective source scenario against area scaling with our L1 attack (first two columns) and L0 attack (last three columns). The attack fails in all cases with respect to objective O2. While traces from the source image are visible, the attack image overwrites the source image considerably.**

**Figure 22: Randomly selected examples before and after restoration with our median filter (first three columns) and random filter (last two columns). Without restoration, the attack is successful, as the downscaling of the attack image produces an unrelated target image (1st and 2nd row). With restoration, the attack fails in all cases with respect to objective O1, as the downscaled output from the restored attack image produces the respective content and not an unrelated image (3rd and 4th row). Moreover, the filtering improves quality by removing traces from the attack.**

**Figure 23: Successful examples regarding objective O1 from the adaptive attack against the median filter if 20% of the pixels in each block can be changed. The target class is detected, but the attack image is a mix between the source and target class. The results thus violate objective O2.**