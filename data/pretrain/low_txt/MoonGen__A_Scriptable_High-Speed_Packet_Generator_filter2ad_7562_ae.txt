### NICs in Our Testbed and Packet Handling

In our testbed, Network Interface Cards (NICs) discard packets with errors early in the receive flow. These packets are dropped before they are assigned to a receive queue, and the NIC only increments an error counter [11, 12, 13]. Consequently, the packet processing logic remains unaffected by this software rate control mechanism.

### Illustration of the Concept

Figure 9 illustrates this concept. Packets \( p_{ij} \) are sent with incorrect CRC checksums, while all other packets \( p_k \) have correct checksums. Note that the wire and all transmission queues are fully utilized, meaning the generated rate must match the line rate.

### Forwarding Latencies with CBR Traffic

Figure 10 shows the differences in forwarding latencies of Open vSwitch with Constant Bit Rate (CBR) traffic generated by hardware and our software approach. In theory, arbitrary inter-packet gaps should be possible. However, the NICs we tested refused to send frames with a wire-length (including Ethernet preamble, start-of-frame delimiter, and inter-frame gap) of less than 33 bytes. This means gaps between 1 and 32 bytes (0.8 ns to 25.6 ns) cannot be generated. Generating small frames also places the NIC under unusually high load, for which it was not designed. We found that the maximum achievable packet rate with short frames is 15.6 Mpps on Intel X540 and 82599 chips, only 5% above the line rate for packets with the regular minimal size. Therefore, MoonGen enforces a minimum wire-length of 76 bytes (8 bytes less than the regular minimum) by default for invalid packets. As a result, gaps between 0.8 ns and 60.8 ns cannot be represented.

### Evaluation of CBR Traffic Generation

We generate CBR traffic using our approach and compare it to CBR traffic generated by the hardware facilities of our NIC by measuring the response of a Device Under Test (DuT). We use Intel’s hardware implementation as the reference generator. Other software-based packet generators do not support accurate timestamping, making such comparisons impossible. However, results from Section 7.4 indicate that latency would be affected at low rates due to the different interrupt rate (cf. Figure 7).

Figure 10 shows the difference in the 25th, 50th, and 75th percentiles of the forwarding latency on a server running Open vSwitch. The test is restricted to the range 0.1 Mpps to 1.9 Mpps, as the DuT becomes overloaded at higher rates. The relative deviation is within 1.2σ of 0% for almost all measurement points, with the 1st quartile at 0.23 Mpps deviating by 1.5% ± 0.5%. Minor activity on the DuT, such as an active SSH session, shows a significantly larger (≥ 10%) effect on latency with both rate control methods. This indicates that loading the DuT with a large number of invalid packets does not cause system activity; the DuT does not notice the invalid packets.

### Example: Poisson Traffic

CBR traffic is often an unrealistic test scenario for latency measurements. Bursts or a Poisson process allow for more sophisticated tests that also stress buffers as the DuT becomes temporarily overloaded.

Figure 11 shows measured latencies of Open vSwitch configured to forward packets between two ports. We generate packets with CBR (hardware rate control) and Poisson (CRC-based software rate control) traffic patterns and compare their latencies. The outlier at 0.4 Mpps for CBR traffic was reproducible across multiple re-measurements on different servers. The sudden drop in latency before the system becomes overloaded was also reproducible. Both are likely artifacts of the interaction between the interrupt throttle algorithm in the Intel driver [10] and the dynamic interrupt adaption of Linux [25] on the DuT. These artifacts are present regardless of how the CBR traffic is generated (cf. Figure 10), indicating they are not caused by MoonGen but by the traffic pattern on the DuT.

The system becomes overloaded at about 1.9 Mpps, resulting in packet drops and a very large latency (about 2 ms in this test setup) as all buffers are filled. The overall achieved throughput is the same regardless of the traffic pattern and method used to generate it. This result highlights that the traffic pattern can measurably affect the DuT, underscoring the importance of a reliable and precise packet generator.

### Limitations of Our Approach

A shorter per-byte transmission time improves both the granularity and the minimum length that can be generated, making our solution most effective on 10 GbE, where the granularity is 0.8 ns. Due to the artificially enforced minimum size of 76 bytes, gaps between 1 and 75 bytes (0.8 ns to 60 ns) cannot be precisely represented (cf. Section 8.1). It is possible to reduce this range for tests with larger packets or lower rates. We approximate gaps that cannot be generated by occasionally skipping an invalid packet and increasing the length of other gaps. The overall rate still reaches the expected average, maintaining high accuracy but relatively low precision for these delays.

A possible workaround for gaps with a length between 1 and 75 bytes is using multiple NICs to generate traffic that is sent to a switch. The switch then drops the invalid frames and multiplexes the different streams before forwarding them to the DuT. This works if the generated pattern can be split into multiple streams, e.g., by overlaying several Poisson processes.

However, short delays are often not meaningful in modern networks. For example, the 10GBASE-T transmission standard used by most experiments for this paper operates on frames with a payload size of 3200 bits on the physical layer as defined in IEEE 802.3 Section 4 55.1.3.1 [9]. This means that any layers above the physical layer will receive multiple packets encoded in the same frame as a burst. So, two back-to-back packets cannot be distinguished from two packets with a gap of 232 bytes (185.6 ns) in the worst case, and failure to represent gaps between 1 and 75 bytes should not be noticeable. Note that this limit on the physical layer only applies to relatively short inter-arrival times, and bad rate control generating bursts is still inferior to our approach (cf. Figure 7 in Section 7.3).

Another limitation is that our approach is optimized for experiments where the DuT (or the first hop in a system under test) is a software-based packet processing system, not a hardware appliance. Hardware might be affected by an invalid packet. In such a scenario, we suggest routing the test traffic through a store-and-forward switch that drops packets with invalid CRC checksums, effectively replacing the invalid packets with real gaps on the wire. The effects of the switch on the inter-arrival times need to be carefully evaluated first.

### Reproducible Research

We encourage you to install MoonGen and reproduce the results from this paper to verify our work. All experiments presented here can be reproduced with the included example scripts and NICs based on Intel 82599, X540, 82580, and XL710 chips.

The performance evaluation in Section 5 is based on the scripts found in `examples/benchmarks`, an Intel Xeon E5-2620 v3 CPU, and Intel X540 NICs. The timestamping accuracy in Section 6 was measured with the script `timestamps.lua`, and the clock drift measurements with `drift.lua`. Inter-arrival times in Section 7 were measured with `inter-arrival-times.lua`. The script `l2-load-latency.lua` with the timestamping task disabled was used to generate the analyzed traffic. The suggested workaround for the hardware rate control limitations at high rates is also implemented in `l2-load-latency.lua`. Sending bursty traffic is implemented in `l2-bursts.lua`. The example measurement of the interrupt rate in Section 7.4 was conducted with `l2-load-latency.lua` and `zsend 6.0.2`.

The script `compare-rate-control-mechanisms.lua` was used for the evaluation in Section 8.2. The latency measurements with Poisson and CBR traffic in Section 8.3 are based on `l2-load-latency.lua` and `l2-poisson-load-latency.lua`.

The DuT for these tests was Open vSwitch 2.0.0 on Debian Linux 3.7 with ixgbe 3.14.5 running on a server with a 3.3 GHz Intel Xeon E3-1230 v2 CPU. Only a single CPU core was used by configuring the NIC with only one queue. Each test was run for at least 30 seconds with at least 30,000 timestamped packets.

All measurements were conducted on Intel X540 NICs except for the inter-arrival times (Intel 82580), fiber loopback measurements (Intel 82599), and 40 GbE tests (Intel XL710). We used different development versions of MoonGen for the experiments described throughout this paper. The performance evaluation with 10 GbE in Section 5 was done with commit 492c0e4, and on 40 GbE with commit a70ca21. We confirmed that all described experiments still work with the example scripts from commit a70ca21 in our git repository [5].

### Conclusions and Future Work

We have presented a general-purpose packet generator, MoonGen, that uses hardware features of commodity NICs to implement functionality previously available only on expensive special-purpose hardware. MoonGen represents a hybrid between a pure software-based solution and one based on hardware, combining the advantages of both approaches while mitigating shortcomings by using both hardware-specific features and novel software approaches.

MoonGen measures latencies with sub-microsecond accuracy and precision. The desired packet rate can be controlled precisely through both hardware support and our rate control algorithm based on filling gaps with invalid packets. We have shown that it is feasible to use modern implementations of scripting languages to craft packets without sacrificing speed, making MoonGen flexible and extensible. The flexibility goes beyond the capabilities provided by hardware load generators, as each packet can be crafted in real-time by a script. Tests that respond to incoming traffic in real-time are possible as MoonGen also features packet reception and analysis.

In the future, we will add additional example scripts and support for hardware features of more NICs. MoonGen currently comes with example scripts to handle IPv4, IPv6, UDP, TCP, ICMP, IPsec, and ARP traffic. MoonGen's flexible architecture allows for further applications like analyzing traffic in line rate on 10 GbE networks or conducting Internet-wide scans from 10 GbE uplinks. MoonGen is under active development, and the latest version is available in our public git repository [5].

### Acknowledgments

This research was supported by the DFG MEMPHIS project (CA 595/5-2), the KIC EIT ICT Labs on SDN, and the EUREKA-Project SASER (01BP12300A). We would like to thank the anonymous reviewers and our colleagues Dominik Scholz, Johannes Reiﬀerscheid, Rainer Sch¨onberger, Patrick Werneck, Lukas M¨ardian, Lukas Erlacher, and Stephan M. G¨unther for valuable contributions to MoonGen and this paper.

### References

[1] Nicola Bonelli, Andrea Di Pietro, Stefano Giordano, and Gregorio Procissi. Flexible High Performance Traffic Generation on Commodity Multi–Core Platforms. In Proceedings of the 4th International Conference on Traffic Monitoring and Analysis, pages 157–170. Springer, 2012.
[2] Alessio Botta, Alberto Dainotti, and Antonio Pescapé. Do You Trust Your Software-Based Traffic Generator? IEEE Communications Magazine, 48(9):158–165, 2010.
[3] Scott Bradner and Jim McQuaid. Benchmarking Methodology for Network Interconnect Devices. RFC 2544 (Informational), March 1999.
[4] G. Adam Covington, Glen Gibb, John W. Lockwood, and Nick Mckeown. A Packet Generator on the NetFPGA Platform. In 17th IEEE Symposium on Field Programmable Custom Computing Machines, pages 235–238, 2009.
[5] Paul Emmerich. MoonGen. https://github.com/emmericp/MoonGen.
[6] Sebastian Gallenm¨uller, Paul Emmerich, Florian Wohlfart, Daniel Raumer, and Georg Carle. Comparison of Frameworks for High-Performance Packet IO. In ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS 2015), May 2015.
[7] Luke Gorrie. Snabb Switch. https://github.com/SnabbCo/snabbswitch/.
[8] IEEE Standard for a Precision Clock Synchronization Protocol for Networked Measurement and Control Systems. IEEE 1588-2008, July 2008.
[9] IEEE. IEEE 802.3-2012 IEEE Standard for Ethernet Section Four, 2012.
[10] Intel. Intel Server Adapters - Linux ixgbe Base Driver. http://www.intel.com/support/network/adapter/pro100/sb/CS-032530.htm. Last visited 2015-08-24.
[11] Intel 82580EB Gigabit Ethernet Controller Datasheet Rev. 2.6. Intel, 2014.
[12] Intel 82599 10 GbE Controller Datasheet Rev. 2.76. Intel, 2012.
[13] Intel Ethernet Controller X540 Datasheet Rev. 2.7. Intel, 2014.
[14] Data Plane Development Kit. http://dpdk.org/. Last visited 2015-08-24.
[15] Intel Ethernet Controller XL710 Datasheet Rev. 2.1. Intel, December 2014.
[16] Product Brief - Intel Ethernet Controller XL710 10/40 GbE. Intel, 2014.
[17] NetFPGA. http://netfpga.org/. Last visited 2015-08-24.
[18] Ntop. PF RING ZC (Zero Copy). http://www.ntop.org/products/pf_ring/pf_ring-zc-zero-copy/. Last visited 2015-04-28.
[19] Srivats P. Ostinato. http://ostinato.org/. Last visited 2015-08-24.
[20] Mike Pall. LuaJIT. http://luajit.org/. Last visited 2015-08-24.
[21] Mike Pall. LuaJIT in realtime applications. http://www.freelists.org/post/luajit/LuaJIT-in-realtime-applications,3, July 2012. Mailing list post.
[22] Luigi Rizzo. The netmap project. http://info.iet.unipi.it/~luigi/netmap/. Last visited 2015-08-24.
[23] Luigi Rizzo. netmap: A Novel Framework for Fast Packet I/O. In USENIX Annual Technical Conference, pages 101–112, 2012.
[24] Charalampos Rotsos, Nadi Sarrar, Steve Uhlig, Rob Sherwood, and Andrew W Moore. Oﬂops: An Open Framework for OpenFlow Switch Evaluation. In Passive and Active Measurement, pages 85–95. Springer, 2012.
[25] Jamal Hadi Salim, Robert Olsson, and Alexey Kuznetsov. Beyond Softnet. In Proceedings of the 5th Annual Linux Showcase & Conference, volume 5, pages 18–18, 2001.
[26] Joel Sommers and Paul Barford. Self-Configuring Network Traffic Generation. In Proceedings of the 4th ACM SIGCOMM Conference on Internet Measurement, IMC ’04, pages 68–81, New York, NY, USA, 2004. ACM.
[27] Keith Wiles. Pktgen-DPDK. http://github.com/Pktgen/Pktgen-DPDK/.
[28] Yinglin Yang, Sudeep Goswami, and Carl G. Hansen. 10GBASE-T Ecosystem is Ready for Broad Adoption, 2012. White paper.