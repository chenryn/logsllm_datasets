# A Framework for Evaluating Storage System Dependability

**Authors:**
- Kimberly Keeton
- Arif Merchant

**Affiliation:**
Storage Systems Department, HP Labs, Palo Alto, CA 94304  
Email: kimberly.keeton@hp.com, arif.merchant@hp.com

## Abstract
Designing storage systems to ensure business continuity in the face of failures requires the use of various data protection techniques, such as backup, remote mirroring, point-in-time copies, and vaulting, often in combination. Predicting the dependability provided by such combinations is challenging but essential for reliable system design. This paper presents a framework for evaluating the dependability of data storage systems, including both individual data protection techniques and their compositions. Our models estimate storage system recovery time, data loss, normal mode system utilization, and operational costs under various failure scenarios. We demonstrate the effectiveness of these modeling techniques through a case study using real-world storage system designs and workloads.

## 1. Introduction
In the information age, data is a primary asset for most corporations, and businesses must have continuous access to this data to operate effectively. A 2001 survey found that a quarter of respondents estimated their outage costs at over $250,000 per hour, with 8% estimating costs exceeding $1 million per hour [5]. The cost of data loss can be even more severe, potentially leading to bankruptcy. Therefore, dependable data storage systems are crucial to avoid such issues.

Several techniques exist for protecting data, including tape backup [3], mirroring and parity-based RAID schemes for disk arrays [20], wide-area inter-array mirroring [12], snapshots [1], and wide-area erasure-coding schemes [15]. Each technique protects against a subset of possible failure scenarios, and they are often used in combination to provide broader coverage.

However, the variety of data protection techniques and their configuration parameters make it difficult to apply each technique appropriately. System administrators often use ad hoc methods to design their data storage systems, focusing more on setting configuration parameters (e.g., backup windows) rather than achieving specific dependability goals [3, 4]. Consequently, it is often unclear whether the business's dependability objectives have been met.

If we could quantify the dependability of a storage system, we could evaluate whether an existing system meets its dependability goals, explore future designs and what-if scenarios, and provide the innermost loop of an automated optimization process to choose the best solution for given business requirements [13]. Such a framework should allow the composition of data protection techniques to model complex storage systems and facilitate the incorporation of new techniques as they are developed.

This paper describes a modeling framework for quantitatively evaluating the dependability of storage system designs. By dependability, we mean both data reliability (i.e., the absence of data loss or corruption) and data availability (i.e., that access is always possible when desired). Our target is tools for use in the business continuity community, so we have adopted their metrics for recovery time and recent data loss after a failure. Recovery time measures the elapsed time after a failure before a business service (e.g., application) is up and running again; the recovery time objective (RTO) provides an acceptable upper bound [2]. When a failure occurs, it may be necessary to revert to a consistent point prior to the failure, which will entail the loss of any data written after that point. Recent data loss measures the amount of recent updates (expressed in time) lost during recovery from a failure; the recovery point objective (RPO) provides an upper bound [2]. Both RTO and RPO can range from zero to days. Our models evaluate the recovery time and recent data loss metrics under specified failure scenarios, and we add normal mode system utilization and overall system cost (including capital and service cost outlays and penalties for violating business requirements).

The primary contributions of our work include:
1. A common set of parameters to describe popular data protection techniques.
2. Models for the dependability of individual data protection techniques using these parameters.
3. Techniques for composing these models to determine the dependability of the overall storage system.
4. Analysis of the models and compositional framework using case studies.

The remainder of the paper is organized as follows. Section 2 provides an overview of popular data protection techniques and surveys related work. Section 3 describes our modeling framework in detail, and Section 4 provides an extensive case study drawn from real-world storage designs to demonstrate its operation. Finally, Section 5 concludes the paper.

## 2. Data Protection Techniques and Related Work
Disk arrays are typically used to store the primary copy of data and provide protection against internal hardware failure through RAID techniques [20] and redundant hardware paths to the data. Other failures, such as user or software errors and site failures, are covered by techniques that periodically make secondary copies of the data, often to other hardware. These secondary copies preferably reflect a consistent version of the primary copy at some instant in time, which we call retrieval points (RPs). The main classes of such techniques are mirroring, point-in-time copies, and backup.

### Mirroring
Mirroring keeps a separate, isolated copy of the current data on another disk array, which may be co-located with the primary array or remote. Mirrors can be synchronous, where each update to the primary is also applied to the secondary before write completion, or asynchronous, where updates are propagated in the background. Batched asynchronous mirrors [12, 21] coalesce overwrites and send batches to the secondary to be applied atomically, reducing the peak bandwidth needed between the copies and smoothing out update bursts.

### Point-in-Time (PiT) Images
A PiT image [1] is a consistent version of the data at a single point in time, typically on the same array. The PiT image can be formed as a split mirror, where a normal mirror is maintained on the same array until a "split" operation stops further updates to the mirror, or as a virtual snapshot, where a virtual copy is maintained using copy-on-write techniques, with unmodified data sharing the same physical storage as the primary copy. Most enterprise-class disk arrays (e.g., [6, 9]) support one or more of these techniques.

### Backup
Backup is the process of copying RPs to separate hardware, which could be another disk array, a tape library, or an optical storage device. Backups can be full, where the entire RP is copied; cumulative incremental, where all changes since the last full backup are copied; or differential incremental, where only the portions changed since the last full or incremental are copied. Tape backups typically use a combination of these alternatives (e.g., weekend full backups, followed by a cumulative incremental every weekday). Backups made to physically removable media, such as tape or optical disks, can also be periodically moved to an off-site vault for archival storage.

Backup techniques and tools have been studied from an operational perspective (e.g., [3, 4]). Studies also describe alternative mechanisms for archival and backup (e.g., [15]) and file systems that incorporate snapshots (e.g., [11]). Evaluations of storage system dependability have focused mainly on disk arrays, including the dependability of array hardware and RAID mechanisms and the recovery time after failure (e.g., [7, 17, 18, 19, 20, 22]). Additionally, a great deal of work (e.g., [8]) focuses on the overall area of dependability and performability evaluation of computer systems. Keeton et al. have explored issues in automating data dependability [13]. This paper builds upon and complements these techniques by providing models of individual dependability techniques and a framework for composing them, thus enabling the evaluation of dependability in storage systems that combine multiple dependability techniques. Our models are deliberately simple to allow users to reason about them and are designed to be composable to fit into the overall framework.

## 3. Modeling Storage System Dependability
The goal of our modeling framework is to evaluate the dependability of a storage system design for the specified workload inputs and business requirements under the specified failure scenario. Table 1 summarizes the model's parameters, which are explained in the sections below.

### Model Components
The model components work together as follows:
- **Data Protection Technique Models:** Convert input parameters to bandwidth and capacity workload demands on the storage and interconnect devices they employ.
- **Hardware Device Models:** Perform device-specific calculations to determine each device's bandwidth and capacity utilization and outlay costs.
- **Compositional Models:** Combine the results from the data protection and device models to generate the output metrics.

By isolating the details of each data protection technique and hardware device, we can easily substitute more sophisticated models (e.g., [16, 19]) for these components as needed without modifying the rest of the modeling framework.

### 3.1. Model Inputs
This section outlines how we describe model inputs, including workloads, business requirements, and failure scenarios.

#### 3.1.1. Workload Inputs
A storage system holds the primary copy of data (e.g., file systems or database tablespaces). Data protection techniques exploit the workload's update properties to effectively make secondary copies of the data, with some techniques propagating all updates and others propagating only periodic batches of unique updates. The key workload parameters to capture include:
- **Data Capacity:** Size of the data item.
- **Average Access Rate:** Rate of read and write accesses to the object.
- **Average Update Rate:** Rate of (non-unique) updates to the object.
- **Burstiness:** Ratio of peak update rate to average update rate.
- **Batch Update Rate:** Unique update rate within a given window.

Although most systems store multiple data objects, we assume a single data object and workload for simplicity. Our models can be extended by explicitly tracking each object's workload demands.

### 3.2. Business Requirements
Business requirements include:
- **Data Unavailability Penalty Rate:** Penalty per unit time for unavailability of data.
- **Recent Data Loss Penalty Rate:** Penalty for loss of a time-unit's worth of updates to data.

### 3.3. Failure Scenarios and Recovery Goals
Failure scenarios and recovery goals include:
- **Failure Scope:** Set of data copy sites unavailable due to a failure.
- **Recovery Time Objective (RTO):** Time from failure to having the application running again.
- **Recovery Point Objective (RPO):** Point in time to which restoration is requested.

### 3.4. Model Outputs
The model outputs include:
- **System Utilization:** Utilization of maximally utilized storage component.
- **Recovery Time:** Time from failure to having the application running again.
- **Recent Data Loss:** Recent data updates not recovered by the recovery process.
- **Overall Cost:** Overall system cost, including outlays and penalties.

## 4. Case Study
We demonstrate the effectiveness of our modeling techniques through a case study using real-world storage system designs and workloads. The case study illustrates how the framework can be used to evaluate the dependability of different storage configurations and to make informed decisions about system design and resource allocation.

## 5. Conclusion
This paper presents a comprehensive framework for evaluating the dependability of data storage systems. The framework includes models for individual data protection techniques and techniques for composing these models to determine the dependability of the overall storage system. The case study demonstrates the practical utility of the framework in real-world scenarios. Future work will focus on extending the framework to incorporate additional data protection techniques and to refine the models for more accurate and detailed analysis.

---

**References:**
- [1] ...
- [2] ...
- [3] ...
- [4] ...
- [5] ...
- [6] ...
- [7] ...
- [8] ...
- [9] ...
- [10] ...
- [11] ...
- [12] ...
- [13] ...
- [14] ...
- [15] ...
- [16] ...
- [17] ...
- [18] ...
- [19] ...
- [20] ...
- [21] ...
- [22] ...

---

**Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04)**  
0-7695-2052-9/04 $ 20.00 © 2004 IEEE  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021, at 10:04:18 UTC from IEEE Xplore. Restrictions apply.