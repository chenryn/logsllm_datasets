### Detection with Almost Equally Good Accuracy When Dealing with Encrypted Traffic

In Tables 10 and 11, we present detailed performance metrics for the evaluation per label. Specifically, although the detection of Low Definition (LD) and Standard Definition (SD) videos is slightly less accurate, the overall performance remains satisfactory, as indicated by the Precision and Recall values. However, the confusion matrix reveals an increase in LD videos being misclassified as SD. This can be attributed to the current dataset containing a significantly higher number of 240p videos in the LD category compared to 144p, leading to a shift in the average quality distribution towards the higher end. Consequently, some LD videos are incorrectly classified as SD.

Another factor contributing to the reduced accuracy is the diminished detection capabilities for High Definition (HD) videos. The Precision and Recall for this class have both decreased significantly. The confusion matrix shows that a significant number of HD videos were incorrectly identified as SD. This poor performance is due to the very small number of HD videos available in the dataset, which, combined with the limited number of HD videos used for training, results in a model with reduced detection capabilities for this class.

This issue can be mitigated by using a training set that includes a larger and more diverse set of HD videos. This will enable the creation of a more robust predictive model capable of accurately detecting the average quality of HD videos with different characteristics.

#### Table 10: Accuracies from the Evaluation for the Average Representation Detection

| Class | TP Rate | FP Rate | Precision | Recall |
|-------|---------|---------|-----------|--------|
| LD    | 0.845   | 0.789   | 0.513     | 0.819  |
| SD    | 0.853   | 0.775   | 0.641     | 0.819  |
| HD    | 0.203   | 0.157   | 0.003     | 0.183  |
| Weighted Avg. | 0.845 | 0.789 | 0.513 | 0.819 |

#### Table 11: Confusion Matrix from the Average Representation Evaluation

| Original Label | Predicted Label | LD (%) | SD (%) | HD (%) |
|----------------|-----------------|--------|--------|--------|
| LD             | 84.5            | 15.4   | 0.1    |        |
| SD             | 20.4            | 78.9   | 0.7    |        |
| HD             | 15              | 51.25  | 33.75  |        |

### 5.6 Representation Quality Switch Detection

The final phase of the evaluation focuses on detecting quality switches. Unlike previous phases, there is no pre-trained model that can be directly applied to encrypted data. Instead, the methodology relies on detecting changes in the time intervals between segment downloads and the differences in size between consecutive segments.

For this evaluation, feature construction or selection is not required. We only need to calculate the time series of the product ∆size × ∆t for each video in the dataset, which serves as input for the change detection algorithm. The standard deviation of the change detection output is then computed for each session.

To validate the methodology, we use the same threshold for the standard deviation proposed in Section 4.3:

\[ \text{ST D(CU SU M (∆size × ∆t))} = 500 \]

According to the proposed methodology, sessions below the threshold should represent approximately 78% of the sessions without quality switches, while those above the threshold should represent 76% of the sessions with quality switches (Figure 4).

The dataset is split into two parts: sessions with scores below the threshold and those with scores above it. Using ground truth from the encrypted data, we evaluate whether the predefined threshold allows for accurate detection of variance, as demonstrated in Section 4.3.

Our analysis shows that the first part of the dataset consists of 76.9% of videos without any quality change, while the second part contains 71.7% of sessions with quality switches. These accuracies are lower by 1.1% and 4.3%, respectively, compared to the results from the evaluation with unencrypted data.

The decrease in accuracy for detecting videos with quality switches suggests that the encrypted data includes videos with smaller average quality variances than those observed in the previous section. As a result, the distribution of (3) shifted towards smaller values, leading to a lower percentage of problematic sessions being correctly identified.

### 6. Related Work

**Prometheus [15]** uses passive measurements on mobile networks to estimate the Quality of Experience (QoE) for Video on Demand and VoIP applications. For video QoE, only Buffering Ratio is considered as a QoE indicator, and the system is evaluated on unencrypted traffic using binary classification to detect buffering issues with 84% accuracy.

**OneClick [19]** and **HostView [20]** develop predictive models to detect the QoE of multiple applications, including video streaming, using network performance metrics. However, both approaches require instrumented devices to capture user feedback.

**Hossfeld et al. [11]** study the impact of representation switch amplitude and frequency on user experience. They re-encoded a video in multiple qualities, introduced different levels and frequencies of switching, and conducted crowd-sourced experiments to detect correlations with the received Mean Opinion Score (MOS) from users. However, their work is limited to a single short video, which may not represent the diverse content found in popular services.

**Liu et al. [21]** investigate three factors influencing user-perceived quality: initial delay, stalling, and quality level variation. They conducted lab experiments under different network conditions to derive functions for calculating each impairment factor. However, the lab setting limits the generalization of the results to real network conditions and streaming services.

Overall, while significant work has been done in detecting and quantifying factors affecting video streaming quality, our work is the first to extensively study these factors in a large-scale network using encrypted traffic.

### 7. Limitations

The methodology presented in this paper was developed using information from YouTube video sessions with the service’s current configuration. If YouTube changes its video delivery scheme, the predictive power of the models responsible for detecting QoE impairments may be limited, requiring retraining and reevaluation with an updated dataset.

We did not evaluate the methodology with other video streaming services to verify its generalizability. However, our analysis of popular services like Vevo, Vimeo, and Dailymotion indicates that they use similar technologies for content delivery, such as adaptive streaming, rate limiting, a wide range of codecs and qualities, and HTML5-based playback. This commonality suggests that our methodology can be generalized to other streaming services, motivating further research in this direction.

### 8. Conclusions

In this work, we presented a novel framework for detecting from encrypted traffic the three key factors impacting both adaptive and classical video streaming QoE: stalls, average quality, and quality switching. Evaluations on encrypted and unencrypted traffic from a large mobile network showed that the proposed models can detect different levels of impairments with accuracies as high as 93.5%.

A key finding is that changes in the size and inter-arrival times of video segments are among the most important indicators of quality impairments. Incorporating these features in our detection framework resulted in significant improvements in accuracy.

The framework performs well on a real production network using a few key performance metrics from a single vantage point, without the need for instrumented clients or additional vantage points. This makes it easy for network operators to deploy and apply the trained models to passively monitored traffic, enabling real-time issue reporting.

### 9. Acknowledgments

This work was supported by the Spanish Ministry of Economy and Competitiveness and EU FEDER under grant TEC2014-59583-C2-2-R (SUNSET project) and by the Catalan Government (ref. 2014SGR-1427).

### 10. References

[1] Cisco. “Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update”. White Paper, February 2016.
[2] Sandvine. “Global Internet Phenomena Report”. December 2015.
[3] A. Finamore et al. “Is there a case for mobile phone content pre-staging?”. In 9th ACM conference on Emerging networking experiments and technologies (CoNEXT), pages 321–326. ACM, 2013.
[4] Vasona. “How encryption threatens mobile operators, and what they can do about it”. http://goo.gl/fe3xpB. (Accessed on 05/11/2016).
[5] A. Rao et al. “Network Characteristics of Video Streaming Traffic”. 7th ACM conference on Emerging networking experiments and technologies (CoNEXT), 2011.
[6] R. Mok et al. “Inferring the QoE of HTTP video streaming from user-viewing activities”. 1st ACM SIGCOMM workshop on Measurements up the stack (W-MUST), 2011.
[13] G. Dimopoulos et al. “Analysis of YouTube user experience from passive measurements”. In 9th International Conference on Network and Service Management (CNSM), pages 260–267. IEEE, 2013.
[14] S. Krishnan et al. “Video stream quality impacts viewer behavior: inferring causality using quasi-experimental designs”. Networking, IEEE/ACM Transactions on, 21(6):2001–2014, 2013.
[15] V. Aggarwal et al. “Prometheus: toward quality-of-experience estimation for mobile apps from passive network measurements”. In Proceedings of the 15th Workshop on Mobile Computing Systems and Applications, page 18. ACM, 2014.
[16] X. Yin et al. “A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP”. In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, pages 325–338. ACM, 2015.
[17] ES Page. “Continuous inspection schemes”. Biometrika, 41(1/2):100–115, 1954.
[18] “YouTube: Most Viewed Videos of All Time”. https://www.youtube.com/playlist?list=PLirAqAtlh2r5g8xGajEwdXd3x1sZh8hC.
[19] K. Chen et al. “OneClick: A framework for measuring network quality of experience”. In INFOCOM 2009, IEEE, pages 702–710. IEEE, 2009.
[7] Z. Guangtao et al. “Cross-Dimensional Perceptual Quality Assessment for Low Bit-Rate Videos”. IEEE Transactions on Multimedia, 10(7):1316–1324, 2008.
[20] D. Joumblatt et al. “Predicting user dissatisfaction with internet application performance at end-hosts”. In INFOCOM, pages 235–239. IEEE, 2013.
[8] T. Hoßfeld et al. “Quantification of YouTube QoE via crowdsourcing”. In IEEE International Symposium on Multimedia (ISM), pages 494–499. IEEE, 2011.
[21] Y. Liu et al. User experience modeling for DASH video. In 20th International Packet Video Workshop (PV), pages 1–8. IEEE, 2013.
[22] A. Balachandran et al. “Developing a predictive model of quality of experience for internet video”. In ACM SIGCOMM Computer Communication Review, volume 43, pages 339–350. ACM, 2013.
[23] Z. M. Shafiq et al. “Understanding the impact of network dynamics on mobile video user engagement”. In ACM SIGMETRICS Performance Evaluation Review, volume 42, pages 367–379. ACM, 2014.
[9] R. Mok et al. “Measuring the quality of experience of HTTP video streaming”. In IFIP/IEEE International Symposium on Integrated Network Management (IM), pages 485–492. IEEE, 2011.
[10] B. Lewcio et al. “Video quality in next generation mobile networks – perception of time-varying transmission”. IEEE International Workshop Technical Committee on Communications Quality and Reliability (CQR), pages 1–6, 2011.
[11] T. Hoßfeld et al. “Assessing effect sizes of influence factors towards a QoE model for HTTP adaptive streaming”. In 6th International Workshop on Quality of Multimedia Experience (QoMEX), pages 111–116. IEEE, 2014.
[12] R. Schatz et al. “Passive YouTube QoE monitoring for ISPs”. In Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), 2012 Sixth International Conference on, pages 358–364. IEEE, 2012.