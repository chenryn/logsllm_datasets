### Conducted to Identify Faster Routes for Traffic, Not for Stability Purposes

The backoff and capacity parameters are designed to prevent oscillations between vias. Circuits never attempt to send traffic through a via without first conducting a race, though they may revert to their direct path at any point. 

1. **Via Capacity and Data Race Cells**: Vias without available capacity will drop data race cells, preventing them from being selected, with the small cost of processing a single packet.
2. **Backoff Mechanism**: When a circuit relay is dropped from a via path, it applies a randomized exponential backoff and will not include that via in data races again until a set period of time has passed.

The exact parameters for via capacity and backoff timing are network-dependent and may evolve based on the current state of the Tor network. Since ShorTor is an optional performance enhancement, these values can initially be set conservatively and then decreased adaptively.

### Integration with Tor

While the ShorTor approach has potential applications to other networks, it was specifically designed and evaluated to integrate with Tor. Maintaining Tor’s existing security guarantees is a primary focus of this design, which informed the structure of our data races and the avoidance of loops. However, successful integration also requires that ShorTor be deployable. In this section, we discuss the components of ShorTor most relevant to Tor deployment, including support for load balancing and fairness, required modifications to Tor relays, and incremental deployment.

#### 1. Load Balancing & Fairness

Fairness to circuit traffic and load balancing are essential to ensure that ShorTor does not inadvertently increase latency for some circuits while reducing it for others. This could occur if via traffic were allowed to consume more resources than a relay had available, resulting in increased processing times or congestion. ShorTor provides both fairness and load balancing through the same mechanism: prioritizing circuit traffic over via traffic. Tor already recognizes different traffic priorities—web browsing is prioritized over large file downloads [26]. We extend this to ensure that relays will preferentially schedule traffic from circuit queues over via queues (Figure 5).

Circuits will select vias that have lower latency than the default path, including the transit time through the via itself. This is crucial as relay congestion and associated queuing delays are a primary source of latency in Tor [43, 44, 46].

**Figure 5: Tor Relay with Circuit and Via Traffic**
- **1. Multiplexing**: Via and circuit traffic are multiplexed on a TCP connection entering the Tor relay.
- **2. TLS Decryption**: The TLS layer is decrypted, and circuit cells are onion-encrypted/decrypted.
- **3. Queuing**: Circuit and via cells are sent to their individual queues.
- **4. Scheduling**: Cells are scheduled for release to the output buffer based on priority order.
- **5. Encryption and Transit**: The contents of the output buffer are encrypted using TLS and then sent to the kernel for transit over a TCP connection to the next hop.

Congestion at a via will appear naturally during the data race in the form of increased latency or could be indicated explicitly by dropping race packets. Vias are transitory and can be dropped or swapped at will with minimal cost compared to circuit construction/teardown.

ShorTor ensures:
- **(1) No Delay by Via Traffic**: Circuit traffic on a relay is never delayed by via traffic.
- **(2) Distributed Load**: Load from via traffic is distributed only across relays with available capacity.

#### 2. Tor Modifications

ShorTor’s primary modification to Tor is the introduction of data races; all other components are simple extensions of Tor’s existing mechanisms for routing and prioritizing circuits. To support ShorTor, Tor relays (though not clients) require additional protocol messages, a new data path for via traffic, and state for managing via traffic.

- **New Cell Headers**: Protocol requires new cell headers for data races, ping, and via traffic.
- **Priority Levels**: Via traffic needs a new priority level lower than circuit traffic (optionally, this level can be higher than that of bulk download traffic, such as torrenting, which is currently of lower priority than circuit traffic) [26].
- **Data Path**: Incoming via traffic needs a new data path that bypasses onion encryption and decryption.
- **State Management**: Relays must hold two additional pieces of state: a new field in the routing table to indicate the via (if any) for each circuit and the list of candidate via nodes for each possible next hop (see Section III-B2 for details).

These modifications are relatively minor, do not touch Tor’s onion encryption layer, and represent an optional overlay on Tor’s routing. The high upfront cost of integrating and deploying modifications to the Tor protocol was a significant factor in the ultimate design of ShorTor. This consideration motivated ShorTor’s construction as an extension to Tor’s existing architecture that operates largely separately from the baseline protocol.

Furthermore, ShorTor’s modifications are configurable, trivially backwards compatible, and support incremental deployment. This allows relay operators to choose whether to support ShorTor and how much capacity to dedicate to the protocol. As shown in Section IV-C3, ShorTor can substantially reduce tail latencies even with relatively low support. This minimizes the risk of upfront development efforts being wasted due to slow deployment.

#### 3. Incremental Deployment

Tor relays are volunteer-run and notoriously slow to update [58]. Any proposal requiring support from all—or even a majority of—Tor relays is unlikely to be effective. ShorTor is incrementally deployable and improves the latency of any Tor circuits that meet the following requirements:
- **(1) Two adjacent circuit relays support ShorTor.**
- **(2) Some other relay supporting ShorTor provides a faster path between the two circuit relays.**

Because Tor does not select its relays with uniform probability, a small set of popular relays could meet these conditions for many circuits without support from the rest of the network. We demonstrate this concretely in Section IV-C3.

Security is another important consideration—incremental deployment inherently creates differences between Tor clients or relays that have adopted a modification and those who have not. This has been an issue for client-side proposals as anonymity in Tor relies on all clients behaving uniformly [12, 14, 36, 60, 61, 74, 85].

ShorTor avoids this issue entirely as it is a fully server-side protocol that does not require participation from, or modify the behavior of, Tor clients in any way. While ShorTor is an observable modification to the Tor protocol, it is in no way correlated to client identity. Support for ShorTor cannot be used to distinguish between clients. In fact, Tor clients should not try to preferentially select relays with support for ShorTor. While this would improve their latency, it would also differentiate them from Tor clients following Tor’s baseline circuit selection algorithm, reducing their anonymity.

### Evaluation

We evaluate ShorTor using a dataset of approximately 400,000 latency measurements collected from the live Tor network over 42 days during summer 2021. Our measurements allow us to compare the direct latency between relays to the latency when routing through an intermediate hop, as in ShorTor.

Using measured latencies allows us to avoid relying on simulated or approximate data. While simulations can be useful, prior work [75] has shown that routing protocols are best evaluated using live internet paths rather than through a simulation with reduced scale and complexity.

We evaluate the performance of ShorTor in terms of its direct impact on the latency between pairs of Tor relays and its ability to reduce the latency of Tor circuits. Evaluating on circuits as well as pairs allows us to account for the relative popularity of relays and more closely model the expected reduction in latency ShorTor can provide to Tor’s end users.

#### A. Measurement Methodology

##### 1. Ting

For our measurements, we adapt the Ting method of Cangialosi et al. [23] for estimating latencies between Tor relays. Ting creates a set of three circuits involving observers, which are Tor relays run solely for the purpose of obtaining latency measurements. Specifically, to obtain the latency between two Tor relays, RelayA and RelayB, we run two observer relays, Obs1 and Obs2, along with a measurement client on the same physical machine. Once each circuit is established, the measurement client “pings” itself through the circuit to estimate round-trip latencies for the following circuits:
- **rttAB = RTT (Obs1 → RelayA → RelayB → Obs2)**
- **rttA = RTT (Obs1 → RelayA → Obs2)**
- **rttB = RTT (Obs1 → RelayB → Obs2)**

With these, we estimate the round-trip time between RelayA and RelayB as rttAB − 1/2 (rttA + rttB). This approximates the RTT including the forwarding delay at both RelayA and RelayB. We repeat this process to find the minimum observed latency between RelayA and RelayB: after 10 iterations, 95.5% of circuits are within 5% or 5 ms of the minimum observed in 100 samples.

##### 2. Directional Latencies

The Ting protocol does not account for directional latencies where the outgoing latency between two nodes may not be equivalent to that of the return trip. To detect asymmetry in our RTT measurements, we include a timestamp halfway through the round-trip “ping” (all timestamps are with respect to the same clock). In our dataset, the median asymmetry was 2.4%, and only 0.2% of measurements had an asymmetry of 2× or greater. Importantly, asymmetric RTTs impact only our evaluation as, when deployed, ShorTor naturally accounts for directional latencies using data races (Section III-B4).

##### 3. Infrastructure

To collect latency measurements at scale, we adapted the Ting protocol to support parallel measurements across multiple machines. Our larger scale also required changes to respect a safe maximum load on the Tor network: we impose a global maximum limit on concurrent measurements and spread measurements of individual relays across time. Our infrastructure also compensates for the high churn in the Tor network (13% of relays we observed were online less than half the time) by enqueuing measurement jobs based on the currently online relays, with automated retries. We handled hardware and power failures using a fault-tolerant system design: we separated data persistence, measurement planning, and the measurements themselves.

We deployed to a private OpenStack [78] cloud but provide a Terraform [41] template supporting any provider. Our open-source [1] measurement infrastructure is approximately 3,300 lines of code, consisting primarily of Python and shell scripts.

##### 4. Geographic Location of Relays

We obtain country codes for the relays in our dataset using the GeoIP database [59], which is also used by Tor in practice. However, GeoIP locations are not guaranteed to be 100% accurate [37, 51, 66]. Upon careful inspection, we observed a handful of relay pairs with physically impossible RTTs for their purported locations. All of these pairs involved the same twelve relays allegedly located in the US. We determined that these twelve relays have higher average RTTs inside the Americas than they do to relays located in other regions. Because of this, all location-related figures (Figure 6(b), Figure 9, Figure 10) exclude these relays as, while we are confident that their reported location is incorrect, we cannot accurately determine their true location.

##### 5. Ethics and Safety

We designed our measurement process to minimize impact on Tor users and relay operators and to comply with security best practices for Tor. To this end, we submitted a proposal to the Tor Research Safety Board [69] for review prior to measurement and adhered to their recommendations. We also received an IRB exemption from each author’s institution for this work.

Collecting our data required us to run several live Tor relays. These relays recorded only our measurement traffic—at no point did we observe or record any information about any traffic from Tor users. We also minimized the likelihood of a user choosing our relays for their circuits by advertising the minimum allowed bandwidth of 80 KiB/s [26].

Our measurement collection was spread over 42 days to reduce concurrent load, including a limit on simultaneous measurements (detailed in Section IV-A3). We also notified a Tor relay operator mailing list and allowed operators to opt out of our measurements; we excluded four such relays.

In light of recent work by Schnitzler et al. [76] on the security implications of fine-grained latency measurements for Tor circuits, we have not published our full latency dataset. However, we will share this data with researchers upon request and are in communication with our reviewers from the Tor Research Safety Board about safely releasing it in the future.

##### 6. Latency Dataset

In this work, we directly measure pairwise latencies within Tor rather than relying on outside estimates. We focus our measurements on the 1,000 most popular Tor relays (by consensus weight) for two main reasons:
- **Measuring all 36,325,026 possible pairs of the 8,524 Tor relays we observed was intractable for this work.**
- **These popular relays are present on over 75% of circuits [39] and thus can provide disproportionate utility.**

Our dataset contains 406,074 measured latencies or 81.3% of all pairs of the 1,000 most popular relays.

### B. Applying ShorTor to Relay Pairs

We begin evaluating ShorTor by comparing the potential latency between our set of relay pairs when routing via ShorTor to our measured latencies observed using Tor’s default routing. Figure 6(a) shows the relative frequency of RTTs experienced by pairs of Tor relays using ShorTor and when routing normally, while Figure 6(b) focuses on the relationship between default RTT and ShorTor RTT for each relay pair. Using ShorTor, all of Tor’s high tail latencies were resolved: ShorTor sees a maximum absolute RTT of 157 ms, while 0.09% of pairs in the default routing had RTTs exceeding 1,000 ms.