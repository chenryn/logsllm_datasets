### Abstract

An adversary can accurately infer the presence or absence of links between any pair of nodes in a graph used to train a Graph Neural Network (GNN) model. We propose a threat model to systematically characterize an adversary's background knowledge across three dimensions. By jointly considering these dimensions, we define eight distinct link stealing attacks and propose novel methods to execute them. Extensive evaluation over eight real-world datasets demonstrates that our attacks can accurately steal links. Future work includes generalizing these attacks to GNNs for graph classification and developing defenses against them.

### Acknowledgments

We are grateful to the anonymous reviewers and our shepherd, Minhui Xue, for their constructive feedback. This research is partially supported by the Helmholtz Association through the project "Trustworthy Federated Data Analytics" (TFDA) (funding number ZT-I-OO1 4) and by the National Science Foundation under grant No. 1937787.

### References

1. **James Atwood and Don Towsley.** *Diffusion-Convolutional Neural Networks.* In NIPS, pages 1993–2001, 2016.
2. **Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz.** *GAN-Leaks: A Taxonomy of Membership Inference Attacks against GANs.* In CCS, 2020.
3. **Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang.** *When Machine Unlearning Jeopardizes Privacy.* CoRR abs/2005.02205, 2020.
4. **Qingrong Chen, Chong Xiang, Minhui Xue, Bo Li, Nikita Borisov, Dali Kaarfar, and Haojin Zhu.** *Differentially Private Data Generative Models.* CoRR abs/1812.02274, 2018.
5. **Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto Perdisci, Manos Antonakakis, and Nikolaos Vasiloglou.** *Practical Attacks Against Graph-based Clustering.* In CCS, pages 1125–1142, 2017.
6. **Yizheng Chen, Shiqi Wang, Dongdong She, and Suman Jana.** *On Training Robust PDF Malware Classifiers.* In USENIX Security, 2020.
7. **Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song.** *Adversarial Attack on Graph Structured Data.* In ICML, pages 1123–1132, 2018.
8. **Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst.** *Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.* In NIPS, pages 3837–3845, 2016.
9. **Michael Backes, Mathias Humbert, Jun Pang, and Yang Zhang.** *walk2friends: Inferring Social Links from Mobility Profiles.* In CCS, pages 1943–1957, 2017.
10. **Paul D. Dobson and Andrew J. Doig.** *Distinguishing Enzyme Structures from Non-Enzymes without Alignments.* Journal of Molecular Biology, 2003.
11. **Aleksandar Bojchevski and Stephan Günnemann.** *Adversarial Attacks on Node Embeddings via Graph Poisoning.* In ICML, pages 695–704, 2019.
12. **Yuxiao Dong, Reid A. Johnson, and Nitesh V. Chawla.** *Will This Paper Increase Your h-index?: Scientific Impact Prediction.* In WSDM, pages 149–158, 2015.
13. **Aleksandar Bojchevski and Stephan Günnemann.** *Certifiable Robustness to Graph Perturbations.* In NeurIPS, pages 8317–8328, 2019.
14. **Karsten M. Borgwardt, Cheng Soon Ong, Stefan Schönauer, S. V. N. Vishwanathan, Alexander J. Smola, and Hans-Peter Kriegel.** *Protein Function Prediction via Graph Kernels.* Bioinformatics, 2005.
15. **Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.** *Benchmarking Graph Neural Networks.* CoRR abs/2003.00982, 2020.
16. **Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli.** *A Fair Comparison of Graph Neural Networks for Graph Classification.* In ICLR, 2020.
17. **Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song.** *The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks.* In USENIX Security, pages 267–284, 2019.
18. **Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin.** *Graph Neural Networks for Social Recommendation.* In WWW, pages 417–426, 2019.
19. **Varun Chandrasekaran, Kamalika Chaudhuri, Irene Giacomelli, Somesh Jha, and Songbai Yan.** *Exploring Connections Between Active Learning and Model Extraction.* In USENIX Security, 2020.
20. **Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.** *Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures.* In CCS, pages 1322–1333, 2015.
21. **Matt Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart.** *Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.* In USENIX Security, pages 17–32, 2014.
22. **Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.** *Neural Message Passing for Quantum Chemistry.* In ICML, pages 1263–1272, 2017.
23. **Neil Zhenqiang Gong and Bin Liu.** *You are Who You Know and How You Behave: Attribute Inference Attacks via Users’ Social Friends and Behaviors.* In USENIX Security, pages 979–995, 2016.
24. **Neil Zhenqiang Gong, Ameet Talwalkar, Lester W. Mackey, Ling Huang, Eui Chul Richard Shin, Emil Stefanov, Elaine Shi, and Dawn Song.** *Joint Link Prediction and Attribute Inference Using a Social-Attribute Network.* ACM Transactions on Intelligent Systems and Technology, 2014.
25. **Aditya Grover and Jure Leskovec.** *node2vec: Scalable Feature Learning for Networks.* In KDD, pages 855–864, 2016.
26. **Inken Hagestedt, Yang Zhang, Mathias Humbert, Pascal Berrang, Haixu Tang, XiaoFeng Wang, and Michael Backes.** *MBeacon: Privacy-Preserving Beacons for DNA Methylation Data.* In NDSS, 2019.
27. **William L. Hamilton, Zhitao Ying, and Jure Leskovec.** *Inductive Representation Learning on Large Graphs.* In NIPS, pages 1025–1035, 2017.
28. **Michael Hay, Chao Li, Gerome Miklau, and David D. Jensen.** *Accurate Estimation of the Degree Distribution of Private Networks.* In ICDM, pages 169–178, 2009.
29. **Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro.** *LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks.* Symposium on Privacy Enhancing Technologies Symposium, 2019.
30. **Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot.** *High Accuracy and High Fidelity Extraction of Neural Networks.* In USENIX Security, 2020.
31. **Jinyuan Jia and Neil Zhenqiang Gong.** *AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning.* In USENIX Security, pages 513–529, 2018.
32. **Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong.** *MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples.* In CCS, pages 259–274, 2019.
33. **Jinyuan Jia, Binghui Wang, Xiaoyu Cao, and Neil Zhenqiang Gong.** *Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing.* In WWW, pages 2718–2724, 2020.
34. **Mika Juuti, Sebastian Szyller, Samuel Marchal, and N. Asokan.** *PRADA: Protecting Against DNN Model Stealing Attacks.* In Euro S&P, pages 512–527, 2019.
35. **Thomas N. Kipf and Max Welling.** *Semi-Supervised Classification with Graph Convolutional Networks.* In ICLR, 2017.
36. **Klas Leino and Matt Fredrikson.** *Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference.* In USENIX Security, 2020.
37. **Shaofeng Li, Shiqing Ma, Minhui Xue, and Benjamin Zi Hao Zhao.** *Deep Learning Backdoors.* CoRR abs/2007.08273, 2020.
38. **Zheng Li, Chengyu Hu, Yang Zhang, and Shanqing Guo.** *How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN.* In ACSAC, pages 126–137, 2019.
39. **Zheng Li and Yang Zhang.** *Label-Leaks: Membership Inference Attack with Label.* CoRR abs/2007.15528, 2020.
40. **David Liben-Nowell and Jon Kleinberg.** *The Link-prediction Problem for Social Networks.* Journal of the American Society for Information Science and Technology, 2007.
41. **Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.** *Exploiting Unintended Feature Leakage in Collaborative Learning.* In S&P, pages 497–512, 2019.
42. **Milad Nasr, Reza Shokri, and Amir Houmansadr.** *Machine Learning with Membership Privacy using Adversarial Regularization.* In CCS, pages 634–646, 2018.
43. **Milad Nasr, Reza Shokri, and Amir Houmansadr.** *Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning.* In S&P, pages 1021–1035, 2019.
44. **Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.** *Knockoff Nets: Stealing Functionality of Black-Box Models.* In CVPR, pages 4954–4963, 2019.
45. **Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.** *Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks.* In ICLR, 2020.
46. **Congzheng Song and Vitaly Shmatikov.** *Auditing Data Provenance in Text-Generation Models.* In KDD, pages 196–206, 2019.
47. **Congzheng Song and Reza Shokri.** *Robust Membership Encoding: Inference Attacks and Copyright Protection for Deep Learning.* In ASIACCS, 2020.
48. **Jun Pang and Yang Zhang.** *DeepCity: A Feature Learning Framework for Mining Location Check-Ins.* In ICWSM, pages 652–655, 2017.
49. **Jun Pang and Yang Zhang.** *Quantifying Location Sociality.* In HT, pages 145–154, 2017.
50. **Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman.** *SoK: Towards the Science of Security and Privacy in Machine Learning.* In Euro S&P, pages 399–414, 2018.
51. **Nicolas Papernot, Patrick D. McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami.** *Practical Black-Box Attacks Against Machine Learning.* In ASIACCS, pages 506–519, 2017.
52. **Erwin Quiring, Alwin Maier, and Konrad Rieck.** *Misleading Authorship Attribution of Source Code using Adversarial Learning.* In USENIX Security, pages 479–496, 2019.
53. **Kaspar Riesen and Horst Bunke.** *Structural, Syntactic, and Statistical Pattern Recognition.* Springer, 2008.
54. **Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario Fritz, and Yang Zhang.** *Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning.* In USENIX Security, pages 1291–1308, 2020.
55. **Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang.** *Dynamic Backdoor Attacks Against Machine Learning Models.* CoRR abs/2003.03675, 2020.
56. **Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes.** *ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models.* In NDSS, 2019.
57. **Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein.** *Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks.* In NeurIPS, pages 6103–6113, 2018.
58. **Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.** *Membership Inference Attacks Against Machine Learning Models.* In S&P, pages 3–18, 2017.
59. **Jeffrey Sutherland, Lee O’Brien, and Donald Weaver.** *SplineFitting with a Genetic Algorithm: A Method for Developing Classification Structure Activity Relationships.* Journal of Chemical Information and Computer Sciences, 2003.
60. **Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart.** *Stealing Machine Learning Models via Prediction APIs.* In USENIX Security, pages 601–618, 2016.
61. **Laurens van der Maaten and Geoffrey Hinton.** *Visualizing Data using t-SNE.* Journal of Machine Learning Research, 2008.
62. **Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.** *Graph Attention Networks.* In ICLR, 2018.
63. **Binghui Wang and Neil Zhenqiang Gong.** *Stealing Hyperparameters in Machine Learning.* In S&P, pages 36–52, 2018.
64. **Binghui Wang and Neil Zhenqiang Gong.** *Attacking Graph-based Classification via Manipulating the Graph Structure.* In CCS, pages 2023–2040, 2019.
65. **Binghui Wang, Jinyuan Jia, and Neil Zhenqiang Gong.** *Graph-based Security and Privacy Analytics via Collective Classification with Joint Weight Learning and Propagation.* In NDSS, 2019.
66. **Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu.** *Adversarial Examples for Graph Data: Deep Insights into Attack and Defense.* In IJCAI, pages 4816–4823, 2019.
67. **Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha.** *Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting.* In CSF, pages 268–282, 2018.
68. **Jun Zhang, Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, and Xiaokui Xiao.** *Private Release of Graph Statistics using Ladder Functions.* In SIGMOD, pages 731–745, 2015.
69. **Yang Zhang.** *Language in Our Time: An Empirical Analysis of Hashtags.* In WWW, pages 2378–2389, 2019.
70. **Yang Zhang, Mathias Humbert, Bartlomiej Surma, Praveen Manoharan, Jilles Vreeken, and Michael Backes.** *Towards Plausible Graph Anonymization.* In NDSS, 2020.
71. **Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong.** *Backdoor Attacks to Graph Neural Networks.* CoRR abs/2006.11165, 2020.
72. **Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu.** *Robust Graph Convolutional Networks Against Adversarial Attacks.* In KDD, pages 1399–1407, 2019.
73. **Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann.** *Adversarial Attacks on Neural Networks for Graph Data.* In KDD, pages 2847–2856, 2018.

---

This version of the text is more structured and professional, with clear headings and a consistent format for references.