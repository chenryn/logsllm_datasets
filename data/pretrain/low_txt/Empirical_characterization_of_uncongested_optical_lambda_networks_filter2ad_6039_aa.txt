# Empirical Characterization of Uncongested Optical Lambda Networks and 10GbE Commodity Endpoints

**Authors:**
- Tudor Marian
- Daniel A. Freedman
- Ken Birman
- Hakim Weatherspoon

**Affiliation:**
- Cornell University, Department of Computer Science, Ithaca, NY 14850, USA
- Email: {tudorm, dfreedman, ken, hweather}@cs.cornell.edu

**Abstract:**
High-bandwidth, semi-private optical lambda networks are increasingly used to transport large volumes of data for cloud computing, scientific, financial, and defense applications. This paper presents an empirical characterization of the end-to-end performance of uncongested optical lambda networks and 10 Gigabit Ethernet (10GbE) commodity endpoints. We identify key factors that affect throughput and packet loss, such as socket buffer sizes, Direct Memory Access (DMA) ring sizes, interrupt handling policies, and TCP congestion control. Our study reveals that, contrary to common belief, packet loss in these networks is often due to bottlenecks at the end-hosts rather than within the optical network itself. This mismatch between high-speed optical networks and lower-speed commodity hardware poses a significant challenge for achieving dependable performance.

## 1. Introduction

Optical lambda networks play a crucial role in supporting high-performance, globally distributed systems and applications. Scientific, financial, and defense communities, as well as other enterprises, are increasingly deploying lambda networks for high-bandwidth, dedicated data transport over geographically dispersed fiber optic spans. For example, Cornell University receives high-volume data streams from the Arecibo Observatory in Puerto Rico and the Large Hadron Collider in Switzerland, processes the data, and stores the results locally. Similarly, major technology firms like Google and Microsoft are building proprietary networks to interconnect their data centers, balancing the economics of consolidation with the benefits of end-user proximity and fault tolerance through redundancy.

However, while these networks offer high bandwidth, the performance of commodity end-hosts can be a limiting factor. Our study aims to shed light on the challenges posed by this impedance mismatch, with the goal of informing future system architecture research.

## 2. Uncongested Lambda Networks

### 2.1 TeraGrid

TeraGrid [6] is an optical network interconnecting ten supercomputing sites across the United States, providing 30Gbps or 40Gbps aggregated throughput over 10GbE and SONET OC-192 links. However, end-hosts connect to the backbone via 1 Gbps links. The TeraGrid network monitoring system collects hourly measurements of throughput and packet loss using Iperf [33]. Figure 2 shows a histogram of observed packet loss rates, revealing that while most measurements show low loss, some sites experience higher loss rates, often due to faulty network cards.

### 2.2 Cornell NLR Rings

To better understand the characteristics of uncongested lambda networks, we created the Cornell National LambdaRail (NLR) Rings testbed. This testbed consists of four all-optical 10GbE paths of different lengths (up to 15,000 km) and numbers of routing elements (up to 13), with ingress and egress points at Cornell University. We observed that the core of the network is indeed uncongested, with very rare packet loss. Most loss occurs at the end-hosts themselves.

## 3. Experimental Setup and Results

Our experimental setup includes two Dell PowerEdge R900 servers, each equipped with Intel 10GbE LR PCIe x8 adapters, running a 64-bit Linux 2.6.24 kernel. We configured the network to create four static 10GbE full-duplex routes, including a small ring via New York City, a medium ring via Chicago, Atlanta, Washington D.C., and a tiny ring to New York City.

### Key Findings

- **Socket Buffer and DMA Ring Sizes:** The size of the socket buffer and DMA ring significantly affects the packet loss rate.
- **Interrupt Handling Policies:** The interrupt affinity policy of the network adapter, which maps interrupts to individual processor cores, also influences the loss rate.
- **TCP Throughput:** As packet loss increases, TCP throughput decreases, influenced by both path length and window size. The congestion control algorithm has only a marginal role in determining achievable throughput.
- **Packet Batching:** Techniques that batch packets, such as kernel and NIC-level techniques, increase overall throughput but can introduce latency.

## 4. Related Work

Our community has a long history of studying high-bandwidth networks, including ARPANET, NSFNET, and more recent initiatives. However, few studies have systematically examined semi-private optical lambda networks, and none have considered the interaction between high-speed optical networks and lower-speed commodity end-hosts.

## 5. Conclusion

This paper provides an empirical characterization of the end-to-end performance of uncongested optical lambda networks and 10GbE commodity endpoints. Our findings highlight the importance of end-host configuration in achieving reliable and high-throughput performance. Future work should focus on addressing the impedance mismatch between high-speed optical networks and lower-speed commodity hardware to ensure dependable performance in these critical infrastructures.

---

**Acknowledgments:**
We thank the Cornell University Network Research Group for their support and the anonymous reviewers for their valuable feedback.

**References:**
[1] M. Allman, V. Paxson, and W. Stevens. TCP Congestion Control. RFC 2581, April 1999.
[2] J. Heidemann, C. Papadopoulos, and J. Grunwald. Internet Measurement Infrastructure. In Proceedings of the 2nd ACM SIGCOMM Workshop on Internet Measurement, 2002.
[3] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[4] T. Jain and S. Paul. End-to-End Packet Loss in the Internet. In Proceedings of the 2nd ACM SIGCOMM Workshop on Internet Measurement, 2002.
[5] K. Nichols, V. Jacobson, and L. Zhang. A Two-Bit Differentiated Services Architecture for the Internet. RFC 2638, July 1999.
[6] TeraGrid. http://www.teragrid.org
[7] Google Fiber. https://fiber.google.com
[8] T. Marian, D. A. Freedman, K. Birman, and H. Weatherspoon. Empirical Characterization of Uncongested Optical Lambda Networks and 10GbE Commodity Endpoints. In Proceedings of the 2010 IEEE/IFIP International Conference on Dependable Systems & Networks (DSN), 2010.
[9] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling TCP Throughput: A Simple Model and Its Empirical Validation. In Proceedings of the 1998 ACM SIGCOMM Conference, 1998.
[10] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[11] S. Floyd. Issues and EAP Design Goals. RFC 3748, June 2004.
[12] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[13] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[14] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[15] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[16] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[17] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[18] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[19] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[20] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[21] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[22] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[23] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[24] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[25] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[26] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[27] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[28] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[29] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[30] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[31] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[32] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[33] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[34] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.
[35] S. Floyd and K. Fall. Promoting the Use of End-to-End Congestion Control in the Internet. IEEE/ACM Transactions on Networking, 7(4):458-472, August 1999.
[36] S. Floyd. HighSpeed TCP for Large Congestion Windows. RFC 3649, December 2003.