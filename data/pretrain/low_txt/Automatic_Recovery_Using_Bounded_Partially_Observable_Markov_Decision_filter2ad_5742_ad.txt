### Faults and Recovery in Distributed Systems

In the context of distributed systems, a "zombie" fault occurs when a component responds to pings from monitors but fails to perform its intended functions correctly. Path monitors can detect such faults but cannot pinpoint the exact faulty component. However, the diversity in the paths taken by the two monitors allows for a probabilistic determination of the faulty component.

The recovery controller has several options: restarting a component, rebooting a host, or passively observing the system through monitors. Recovery actions are deterministic, meaning the correct action will always fix the fault. Each action has a specific duration: 5 minutes for a host reboot, 4 minutes for a database restart, 2 minutes for a voice gateway restart, 1 minute each for an HTTP or EMN server restart, and 5 seconds for monitor execution. During recovery, costs accrue based on the fraction of requests dropped due to failure or unavailability caused by recovery actions. It is assumed that 80% of the requests are HTTP and 20% are voice requests. The system lacks a recovery notification mechanism, as an "all clear" from monitors might only indicate that an EMN server has become a zombie, with path monitor requests being rerouted. Termination costs are specified using a mean human response time of 6 hours. The model, while small, is sufficient to represent a realistic system.

### Experimental Setup

Experiments were conducted on 2GHz Athlon machines with 512MB of memory. Only zombie faults, which are difficult to diagnose, were injected into the simulations. Two bootstrapping procedures were evaluated: "Random," where faults were randomly selected with uniform distribution and observations were chosen according to monitor coverage probabilities, and "Average," where all faults were equally likely.

### Convergence Behavior

Figure 5(a) illustrates the improvement in lower bounds as a function of the number of iterations during the bootstrapping phase, with a tree depth set to one. The y-axis shows the negative values of the POMDP lower-bound function (or upper bounds of the cost function) evaluated at the belief-state {1/|S|}, indicating that all faults are equally likely. The graph confirms that iterative updates improve the lower bounds, with rapid tightening in the initial iterations followed by a slower rate. For general POMDPs, it is undecidable whether the lower bound is within a certain distance from the optimal solution. However, upper bounds can be used to estimate this distance without guaranteeing a reduction.

Figure 5(b) shows the increase in the number of bounds vectors (hyperplanes) in the lower bound with the number of iterative update steps. Since at most one new bounds vector is added per update, the growth is linear. For the example model, bounds refinement was fast, and limiting the number of bounds was unnecessary. There are no guarantees that the number of bound vectors will stabilize, so finite storage for the lower bound is practical. The average bootstrapping procedure achieves faster improvement and a tighter bound, with a slower increase in the number of bound vectors compared to the random method.

### Decision Quality Evaluation

The second set of experiments evaluates the quality of decisions made by the controller by injecting 10,000 faults into the system and measuring per-fault metrics. The bounded controller with a recursion depth of 1 and bootstrapped with 10 runs of depth 2 is compared to three other types of controllers: "most likely," which uses Bayes rule to choose the cheapest recovery action for the most likely fault; "heuristic," which uses a heuristic approximation of the value function; and "Oracle," which knows the fault and can recover with a single action.

Table 1 shows the per-fault metrics. The bounded controller outperforms both the "most likely" and heuristic depth one controllers by significant cost margins. Even though heuristic controllers with depths two and three perform well, the bounded controller, with a depth of one, still outperforms them. Additionally, the bounded controller generates decisions faster than the nearest comparable heuristic controller, which requires a lookahead of 2.

The bounded controller also excels in terminating recovery. Other controllers require a termination probability, set to 0.9999 in these experiments, leading to disproportionately high recovery times. The bounded controller, using operator response time, determines recovery completion more efficiently, as shown in Table 1.

### Conclusion

This paper addresses the problem of system recovery with imperfect monitoring information by formulating it as an undiscounted mean accumulated reward optimization problem in the POMDP framework. We demonstrated how to use specific properties of system recovery to formulate lower bounds on the solution of recovery POMDPs. Experimental results on a sample e-commerce system show that these bounds can be iteratively improved, and the resulting controller outperforms heuristic-based controllers. Future work includes providing guarantees against early termination, formally investigating iterative improvement, and generating upper bounds to facilitate branch and bound techniques.

### Acknowledgments

This material is based on work supported in part by the National Science Foundation under Grant No. CNS-0406351. Kaustubh R. Joshi is supported by an AT&T Virtual University Research Initiative (VURI) fellowship. The authors thank Jenny Applequist for improving the readability of the material.

### References

[1] M. Abdeen and M. Woodside. Seeking optimal policies for adaptive distributed computer systems with multiple controls. In Proc. of Third Intl. Conf. on Parallel and Dist. Comp., Appl. and Technologies, Kanazawa, Japan, 2002.

[2] A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. Acting optimally in partially observable stochastic domains. In Proc. of the 12th Natl. Conf. on Artiﬁcial Intelligence (AAAI-94), volume 2, pages 1023–1028, Seattle, 1994.

[3] H. de Meer and K. S. Trivedi. Guarded repair of dependable systems. Theoretical Computer Sci., 128:179–210, 1994.

[4] M. Fischer, N. Lynch, and M. Paterson. Impossibility of distributed consensus with one faulty process. J. ACM, 32(2):374–382, Apr. 1985.

[5] L. J. Franken and B. R. Haverkort. Reconﬁguring distributed systems using Markov-decision models. In Proceedings of the Workshop on Trends in Distributed Systems (TreDS’96), pages 219–228, Aachen, Oct. 1996.

[6] M. Hauskrecht. Incremental methods for computing bounds in partially observable Markov decision processes. In Proc. of AAAI, pages 734–739, Providence, RI, 1997.

[7] M. Hauskrecht. Value-function approximations for partially observable Markov decision processes. Journal of Artiﬁcial Intell. Research, 13:33–94, 2000.

[8] K. R. Joshi, M. Hiltunen, W. H. Sanders, and R. Schlichting. Automatic model-driven recovery in distributed systems. In Proc. of Symp. on Reliable Dist. Systems (SRDS 05), pages 25–36, Oct 2005.

[9] O. Madani, S. Hanks, and A. Condon. On the undecidability of probabilistic planning and related stochastic optimization problems. Artif. Intell., 147(1-2):5–34, 2003.

[10] G. E. Monahan. A survey of partially observable Markov decision processes: Theory, models, and algorithms. Management Science, 28(1):1–16, 1982.

[11] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-Intersci., 1994.

[12] K. G. Shin, C. M. Krishna, and Y.-H. Lee. Optimal dynamic control of resources in a distributed system. IEEE Trans. on Software Eng., 15(10):1188–1198, Oct. 1989.

[13] E. J. Sondik. The Optimal Control of Partially Observable Markov Processes. PhD thesis, Stanford University, 1971.

[14] R. Washington. BI-POMDP: Bounded, incremental, partially-observable Markov-model planning. In S. Steel and R. Alami, editors, Proc. of European Conf. on Planning, volume 1348 of Lecture Notes in Computer Sci., pages 440–451. Springer, 1997.