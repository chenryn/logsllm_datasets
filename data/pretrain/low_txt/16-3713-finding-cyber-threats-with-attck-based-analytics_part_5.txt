### Web Shell Remote File Copy and Rundll32 Screen Capture

- **Blocking Peripheral Multiband Device Discovery Communication**
- **Basic Exploitation of Vulnerability in Remote Services via Scheduled Task**
- **Exfiltration Over Input/Output Replication Using Physical Medium and Multilayer System Bypass**
- **User Account Control Permission Scripting Through Encryption Groups**
- **Removable Service Scheduled Bootkit with DLL Injection for Peer Connections Media Execution Transfer Process**
- **Indicator Shared Webroot Windows Remote File Copy**
- **Change Default File Association Taint Shared Tools Query Registry Instrumentation Content Standard Application Layer Component**
- **Remote System Windows Admin Indicator Protocol Firmware Discovery Shares Removal on Host Hypervisor Security Standard Software Cryptographic Logon Scripts InstallUtil Discovery Protocol Modify Existing Masquerading System Standard Non-Service Information Application Layer Redundant Discovery Protocol Modify Registry Access Registry Run NTFS Extended System Uncommonly Used Port Folder Discovery Web Service Obfuscated Files Security Support or Information System Service Provider Discovery Shortcut Process Modification Hollowing Redundant Windows Access Management Regsvcs / Regasm Event Subscription Regsvr32 Winlogon Helper Rootkit DLL Rundll32 Scripting Software Packing Timestomp**

**Figure 6. Scenario 1 Plan ATT&CK Matrix**

---

### Example Scenario 2

A second cyber exercise (Scenario 2) can be developed based on the outcomes of the first. The objective of the second scenario is to validate any new data sources and analytics that were effective against the previously emulated behavior in the first exercise. Additionally, Scenario 2 will introduce new adversarial tactics by the Red Team.

Since the conclusion of Scenario 1, the Blue Team has implemented persistent process monitoring tools and capabilities, such as Sysmon, to capture process invocations and command-line arguments. This makes the ATT&CK behaviors from the first exercise available for defensive analysis.

In Scenario 2, it is assumed that the Blue Team now has a high detection rate of the Red Team’s adversary emulation activities from Scenario 1. Therefore, one of the goals for Scenario 2 is to test sensor and analytic capabilities against prior ATT&CK behaviors and technique variations that may not be effectively detected by process monitoring. In this scenario, command-line arguments are used by the Blue Team to assess the resilience of its analytics.

Recent threat reports indicate that some adversaries have adopted the use of PowerShell, as referenced in the ATT&CK framework. Other reports suggest that both advanced targeted attacks and crimeware are using PowerShell to evade detection, making it a growing trend. Depending on how PowerShell is used and the type of PowerShell logging on target systems, it could mask adversary techniques that were effectively detected by process monitoring and command-line arguments in Scenario 1.

The high-level sequence of events in Scenario 2 mirrors that of Scenario 1. The goal and use of tactics and techniques remain the same, but the techniques are executed through PowerShell equivalents instead of common Windows-based utilities.

Freely available PowerShell tools, such as Empire, can be used to replace some common Windows utilities to test defenses against PowerShell-based behaviors. For Scenario 2, the Red Team could use PowerShell alternatives for a subset of ATT&CK techniques successfully used during Scenario 1 to avoid detection by Blue Team analytics. Figure 7 is a color-coded ATT&CK matrix displaying the techniques used in Scenario 2. Techniques that can be modified using PowerShell alternatives appear with an orange border. See Appendix B for a detailed list of techniques for Scenario 2.

**Figure 7. Scenario 2 ATT&CK Matrix**

---

### Step 5: Emulate Threat

After designing the scenarios and analytics, the next step is to use the scenarios to emulate the adversary and test the functionality of the analytics in Step 6. This is done by having a Red Team emulate threat behavior and perform techniques as scoped by a White Team. Adversary emulation operations allow analytic developers to verify the efficacy of their cyber defenses. To focus on post-compromise adversary behavior, the Red Team begins with access to the enterprise network through a remote access tool on a particular system in a given network environment. This access expedites the assessment and ensures that post-compromise defenses are adequately tested. The Red Team then follows the plan and guidelines outlined by the White Team.

The White Team should coordinate any adversary emulation activity with the organization’s network asset owners and security organizations to ensure awareness in case of network issues, user concerns, security incidents, or other issues that may arise.

### Step 6: Investigate Attack

Once the Red Team component has been conducted in a given cyber game, the Blue Team gathers to attempt to discover what the Red Team did. In many of MITRE’s cyber games, the Blue Team includes the developers responsible for creating the analytics being used. This allows the developers to experience first-hand how well their analytics perform in a near real-life situation, and their lessons learned can drive future development and refinement. Occasionally, it is beneficial to conduct a Blue Team with analysts who have no experience with the analytics being used. Testing in this way helps to ensure that the Blue Team’s success is not dependent on institutional knowledge and that the analytics are intuitive for all users, not just for those who developed them.

The Blue Team starts their portion of the cyber game with a set of high-confidence analytics that, if successful, provide an initial indicator of where and when the Red Team may have been active. This is important, as the Blue Team is not given any information regarding Red Team activity, other than a vague window of time, normally on the order of a month. Oftentimes, the Blue Team’s high-confidence analytics fall into the “behavioral” category, although some may belong in the “anomaly/outlier” category. The results of applying these high-confidence analytics drive the Blue Team to further investigate individual hosts using the other types of analytics previously described (situational awareness, outlier/anomaly, and forensic). The information derived from the higher confidence analytics helps to focus and refine the higher noise analytics, increasing the confidence that their output is indicative of Red Team activity.

This process of using the output of one analytic to help refine another is iterative and is repeated throughout the exercise as new information is gathered. Eventually, as events are identified as belonging to the Red Team, a timeline of activity begins to form. Understanding the timeline of events is important and can help analysts infer information that cannot be gained purely through the analytics. Gaps in activity in the timeline can identify windows of time where further investigation is needed. Also, by looking at the data this way, Blue Team members may be able to infer where activity may be found, even without having any other evidence of such activity. For example, seeing a new executable run but having no evidence of how it was placed on the machine may alert analysts of potential Red Team behavior and can provide details on how the Red Team accomplished its lateral movement. These clues can also lead to ideas for new analytics that need to be written for the next iteration of the ATT&CK-based analytic development method.

While investigating the Red Team’s attack, the Blue Team develops several overarching categories of information as their portion of the exercise progresses. These can take the form of pieces of information they wish to discover, such as:
- **Hosts Involved/Compromised:** Often represented during the exercise as a list of hosts and the reason(s) why each host has been identified as suspicious. This is critical information when trying to remediate an incident.
- **Accounts Compromised:** It is crucial that the Blue Team be able to identify the accounts that have been compromised on a network. Failure to do so allows the Red Team, or an adversary in real life, to regain access to the network from other vectors, thus negating all previously made remediation efforts.
- **Objective:** The Blue Team also needs to endeavor to discover the Red Team’s objectives and whether they achieved them or not. This is often one of the hardest aspects to uncover as it requires a large corpus of data to determine with confidence.
- **TTPs Used:** Red Team TTPs are important to note at the end of the exercise as a way to identify future work that needs to be done. The Red Team may have exploited misconfigurations in a network that need to be addressed, or the Blue Team may discover a technique that the Blue Team cannot identify currently without further sensing. The TTPs that the Blue Team identifies should be compared to the list of TTPs the Red Team claims to have used to identify any defense gaps.

### Step 7: Evaluate Performance

After both Blue and Red Team activities are complete, the White Team facilitates an analysis by team members that compares the Red Team activity to what was reported by the Blue Team. This allows a comprehensive comparison from which the Blue Team gains invaluable information about how successful they were at discovering Red Team actions. Using this information, the Blue Team refines existing analytics and identifies adversary behaviors for which they need to develop or install new sensors, collect new sets of data, or create new analytics.

**Real-World Experiences**

To validate its approach, MITRE created a living lab—an enclave of about 250 computers on MITRE’s corporate networks. These were the real PCs that MITRE employees use every day for their actual work. Host and network sensors were then added to these PCs before developing behavioral analytics using the ATT&CK-based analytics development method outlined in Section 4. MITRE developed its analytics in a live environment (as opposed to a lab environment), because analytics that work well in the lab may fail in the real world due to various factors.