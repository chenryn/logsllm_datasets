**Title: Once is Never Enough: Foundations for Sound Statistical Inference in Tor Network Experimentation**

**Authors:**
- Rob Jansen, U.S. Naval Research Laboratory
- Justin Tracey, University of Waterloo
- Ian Goldberg, University of Waterloo

**Publication:**
- Proceedings of the 30th USENIX Security Symposium
- August 11â€“13, 2021
- ISBN: 978-1-939133-24-3
- Open access sponsored by USENIX

**Abstract:**
Tor is a widely used low-latency anonymous communication system that emphasizes usability and performance. A faster network attracts more users, thereby enhancing the anonymity of all users. Previous research aimed at improving Tor's performance typically draws conclusions from a single simulation run for both standard Tor and each proposed variant. However, since these simulations are conducted on sampled Tor networks, sampling errors can significantly affect the observed results. This paper questions the practical significance of such conclusions without considering statistical significance.

We enhance the Tor experimental methodology in two key ways:
1. We introduce a new Tor network modeling approach that generates more representative networks and improved experimentation tools, enabling faster and larger-scale simulations.
2. We present new statistical methodologies to:
   - Demonstrate the necessity of running multiple simulations in independently sampled networks for informative results.
   - Show how to use multiple simulation results for sound statistical inference.

We illustrate our contributions with a case study involving 420 simulations, demonstrating the application of our methodologies to concrete Tor experiments and the analysis of their results.

**1. Introduction**
Tor is a privacy-enhancing technology and the most popular anonymous communication system. It consists of a network of relays that forward traffic on behalf of clients and internet destinations. The Tor Project estimates approximately 2 million daily active users, while recent studies suggest up to 8 million daily active users and 792,000 simultaneously active users. Tor is used for various purposes, including blocking trackers, defending against surveillance, resisting fingerprinting and censorship, and freely browsing the internet.

Usability is crucial for the security Tor provides. Improved usability retains more users, which generally enhances anonymity. Tor has made significant improvements in three main areas:
- Interface design through usability studies.
- Performance enhancements via new traffic scheduling algorithms.
- Increased network resources, growing from about 100 Gbit/s to 400 Gbit/s over five years.

Researchers have proposed numerous methods to improve Tor's performance, focusing on path selection, load balancing, traffic admission control, and congestion control. The standard practice in these proposals is to conduct a single experiment for each configuration and compare empirical distributions of performance metrics. These experiments, often simulations or emulations, are run on scaled-down Tor test networks sampled from a static point in time, leading to questionable conclusions due to the lack of statistical inference techniques.

**Contributions:**
We advance Tor performance research by:
1. Designing and validating new Tor experimentation models and tools that allow for the creation and execution of more representative Tor test networks.
2. Developing statistical methodologies for sound statistical inference of experimental results, demonstrated through a case study.

**Models and Tools:**
In Section 3, we present a new Tor network modeling methodology that considers the network's state over time, producing more representative models. Our tools support flexible generation of Tor network models with configurable parameters, suitable for various computing facilities.

In Section 4, we introduce improved experimentation tools, optimized to run Tor experiments faster and at a larger scale. We showcase these contributions with the largest known Tor simulations, featuring full-scale networks with 6,489 relays and 792,000 simultaneously active users. Compared to prior work, we observe significant reductions in RAM usage and run time.

**Statistical Methodologies:**
In Section 5, we describe a methodology for conducting sound statistical inference using results from scaled-down Tor networks. We find that multiple simulations in independently sampled networks are necessary for statistically significant results. We discuss how to estimate the distribution of a random variable and compute confidence intervals, and how network sampling choices affect these estimations.

In Section 6, we present a case study demonstrating the application of our modeling and statistical methodologies. We analyze the results from 420 Tor simulations across different network scales and traffic loads, concluding that one simulation is insufficient for achieving statistically significant results.

**Availability:**
Our new modeling tools and improvements to Shadow are released as open-source software as part of OnionTrace v1.0.0, TorNetTools v1.1.0, TGen v1.0.0, and Shadow v1.13.2. All research artifacts are publicly available.

**2. Background and Related Work**
We provide a brief background on Tor, followed by an overview of prior work on Tor experimentation, modeling, and performance.

**2.1 Tor**
The primary function of the Tor network is to anonymize user traffic. It consists of relays that forward traffic on behalf of clients. Directory authorities publish a consensus document containing relay information, and clients build circuits through a telescopically connected path of relays to send and receive traffic. Tor also supports onion services, anonymizing both clients and servers.

**2.2 Tor Experimentation Tools**
Early Tor experimentation tools included packet-level simulators, but they became outdated. More realistic tools, such as ExperimenTor and Shadow, were developed. ExperimenTor uses network emulation, while Shadow is a hybrid discrete-event network simulator. We make further contributions to improve Shadow's efficiency and correctness.

**2.3 Tor Modeling**
Previous Tor network models were sensitive to short-term changes in the true network. Our new techniques use Tor metrics data over a selectable time period to create more representative simulated networks. We also use Markov models informed by privacy-preserving statistics from real Tor traffic, significantly improving experiment scalability.

**2.4 Tor Performance Studies**
Existing tools and models have helped researchers explore various aspects of Tor performance. The standard practice involves sampling a single scaled-down network model and comparing results across experiments. While some studies use multiple trials, none involve running experiments in multiple sampled networks, which is necessary for estimating real-world effects. Our work advances the experimental process by introducing new statistical methodologies and demonstrating their application in a case study.

**3. Models for Tor Experimentation**
To conduct meaningful Tor experiments, we need accurate network and traffic models. In this section, we describe new modeling techniques that utilize the latest data from privacy-preserving measurement studies.