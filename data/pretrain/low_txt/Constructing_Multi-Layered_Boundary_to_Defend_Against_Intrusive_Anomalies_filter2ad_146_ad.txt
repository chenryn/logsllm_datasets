### 4.2.1 Probabilistic Policy and Reward Calculation

In the probabilistic policy, the long-term expected average reward is calculated based on the probabilities of two outcomes: \( N_{\text{correct}} \) and \( N_{\text{incorrect}} \). Therefore, the expected reward \( E[R] \) can be simplified as:

\[ E[R] = P(N_{\text{correct}}) \cdot R_{\text{correct}} + P(N_{\text{incorrect}}) \cdot R_{\text{incorrect}} \]

Additionally, since the ADC receives a reward signal at every decision step, the expected reward can be further simplified as:

\[ E[R] = P(\text{Observe}) \cdot R_{\text{Observe}} + P(\text{Alarm}) \cdot R_{\text{Alarm}} \]

Specifically, at each time step, if the ADC takes the action "Observe," the reward signal is assigned 0. If the action is "Alarm," the reward signal is assigned -1. The total reward signal is then calculated after one pass through the sequence data concatenated by observation traces (the ideal value should be 0).

To simplify the consensus strategy, any false alarm reported by any elemental AD would lead to the "Alarm" action of the ADC, with a penalty applied to all ADs.

### 4.2.2 Training Phase Analysis

Figure 3 depicts the ADC's behavior during the training phase, which consists of 500 training epochs with parameters \(\gamma = 0.90\) and \(\alpha_1 = \alpha_2 = \ldots = 10^{-3}\). The upper part of the figure shows the number of false alarms in the training phase, while the lower part shows the average reward signal. To better illustrate the trend, the ADC only considered the past 10 passes (i.e., \(\lambda = 10\) in equation (2)).

The figure clearly shows that the ADC's performance improved incrementally during the training phase, as the reward signal increased over time to an optimum. After the 462nd pass, no false alarms were triggered.

After training, the parameter vector of the ADC \(\theta\) is:

\[
\theta = \begin{pmatrix}
0.42 & 0.84 & 0.69 & 0.79
\end{pmatrix}
\]

### 4.2.3 Testing for False Alarms

To evaluate the ADC's capability to suppress false alarms, we tested the trained ADC using the normal testing set in Table 2. The testing data was divided into 188 command traces, each containing 30 command tokens, along with their underlying audit events and processes.

Figure 4 illustrates the relationship between the average false alarm rate (number of false alerts over the number of command traces) and the number of command traces used for testing. Since the ADC reports at the pace of each command trace, its performance was compared with that of MCE (with initial parameters), which also reports once per command trace.

The figure shows that the ADC triggered fewer false alerts compared to MCE. The ADC generated its first false alarm at the 101st command trace (F.P. = 0.99%). At the 183rd command trace, MCE had generated 11 alerts (F.P. = 6.01%), while the ADC generated only 4 alerts (F.P. = 2.19%).

At the 128th and 171st command traces, MCE did not report false alarms, but the ADC did, indicating that one of the other three ADs made a wrong action. Although analyzing the other three ADs could provide insights, it was not carried out due to the complexity of data partitioning and the lack of a compelling need.

### 4.2.4 Detection of Common Exploits

First, we evaluated the ADC's masquerade detection performance. 850 command tokens (with underlying 2127 audit events and 272 processes) of another user were truncated into 28 command traces (each login session containing about 30 command tokens) and injected at randomly selected positions, without replacement, into the original 188 command traces. The underlying audit events and processes executed by the 'masquerader' were also injected into the respective normal observation traces.

Table 5 shows the results: among the total 216 command traces (188 normal + 28 anomalous), MCE detected 20 out of 28 anomalous command traces with a F.P. of 11.17% by regulating the threshold to 0.38. After this point, the F.P. sharply increased to 100% with a total of 21 anomalous command traces detected. The trained ADC, however, detected 23 anomalous command traces with a F.P. of 9.57%.

Second, the trained ADC was used to detect the injected attacks shown in Table 3, and its performance was compared with that of the individual ADs. In our work, detection accuracy is defined as the ratio of detected attacks to all injected attacks (hidden in 35 intrusive processes). The false alert rate is the ratio of misreports to all normal processes (total 690).

To simplify the experiment while maintaining validity, we assumed that false alerts would not be generated by the normal traces used in the previous false alarm test. The consensus strategy was adjusted so that any 'Alarm' report from any individual AD would cause the ADC to take the 'Alarm' action. The initial parameters used by the individual ADs were directly derived from the ADC, and they were individually adjusted to investigate the relationship between detection accuracy and F.P.

Table 6 shows the detection result of the ADC and the best trade-off between detection accuracy and F.P. of the elemental ADs by adjusting respective thresholds. Specifically, we observed:

- MCE always took the action 'Observe' since the intrusive processes were injected without corresponding command traces.
- The ADC detected all the injected attacks by combining the reports from elemental ADs, with a very low false alert rate (7 out of 690 processes were misreported).
- All ADs detected all buffer overflow attacks, but some DoS attacks were not discovered.

### 5 Conclusion and Future Work

Based on the assumption that an optimal combination of several observation-specific ADs may broaden detection coverage, suppress the false positive rate, and potentially capture "root-cause" attacks, a POMDP model was formulated, and a policy-gradient reinforcement learning algorithm was applied to address the delayed reward, partially observable, multi-agent learning problem.

In the next stage, we plan to collect more real trace data (and some artificial anomalies) to enrich the experiments. Additional problems, such as computational cost, real-time response ability, and consensus efficiency, also need careful consideration. Furthermore, we will extend our work to computer networks to verify whether our ADC can detect distributed attacks with ADs located in several dominated hosts. Anomalies in wireless networks or sensor networks are also expected to be detected through the optimal cooperation of location-centric ADs.

### 6 Acknowledgement

This research is conducted as part of the "Fostering Talent in Emergent Research Fields" program, funded by the Special Coordination Funds for Promoting Science and Technology by the Ministry of Education, Culture, Sports, Science and Technology.

### References

[1] Douglas Aberdeen, “A Survey of Approximate Methods for Solving Partially Observable Markov Decision Processes,” National ICT Australia Report, Canberra, Australia, 2003.

[2] Peter L. Bartlett and Jonathan Baxter, “Hebbian Synaptic Modifications in Spiking Neurons That Learn,” Technical Report, Computer Sciences Laboratory, RSISE, ANU, 1999.

[3] Jonathan Baxter and Peter L. Bartlett, “Stochastic Optimization of Controlled Partially Observable Markov Decision Processes,” Proceedings of the 39th IEEE Conference on Decision and Control (CDC00).

[4] Jonathan Baxter and Peter L. Bartlett, “Direct Gradient-Based Reinforcement Learning: I. Gradient Estimation Algorithms,” Technical Report, ANU, 1999.

[5] J. Baxter, L. Weaver, and P.L. Bartlett, “Direct Gradient-Based Reinforcement Learning: II. Gradient Descent Algorithms and Experiments,” Technical Report, Research School of Information Sciences and Engineering, Australian National University, September 1999.

[6] Sung-Bae Cho and Hyuk-Jang Park, “Efficient Anomaly Detection by Modeling Privilege Flows Using Hidden Markov Model,” Computers and Security, Vol. 22, No. 1, pp. 45-55, 2003.

[7] S. Forrest, S.A. Hofmeyr, and T.A. Longstaff, “A Sense of Self for UNIX Processes,” Proceedings of the 1996 IEEE Symposium on Security and Privacy, Los Alamitos, CA: IEEE Computer Society Press.

[8] Giorgio Giacinto, Fabio Roli, and Luca Didaci, “Fusion of Multiple Classifiers for Intrusion Detection in Computer Networks,” Pattern Recognition Letters 24 (2003) 1795-1803.

[9] Sang-Jun Han and Sung-Bae Cho, “Combining Multiple Host-Based Detectors Using Decision Tree,” Artificial Intelligence, LNAI 2903, pp. 208-220, 2003.

[10] Joshua H., Dorene K.R., Larra T., and Stephen T., “Validation of Sensor Alert Correlators,” IEEE Security and Privacy, pp. 46-56, 2003.

[11] Yihua Liao and V. Rao Vemuri, “Use of K-Nearest Neighbor Classifier for Intrusion Detection,” Computers and Security, Vol. 21, No. 5, pp. 439-448, 2002.

[12] Roy A. Maxion, “Masquerade Detection Using Enriched Command Lines,” International Conference on Dependable Systems & Networks: San Francisco, CA, 22-25 June 2003.

[13] Roy A. Maxion, “Masquerade Detection Using Truncated Command Lines,” International Conference on Dependable Systems & Networks: Washington, DC, 23-26 June 2002.

[14] Ning, P., Cui, Y., Reeves, D.S., and Ding X., “Techniques and Tools for Analyzing Intrusion Alerts,” ACM Transactions on Information and Systems Security, Vol. 7, No. 2, May 2004, Pages 274-318.

[15] Porras, P.A., and Neumann, P.G., “EMERALD: Event Monitoring Enabling Responses to Anomalous Live Disturbances,” Proceedings of the 20th National Information Systems Security Conference, 1997.

[16] Snapp, S.R., Smaha, S.E., Teal, D.M., and Grance, T., “The DIDS (Distributed Intrusion Detection System) Prototype,” Summer USENIX Conference, San Antonio, Texas, USENIX Association (1992) 227-233.

[17] Nigel Tao, Jonathan Baxter, and Lex Weaver, “A Multi-Agent, Policy-Gradient Approach to Network Routing,” 18th International Conference on Machine Learning, ICML 2001.

[18] Kymie M.C. Tan and Roy A. Maxion, “‘Why 6’ Defining the Operational Limits of STIDE, an Anomaly-Based Intrusion Detector,” Proceedings of the 2002 IEEE Symposium on Security and Privacy, 2002.

[19] Kymie M.C. Tan, Kevin S. Killourhy, and Roy A. Maxion, “Undermining an Anomaly-Based Intrusion Detection System Using Common Exploits,” RAID 2002, LNCS 2516, pp. 54-73, 2002.

[20] C. Warrender, S. Forrest, and B. Pearlmutter, “Detecting Intrusion Detection Using System Calls: Alternative Data Models,” Proceedings of the 1999 IEEE Symposium on Security and Privacy, pp. 133-145, Oakland, 1999.

[21] Nong Ye, Xiangyang Li, Qiang Chen, Syed Masum Emran, and Mingming Xu, “Probabilistic Techniques for Intrusion Detection Based on Computer Audit Data,” IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, Vol. 31, No. 4, July 2001.

[22] Nong Ye, Timothy Ehiabor, and Yebin Zhang, “First-Order Versus High-Order Stochastic Models for Computer Intrusion Detection,” Quality and Reliability Engineering International, 2002; 18: 243-250.

[23] Dit-Yan Yeung and Yuxin Ding, “Host-Based Intrusion Detection Using Dynamic and Static Behavioral Models,” Pattern Recognition 36 (2003) 229-243.