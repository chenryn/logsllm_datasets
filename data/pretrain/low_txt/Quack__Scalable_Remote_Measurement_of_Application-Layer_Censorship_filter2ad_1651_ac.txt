### 4. Characterization

To better understand any inherent biases in our data, we first characterize the population of echo servers used in our study.

#### 4.1 Discovery

To discover echo servers across diverse subnets and geographic locations, we performed Internet-wide scans using the ZMap toolchain [15] on the IPv4 address space. We conducted daily scans over two months, from June 1st to July 31st, discovering more than 50,000 echo servers each day. Upon identifying hosts that respond to our SYN packets on port 7, we initiated connections to these potential echo servers. We sent a randomly generated string and verified that they replied with an identical string. In our initial trial, 57,890 servers responded correctly, spanning 3,766 ASNs (Autonomous System Numbers).

Many of our experiments run over the course of a day, so we measured the stability of echo servers by checking their online status 24 hours later. We found that 92% of ASNs had at least one echo server that was online during this second test. Figure 4 illustrates the number of servers still online after 24 hours, which is significant for our experiments. Only those servers stable for at least 24 hours will test all keywords. This reduces the diversity of our coverage, but not significantly, and biases our results towards stable echo servers.

#### 4.2 Churn

We analyzed our daily scans to understand the stability of echo server IP addresses over time. On average, 17% of echo servers changed their IP address within 24 hours, while 18% remained stable and responsive throughout the entire measurement period. The churn rate decelerates over time, with the highest churn rate observed on the first day of the study.

#### 4.3 Identification

To understand the composition of machines running echo servers, we randomly selected 1% of responding echo servers on July 17, 2017. We performed OS detection on each IP address using Nmap. The most common system families, as defined by Nmap, are shown in Figure 6. There were 56,228 working echo servers on this date. Of the 562 we tested, Nmap identified 463 (82.4%) of the operating systems, with a median accuracy of 99%. This test covered 54 countries.

Of the echo servers scanned with Nmap, 251 (44.7%) had full device labels containing the words "server," "router," or "switch." Of the remaining echo servers, 70 (12.5%) were Linux, and 26 (4.6%) were Windows. The rest were identified as various other systems such as firewalls, controllers, and embedded systems. In total, 4% of echo servers were given device labels that left doubt as to whether they were infrastructure machines, as they were identified as non-server Windows machines, and 2 devices were identified as running Android.

While it would be infeasible to run Nmap’s OS detection service against all echo servers, we do not believe this is necessary to safely use all functioning echo servers, as discussed in Section 3.2.

#### 4.4 Coverage

Echo servers provide us with diverse vantage points in a majority of countries. We associated IPs with autonomous systems using the publicly available Route Views dataset [39] and located each server to a country using the MaxMind GeoIP2 service [29].

On average, we observed echo servers in 177 countries. Of these, 39 countries had more than one hundred echo servers, and 82 countries had more than fifteen echo servers. This provides insight into a large number of countries.

We compared our method’s coverage with that of the OONI project [19], which enlists volunteers worldwide to run scans from local devices to measure network disruption. OONI makes this data public with the consent of the volunteers, but probes do not have unique identifiers; therefore, we used the number of distinct autonomous systems per country to estimate coverage.

During the week of July 8–15, 2017, we compared the number of unique ASes observed for both tests. As shown in Figure 7, echo servers provided a much more diverse set of vantage points and were present in more countries. During the comparison week, OONI data was available for 113 countries, while echo servers were responsive in 184. Furthermore, the total number of ASes seen in the echo measurements was nearly an order of magnitude larger: we observed echo servers in 4,458 unique ASes, while OONI measured 678. While OONI probes provide rich measurements for the locations they have access to, our technique provides broader and more consistent measurements.

### 5. Evaluation

In this section, we provide the results of the studies described in Section 3. Our evaluation supports the practicality of Quack as an application-layer measurement tool in two ways. First, we describe what behavior our measurements detected given a set of URLs known to be censored, to verify that our results correlate with previously observed phenomena. Second, we support our claim that our system works at scale, presenting the results of an experiment that measured a larger corpus of domains across a greater number of countries than any previous study.

#### 5.1 Validation

To control for noise, non-protocol-compliant servers, and other anomalous behaviors, we measured echo server behavior using innocuous domains of the form `testN.example.com`. Mock queries to these domains demonstrate behavior in the absence of disruption, as these domains are unlikely to be blocked. This allows us to identify a baseline for ordinary network and echo server failure when interacting with each remote network and understand our subsequent test results in light of a baseline model of expected behavior.

Our first assumption in designing our control tests is that the class of domains `testN.example.com` will face no blocking by the network between our server and the echo server. To validate this assumption, we performed a set of measurements to all echo servers using only this control class of domains and considered the failure percentages observed. The distribution of failures per domain tested is shown in Figure 8.

We observed a median domain failure rate of less than 0.01%, with a maximum failure rate across 1,109 domains of 0.08%. Additionally, the domains in the upper quartile of disruption rates were evenly distributed over the class of innocuous domains, independent of the value of N.

Using the technique described in Section 3.1, we classified no country as interfering with any of our control domains. We also confirmed these results using another control domain: `echotest.[redacted].edu`, validating our control.

We assume failures in the absence of network interference are independent of which server is used. This allows us to present a distribution for the null hypothesis that is independent of either variable and therefore constant. A few factors could cause a given server to fail many innocuous domains: network unreliability, echo server unreliability, or actual blocking occurring for our innocuous domains. Despite this, in Figure 9, we see that 98% of servers saw no blocking events.

We observed that during the duration of our experiment, 17% of echo servers appeared to churn away, indicated by yielding two No Result tests sequentially. This is roughly the same number we observed churning away in a day for our discovery scans. This confirms that our results will be biased toward networks with stable echo servers.

Finally, we empirically determined how long measurements should wait when a blocking event is detected to allow stateful DPI (Deep Packet Inspection) disruption to disengage. Shorter timeouts allow us to test more domains against a given server in a shorter time, while longer timeouts are less likely to incorrectly classify a domain as a failure due to a previous sensitive domain having triggered stateful blocking. Our system, as implemented, is not fundamentally limited by a longer timeout because there are more servers to test at any given time than there are servers waiting for that timeout to expire. As such, the two-minute delay we empirically determined, as shown in Figure 3, is a minimum, and the system may take longer to schedule the subsequent trial in a test against a disrupted server. We observed that all delays were less than five minutes in practice.

#### 5.2 Detection of Disruption

Next, we tested each of the domains on the Citizen Lab global list against all echo servers by formatting them as valid HTTP GET requests. We expected to see interruption in this test because the Citizen Lab domains are known to be blocked in countries around the world. This is confirmed by the difference to the control in Figures 8 and 9.

Using our method of classifying interference, as described in Section 3.1, only 12 of the 74 countries tested demonstrated evidence of keyword blocking in this test. The interfering countries, the number of domains for which we observed interference, and the categories those domains are contained in are given in Figure 10.

For each country listed in Figure 10, we looked for external evidence to support the conclusion that we observed government-sanctioned censorship. One source of external evidence is the Freedom on the Net report by Freedom House [21]. Of the countries in the table, nine are rated as "Not Free" and two as "Partially Free."

South Korea and Jordan are listed as Partially Free by Freedom House; however, both are indicated in ONI’s most recent country profiles as performing filtering [31, 32]. In the case of South Korea, blocking based on HTTP request content is specifically identified. In further support of the observed phenomenon being action at a national level, the echo responses in South Korea that did not match the echo requests were HTTP redirects to a government-run website outlining the reason the requested domain was blocked. This is another advantage of the Echo Protocol — we are able to see the responses injected to the echo server because they are then echoed back to us.

Two countries were identified by our system as having a significant proportion of blocking but had no evidence from other sources that there would be restrictions on the Internet: Ghana and New Zealand. Ghana is not evaluated by Freedom Net, but the Department of State stated in its 2016 Human Rights report [45] that there were no governmental restrictions to the Internet. Upon inspecting the scope of blocking, in both cases, it was restricted to a single academic network in the country, and all echo servers in that AS reported interference. In all other countries identified by our system as performing blocking, we observed interference in more than one AS. Our technique is not fine-grained enough to detect censorship across all networks, and in these cases, we have visibility into only a few locations that have close proximity. For these reasons, we excluded Ghana and New Zealand from Figure 10.

While this presents a case that the interference we identify is genuine, we do not claim that we identify all instances of censorship.