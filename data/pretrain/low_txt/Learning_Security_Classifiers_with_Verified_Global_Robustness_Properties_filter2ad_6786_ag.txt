### References

1. **IEEE Transactions on Neural Networks 21, 6 (2010), 906–917.**
   
2. **Wouter Duivesteijn and Ad Feelders.** 2008. Nearest neighbour classification with monotonicity constraints. In *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*. Springer, 301–316.

3. **Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari.** 2018. Output Range Analysis for Deep Feedforward Neural Networks. In *NASA Formal Methods Symposium*. Springer, 121–138.

4. **Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O’Donoghue, Jonathan Uesato, and Pushmeet Kohli.** 2018. Training verified learners with learned verifiers. *arXiv preprint arXiv:1805.10265* (2018).

5. **Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli.** 2018. A dual approach to scalable verification of deep networks. *arXiv preprint arXiv:1803.06567* (2018).

6. **Ruediger Ehlers.** 2017. Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks. *15th International Symposium on Automated Technology for Verification and Analysis* (2017).

7. **Farzan Farnia, Jesse Zhang, and David Tse.** 2018. Generalizable Adversarial Training via Spectral Normalization. In *International Conference on Learning Representations*.

8. **Ad Feelders.** 2010. Monotone relabeling in ordinal classification. In *2010 IEEE International Conference on Data Mining*. IEEE, 803–808.

9. **Chris Finlay and Adam M Oberman.** 2021. Scalable input gradient regularization for adversarial robustness. *Machine Learning with Applications 3* (2021), 100017.

10. **Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin Vechev.** 2019. DL2: Training and Querying Neural Networks with Logic. In *International Conference on Machine Learning (ICML)*.

11. **Matteo Fischetti and Jason Jo.** 2017. Deep Neural Networks as 0-1 Mixed Integer Linear Programs: A Feasibility Study. *arXiv preprint arXiv:1712.06174* (2017).

12. **Jerome H Friedman, Bogdan E Popescu, et al.** 2008. Predictive learning via rule ensembles. *The Annals of Applied Statistics 2, 3* (2008), 916–954.

13. **Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev.** 2018. AI2: Safety and robustness certification of neural networks with abstract interpretation. In *IEEE Symposium on Security and Privacy (SP)*.

14. **Maxim Goncharov.** [n.d.]. Traffic direction systems as malware distribution tools. http://www.trendmicro.es/media/misc/malware-distribution-tools-research-paper-en.pdf.

15. **Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree.** 2021. Regularisation of neural networks by enforcing Lipschitz continuity. *Machine Learning 110, 2* (2021), 393–416.

16. **Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel.** 2016. Adversarial perturbations against deep neural networks for malware classification. *arXiv preprint arXiv:1606.04435* (2016).

17. **Maya Gupta, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van Esbroeck.** 2016. Monotonic calibrated interpolated look-up tables. *The Journal of Machine Learning Research 17, 1* (2016), 3790–3836.

18. **Matthias Hein and Maksym Andriushchenko.** 2017. Formal guarantees on the robustness of a classifier against adversarial manipulation. In *Advances in Neural Information Processing Systems*.

19. **Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu.** 2017. Safety verification of deep neural networks. In *International Conference on Computer Aided Verification (CAV)*. Springer, 3–29.

20. **Inigo Incer, Michael Theodorides, Sadia Afroz, and David Wagner.** 2018. Adversarially Robust Malware Detection Using Monotonic Classification. In *Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics*. ACM, 54–63.

21. **Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong.** 2019. Certified robustness for top-k predictions against adversarial perturbations via randomized smoothing. *International Conference on Learning Representations (ICLR)* (2019).

22. **Alex Kantchelian, JD Tygar, and Anthony Joseph.** 2016. Evasion and hardening of tree ensemble classifiers. In *International Conference on Machine Learning*. 2387–2396.

23. **Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.** 2017. Reluplex: An efficient SMT solver for verifying deep neural networks. In *International Conference on Computer Aided Verification (CAV)*. Springer, 97–117.

24. **Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljić, et al.** 2019. The Marabou framework for verification and analysis of deep neural networks. In *International Conference on Computer Aided Verification (CAV)*.

25. **Herbert Kay and Lyle H Ungar.** 2000. Estimating monotonic functions and their bounds. *AIChE Journal 46, 12* (2000), 2426–2434.

26. **Amin Kharraz, Zane Ma, Paul Murley, Charles Lever, Joshua Mason, Andrew Miller, Nikita Borisov, Manos Antonakakis, and Michael Bailey.** 2019. Outguard: Detecting in-browser covert cryptocurrency mining in the wild. In *The World Wide Web Conference*. 840–852.

27. **Alexey Kurakin, Ian Goodfellow, and Samy Bengio.** 2017. Adversarial machine learning at scale. In *International Conference on Learning Representations (ICLR)*.

28. **Heeyoung Kwon, Mirza Basim Baig, and Leman Akoglu.** 2017. A domain-agnostic approach to spam-URL detection via redirects. In *Pacific-Asia Conference on Knowledge Discovery and Data Mining*. Springer, 220–232.

29. **Pavel Laskov et al.** 2014. Practical evasion of a learning-based classifier: A case study. In *Security and Privacy (SP), 2014 IEEE Symposium on*. IEEE, 197–211.

30. **Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana.** 2019. Certified robustness to adversarial examples with differential privacy. In *2019 IEEE Symposium on Security and Privacy (SP)*. IEEE.

31. **Kyumin Lee, James Caverlee, and Steve Webb.** 2010. Uncovering social spammers: social honeypots + machine learning. In *Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval*. 435–442.

32. **Kyumin Lee, Brian Eo, and James Caverlee.** 2011. Seven months with the devils: A long-term study of content polluters on Twitter. In *Proceedings of the International AAAI Conference on Web and Social Media, Vol. 5*.

33. **Sangho Lee and Jong Kim.** 2013. Warningbird: A near real-time detection system for suspicious URLs in Twitter stream. *IEEE transactions on dependable and secure computing 10, 3* (2013), 183–195.

34. **Sungyoon Lee, Jaewook Lee, and Saerom Park.** 2020. Lipschitz-Certifiable Training with a Tight Outer Bound. *Advances in Neural Information Processing Systems (NeurIPS)* (2020).

35. **Klas Leino, Zifan Wang, and Matt Fredrikson.** 2021. Globally-Robust Neural Networks. *arXiv preprint arXiv:2102.08452* (2021).

36. **Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin.** 2018. Second-order adversarial attack and certifiable robustness. (2018).

37. **Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin.** 2019. Certified adversarial robustness with additive noise. *Advances in Neural Information Processing Systems (NeurIPS)* (2019).

38. **Linyi Li, Xiangyu Qi, Tao Xie, and Bo Li.** 2020. SoK: Certified Robustness for Deep Neural Networks. *arXiv preprint arXiv:2009.04131* (2020).

39. **Linyi Li, Zexuan Zhong, Bo Li, and Tao Xie.** 2019. Robustra: Training Provable Robust Neural Networks over Reference Adversarial Space. In *IJCAI*.

40. **Zhou Li, Sumayah Alrwais, Yinglian Xie, Fang Yu, and XiaoFeng Wang.** 2013. Finding the linchpins of the dark web: a study on topologically dedicated hosts on malicious web infrastructures. In *2013 IEEE Symposium on Security and Privacy*. IEEE, 112–126.

41. **Xuankang Lin, He Zhu, Roopsha Samanta, and Suresh Jagannathan.** 2020. ART: Abstraction refinement-guided training for provably correct neural networks. In *2020 Formal Methods in Computer Aided Design (FMCAD)*. IEEE, 148–157.

42. **Alessio Lomuscio and Lalit Maganti.** 2017. An approach to reachability analysis for feed-forward ReLU neural networks. *arXiv preprint arXiv:1706.07351* (2017).

43. **Justin Ma, Lawrence K Saul, Stefan Savage, and Geoffrey M Voelker.** 2009. Identifying suspicious URLs: an application of large-scale online learning. In *Proceedings of the 26th annual international conference on machine learning*. 681–688.

44. **Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.** 2018. Towards deep learning models resistant to adversarial attacks. *International Conference on Learning Representations (ICLR)* (2018).

45. **Stefano Melacci, Gabriele Ciravegna, Angelo Sotgiu, Ambra Demontis, Battista Biggio, Marco Gori, and Fabio Roli.** 2020. Can Domain Knowledge Alleviate Adversarial Attacks in Multi-Label Classifiers? *arXiv preprint arXiv:2006.03833* (2020).

46. **Brad Miller, Alex Kantchelian, Michael Carl Tschantz, Sadia Afroz, Rekha Bachwani, Riyaz Faizullabhoy, Ling Huang, Vaishaal Shankar, Tony Wu, George Yiu, et al.** 2016. Reviewer integration and performance measurement for malware detection. In *International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment*. Springer, 122–141.

47. **Matthew Mirman, Timon Gehr, and Martin Vechev.** 2018. Differentiable Abstract Interpretation for Provably Robust Neural Networks. In *International Conference on Machine Learning (ICML)*. 3575–3583.

48. **Christoph Müller, François Serre, Gagandeep Singh, Markus Püschel, and Martin Vechev.** 2021. Scaling Polyhedral Neural Network Verification on GPUs. *Proceedings of Machine Learning and Systems 3* (2021).

49. **Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev.** 2021. Precise Multi-Neuron Abstractions for Neural Network Certification. *arXiv preprint arXiv:2103.03638* (2021).

50. **Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgower.** 2021. Training robust neural networks using Lipschitz bounds. *IEEE Control Systems Letters* (2021).

51. **Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney, Johannes Kinder, and Lorenzo Cavallaro.** 2019. TESSERACT: Eliminating experimental bias in malware classification across space and time. In *28th USENIX Security Symposium (USENIX Security 19)*. 729–746.

52. **Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro.** 2020. Intriguing Properties of Adversarial ML Attacks in the Problem Space. In *2020 IEEE Symposium on Security and Privacy (SP)*. IEEE Computer Society, 1308–1325. https://doi.org/10.1109/SP40000.2020.00073

53. **Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli.** 2019. Adversarial robustness through local linearization. *Advances in Neural Information Processing Systems (NIPS)* (2019).

54. **Mukund Raghothaman, Jonathan Mendelson, David Zhao, Mayur Naik, and Bernhard Scholz.** 2019. Provenance-guided synthesis of Datalog programs. *Proceedings of the ACM on Programming Languages 4, POPL* (2019), 1–27.

55. **Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.** 2018. Certified defenses against adversarial examples. *International Conference on Learning Representations (ICLR)* (2018).

56. **Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang.** 2018. Semidefinite relaxations for certifying robustness to adversarial examples. In *Advances in Neural Information Processing Systems*. 10900–10910.

57. **Gabriel Ryan, Justin Wong, Jianan Yao, Ronghui Gu, and Suman Jana.** 2020. CLN2INV: Learning Loop Invariants with Continuous Logic Networks. In *International Conference on Learning Representations (ICLR)*.

58. **Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sébastien Bubeck.** 2019. Provably robust deep learning via adversarially trained smoothed classifiers. *Advances in Neural Information Processing Systems (NeurIPS)* (2019).

59. **Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang.** 2019. A convex relaxation barrier to tight robustness verification of neural networks. *Advances in Neural Information Processing Systems (NeurIPS)* (2019).

60. **Xujie Si, Woosuk Lee, Richard Zhang, Aws Albarghouthi, Paraschos Koutris, and Mayur Naik.** 2018. Syntax-guided synthesis of Datalog programs. In *Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering*. 515–527.

61. **Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin Vechev.** 2019. Beyond the single neuron convex barrier for neural network certification. *Advances in Neural Information Processing Systems (NeurIPS)* (2019).

62. **Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev.** 2019. An abstract domain for certifying neural networks. *Proceedings of the ACM on Programming Languages 3, POPL* (2019), 1–30.

63. **Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T Vechev.** 2019. Boosting robustness certification of neural networks. In *ICLR (Poster)*.

64. **Sahil Singla and Soheil Feizi.** 2019. Bounding singular values of convolution layers. *arXiv preprint arXiv:1911.10258* (2019).

65. **Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia, and Vijay Saraswat.** 2006. Combinatorial sketching for finite programs. In *Proceedings of the 12th international conference on Architectural support for programming languages and operating systems*. 404–415.

66. **Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.** 2013. Intriguing properties of neural networks. *International Conference on Learning Representations (ICLR)* (2013).

67. **Kurt Thomas, Chris Grier, Justin Ma, Vern Paxson, and Dawn Song.** 2011. Design and evaluation of a real-time URL spam filtering service. In *2011 IEEE symposium on security and privacy*. IEEE, 447–462.

68. **Vincent Tjeng, Kai Xiao, and Russ Tedrake.** 2017. Evaluating robustness of neural networks with mixed integer programming. *arXiv preprint arXiv:1711.07356* (2017).

69. **David Wagner and Paolo Soto.** 2002. Mimicry attacks on host-based intrusion detection systems. In *Proceedings of the 9th ACM Conference on Computer and Communications Security*. ACM, 255–264.

70. **Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana.** 2018. MixTrain: Scalable Training of Formally Robust Neural Networks. *arXiv preprint arXiv:1811.02625* (2018).

71. **Shiqi Wang, Kexin Pei, Whitehouse Justin, Junfeng Yang, and Suman Jana.** 2018. Efficient formal safety analysis of neural networks. *Advances in Neural Information Processing Systems (NIPS)* (2018).

72. **Shiqi Wang, Kexin Pei, Whitehouse Justin, Junfeng Yang, and Suman Jana.** 2018. Formal security analysis of neural networks using symbolic intervals. *27th USENIX Security Symposium* (2018).

73. **Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.** 2021. Beta-CROWN: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification. *arXiv preprint arXiv:2103.06624* (2021).

74. **Antoine Wehenkel and Gilles Louppe.** 2019. Unconstrained monotonic neural networks. In *Advances in Neural Information Processing Systems*.

75. **Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon.** 2018. Towards fast computation of certified robustness for ReLU networks. In *International Conference on Machine Learning (ICML)*.

76. **Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel.** 2018. Evaluating the robustness of neural networks: An extreme value theory approach. In *International Conference on Learning Representations (ICLR)*.

77. **Eric Wong and Zico Kolter.** 2018. Provable defenses against adversarial examples via the convex outer adversarial polytope. In *International Conference on Machine Learning*. 5283–5292.

78. **Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter.** 2018. Scaling provable adversarial defenses. *Advances in Neural Information Processing Systems (NIPS)* (2018).

79. **Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh.** 2021. Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. *International Conference on Learning Representations (ICLR)* (2021).

80. **Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li.** 2020. Randomized smoothing of all shapes and sizes. In *International Conference on Machine Learning (ICML)*. PMLR.

81. **Jianan Yao, Gabriel Ryan, Justin Wong, Suman Jana, and Ronghui Gu.** 2020. Learning Nonlinear Loop Invariants with Gated Continuous Logic Networks. In *Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation*.

82. **Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning, and Cho-Jui Hsieh.** 2020. Towards stable and efficient training of verifiably robust neural networks. *International Conference on Learning Representations (ICLR)* (2020).

83. **Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.** 2018. Efficient neural network robustness certification with general activation functions. *arXiv preprint arXiv:1811.00866* (2018).

84. **Xiao Zhang and David Evans.** 2019. Cost-Sensitive Robustness against Adversarial Examples. *International Conference on Learning Representations (ICLR)* (2019).

### Stability for Twitter Account Classifier

To classify Twitter accounts that broadcast spam URLs, we can use the number of followers and the ratio of posted URLs over the total number of tweets as features [47]. It is difficult for spammers to obtain a large number of followers, and they are likely to post more URLs than benign users. We specify the `URLRatio` feature to be stable, such that arbitrarily changing the feature will not change the classifier’s output by more than 1.

**Figure 4** shows one Counterexample-Guided Inductive Synthesis (CEGIS) iteration to train the stability property. The starting classifier is a decision tree. For example, "1.0 * URLRatio ≤ 1" is a condition in the initial classifier. The constraint specifies that the classifier's output score should change by at most 1 when the `URLRatio` feature is arbitrarily perturbed. Multiple weights of the classifier are updated by gradient-guided optimization, and the classifier after training no longer forms a tree structure.

**Figure 4: One CEGIS iteration to train stability property for the Twitter account classifier.**

- **Classifier Output:**
  - \( FR(x) = R3 \)
  - \( FR(x') = R1 \)
  - \( |FR(x) - FR(x')| > 1 \)
  - **Constraint:**
    - \( |R3 - R1| \leq 1 \)

- **CLN: Gradient-guided Optimization**
  - \( 0.99 * URLRatio < 0.294 \land 0.99 * followers < 1429.5 \rightarrow -1.71 \)
  - \( 1.01 * URLRatio < 0.272 \land 1.0 * followers \geq 1429.49 \rightarrow -0.11 \)
  - \( 1.01 * URLRatio \geq 0.274 \land 0.98 * followers < 104.52 \rightarrow -0.59 \)
  - \( 1.01 * URLRatio \geq 0.272 \land 0.98 * followers \geq 104.51 \rightarrow 0.89 \)

- **Updated Classifier:**
  - \( R0 \)
  - \( R1 \)
  - \( R2 \)
  - \( R3 \)

### Proof

**Lemma 1.** If a classifier satisfies Property 3a, then it also satisfies Property 3.

**Proof.** For all \( x, x' \in \mathbb{R}^n \) such that \( \forall i < n, x_i = x'_i \) and \( |\Delta(F(x))| \leq \epsilon \), we have \( F(x) \leq \epsilon^{-1}(\Delta) \). Since \( F \) satisfies Property 3a, we also have \( F(x) - F(x') \leq \epsilon^{-1}(\Delta) \). Therefore, \( F(x') - F(x) \leq \epsilon^{-1}(\Delta) - 0 \). 

\(\blacksquare\)