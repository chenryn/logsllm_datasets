### URL Manager

The host crawler tracks compromised sites using a single probe URL at a time. However, a site can have multiple infected pages, such as multiple blogs, comment pages, and articles. Over time, a site owner may clean or remove an infected page while other URLs on the site remain compromised and active with the same SEO kit.

In such cases, the host crawler switches to a new URL to continue tracking and monitoring the compromised site. The URL manager addresses this need by maintaining a list of all discovered URLs for a given site in the crawling set. It periodically checks each URL to determine if it can serve as a probe URL by attempting to fetch a diagnostic page from that URL. If the host crawler cannot fetch a diagnostic page for a site, it consults the URL manager to find another representative probe URL. If no suitable URL is found, the host crawler continues to use the same probe URL, eventually timing out after eight days if all URLs to the site are non-operational. In this case, the site is declared "sanitized" since the SEO kit is no longer operational. Given the large number of URLs compared to sites, the URL manager crawls newly discovered URLs just once a day.

### Dagger Search Crawler

Before we began crawling the SEO botnet, we explored the general dynamics of cloaking on the Web [19]. We knew from examining the code of previous versions of the SEO kit that the botnet poisoned trending search terms from April 2011 through September 2011. Therefore, we suspected that poisoned search results from the SEO botnet would also appear in our previous data set.

We collected cloaking data using a crawler called Dagger, which ran every four hours to:
1. Download trending search terms.
2. Query each trending search term on various search engines.
3. Visit the page linked from each search result.
4. Run a cloaking detection algorithm to identify poisoned search results.

The Dagger cloaking data allows us to analyze the impact of the SEO botnet on trending search results in near real-time over a seven-month period (Section 5.3). Unfortunately, although we continued to crawl cloaking search results, the SEO botnet changed its SEO policy to target first OEM software and then random search terms, leading to only accidental overlap with the Dagger data after September 2011.

### Trajectory Redirection Crawler

While the host crawler downloads the contents of the doorway pages directly linked by poisoned search results, we also want to identify the final landing pages (e.g., a fake antivirus scam page) to infer how the botmaster monetizes user traffic. Following the doorway pages to final landing pages often involves navigating complex redirection chains, including JavaScript and Flash. To handle this, we used the high-fidelity Trajectory crawler from another project [11].

This crawler uses an instrumented version of Mozilla Firefox to visit URLs, follow all application-level redirects, log the HTTP headers of intermediate and final pages, and capture the HTML and a screenshot of the final page. For all poisoned search results crawled by Dagger, we also used the Trajectory crawler to track scams.

### Results

With the gathered data sets, we now characterize the activities of the SEO botnet and its compromised hosts.

#### Infrastructure

Using nine months of data collected by Odwalla, we start by analyzing the botnet infrastructure used in the SEO campaigns: the scale of the botnet, the lifetime of compromised sites, and the extent to which the botmaster monitors and manages the botnet.

##### Scale

Compared to well-known botnets like spamming botnets with tens to hundreds of thousands of nodes, the SEO botnet is modest in size. Figure 3 presents the measured size of the botnet over time. Each line shows the number of nodes operating a specific version of the SEO kit, and the SUM line shows the total number of all nodes across all versions. For example, on December 1, 2011, we found 821 compromised sites in total, of which 585 sites were running the MAC version of the SEO kit, 42 were running OEM, and 194 were running V7.

Unlike other botnets, the SEO botnet does not exhibit frequent churn. Over nine months, the botnet consisted of 695 active nodes on average, with a maximum size of 939 nodes on December 11, 2011. We observed the botnet running on a total of just 1,497 unique compromised sites across the entire measurement period. In contrast, spamming botnets like Storm would experience churn of thousands of hosts a day [6].

Instead, we see a few key points in time where the botnet membership fluctuates in response to SEO kit updates by the botmaster, rather than from external intervention. During these upgrades, the botmaster also changes the cross-linking policy among nodes, potentially revealing new nodes. Between upgrades, the botnet size primarily fluctuates due to variations in host availability, with a degree of slow attrition.

For example, on November 1, 2011, the botmaster updated the SEO kit from OEM to MAC. Even though the OEM nodes appeared to have entirely switched over to MAC, the botnet size increased by over 200 nodes, all due to nodes running the older version V7. It appears that during the update, the botmaster changed the cross-linking policy to include additional nodes running V7, incidentally widening our vantage point but only for stagnant sites running an older version. On March 6, 2012, there was a similar version switch from MAC to V8 in response to another software upgrade.

##### Lifetime

The relatively stable botnet size and membership suggest that compromised sites are long-lived in the botnet. We define the lifetime of a compromised site as the time between the first and last observation of the SEO kit running on the site. This estimate is conservative, as a site may have been compromised before we first crawled it. However, our measurement period of compromised sites is nine months, and we began monitoring 74% of all 1,497 compromised sites within the first 40 days of our study. Thus, even without the exact time of compromise, we are still able to observe the sites for long periods of time. (As further evidence, for the 537 sites that also appear in the earlier Dagger search results (Section 4.2), the majority were compromised back to April 2011.)

We consider a site cleaned when it does not respond to the SEO C&C protocol for eight consecutive days, suggesting that the site no longer runs the SEO kit. Typically, a site stops running the SEO kit because the site owner removed the SEO malware, sanitizing the site, or the web host or DNS registrar made the site unavailable by preventing visitors from loading the site or resolving the domain.

Consequently, the botmaster is able to use compromised sites for SEO campaigns for long periods of time. Figure 4a presents a histogram of the lifetime of the compromised sites. We distinguish between sites that have been sanitized, avoiding right-censoring of their lifetimes, and sites that have not yet been sanitized. For compromised sites that are eventually sanitized, we bin them according to their respective lifetimes using monthly intervals (30 days). Over 74% of sanitized sites have a lifetime greater than a month, and over 54% have a lifetime greater than two months. There is also a long tail, with the lifetime of some sanitized sites extending beyond even eight months. For compromised sites that have not yet been sanitized, we show them in the ‘*’ bin. These remaining 549 sites are still compromised at the time of writing, and the majority of those have been compromised for at least seven months. This distribution indicates that the majority of compromised sites are indeed long-lived and able to support the SEO campaign for months with high availability.

Figure 4b shows the number of sites sanitized each day, indicating a low daily attrition rate of sites leaving the botnet over time (9.9 sites on average). The few spikes in the graph are specific points in time when many compromised sites were sanitized. In some cases, the spikes are partially attributable to a single entity, owning or hosting multiple sites, who cleans multiple sites at the same time. By manually comparing the resolved IP address for domain names and parsing WHOIS records, we were able to confirm shared hosting and shared owners, respectively. Note that the largest spike on January 26, 2012, corresponds to the outage of the botnet directory server.

One reason sites remain compromised for long periods is that the SEO kit camouflages its presence to site owners. As discussed in Section 3.2.1, the SEO kit returns the original contents of the page to a visitor unless it can determine if the visitor is a search engine crawler or has clicked on a result returned from a search engine. Hence, site owners accessing their own pages typically will not notice an installed SEO kit. Even when they discover the presence of the SEO kit, oftentimes they are unable or unwilling to remove it. In December and January, for instance, we contacted nearly 70 site owners to inform them that their site was infected with the SEO kit, yet only seven sites subsequently removed it.

##### Control

We use two different approaches to assess the botmaster’s ability to monitor and manage the botnet. In the first approach, we observe what fraction of the compromised sites update their SEO kit when the botmaster deploys a new version. We can detect both version changes and site updates by parsing the version information from the diagnostic pages periodically fetched by the host crawler.

As discussed in Section 5.1.1, the data collected by the host crawler overlaps with two version updates. On November 1, 2011, version OEM updated to MAC, and on March 6, 2012, MAC updated to V8. In both cases, we see a near-instantaneous update to the respective new versions from the majority of the compromised sites, followed by a sudden addition of newly seen compromised sites.

In the OEM to MAC update, we see many stragglers—sites that continue running older versions of the SEO kit after the majority of sites update themselves to the latest version. Within a month after the first update, 324 out of 970 sites (33%) that comprise the botnet were stragglers. These stragglers suggest that the botmaster lacks full installation privileges on the compromised sites and is unable to force an update. There is no advantage to running old versions because they poison an outdated set of search terms, are not well optimized in search results, and consequently will not attract much traffic. Therefore, the 324 stragglers represent a substantial inefficiency in the botnet. The straggler phenomenon also occurs during the second update, but the numbers are less pronounced.

Our second approach for assessing control looks at how the botmaster adjusts the cross-linking policy once a compromised site is sanitized and no longer part of the botnet. Recall that each compromised site is cross-linked to other compromised sites to increase search result ranking (Section 5.2). Therefore, when a site is no longer compromised, there is no value for the site to receive backlinks. Assuming the botmaster is actively monitoring the sites in the botnet, he should be able to adjust the cross-linking policy to only link to sites that are still part of the botnet.

Using the set of sanitized sites described in Section 5.1.2, we track the number of backlinks received by each site over time from other compromised sites, noting whether a sanitized site still receives backlinks and for how long. Additionally, we measure the average number of backlinks received before a site is sanitized, and after, to see whether the botmaster updates the cross-linking policy to decrease the number of backlinks given to sanitized sites. Surprisingly, sanitized sites still overwhelmingly receive backlinks, and do so for long periods of time. Out of 508 sanitized sites, nearly all sites still receive backlinks even after being sanitized: all but two sanitized sites receive backlinks through February 26, and 488 (96%) through March 2.

In summary, it appears that the botmaster exerts only limited control over many compromised sites, allowing many to degrade over time. Further, this is one of the inefficiencies in how the botnet is operated. While we do not have insight into the reasons for these lapses—whether negligence, lack of insight, or lack of need—the large numbers of stragglers and useless cross-linking to sanitized sites make it clear that in its existing regime, the botnet does not reach its full potential.