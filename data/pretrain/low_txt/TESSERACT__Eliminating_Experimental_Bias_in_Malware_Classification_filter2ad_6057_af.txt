### USENIX Association
28th USENIX Security Symposium    741

Recent studies have explored various aspects of malware classification, but often without a comprehensive evaluation of classifier performance. Zhou et al. [58] demonstrated that Hardware Performance Counters (HPCs) are not effective for malware classification. Although their work aligns with our research, it is narrowly focused and relies on 10-fold cross-validation for evaluation.

Allix et al. [2] pioneered the evaluation of malware classifiers in relation to time, highlighting how future knowledge can inflate performance. However, they did not propose solutions for comparable evaluations and only identified constraint C1. In a separate study, Allix et al. [1] examined the differences between in-the-lab and in-the-wild scenarios, finding that the presence of more goodware leads to lower performance. Our work systematically analyzes and explains these issues, formalizes a set of constraints (considering both temporal and spatial bias), introduces AUT as a unified performance metric for fair time-aware comparisons, and offers a tuning algorithm to address the effects of training data distribution.

Miller et al. [36] identified temporal sample consistency (equivalent to our constraint C1) but did not consider C2 or C3, which are fundamental (§4.4). Additionally, they assumed the test period to be a uniform time slot, whereas we account for time decay. Roy et al. [45] questioned the use of recent or older malware as training objects and the performance degradation when testing real-world object ratios. However, most of their experiments did not consider time, reducing the reliability of their conclusions.

While past work has highlighted some sources of experimental bias [1, 2, 36, 45], it has given little consideration to the specific goals of different classifiers. Different scenarios may have different objectives (not necessarily maximizing F1). Our work examines the impact of different training settings on performance goals and proposes an algorithm to properly tune a classifier accordingly (§4.3).

Other machine learning (ML) literature investigates imbalanced datasets, showing how training and testing ratios can influence algorithm results [9, 25, 55]. However, these studies, not originating from the security domain, focus only on some aspects of spatial bias and do not consider temporal bias. Concept drift is less problematic in some applications, such as image and text classification, compared to Android malware [26]. Fawcett [16] addresses challenges in spam detection, one of which resembles spatial bias, but does not provide a solution. We introduce C3 to address this issue and demonstrate how its violation inflates performance (§4.4).

Torralba and Efros [52] discuss dataset bias in computer vision, which is distinct from our security setting, where fewer benchmarks exist. In images, the negative class (e.g., "not cat") can grow arbitrarily, which is less likely in the malware context. Moreno-Torres et al. [38] systematize different types of drift and mention sample-selection bias, but do not propose solutions or experiments for its impact on ML performance. Other related work emphasizes the importance of choosing appropriate performance metrics to avoid misinterpretation, such as the misleading nature of ROC curves in imbalanced datasets [14, 24]. Our paper accounts for imbalance and proposes actionable constraints and metrics, supported by tools, to evaluate the performance decay of classifiers over time.

### Summary
Several studies on bias have motivated our research, but none address the entire problem in the context of evolving data, where the i.i.d. assumption no longer holds. Constraint C1, introduced by Miller et al. [36], is insufficient to eliminate bias, as evident from the original evaluation in MAMADROID [33], which enforces only C1. The evaluation in §4.4 clarifies why our novel constraints C2 and C3 are fundamental and shows how our AUT metric can effectively reveal the true performance of algorithms, providing counter-intuitive results.

### Availability
We make TESSERACT’s code and data available to the research community to promote sound and unbiased evaluation of classifiers. The TESSERACT project website, with instructions to request access, is at https://s2lab.kcl.ac.uk/projects/tesseract/. We will also maintain an updated list of publicly available security-related datasets with timestamped objects.

### Conclusions
We have identified novel temporal and spatial biases in the Android domain and proposed new constraints, metrics, and tuning methods to address these issues. We have built and released TESSERACT as an open-source tool that integrates our methods. TESSERACT reveals the real performance of malware classifiers, which remains hidden in incorrect experimental settings in non-stationary contexts. It is essential for the correct evaluation and comparison of different solutions, especially when considering mitigation strategies for time decay. We are currently working on integrating a time-varying percentage of malware into our framework to model more realistic scenarios and using the slope of the performance decay curve to better differentiate algorithms with similar AUT.

We envision that future work on Android malware classification will use TESSERACT to produce realistic, comparable, and unbiased results. We also encourage the security community to adopt TESSERACT to evaluate the impact of temporal and spatial bias in other security domains where concept drift still needs to be quantified.

### Acknowledgements
We thank the anonymous reviewers and our shepherd, Roya Ensafi, for their constructive feedback, which has improved the overall quality of this work. This research has been partially sponsored by the UK EP/L022710/1 and EP/P009301/1 EPSRC research grants.

### References
[1] Kevin Allix, Tegawendé F. Bissyandé, Quentin Jérome, Jacques Klein, Radu State, and Yves Le Traon. Empirical Assessment of Machine Learning-Based Malware Detectors for Android. Empirical Software Engineering, 2016.
[2] Kevin Allix, Tegawendé F Bissyandé, Jacques Klein, and Yves Le Traon. Are Your Training Datasets Yet Relevant? In ESSoS. Springer, 2015.
[3] Kevin Allix, Tegawendé F Bissyandé, Jacques Klein, and Yves Le Traon. Androzoo: Collecting Millions of Android Apps for the Research Community. In Mining Software Repositories. ACM, 2016.
[4] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, and Konrad Rieck. DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket. In NDSS, 2014.
[5] Stefan Axelsson. The Base-Rate Fallacy and the Difficulty of Intrusion Detection. ACM TISSEC, 2000.
[6] Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss. JMLR, 2008.
[7] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 2018.
[8] Christopher M Bishop. Pattern Recognition and Machine Learning. 2006.
[9] Nitesh V Chawla, Nathalie Japkowicz, and Aleksander Kotcz. Special Issue on Learning From Imbalanced Data Sets. ACM SIGKDD Explorations Newsletter, 2004.
[10] François Chollet et al. Keras. https://github.com/fchollet/keras, 2015.
[11] Charlie Curtsinger, Benjamin Livshits, Benjamin G Zorn, and Christian Seifert. ZOZZLE: Fast and Precise In-Browser JavaScript Malware Detection. In USENIX Security, 2011.
[12] George E Dahl, Jack W Stokes, Li Deng, and Dong Yu. Large-Scale Malware Classification Using Random Projections and Neural Networks. In Int. Conf. Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013.
[13] Santanu Kumar Dash, Guillermo Suarez-Tangil, Salahuddin Khan, Kimberly Tam, Mansour Ahmadi, Johannes Kinder, and Lorenzo Cavallaro. Droidscribe: Classifying Android Malware Based on Runtime Behavior. In MoST-SPW. IEEE, 2016.
[14] Jesse Davis and Mark Goadrich. The Relationship Between Precision-Recall and ROC Curves. In Proceedings of the 23rd international conference on Machine learning, pages 233–240. ACM, 2006.
[15] Jun Du and Charles X Ling. Active Learning with Human-Like Noisy Oracle. In ICDM. IEEE, 2010.
[16] Tom Fawcett. In vivo spam filtering: a challenge problem for kdd. ACM SIGKDD Explorations Newsletter, 2003.
[17] Giorgio Fumera, Ignazio Pillai, and Fabio Roli. Classification with reject option in text categorisation systems. In Int. Conf. Image Analysis and Processing. IEEE, 2003.
[19] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. MIT press Cambridge, 2016.
[20] Google. VirusTotal, 2004.
[21] Google. Android Security 2017 Year In Review. https://source.android.com/security/reports/Google_Android_Security_2017_Report_Final.pdf, March 2018.
[22] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. Adversarial examples for malware detection. In ESORICS. Springer, 2017.
[23] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu Xing. LEMNA: Explaining Deep Learning based Security Applications. In CCS. ACM, 2018.
[24] David J Hand. Measuring Classifier Performance: a Coherent Alternative to the Area Under the ROC Curve. Machine Learning, 2009.
[25] Haibo He and Edwardo A Garcia. Learning From Imbalanced Data. IEEE TKDE, 2009.
[26] Roberto Jordaney, Kumar Sharad, Santanu Kumar Dash, Zhi Wang, Davide Papini, Ilia Nouretdinov, and Lorenzo Cavallaro. Transcend: Detecting Concept Drift in Malware Classification Models. In USENIX Security, 2017.
[27] Pavel Laskov and Nedim Šrndić. Static Detection of Malicious JavaScript-Bearing PDF Documents. In ACSAC. ACM, 2011.
[28] Sangho Lee and Jong Kim. WarningBird: Detecting Suspicious URLs in Twitter Stream. In NDSS, 2012.
[29] Li Li, Tegawendé Bissyandé, and Jacques Klein. Moonlight-Box: Mining Android API Histories for Uncovering Release-time Inconsistencies. In Symp. on Software Reliability Engineering. IEEE, 2018.
[30] Martina Lindorfer, Stamatis Volanis, Alessandro Sisto, Matthias Neugschwandtner, Elias Athanasopoulos, Federico Maggi, Christian Platzer, Stefano Zanero, and Sotiris Ioannidis. AndRadar: Fast Discovery of Android Applications in Alternative Markets. In DIMVA. Springer, 2014.
[31] Federico Maggi, Alessandro Frossi, Stefano Zanero, Gianluca Stringhini, Brett Stone-Gross, Christopher Kruegel, and Giovanni Vigna. Two Years of Short URLs Internet Measurement: Security Threats and Countermeasures. In WWW. ACM, 2013.
[32] Davide Maiorca, Giorgio Giacinto, and Igino Corona. A Pattern Recognition System for Malicious PDF Files Detection. In Intl. Workshop on Machine Learning and Data Mining in Pattern Recognition. Springer, 2012.
[33] Enrico Mariconti, Lucky Onwuzurike, Panagiotis Andriotis, Emiliano De Cristofaro, Gordon Ross, and Gianluca Stringhini. MaMaDroid: Detecting Android Malware by Building Markov Chains of Behavioral Models. In NDSS, 2017.
[34] Zane Markel and Michael Bilzor. Building a Machine Learning Classifier for Malware Detection. In Anti-malware Testing Research Workshop. IEEE, 2014.
[18] Hugo Gascon, Fabian Yamaguchi, Daniel Arp, and Konrad Rieck. Structural Detection of Android Malware using Embedded Call Graphs. In AISec. ACM, 2013.
[35] Marco Melis, Davide Maiorca, Battista Biggio, Giorgio Giacinto, and Fabio Roli. Explaining Black-box Android Malware Detection. EUSIPCO, 2018.
USENIX Association
28th USENIX Security Symposium    743

[36] Brad Miller, Alex Kantchelian, Michael Carl Tschantz, Sadia Afroz, Rekha Bachwani, Riyaz Faizullabhoy, Ling Huang, Vaishaal Shankar, Tony Wu, George Yiu, et al. Reviewer Integration and Performance Measurement for Malware Detection. In DIMVA. Springer, 2016.
[37] Bradley Austin Miller. Scalable Platform for Malicious Content Detection Integrating Machine Learning and Manual Review. University of California, Berkeley, 2015.
[38] Jose G Moreno-Torres, Troy Raeder, Rocío Alaiz-Rodríguez, Nitesh V Chawla, and Francisco Herrera. A unifying view on dataset shift in classification. Pattern Recognition, 2012.
[39] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-Learn: Machine Learning in Python. JMLR, 2011.
[40] Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney, Johannes Kinder, and Lorenzo Cavallaro. POSTER: Enabling Fair ML Evaluations for Security. In CCS. ACM, 2018.
[41] Babak Rahbarinia, Marco Balduzzi, and Roberto Perdisci. Exploring the Long Tail of (Malicious) Software Downloads. In DSN. IEEE, 2017.
[42] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why Should I Trust You?: Explaining the Predictions of Any Classifier. In KDD. ACM, 2016.
[43] Konrad Rieck, Tammo Krueger, and Andreas Dewald. Cujo: Efficient Detection and Prevention of Drive-By-Download Attacks. In ACSAC. ACM, 2010.
[44] Christian Rossow, Christian J Dietrich, Chris Grier, Christian Kreibich, Vern Paxson, Norbert Pohlmann, Herbert Bos, and Maarten Van Steen. Prudent Practices for Designing Malware Experiments: Status Quo and Outlook. In Symp. S&P. IEEE, 2012.
[45] Sankardas Roy, Jordan DeLoach, Yuping Li, Nic Herndon, Doina Caragea, Xinming Ou, Venkatesh Prasad Ranganath, Hongmin Li, and Nicolais Guevara. Experimental Study with Real-World Data for Android App Security Analysis Using Machine Learning. In ACSAC. ACM, 2015.
[46] Burr Settles. Active Learning Literature Survey. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2012.
[47] Robin Sommer and Vern Paxson. Outside the Closed World: On Using Machine Learning for Network Intrusion Detection. In Symp. S&P. IEEE, 2010.
[48] Gianluca Stringhini, Christopher Kruegel, and Giovanni Vigna. Shady Paths: Leveraging Surfing Crowds to Detect Malicious Web Pages. In CCS. ACM, 2013.
[49] Guillermo Suarez-Tangil, Santanu Kumar Dash, Mansour Ahmadi, Johannes Kinder, Giorgio Giacinto, and Lorenzo Cavallaro. DroidSieve: Fast and Accurate Classification of Obfuscated Android Malware. In CODASPY. ACM, 2017.
[50] Masashi Sugiyama, Neil D Lawrence, Anton Schwaighofer, et al. Dataset Shift in Machine Learning. The MIT Press, 2009.
[51] Gil Tahan, Lior Rokach, and Yuval Shahar. Mal-id: Automatic malware detection using common segment analysis and meta-features. JMLR, 2012.
[52] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR. IEEE, 2011.
[53] Phani Vadrevu, Babak Rahbarinia, Roberto Perdisci, Kang Li, and Manos Antonakakis. Measuring and Detecting Malware Downloads in Live Network Traffic. In ESORICS. Springer, 2013.
[54] Erik van der Kouwe, Dennis Andriesse, Herbert Bos, Cristiano Giuffrida, and Gernot Heiser. Benchmarking Crimes: An Emerging Threat in Systems Security. arXiv preprint, 2018.
[55] Gary M Weiss and Foster Provost. Learning when Training Data Are Costly: The Effect of Class Distribution on Tree Induction. JMLR, 2003.