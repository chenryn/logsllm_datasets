### Figure 9: Shadow Stack Overhead
Figure 9 illustrates the overhead associated with the shadow stack implementation.

### Figure 10: Cumulative Overhead of the First Two Layers with Call Validation
Figure 10 shows the cumulative overhead of the first two layers, including call validation. The results are based on software simulations of hardware solutions, and we can expect much lower values in a hardware implementation.

### Figure 11: Cumulative Overhead of the First Two Layers with a Shadow Stack
Figure 11 presents the cumulative overhead of the first two layers when a shadow stack is used.

### 4) The Overhead of Layer 3
As discussed in Section III-C, false positives can be avoided either at the hardware level or at the compiler level. If Layer 2 is implemented using the call validation strategy from Section III-B, a solution at the compilation level is possible. However, the CET-based implementation of Layer 2 requires hardware interventions. In this section, we will discuss the overhead of a hardware-based implementation of Layer 3.

#### Measurement Methodology
To eliminate false positives at the compiler level, the return instruction is replaced by a sequence of `pop reg` and `jmp reg`. This replacement is necessary only for functions that might be invoked by means other than the execution of a `call` instruction. To estimate the overhead of this replacement, we manually performed it on every function of the twelve programs in the Shootout collection, available in the LLVM test suite. We had to perform 52 replacements, and tail call optimization was not allowed. Figure 12 shows an example of code replacement.

#### Analysis of Results
Figure 13 displays the results of this experiment. All benchmarks ran for more than one second, with the Hash (hsh) benchmark running for 35 seconds, the longest runtime. Each benchmark performed thousands of function calls. Each box in the figure represents the runtime of the modified program divided by the runtime of the original program. A t-test applied to the two populations never yielded a p-score lower than 0.1, indicating that it is statistically difficult to distinguish between the original and modified binaries by observing their runtimes.

### Figure 12: Code Replacement Example
To carry out the experiment described in this section, we compile a program such as (a) into its assembly version (b). Then, we manually replace occurrences of `ret` with the two-instruction sequence seen in (c).

### Figure 13: Runtime Comparison
- **Original vs. Modified Programs**: An average of 1.00 indicates that the original and modified programs have the same runtime.
- **Runtime Ratios**:
  - 1.04
  - 1.03
  - 1.02
  - 1.01
  - 1.00
  - 0.99
  - 0.98

This data suggests that the overhead introduced by the code replacement is minimal and does not significantly affect the overall performance of the programs.