### Figure 12: Cumulative Distribution Function (CDF) for ePVF and PVF Values

The figure presents the CDF for the ePVF and PVF values of registers used in each instruction of the `nw` (left) and `lud` (right) benchmarks. The PVF values for most instructions are clustered around 1, indicating that they do not provide useful information for protection mechanisms based on instruction-level protection.

\[ \text{ePVF}_{\text{inst}} = \frac{\sum_{\text{register in inst}} (\text{ACEBits} - \text{CrashBits})}{\text{Total bits in inst}} \]

### Evaluation Methodology

We evaluate the coverage of the above schemes through fault injection experiments. We focus on five benchmarks—`mm`, `pathfinder`, `hotspot`, `lud`, and `nw`—whose SDC rates were higher than 10% in Figure 9, to better discriminate the effects of the two schemes. Additionally, we run the fault injection campaigns with different inputs than those used to obtain the ePVF values (these new inputs are much larger in size) to ensure stable performance numbers.

### Evaluation Results

Figure 13 shows the SDC rate of the original application (no protection), the SDC rate when using hot-path protection, and the SDC rate when using ePVF-informed protection, given a performance overhead bound of 24%. Overall, we find that ePVF-based protection outperforms hot-path-based protection, reducing the SDC rate from 20% to 7% (geometric mean). In contrast, hot-path-based protection reduces it to about 10%. Thus, ePVF-based protection is, on average, 30% more effective than hot-path-based protection across the benchmarks, demonstrating its superior discriminative power over execution frequencies for protection. Furthermore, ePVF-based protection outperforms hot-path-based protection for all benchmarks except `hotspot`. This exception is due to the presence of many control-flow structures in `hotspot`, which are marked as sensitive by ePVF but do not cause SDCs.

### Discussion

#### Scalability
Scalability is an important issue as most applications will likely generate ACE graphs with billions of vertices. The ACE-graph sampling technique described in §IV-E offers a significant speedup for applications containing repetitive patterns. We believe that scaling to handle larger applications is primarily a matter of good engineering rather than a fundamental barrier for the following reasons:
1. The current ePVF infrastructure, including building/processing the DDG, is implemented in Python. A tuned C/C++ implementation would likely be orders of magnitude faster and consume less memory.
2. The most time-consuming part of the ePVF analysis is running the crash and propagation models, which start from each load/store individually and search along their backward slices. This process is trivially parallelizable, with threads assigned to one backward slice each with minimal coordination required. The work allocated for each thread scales sub-linearly with the size of the graph.
3. If the DDG does not fit in memory, it can be partitioned to support the parallel backward slice exploration suggested above.

#### Sources of Inaccuracy
While ePVF is a closer upper bound than PVF for the SDC rate of an application, it still overestimates the SDC rate in some cases. The main factors contributing to this overestimation are:
1. **Lucky loads**: ePVF assumes that any fault causing a load to deviate from its intended source address (but still within the bounds of the program’s allocated memory) will lead to an SDC. However, prior work has found that this is not always true. For example, the value loaded from the incorrect address may still be correct, especially if the value loaded is 0, as memory typically has large areas initialized to zeroes.
2. **Y-branches**: Y-branches are branches that do not affect the outcome of the application even when the program executes the wrong part of a branch due to a fault. The ePVF analysis assumes that all branches lead to SDCs if flipped. However, only about 20% of branch flips lead to SDCs in practice.
3. **Application-specific correctness checks**: Similar to PVF, the ePVF model considers as ACE bits all bits that lead to visible changes to the application output. Some of these faults may be characterized as benign by application-specific correctness checks (e.g., based on precision thresholds for floating-point computations).

#### Conservativeness
While ePVF may overestimate the SDC rate, it will never underestimate it (barring specific cases). This is because ePVF conservatively labels every non-crash-causing operation as potentially leading to an SDC. Being conservative is important as it can drive decisions about how much state to protect in the worst-case scenario for the application.

### Related Work

There has been considerable work on estimating the error resilience of a program either through fault injection or vulnerability analysis techniques. The main advantage of fault injection is its simplicity and ability to distinguish between different failure outcomes, though it has limited predictive power and is slow. The main advantage of vulnerability analysis is its predictive power and speed, but it does not distinguish between different kinds of failures. This paper explores whether it is possible to combine the advantages of both approaches by building an architecture-neutral vulnerability analysis technique to distinguish different failure outcomes, particularly SDCs. Therefore, we use fault injection to gather the ground truth of the error resilience characteristics of an application and compare it with the results of the ePVF methodology.

- **Biswas et al.** [34] separate the overall AVF of processor structures into SDC AVF and DUE AVF by considering whether bit-level error protection mechanisms such as ECC or parity are enabled in those structures. While DUE is similar to the notion of a crash in this paper, DUE is defined at the hardware level only and does not consider software-level mechanisms.
- **Bronovetsky et al.** [35] use standard machine learning algorithms to predict the vulnerability profiles of different routines under soft errors, focusing on linear algebra applications.
- **Lu et al.** [36] and **Laguna et al.** [37] identify SDC-causing code regions through a combination of static analysis and machine learning. However, their techniques do not provide foundational understanding behind why some faults cause SDCs and others do not.
- **Yu et al.** [38] introduce a novel resilience metric called data vulnerability factor (DVF) to quantify the vulnerability of individual data structures. By combining the DVF of different data structures, the vulnerability of an application can be evaluated. However, this technique requires the program to be written in a domain-specific language, which is restricted in terms of its expressiveness.

### Summary

This paper presents ePVF, a methodology to extend the PVF analysis by distinguishing crash-causing bits from the ACE bits to get a tighter bound on the SDC rate. Our methodology consists of two models: (1) a propagation model to predict the dependent bits of memory address calculations based on a range propagation analysis, and (2) a crash model to predict the platform-specific behavior of program crashes. We implement the ePVF methodology in the LLVM compiler and evaluate its accuracy. The results show that ePVF can predict crashes with high confidence (89% recall and 92% precision on average). Further, ePVF significantly lowers the upper bound of the estimated SDC rate of a program (61% on average) compared to the original PVF. Finally, we present a use case for this methodology: an ePVF-informed selective duplication technique, which leads to 30% lower SDCs than hot-path instruction duplication.

While we have focused on using the ePVF methodology for SDC rate estimation and reduction in software, there are two other potential uses in the future:
1. Determining which architectural structures are more likely to cause SDCs and selectively protecting these structures through hardware techniques such as selective ECC.
2. Using the ePVF methodology to determine the total number of crash-causing bits in the program and inform a fault-tolerance mechanism for crash-causing faults (e.g., checkpointing).

### Acknowledgements

We thank Vilas Sridharan for his insightful feedback on this work. We also thank the reviewers of DSN 2016 for their feedback, which helped improve this work. This work was funded in part by Discovery grants from the Natural Science and Engineering Research Council (NSERC).

### References

[1] C. Constantinescu, “Trends and challenges in VLSI circuit reliability,” in IEEE MICRO, 2003.
[2] T. Karnik and P. Hazucha, “Characterization of soft errors caused by single event upsets in CMOS processes,” Dependable and Secure Computing, IEEE Transactions on, vol. 1, no. 2, April 2004.
[3] L. Tan, S. L. Song, P. Wu, Z. Chen, R. Ge, and D. J. Kerbyson, “Investigating the interplay between energy efficiency and resilience in high-performance computing,” in IPDPS. IEEE, 2015, pp. 786–796.
[4] K. S. Yim, C. Pham, M. Saleheen, Z. Kalbarczyk, and R. Iyer, “Hauberk: Lightweight silent data corruption error detector for GPGPU,” in IPDPS, 2011.
[5] W. Gu, Z. Kalbarczyk, and R. Iyer, “Error sensitivity of the Linux kernel executing on PowerPC G4 and Pentium 4 processors,” in DSN 2003.
[6] B. Atkinson, N. DeBardeleben, Q. Guan, R. Robey, and W. M. Jones, “Fault injection experiments with the Clamr hydrodynamics mini-app,” in 2014 ISSREW.
[7] C. da Lu and D. Reed, “Assessing fault sensitivity in MPI applications,” in Supercomputing, 2004. Proceedings of the ACM/IEEE SC2004.
[8] V. Sridharan and D. Kaeli, “Eliminating microarchitectural dependency from architectural vulnerability,” in HPCA 2009., 2009, pp. 117–128.
[9] C. Lattner and V. Adve, “LLVM: a compilation framework for lifelong program analysis & transformation,” in CGO, ser. CGO ’04, 2004.
[10] J. Wei, A. Thomas, G. Li, and K. Pattabiraman, “Quantifying the accuracy of high-level fault injection techniques for hardware faults,” in DSN, June 2014.
[11] A. Meixner, M. Bauer, and D. Sorin, “Argus: Low-cost, comprehensive error detection in simple cores,” in Microarchitecture, 2007. MICRO 2007. 40th Annual IEEE/ACM International Symposium on, Dec 2007.
[12] M. de Kruijf, S. Nomura, and K. Sankaralingam, “Relax: An architectural framework for software recovery of hardware faults,” in ISCA 14, pp. 497–508.
[13] S. Feng, S. Gupta, A. Ansari, and S. Mahlke, “Shoestring: Probabilistic soft error reliability on the cheap,” SIGPLAN Not., vol. 45, no. 3, Mar.
[14] G. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. August, “SWIFT: Software implemented fault tolerance,” in CGO, 2005, pp. 243–254.
[15] D. S. Khudia and S. A. Mahlke, “Harnessing Soft Computations for Low-Budget Fault Tolerance.” MICRO, pp. 319–330, 2014.
[16] M.-C. Hsueh, T. Tsai, and R. Iyer, “Fault injection techniques and tools,” Computer, vol. 30, no. 4, pp. 75–82, 1997.
[17] D. Stott, B. Floering, D. Burke, Z. Kalbarczpk, and R. Iyer, “Nftape: a framework for assessing dependability in distributed systems with lightweight fault injectors,” in IPDPS 2000, 2000, pp. 91 –100.
[18] J. Aidemark, J. Vinter, P. Folkesson, and J. Karlsson, “Goofi: generic object-oriented fault injection tool,” in DSN, 2001, pp. 83–88.
[19] J. Carreira, H. Madeira, and J. Silva, “Xception: a technique for the experimental evaluation of dependability in modern computers,” Software Engineering, IEEE Transactions on, Feb 1998.
[20] S. K. S. Hari, S. V. Adve, H. Naeimi, and P. Ramachandran, “Relyzer: exploiting application-level fault equivalence to analyze application resiliency to transient faults,” in ASPLOS 2012.
[21] S. Hari, R. Venkatagiri, S. Adve, and H. Naeimi, “Ganges: Gang error simulation for hardware resiliency evaluation,” in ISCA 2014, 2014.
[22] S. Mukherjee, C. Weaver, J. Emer, S. Reinhardt, and T. Austin, “Measuring architectural vulnerability factors,” in IEEE MICRO, vol. 23, no. 6.
[23] H. Cho, S. Mirkhani, C.-Y. Cher, J. Abraham, and S. Mitra, “Quantitative evaluation of soft error injection techniques for robust system design,” in DAC, 2013 50th ACM/EDAC/IEEE, May, pp. 1–10.
[24] J. Wei and K. Pattabiraman, “BLOCKWATCH: Leveraging similarity in parallel programs for error detection,” in DSN, 2012.
[25] Q. Lu, K. Pattabiraman, M. S. Gupta, and J. A. Rivers, “Sdctune: A model for predicting the SDC proneness of an application for configurable protection,” in CASE 2014. New York, New York, USA: ACM Press, 2014, pp. 1–10.
[26] F. Ayatolahi, B. Sangchoolie, R. Johansson, and J. Karlsson, “A study of the impact of single bit-flip and double bit-flip errors on program execution,” in Computer Safety, Reliability, and Security, 2013, vol. 8153, pp. 265–276.
[27] H. Agrawal and J. R. Horgan, “Dynamic program slicing,” in Proceedings of Programming Language Design and Implementation (PLDI), New York, NY, USA, 1990.
[28] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee, and K. Skadron, “Rodinia: A benchmark suite for heterogeneous computing,” in IISWC, ser. IISWC ’09.
[29] I. Karlin, A. Bhatele, J. Keasler, B. L. Chamberlain, J. Cohen, Z. DeVito, R. Haque, D. Laney, E. Luke, F. Wang, D. Richards, M. Schulz, and C. Still, “Exploring traditional and emerging parallel programming models using a proxy application,” in IEEE IPDPS 2013, Boston, USA.
[30] I. Karlin, J. Keasler, and R. Neely, “Lulesh 2.0 updates and changes,” Tech. Rep. LLNL-TR-641973, August 2013.
[31] S. K. S. Hari, S. V. Adve, and H. Naeimi, “Low-cost program-level detectors for reducing silent data corruptions,” in 2012 42nd DSN. IEEE, 2012, pp. 1–12.
[32] J. Cook and C. Zilles, “A characterization of instruction-level error derating and its implications for error detection,” in DSN, 2008.
[33] N. J. Wang, A. Mahesri, and S. J. Patel, “Examining ACE analysis reliability estimates using fault-injection,” in ISCA ’07, 2007.
[34] A. Biswas, P. Racunas, R. Cheveresan, J. Emer, S. S. Mukherjee, and R. Rangan, “Computing architectural vulnerability factors for address-based structures,” SIGARCH Comput. Archit. News, vol. 33, no. 2, May.
[35] G. Bronevetsky, B. de Supinski, and M. Schulz, “A foundation for the accurate prediction of the soft error vulnerability of scientific applications,” in SELSE, 2009.
[36] Q. Lu, K. Pattabiraman, M. S. Gupta, and J. A. Rivers, “Sdctune: A model for predicting the SDC proneness of an application for configurable protection,” in CASE, 2014.
[37] I. Laguna, M. Schulz, D. F. Richards, J. Calhoun, and L. Olson, “IPAS: Intelligent protection against silent output corruption in scientific applications,” ser. CGO 2016, 2016.
[38] L. Yu, D. Li, S. Mittal, and J. S. Vetter, “Quantitatively modeling application resilience with the data vulnerability factor,” in SC, 2014.