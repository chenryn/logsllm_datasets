### Adversarial Example (AE) Detection Methods

Adversarial example (AE) detection methods, such as adversarial training, defensive distillation, and input transformation [20], are widely used to enhance the robustness of machine learning models. In our study, the queried samples do not contain adversarial perturbations. However, we use mainstream methods, such as Projected Gradient Descent (PGD) [37], to generate AEs. Although these AEs are likely to be detected by defensive methods like those mentioned in [20], this is not the primary focus of our study.

### Defense Mechanisms

One possible defense mechanism is to measure the redundancy of queries from a single client, similar to the approach used in DRMI. Generally, the queries generated by DRMI have a much smaller mutual information (MI) value compared to normal samples of the same number, as normal data tend to have more repetitions. However, this method requires counting a large number of queries and establishing a distribution of MI values. In our tests, the defender needs to analyze more than 100 times the number of malicious queries for effective detection, which inevitably leads to significant computational costs. Additionally, this defense becomes even more infeasible when dealing with distributed querying.

### Conclusion

This paper introduces a novel dataset reduction technology based on mutual information, called DRMI, which can be applied in black-box attacks. Our approach accurately measures the overall quality of the dataset, identifying redundancies and repetitions. Compared to other three techniques, DRMI demonstrates superior performance in selecting representative and distinct data for deep neural network (DNN) training. Furthermore, we apply DRMI to reduce the number of queries in model extraction and adversarial attacks. The results show that DRMI effectively reduces the data while maintaining high model accuracy and transferability of adversarial examples.

### Acknowledgements

We thank our shepherd, David Wagner, for his valuable guidance and assistance, and all the anonymous reviewers for their constructive feedback. This work was supported in part by the National Key Research and Development Program of China (Grant No. 2020AAA0107800), NSFC (U1836211, 61902395), Beijing Natural Science Foundation (No. JQ18011), National Top-notch Youth Talents Program of China, Youth Innovation Promotion Association CAS, Beijing Academy of Artificial Intelligence (BAAI), CCF-Tencent Open Fund, and a research grant from Huawei.

### References

[1] USPS Dataset. https://www.kaggle.com/bistaumanga/usps-dataset.

[2] Mutual Information. https://en.wikipedia.org/wiki/Mutual_information, 2019.

[3] PyTorch. https://pytorch.org/, 2020.

[4] Welcome to Foolbox Native. https://foolbox.readthedocs.io/en/latest/, 2020.

[5] Naveed Akhtar and Ajmal S. Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. IEEE Access, 6:14410–14430, 2018.

[6] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square Attack: A query-efficient black-box adversarial attack via random search. CoRR, abs/1912.00049, 2019.

[7] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Exploring the space of black-box attacks on deep neural networks. CoRR, abs/1712.09491, 2017.

[8] Thomas Brunner, Frederik Diehl, Michael Truong-Le, and Alois Knoll. Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks. CoRR, abs/1812.09803, 2018.

[9] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, SP, San Jose, USA, pages 39–57.

[10] Yuxuan Chen, Xuejing Yuan, Jiangshan Zhang, Yue Zhao, Shengzhi Zhang, Kai Chen, and XiaoFeng Wang. Devil’s Whisper: A general approach for physical adversarial attacks against commercial black-box speech recognition devices. In 29th USENIX Security Symposium, August 12-14, 2020, pages 2667–2684.

[11] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior. In NeurIPS 19, Vancouver, Canada, pages 10932–10942, 2019.

[12] Kashyap Chitta, Jose M. Alvarez, Elmar Haussmann, and Clement Farabet. Training data distribution search with ensemble active learning, 2019.

[13] V. Chouvatut, W. Jindaluang, and E. Boonchieng. Training set size reduction in large dataset problems. In 2015 International Computer Science and Engineering Conference (ICSEC), pages 1–5, Nov 2015.

[14] T.M. Cover and J.A. Thomas. Elements of Information Theory. Wiley series in telecommunications. Wiley, 1991.

[15] Terrance DeVries and Graham W. Taylor. Dataset augmentation in feature space. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, Workshop Track Proceedings.

[16] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. Deepstellar: Model-based quantitative analysis of stateful deep learning systems. In 27th ACM Joint Meeting on ESES/FSE, New York, NY, USA, 2019.

[17] Steven Eschrich, Jingwei Ke, Lawrence O. Hall, and Dmitry B. Goldgof. Fast accurate fuzzy clustering through data reduction. IEEE Trans. Fuzzy Systems, 11(2):262–270, 2003.

[18] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov. Property inference attacks on fully connected neural networks using permutation invariant representations. In 2018 ACM SIGSAC Conference on CCS, pages 619–633, Oct. 2018.

[19] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In 3rd ICLR, San Diego, CA, USA, 2015.

[20] Chuan Guo, Mayank Rana, Moustapha Cissé, and Laurens van der Maaten. Countering adversarial images using input transformations. In 6th ICLR, Vancouver, BC, Canada, 2018.

[21] Yiwen Guo, Ziang Yan, and Changshui Zhang. Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3825–3834, 2019.

[22] Jiawei Han, Micheline Kamber, and Jian Pei. Data Mining: Concepts and Techniques. Elsevier, 2012.

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.

[24] Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen He. Towards Security Threats of Deep Learning Systems: A Survey. IEEE Transactions on Software Engineering (TSE), pages 1–28, 2020.

[25] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In 35th International Conference on Machine Learning, ICML 2018, Stockholm, Sweden, pages 2142–2151, 2018.

[26] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. High accuracy and high fidelity extraction of neural networks. In 29th USENIX Security Symposium (USENIX Security 20), pages 1345–1362. USENIX Association, August 2020.

[27] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In 2018 IEEE Symposium on Security and Privacy, San Francisco, USA, pages 19–35.

[28] Uyeong Jang, Xi Wu, and Somesh Jha. Objective metrics and gradient descent algorithms for adversarial examples in machine learning. In 33rd Annual Computer Security Applications Conference, Orlando, FL, USA, pages 262–277.

[29] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N. Asokan. PRADA: Protecting against DNN model stealing attacks. In IEEE European Symposium on Security and Privacy, EuroS&P 2019, Stockholm, Sweden, pages 512–527.

[30] Richard M. Karp. Reducibility among combinatorial problems. In Proceedings of a symposium on the Complexity of Computer Computations, New York, USA, pages 85–103, 1972.

[31] Angelos Katharopoulos and François Fleuret. Not all samples are created equal: Deep learning with importance sampling. In 35th International Conference on Machine Learning, ICML 2018, Stockholm, Sweden.

[32] Alex Krizhevsky. The CIFAR-10 dataset. https://www.cs.toronto.edu/~kriz/cifar.html/.

[33] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, Workshop Track Proceedings.

[34] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, Nov 1998.

[35] Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/.

[36] Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang, Chunming Wu, Bo Li, and Ting Wang. DEEPSEC: A uniform platform for security analysis of deep learning model. In 2019 IEEE Symposium on Security and Privacy, SP, San Francisco, USA, pages 673–690.

[37] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada.

[38] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, pages 86–94.

[39] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C. Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In AISec@CCS 2017, Dallas, TX, USA, pages 27–38.

[40] Seong Joon Oh, Max Augustin, Mario Fritz, and Bernt Schiele. Towards reverse-engineering black-box neural networks. In International Conference on Learning Representations, 2018.

[41] Lucila Ohno-Machado, Hamish S. F. Fraser, and Aleksander Øhrn. Improving machine learning performance by removing redundant cases in medical data sets. In AMIA 1998, Lake Buena Vista, FL, USA.

[42] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box models. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, pages 4954–4963.

[43] Stefanos Ougiaroglou and Georgios Evangelidis. Efficient dataset size reduction by finding homogeneous clusters. In Balkan Conference in Informatics, BCI 2012, Novi Sad, Serbia.

[44] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning: From phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016.

[45] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In ACM AsiaCCS 2017, pages 506–519.

[46] Haekyu Park, Fred Hohman, and Duen Horng Chau. Neuraldivergence: Exploring and understanding neural networks by comparing activation distributions. CoRR, abs/1906.00332, 2019.

[47] Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher Ré. Learning to compose domain-specific transformations for data augmentation. In NeurIPS 2017, Long Beach, USA, pages 3236–3246.

[48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211–252, 2015.

[49] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! Targeted clean-label poisoning attacks on neural networks. In NeurIPS 2018, Montréal, Canada, pages 6106–6116.

[50] Mahmood Sharif, Lujo Bauer, and Michael K. Reiter. On the suitability of lp-norms for creating and preventing adversarial examples. In CVPR Workshops 2018, Salt Lake City, UT, USA, pages 1605–1613.

[51] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy, San Jose, USA, pages 3–18.

[52] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In 34th ICML, volume 70, pages 3145–3153, 2017.

[53] Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, and J. Zico Kolter. Black-box adversarial attacks with Bayesian optimization. CoRR, abs/1909.13857.

[54] Congzheng Song and Vitaly Shmatikov. Overlearning reveals sensitive attributes. In 8th ICLR, Addis Ababa, Ethiopia, April 26-30, 2020.

[55] Fnu Suya, Jianfeng Chi, David Evans, and Yuan Tian. Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries. In 29th USENIX Security Symposium, 2020.

[56] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR 2016, Las Vegas, NV, USA, pages 2818–2826.

[57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In 2nd ICLR, 2014, Banff, AB, Canada.

[58] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction APIs. In 25th USENIX Security Symposium, 2016, Austin, TX, USA, pages 601–618.

[59] Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. In 31st AAAI Conference on Artificial Intelligence, Honolulu, Hawaii, USA, 2019, pages 742–749.

[60] Binghui Wang and Neil Zhenqiang Gong. Stealing hyperparameters in machine learning. In 2018 IEEE Symposium on Security and Privacy (SP), San Francisco, California, USA, pages 36–52.

[61] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A. Gunter. CommanderSong: A systematic approach for practical adversarial voice recognition. In 27th USENIX Security Symposium, USENIX Security 2018, Baltimore, USA, pages 49–64.

[62] L.A. Zadeh. Similarity relations and fuzzy orderings. Information Sciences, 3(2):177 – 200, 1971.

[63] Mingming Zha, Guozhu Meng, Chaoyang Lin, Zhe Zhou, and Kai Chen. ROLMA: A practical adversarial attack against deep learning-based LPR systems. In Information Security and Cryptology (Inscrypt), pages 4701–4708, Dec 2019.

[64] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, and Kai Chen. Seeing isn’t believing: Towards more robust adversarial attack against real-world object detectors. In ACM CCS 2019, London, UK, pages 1989–2004.

[65] Jian Zheng, Wei Yang, and Xiaohua Li. Training data reduction in deep neural networks with partial mutual information based feature selection and correlation matching based active learning. In IEEE ICASSP 2017, New Orleans, LA, USA.