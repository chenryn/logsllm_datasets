# Apparent Availability and Revocation Analysis

The following data points represent the apparent availability and revocation rates of operations in a distributed system under different partition durations:

- **Apparent Availability:**
  - 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.11, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01

- **Partition Duration [s]:**
  - 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20

## Figures

### Figure 7: Apparent Availability
- **Continuous Service (CS)**
- **Stop-the-World (STW)**
- **Pessimistic**

### Figure 8: Revocations over Provisionally Accepted Operations
- **Continuous Service (CS)**
- **Stop-the-World (STW)**

## Analysis

### Conflict and Integrity Constraints
In our model, conflicts arise only when integrity constraints are violated. These violations can occur due to concurrent updates, but not all concurrent updates necessarily lead to conflicts. In Figure 8, as the partition duration increases, the ratio of revoked operations decreases. This counter-intuitive trend is explained by the nature of the synthetic application, where two partitions perform similar client calls. Thus, an operation applied in one partition is likely to be compatible with changes in the other. As the partition duration lengthens, the number of operations performed increases, reducing the risk of different types of operations in the two partitions. The confidence intervals for these measurements are within 6.9%.

### Effect of Load
Previous experiments were conducted with a constant arrival rate of 120 operations per second. To analyze the effect of load, we plotted the reconciliation duration against the load in Figure 9. The 95% confidence intervals are within 1.6% for all measurement points. The handling rate for this experiment was 300 actions per second. The continuous service protocol suffers more under heavy load, especially as it approaches the maximum load. However, this does not reduce its apparent availability, as seen in Figure 5. The unavailability period for the CS protocol is only during the time between receiving a stop message from the reconciliation manager and the installation of the new state.

## Related Work

### Reconciliation After Network Partitions
- **CS Protocol:** Recently formalized using timed I/O automata [1].
- **Bayou [22]:** A distributed storage system for mobile environments, which allows updates in a partitioned system but has limitations with system-wide integrity constraints.
- **Babaoglu et al. [3]:** Propose a method for shared state management in partitionable systems, suggesting tentative writes that can be undone if conflicts occur.
- **Moser et al. [15]:** Designed a fault-tolerant CORBA extension for node crashes and network partitions, with a reconciliation scheme that transfers primary states to secondaries.
- **Yu and Vahdat [25]:** Use consistency units (conits) to specify bounds on allowed inconsistency, though their system does not support partitioning.
- **Lippe et al. [14]:** Order operation logs to avoid conflicts but require enumeration and comparison of a large set of operation sequences.
- **IceCube [13, 18]:** Orders operations to achieve a consistent final state but does not fully address multi-object integrity constraints.
- **Phatak et al. [17]:** Provide reconciliation through multiversioning or a client-provided function, but snapshot isolation is more pessimistic than our approach.

## Conclusions and Future Work

For network partition faults in distributed systems, optimistic replication offers better availability even with data constraints. We have identified additional metrics (e.g., number of accepted operations, proportion of revoked operations) to evaluate these systems. Our results show that for long partition durations, the continuous service protocol provides the best performance in terms of apparent availability and number of applied operations. For some applications, the risk of revoking previously accepted operations can decrease with longer partition durations.

Future work includes evaluating the protocol as a CORBA extension to make it partition-tolerant, measuring latency, and extending it to more dynamic environments with frequent partitions and changing network topologies. The current implementation sends the entire state to update replicas, which is impractical for large states. A modification to send increments representing changes to the state is being considered.

## Acknowledgments

This work was supported by the FP6 IST project DeDiSys on Dependable Distributed Systems. The second author was partially supported by the University of Luxembourg.

## References

[1] M. Asplund and S. Nadjm-Tehrani. Formalising reconciliation in partitionable networks with distributed services. In M. Butler, C. Jones, A. Romanovsky, and E. Troubitsyna, editors, Rigorous Development of Complex Fault-Tolerant Systems, volume 4157 of Lecture Notes in Computer Science, pages 37–58. Springer-Verlag, 2006.

[2] M. Asplund and S. Nadjm-Tehrani. Post-partition reconciliation protocols for maintaining consistency. In Proceedings of the 21st ACM/SIGAPP symposium on Applied computing, April 2006.

[3] Ö. Babaoglu, A. Bartoli, and G. Dini. Enriched view synchrony: A programming paradigm for partitionable asynchronous distributed systems. IEEE Trans. Comput., 46(6):642–658, 1997.

[4] G. Badishi, G. Caronni, I. Keidar, R. Rom, and G. Scott. Deleting files in the Celeste peer-to-peer storage system. In SRDS’06: Proceedings of the 25th IEEE Symposium on Reliable Distributed Systems, October 2006.

[5] P. A. Bernstein, V. Hadzilacos, and N. Goodman. Concurrency control and recovery in database systems. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1987.

[6] S. Beyer, M. Bañuls, P. Galdámez, J. Osrael, and F. D. Muñoz-Escobar. Increasing availability in a replicated partitionable distributed object system. In Proceedings of the Fourth International Symposium on Parallel and Distributed Processing and Applications (ISPA’2006). Springer–Verlag, 2006.

[7] M. Cukier, J. Ren, C. Sabnis, D. Henke, J. Pistole, W. H. Sanders, D. E. Bakken, M. E. Berman, D. A. Karr, and R. E. Schantz. AQUA: An adaptive architecture that provides dependable distributed objects. In SRDS ’98: Proceedings of the 17th IEEE Symposium on Reliable Distributed Systems, page 245, Washington, DC, USA, 1998. IEEE Computer Society.

[8] S. B. Davidson, H. Garcia-Molina, and D. Skeen. Consistency in a partitioned network: a survey. ACM Comput. Surv., 17(3):341–370, 1985.

[9] DeDiSys. European IST FP6 DeDiSys Project. http://www.dedisys.org, 2006.

[10] C. Ferdean and M. Makpangou. A generic and flexible model for replica consistency management. In ICDCIT, pages 204–209, 2004.

[11] J. Gray, P. Helland, P. O’Neil, and D. Shasha. The dangers of replication and a solution. In SIGMOD ’96: Proceedings of the 1996 ACM SIGMOD international conference on Management of data, pages 173–182, New York, NY, USA, 1996. ACM Press.

[12] V. Hadzilacos and S. Toueg. Fault-tolerant broadcasts and related problems. In Distributed systems, chapter 5, pages 97–145. ACM Press, Addison-Wesley, 2nd edition, 1993.

[13] A.-M. Kermarrec, A. Rowstron, M. Shapiro, and P. Druschel. The IceCube approach to the reconciliation of divergent replicas. In PODC ’01: Proceedings of the twentieth annual ACM symposium on Principles of distributed computing, pages 210–218, New York, NY, USA, 2001. ACM Press.

[14] E. Lippe and N. van Oosterom. Operation-based merging. In SDE 5: Proceedings of the fifth ACM SIGSOFT symposium on Software development environments, pages 78–87, New York, NY, USA, 1992. ACM Press.

[15] L. E. Moser, P. M. Melliar-Smith, and P. Narasimhan. Consistent object replication in the Eternal System. Theor. Pract. Object Syst., 4(2):81–92, 1998.

[16] P. Narasimhan, L. E. Moser, and P. M. Melliar-Smith. Replica consistency of CORBA objects in partitionable distributed systems. Distributed Systems Engineering, 4(3):139–150, 1997.

[17] S. H. Phatak and B. Nath. Transaction-centric reconciliation in disconnected client-server databases. Mob. Netw. Appl., 9(5):459–471, 2004.

[18] N. Preguica, M. Shapiro, and C. Matheson. Semantics-based reconciliation for collaborative and mobile environments. Lecture Notes in Computer Science, 2888:38–55, October 2003.

[19] Y. Saito and M. Shapiro. Optimistic replication. ACM Comput. Surv., 37(1):42–81, 2005.

[20] D. P. Siewiorek and R. S. Swarz. Reliable computer systems (3rd ed.): design and evaluation. A. K. Peters, Ltd., Natick, MA, USA, 1998.

[21] D. Szentivanyi and S. Nadjm-Tehrani. Middleware support for fault tolerance. In Q. Mahmoud, editor, Middleware for Communications. John Wiley & Sons, 2004.

[22] D. B. Terry, M. M. Theimer, K. Petersen, A. J. Demers, M. J. Spreitzer, and C. H. Hauser. Managing update conflicts in Bayou, a weakly connected replicated storage system. In SOSP ’95: Proceedings of the fifteenth ACM symposium on Operating systems principles, pages 172–182, New York, NY, USA, 1995. ACM Press.

[23] A.-I. Wang, P. L. Reiher, R. Bagrodia, and G. H. Kuenning. Understanding the behavior of the conflict-rate metric in optimistic peer replication. In DEXA ’02: Proceedings of the 13th International Workshop on Database and Expert Systems Applications, pages 757–764, Washington, DC, USA, 2002. IEEE Computer Society.

[24] H. Ying Tyan. Design, realization and evaluation of a component-based software architecture for network simulation. PhD thesis, Department of Electrical Engineering, Ohio State University, 2001.

[25] H. Yu and A. Vahdat. Design and evaluation of a conit-based continuous consistency model for replicated services. ACM Trans. Comput. Syst., 20(3):239–282, 2002.