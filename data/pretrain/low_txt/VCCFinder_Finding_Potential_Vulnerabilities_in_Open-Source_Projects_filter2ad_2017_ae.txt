# Table 5: Confusion Matrix of the Tools by Top X Commits
| T | F |
|---|---|
| 0.00 | 0.03 |
| 0.06 | 0.26 |
| 0.40 | 0.48 |

**Table 5: Confusion matrix of the tools by top X commits. T: True, F: False, P: Positive, N: Negative.**

## Qualitative Take-Aways from Developing and Evaluating VCCFinder

In addition to the quantitative results, we present some qualitative insights gained during the development and evaluation of VCCFinder. These insights can be valuable even without using the tool. While some of these findings confirm well-known beliefs, our machine-learning approach not only validated these beliefs with quantitative data but also generated new insights.

### Error Handling is Challenging

Our analysis of the features learned by the SVM for classifying Vulnerable Code Commits (VCCs) revealed that the adage "gotos considered harmful" [16] still holds true today. The presence of keywords like `goto` and jump labels such as `out:` and `error:` increases the likelihood of vulnerable code. Additionally, the SVM flags returning error values such as `-EINVAL` as potentially dangerous. Combined with `gotos`, these are common C mechanisms for error and exception handling. Unlike Dijkstra's argument that `gotos` lead to unreadable code, in our context, `gotos` are considered harmful because they frequently occur in an error-handling context. Thus, our SVM gives exception and error-handling code a higher potential vulnerability ranking. This effect can be explained by the fact that it is easy to miss some cases in exception handling, leading to vulnerabilities (e.g., Apple's `goto fail` bug in their TLS implementation [12]).

### Variable Usage and Memory Management

When examining highly ranked features of the SVM, we noticed that certain memory management constructs and variable usage patterns are associated with a higher vulnerability ranking. For instance, frequent use of `sizeof(struct, a high usage of sizeof in general, len, and length` as variable names occurred more often in vulnerable commits. Additionally, specific variable names such as `buf, net, socket, and sk` were more common in VCCs. While these keywords and variables alone do not cause vulnerabilities, they may indicate more critical areas of the code.

### New Contributors and Vulnerability Risk

We found that new contributors, defined as those with less than 1% of all commits in a given project, are about five times more likely to commit a vulnerability compared to frequent contributors. Specifically, new contributors authored 470 out of 95,621 VCCs (0.49%), while frequent contributors authored only 244 out of 255,074 VCCs (0.10%) (Pearson’s χ²: p < 0.0001). While this finding is not surprising, quantifying this risk can help projects implement more stringent review policies for new contributors.

### Final Thoughts

The evaluation and take-aways show that commits can be flagged as VCCs for a myriad of reasons, which can be code-based, metadata-based, or a combination of both. Our main recommendation is to use VCCFinder to classify potentially vulnerable commits to prioritize reviews. However, there are also general recommendations that can be extracted from the classifier results.

## Limitations

Our approach has several limitations. We selected 66 open-source projects written in C or C++ that had at least one CVE, varying in the number of contributors, commits, and governance. We believe that applying our results to other C or C++ projects should not threaten the validity. However, we cannot predict how VCCFinder will perform on projects that have not yet received any CVEs. For generalization to other programming languages, feature extraction and training will need to be redone per language to avoid mis-training based on syntactic differences.

We used a heuristic to map CVEs to VCCs, and our manual analysis of 15% of these mappings showed an error rate of 3.1%. This error rate must be taken into account by any project building on this dataset. Additionally, it is unknown how many unknown vulnerabilities are contained in our annotated database, so our true positives must be considered a lower bound, and the false positives an upper bound. Despite this, VCCFinder significantly outperforms Flawfinder, making it unlikely that the outcome would change.

While VCCFinder can automatically spot vulnerability-contributing commits with high precision, this alone does not ensure that underlying vulnerabilities will be uncovered. Significant work and expertise are still necessary to audit commits for potential security flaws. However, our approach reduces the amount of code to inspect, thereby increasing the effectiveness of code audits.

## Conclusion

In this paper, we present and evaluate VCCFinder, an approach to improve code audits by combining code-metric analysis with metadata gathered from code repositories using machine-learning techniques. Our results show that VCCFinder significantly outperforms Flawfinder. We created a large test database containing 66 C and C++ projects with 170,860 commits to evaluate and compare our approach. Training our classifier on data up until 2010 and testing it against data from 2011 to 2014, VCCFinder produced 99% fewer false positives than Flawfinder, detecting 53 of the 219 known vulnerabilities and only producing 36 false positives compared to Flawfinder’s 5,460 false positives.

To enable future research, we will release our annotated VCC database and results. The community currently lacks such a baseline, and we hope to spur more comparable research in this domain. We see a lot of interesting future work, and while the results are already significantly better than Flawfinder, we believe we have only begun to scratch the surface of what can be achieved by combining different features.

## References

[1] Clang static analyzer. http://clang-analyzer.llvm.org/. Accessed: 2015-05-08.
[2] Trinity: A Linux system call fuzzer. http://codemonkey.org.uk/projects/trinity/. Accessed: 2015-05-08.
[3] Valgrind. http://valgrind.org/. Accessed: 2015-05-08.
[4] CodeSonar R(cid:13) | GrammaTech static analysis. https://www.grammatech.com/codesonar/, visited August, 2015.
[5] Coverity Scan — static analysis. https://scan.coverity.com/, visited August, 2015.
[6] HP Fortify. https://www.hpfod.com/, visited August, 2015.
[7] IBM Security AppScan Source. https://www.ibm.com/software/products/en/appscan-source/, visited August, 2015.
[8] PREfast analysis tool. https://msdn.microsoft.com/en-us/library/ms933794.aspx, visited January, 2015.
[9] Rough auditing tool for security (RATS). https://code.google.com/p/rough-auditing-tool-for-security/, visited January, 2015.
[10] Splint – annotation-assisted lightweight static checking. http://splint.org/, visited January, 2015.
[11] S. Bandhakavi, S. T. King, P. Madhusudan, and M. Winslett. VEX: Vetting browser extensions for security vulnerabilities. In USENIX Security Symposium, volume 10, pages 339–354, 2010.
[26] S. Neuhaus, T. Zimmermann, C. Holler, and A. Zeller. Predicting vulnerable software components. In Proceedings of the 14th ACM conference on Computer and communications security, pages 529–540. ACM, 2007.
[27] K. Rieck, C. Wressnegger, and A. Bikadorov. Sally: A tool for embedding strings in vector spaces. Journal of Machine Learning Research (JMLR), 13(Nov):3247–3251, Nov. 2012.
[28] A. Sadeghi, N. Esfahani, and S. Malek. Mining the categorized software repositories to improve the analysis of security vulnerabilities. In Fundamental Approaches to Software Engineering, pages 155–169. Springer, 2014.
[29] G. Salton. Mathematics and information retrieval. Journal of Documentation, 35(1):1–29, 1979.
[30] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, 1986.
[12] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs. In OSDI, volume 8, pages 209–224, 2008.
[31] R. Scandariato, J. Walden, A. Hovsepyan, and W. Joosen. Predicting vulnerable software components via text mining. Software Engineering, IEEE Transactions on, 40(10):993–1006, Oct 2014.
[13] R.-Y. Chang, A. Podgurski, and J. Yang. Discovering neglected conditions in software by mining dependence graphs. Software Engineering, IEEE Transactions on, 34(5):579–596, Sept 2008.
[14] C. Y. Cho, D. Babic, P. Poosankam, K. Z. Chen, E. X. Wu, and D. Song. MACE: Model-inference-assisted concolic exploration for protocol and vulnerability discovery. In USENIX Security Symposium, pages 139–154, 2011.
[15] J. Dahse and T. Holz. Static detection of second-order vulnerabilities in web applications. In 23rd USENIX Security Symposium (USENIX Security 14), pages 989–1003, San Diego, CA, Aug. 2014. USENIX Association.
[16] E. W. Dijkstra. Letters to the editor: go to statement considered harmful. Communications of the ACM, 11(3):147–148, 1968.
[17] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research (JMLR), 9:1871–1874, 2008.
[18] J. Graylin, J. E. Hale, R. K. Smith, H. David, N. A. Kraft, W. Charles, et al. Cyclomatic complexity and lines of code: empirical evidence of a stable linear relationship. Journal of Software Engineering and Applications, 2(03):137, 2009.
[19] M. H. Halstead. Elements of software science. Elsevier computer science library : operational programming systems series. North-Holland, New York, NY, 1977.
[20] C. Holler, K. Herzig, and A. Zeller. Fuzzing with code fragments. In Presented as part of the 21st USENIX Security Symposium (USENIX Security 12), pages 445–458, Bellevue, WA, 2012. USENIX.
[21] S. Kim, E. J. Whitehead Jr, and Y. Zhang. Classifying software changes: Clean or buggy? Software Engineering, IEEE Transactions on, 34(2):181–196, 2008.
[22] T. J. McCabe. A complexity measure. Software Engineering, IEEE Transactions on, (4):308–320, 1976.
[23] A. Meneely and O. Williams. Interactive churn metrics: socio-technical variants of code churn. SIGSOFT Softw. Eng. Notes, 37(6):1–6, Nov. 2012.
[24] A. Meneely, H. Srinivasan, A. Musa, A. Rodriguez Tejeda, M. Mokary, and B. Spates. When a patch goes bad: Exploring the properties of vulnerability-contributing commits. In Empirical Software Engineering and Measurement, 2013 ACM / IEEE International Symposium on, pages 65–74, Oct 2013.
[25] A. Meneely, A. C. R. Tejeda, B. Spates, S. Trudeau, D. Neuberger, K. Whitlock, C. Ketant, and K. Davis. An empirical investigation of socio-technical code review metrics and security vulnerabilities. In Proceedings of the 6th International Workshop on Social Software Engineering, SSE 2014, pages 37–44. ACM, 2014.
[32] J. ´Sliwerski, T. Zimmermann, and A. Zeller. When do changes induce fixes? ACM Sigsoft Software Engineering Notes, 30(4):1–5, 2005.
[33] H. W. Wendt. Dealing with a common problem in social science: A simplified rank-biserial coefficient of correlation based on the U statistic. European Journal of Social Psychology, 2(4):463–465, 1972.
[34] D. A. Wheeler. Flawfinder. http://www.dwheeler.com/flawfinder/, visited January, 2015.
[35] D. Wijayasekara, M. Manic, J. L. Wright, and M. McQueen. Mining bug databases for unidentified software vulnerabilities. In Human System Interactions (HSI), 2012 5th International Conference on, pages 89–96. IEEE, 2012.
[36] F. Yamaguchi, C. Wressnegger, H. Gascon, and K. Rieck. Chucky: Exposing missing checks in source code for vulnerability discovery. In Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security, pages 499–510. ACM, 2013.
[37] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck. Modeling and discovering vulnerabilities with code property graphs. In Security and Privacy (SP), 2014 IEEE Symposium on. IEEE, 2014.
[38] T. Zimmermann, N. Nagappan, and L. Williams. Searching for a needle in a haystack: Predicting security vulnerabilities for Windows Vista. In Software Testing, Verification and Validation (ICST), 2010 Third International Conference on, pages 421–428. IEEE, 2010.

## Appendix A: List of Repositories

We used the following list of repositories: Portspoof, GnuPG, Kerberos, PHP, MapServer, HHVM, Mozilla Gecko, Quagga, libav, Libreswan, Redland Raptor RDF syntax library, charybdis, Jabberd2, ClusterLabs pacemaker, bdwgc, pango, qemu, glibc, OpenVPN, torque, curl, jansson, PostgreSQL, corosync, tinc, FFmpeg, nedmalloc, mosh, trojita, inspircd, nspluginwrapper, cherokee webserver, openssl, libfep, quassel, polarssl, radvd, tntnet, Android Platform Bionic, uzbl, LibRaw, znc, nbd, Pidgin, V8, SpiderLabs ModSecurity, file, graphviz, Linux Kernel, libtiﬀ, ZRTPCPP, taglib, suhosin, Phusion passenger, monkey, memcached, lxc, libguestfs, libarchive, Beanstalkd, Flac, libX11, Xen, libvirt, Wireshark, and Apache HTTPD.