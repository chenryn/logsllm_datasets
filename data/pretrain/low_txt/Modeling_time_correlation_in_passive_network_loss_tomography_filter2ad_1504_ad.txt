### Experiment B.1: Performance of Different Inference Approaches in the ON-OFF Loss Model

In Figure 9, we present the performance of different inference approaches under the ON-OFF loss model. Specifically, we compare the error rates of three methods: `est.equal`, `est.self`, and `est.perfect`.

- **Figure 9(a) (No Leaf Loss, Common Link Loss Rate ∼2%)**: 
  - When there is no leaf loss, both `est.equal` and `est.self` exhibit lower error rates compared to `est.perfect`. This is because `est.equal` and `est.self` inherently account for on-off transitions in the loss model between two packets, which `est.perfect` does not.
  
- **Figure 9(b) (With Leaf Losses, Common Link Loss Rate ∼2%)**:
  - With leaf losses, `est.equal` and `est.self` still outperform `est.perfect`. However, `est.equal` has a higher error rate than `est.self` when the number of leaf branches is small. Conversely, `est.equal` performs better than `est.self` when the number of leaf branches is large. This observation aligns with our model simulation results (see Experiment A.2 in Section VI).
  - Both `est.equal` and `est.self` maintain robust performance with respect to the number of leaf branches, with `est.equal` keeping its error rate within 10%.

- **Figures 9(c) and 9(d) (Common Link Loss Rate ∼5%)**:
  - When the common link has a higher loss rate (5%), all approaches show smaller error rates compared to Figures 9(a) and 9(b), where the common link loss rate is 2%. This is expected, as end-to-end losses are more likely to occur at the common link.
  - `est.equal` and `est.self` continue to outperform `est.perfect`, with `est.equal` maintaining low error rates (within 4.2%) regardless of the number of leaf branches.

### Experiment B.2: Performance of Different Inference Approaches in the QUEUE Loss Model

In this experiment, we use the QUEUE loss model, where each node is associated with a finite-buffer drop-tail queue. We inject large UDP bursts to cause queue overflows and packet losses, following an exponential on-off distribution.

- **Figure 10(a) and 10(b) (Common Link Loss Rate ∼2%)**:
  - The error rates for all inference approaches in the QUEUE model range from 10-20%, which are higher than those in the ON-OFF model.
  - Similar to Experiment B.1, in the with-leaf-losses cases, `est.equal` introduces higher error rates when the number of leaf branches is small but outperforms the other two approaches when the number of leaf branches increases.

- **Figures 10(c) and 10(d) (Common Link Loss Rate ∼5%)**:
  - The same trends observed in Figures 10(a) and 10(b) hold, with `est.equal` and `est.self` outperforming `est.perfect`.

### Experiment B.3: Performance Under Skewed Traffic Distribution

We evaluate the performance of the inference approaches under skewed traffic loads, where the traffic in the leaf branches is uneven.

- **Figure 11**:
  - As the probability \( \beta \) of selecting Group 1 increases, the performance of both `est.self` and `est.perfect` degrades significantly, although `est.self` is slightly better.
  - `est.equal` remains robust to skewed traffic loads by assuming the same end-to-end loss rates in all leaf branches. This robustness is consistent across different numbers of leaf branches and for the QUEUE loss model.

### Experiment B.4: Large Tree Topology

We now evaluate the inference algorithms on a large four-level tree topology, as shown in Figure 12, which mimics a wireless data architecture. The root node has a bandwidth of 5 Mb/s, and other links have a bandwidth of 1 Mb/s, with a propagation delay of 5 ms.

- **Figure 12**:
  - The topology has four levels with degrees 1, 4, 4, and 10, respectively.
  - Each TCP flow sends traffic from the root to one of the leaf branches in level 4. We focus on inferring the loss rates of the leftmost links in non-leaf levels (levels 1 to 3) using the ON-OFF loss model.
  - For each link of interest, we generate an actual loss rate of approximately 2-3%.
  - We obtain estimates over 30 simulation runs, each with a simulation time of 300 seconds to gather more samples.

These experiments provide insights into the performance and robustness of different inference approaches under various loss models and traffic conditions.