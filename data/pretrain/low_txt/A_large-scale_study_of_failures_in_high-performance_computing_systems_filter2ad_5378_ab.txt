# Systems Analysis and Failure Breakdown

## 1. System Components
- **Hardware**
- **Software**
- **Network**
- **Environment**
- **Human Factors**
- **Unknown**

### Figure 1
- **(a)**: Breakdown of failures by root cause for systems D, E, F, G, and H, with aggregate statistics across all systems (A–H).
- **(b)**: Breakdown of downtime by root cause for the same systems.

## 2. Data Collection and Quality
Failure records are created by system administrators. If the root cause is identified, it is documented; otherwise, it is marked as "Unknown." Follow-up meetings are held to determine the root cause if initially unknown, and the failure record is updated accordingly.

### Implications of Data Collection
1. **Difference from Error Logs**: This data differs from typical error logs, which automatically track all exceptional events, often with multiple entries for the same event.
2. **Data Quality**: The accuracy depends on the administrators' reporting. Potential issues include underreporting and misreporting of root causes. However, underreporting is less of a concern due to automated monitoring and multi-person reporting. Misdiagnosis is minimized by highly trained staff and advanced technology.

## 3. Methodology
We use three key metrics to characterize empirical distributions:
- **Mean**
- **Median**
- **Squared Coefficient of Variation (C²)**: A measure of variability, defined as the squared standard deviation divided by the squared mean. It allows comparison across distributions with different means.

We also evaluate the empirical cumulative distribution function (CDF) and fit it to four common probability distributions in reliability theory:
- **Exponential**
- **Weibull**
- **Gamma**
- **Lognormal**

We use maximum likelihood estimation to parameterize these distributions and assess the goodness of fit through visual inspection and the negative log-likelihood test. We prefer simpler distributions when the fit is adequate.

## 4. Root Cause Breakdown
We analyze the high-level root cause categories:
- **Human**
- **Environment**
- **Network**
- **Software**
- **Hardware**
- **Unknown**

### Figure 1
- **(a)**: Percentage of failures in each category, showing that hardware is the largest contributor (30% to 60%), followed by software (5% to 24%). Type D systems have nearly equal contributions from hardware and software.
- **(b)**: Total downtime per system broken down by root cause, with similar trends to the frequency breakdown.

### Detailed Root Causes
- **Memory Failures**: Significant in all systems, accounting for more than 10% of all failures, and over 25% in systems F and H.
- **Software Failures**: Vary by system, with specific issues like parallel file systems, scheduler software, and operating systems being prominent.

## 5. Analysis of Failure Rates

### 5.1 Failure Rate as a Function of System and Node
- **System-Level Variability**: Annual failure rates vary widely, from 17 to 1159 failures per year. Normalized by the number of processors, the variability decreases, especially among systems of the same type.
- **Node-Level Variability**: Some nodes experience significantly higher failure rates, possibly due to different workloads. For example, nodes 21–23 in system 20, used for visualization, account for 20% of all failures despite making up only 6% of the nodes.

### Figure 3
- **(a)**: Number of failures per node for system 20.
- **(b)**: CDF of failures per node, fitted with Poisson, normal, and lognormal distributions. The Poisson distribution is a poor fit, while the normal and lognormal distributions fit better.

### 5.2 Failure Rate at Different Time Scales
- **Lifetime Trends**: Failure rates typically follow one of two shapes:
  - **Initial High, Then Decreasing**: Common in systems of type E and F, where initial high rates drop as bugs are fixed and administrators gain experience.
  - **Stable or Increasing**: Less common but observed in some systems.

This analysis provides insights into the factors contributing to system failures and helps in optimizing system reliability and recovery mechanisms.

---

**Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06)  
0-7695-2607-1/06 $20.00 © 2006 IEEE  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore. Restrictions apply.**