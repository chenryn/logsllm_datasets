### Detailed Analysis and Discussion

#### Comparison of Schemes
The two schemes exhibit similar Instruction Per Cycle (IPC) values before the addition of any spare elements, which is consistent with previous research discussed in Section 2. Specifically, REESE's IPC is only 11-16% lower than the baseline without any spare elements. When spare elements are added, the difference in IPC decreases from an average of 14.0% to 8.0% across the hardware configurations shown in the earlier figures.

#### Performance Variability
The graphs also highlight that the performance of some programs can be erratic and unpredictable. For instance, ijpeg typically has a higher baseline IPC compared to REESE, while Vortex has a lower baseline IPC than REESE before adding spare elements. These unpredictable results stem from complex interactions within the instruction stream, underscoring the importance of averaging performance across a wide variety of programs before implementing REESE on a larger scale.

#### Cycle Time Considerations
Cycle time, which depends on implementation technology, is another critical factor. Our simulation model could not account for potential cycle time dilation. However, the R-stream Queue operates in parallel with the processor pipeline, and the additional result comparison and forwarding hardware should not significantly burden the clock cycle.

#### Impact of Memory Ports
Figure 6 summarizes the previous results, clearly showing that the addition of memory ports significantly improves REESE's performance. Implementing REESE on a system with four or more memory ports is more feasible. Nevertheless, adding these ports increases cost and complexity more than adding integer Arithmetic Logic Units (ALUs).

#### Scalability with Additional Hardware
According to our results, REESE performs better as more hardware is added to the system. To test this, we increased the size of the Register Update Unit (RUU) and ran more simulations. Figure 7 shows the results for cases where the RUU was increased to 64 and even 256 entries, with the Load Store Queue (LSQ) always set to half the RUU size. We adjusted the RUU because it appeared to be a bottleneck for quick execution and to compare the results of adding functional units alongside a large RUU.

The results indicate that the difference between the Baseline system and REESE remains at approximately 15% when only the RUU size is increased. However, adding additional functional units reduces this difference to about 1.5%. This confirms that adding spare functional units does not significantly impact performance, especially when a large number of functional units are present. The figure also demonstrates how adding just two integer ALUs can dramatically improve REESE's performance.

### Discussion and Conclusions

REESE is an efficient implementation of time redundancy in a microprocessor pipeline, designed to detect soft errors with minimal spare elements. With a small amount of overall hardware, REESE shows a 14% performance decrease compared to the baseline. Simulations of systems with more hardware reduce this difference to about 12%.

Adding two spare integer ALUs to the REESE configuration further reduces the performance gap to 8.2% for simpler systems and 1.5% for more complex systems. As the simulated system grows, the practicality of REESE becomes increasingly dependent on the size of the register update unit and the total number of functional units, unlike the baseline processor.

Our simulations suggest that achieving zero performance degradation with time-redundant instruction execution is challenging but possible as the amount of hardware increases. This indicates that the time penalty due to time-redundant instruction execution should decrease in future generations of microprocessors. Different processors will require different implementations of REESE, but its practicality will increase over time.

To be fully practical, the hardware cost of implementing REESE must be balanced against its benefits. The R-stream Queue requires more die area than other hardware additions, potentially adding about 20% to the die area and 1.5% to the execution time. Future work could explore executing less than 100% of P-stream instructions in the R stream, which might more easily meet processor performance goals but would decrease the number of detectable soft errors.

Adding just two integer ALUs to the execution stage of the pipeline nearly achieves zero performance degradation with a reasonable processor configuration. ALUs are relatively inexpensive, making REESE a cost-effective method for adding soft error detection to General Purpose Processors (GPPs) without drastically increasing program execution time. REESE is a scheme that must be seriously considered for addressing the reliability issues of future microprocessor pipelines.

### References
[1] Borkar S., “Design Challenges of Technology Scaling”, IEEE Micro, Vol. 19, No 4, pp. 23-29, 1999.
[2] Hazucha P., Svensson C., and Wender S.A., “Cosmic-Ray Soft Error Rate Characterization of a Standard 0.6-μm CMOS Process”, IEEE Journal of Solid-State Circuits, Vol. 35, No 10, pp. 1422-1429, 2000.
[3] Tosaka Y., Kanata H., Satoh S., and Itakura T., “Simple Method for Estimating Neutron-Induced Soft Error Rates Based on Modified BGR Model”, IEEE Electron Device Letters, Vol. 20, No 2, pp. 89-91, 1999.
[4] Chen C.L., Hsiao M.Y., “Error-correcting Codes for Semiconductor Memory Applications: A State of the Art Review”, In Reliable Computer Systems - Design and Evaluation, pp. 771-786, Digital Press, 2nd edition, 1992.
[5] Hennessy J., “The Future of Systems Research”, IEEE Computer, Vol. 32, No 8, pp. 27-33, 1999.
[6] Johnson B.W., “Fault-tolerant Microprocessor-based Systems”, IEEE Micro, Vol. 4, No 6, pp. 6-21, 1984.
[7] Patel J.H., Fung L.Y., “Concurrent Error Detection in ALUs by Recomputing with Shifted Operands”, IEEE Transactions on Computers, Vol. 31, No 7, pp. 589-595, 1982.
[8] Rebaudengo M., Sonza Reorda M., Torchiano M., Violante M., “Soft-error Detection through Software Fault-tolerance Techniques”, International Symposium on Defect and Fault Tolerance in VLSI Systems, Albuquerque(NM), USA, IEEE, pp. 210-218, 1999.
[9] Anghel L., Nicolaidis M., Alzaher-Noufal I., “Self-Checking Circuits versus Realistic Faults in Very Deep Submicron”, Proceedings of the 18th IEEE VLSI Test Symposium, Montreal, Canada, pp. 55-63, 2000.
[10] Kanopoulos N., Pantzartzia D., Bartram E.R., “Design of Self-checking Circuits Using DCVS Logic: A Case Study”, IEEE Transactions on Computers, Vol. 41, No 7, pp. 891-896, 1992.
[11] Nicolaidis M., “Time Redundancy Based Soft-Error Tolerance to Rescue Nanometer Technologies”, Proceedings of the 17th IEEE VLSI Test Symposium, Dana Point(CA), USA, IEEE, pp. 86-94, 1999.
[12] Anghel L., Nicolaidis M., “Cost Reduction and Evaluation of a Temporary Faults Detecting Technique”, Proceedings of the Design, Automation and Test in Europe Conference and Exhibition 2000, Paris, France, ACM, pp. 591-598, 2000.
[13] Spainhower L., Gregg T., “G4: A Fault-Tolerant CMOS Mainframe”, Proceedings of the International Symposium on Fault-Tolerant Computing, Munich, Germany, IEEE, pp. 432-440, 1998.
[14] Franklin M., “Incorporating Fault Tolerance in Superscalar Processors”, Proceedings of the 3rd International Conference on High Performance Computing, Trivandrum, India, IEEE, pp. 301-306, 1996.
[15] Sohi G.S., Franklin M., Saluja K.K., “A Study of Time-Redundant Fault-Tolerance Techniques for High-Performance Pipelined Computers”, Proceedings of the International Symposium on Fault-Tolerant Computing, Chicago(IL), USA, IEEE, pp. 436-449, 1989.
[16] Hsu Y.M., Swartzlander, Jr. E.E., “Time Redundant Error Correcting Adders and Multipliers”, IEEE International Workshop on Defect and Fault Tolerance in VLSI Systems, Dallas(TX), USA, IEEE, pp. 247-256, 1992.
[17] Rashid E, Saluja K.K., Ramanathan P., “Fault Tolerance through Re-execution in Multiscalar Architecture”, Proceedings of the International Conference on Dependable Systems and Networks, New York(NY), USA, IEEE, pp. 482-491, 2000.
[18] Rotenberg E., “AR-SMT: A Microarchitectural Approach to Fault Tolerance in Microprocessors”, Proceedings of the International Symposium on Fault-Tolerant Computing, Madison(WI), USA, IEEE, pp. 84-91, 1999.
[19] The Berkeley NOW project, URL: http://now.cs.berkeley.edu, December 2000.
[20] Reddy U., Tridandapani S., Somani A., “Effect of Diagnosis Coverage and Preemption Latencies on Fault Diagnosis Schemes Using Idle Capacity in Multiprocessor Systems”, Proc. 1995 Pacific Rim International Symposium on Fault Tolerant Systems, Newport Beach(CA), USA, IEEE, pp. 213-218, 1995.
[21] Tridandapani, S., Somani, A. K., and Reddy, U., “Low Overhead Multiprocessor Allocation Strategies Exploiting System Spare Capacity for Fault Detection and Location,” IEEE Transactions on Computers, Vol. 44, No. 7, July 1995.
[22] Gong C., Melhem R., Gupta R., “Compiler Assisted Fault Detection for Distributed-Memory Systems”, Proceedings of the International Symposium on Fault-Tolerant Computing, Austin(TX), USA, IEEE, pp. 373-380, 1994.
[23] Tamir Y., Liang M., Lai T., Tremblay M., “The UCLA Mirror Processor: A Building Block for Self-Checking Self-Repairing Computing Nodes”, Proceedings of the International Symposium on Fault-Tolerant Computing, Montreal, Canada, IEEE, pp. 178-185, 1991.
[24] Franklin M., “A Study of Time Redundant Fault Tolerance Techniques for Superscalar Processors”, IEEE International Workshop on Defect and Fault Tolerance in VLSI Systems, Lafayette(LA), USA, IEEE, pp. 207-215, 1995.
[25] Burger D., Austin T.M., “The SimpleScalar Tool Set, Version 2.0’, Computer Sciences Department Technical Report, No. 1342, Univ. of Wisconsin, June 1997.
[26] McFarling S., “Combining Branch Predictors”, TN 36, DEC-WRL, June 1993.
[27] The Standard Performance Evaluation Corporation, URL: http://www.specbench.org, December 2000.