### Figure 2: Distances in Kilometers (Log Scale) from Volume-Weighted Clients to Nearest Front-Ends

To further reduce overhead, each beacon performs only four measurements to front-ends:
1. A measurement to the front-end selected by anycast routing.
2. A measurement to the front-end judged to be geographically closest to the LDNS.
3. Two measurements to front-ends randomly selected from the other nine candidates, with the likelihood of a front-end being selected weighted by its distance from the client LDNS (e.g., the 3rd closest front-end is more likely to be selected than the 4th closest).

For most of our analysis, we aggregate measurements by /24 and consider the performance distribution to a front-end. This approach ensures robustness even if not every client measures to the best front-end every time.

To partially validate our approach, Figure 1 shows the distribution of minimum observed latency from a client /24 to a front-end. The labeled Nth line includes latency measurements from the nearest N front-ends to the LDNS. The results show decreasing latency as more front-ends are included, but little improvement after adding five front-ends per prefix. Thus, we do not expect significant improvements in minimum latencies for many prefixes if we measure more than the nearest ten front-ends included in our beacon measurements.

### 4. CDN Size and Geo-Distribution

The results in this paper are specific to Bing’s anycast CDN deployment. In this section, we characterize the size of the deployment, showing that it is similar in scale to most other CDNs, particularly anycast CDNs, with a few dozen front-end server locations. We then measure the distance from clients to the nearest front-ends. Our characterization of the performance of this CDN is an important first step towards understanding anycast performance. Future work could explore extending these performance results to CDNs with different numbers and locations of servers and varying interdomain connectivity [18].

We compare our CDN to others based on the number of server locations, which is a key factor in CDN and anycast performance. We examined 21 CDNs and content providers with publicly available data [3]. Four CDNs are extreme outliers: ChinaNetCenter and ChinaCache each have over 100 locations in China. Previous research found Google to have over 1000 locations worldwide [16], and Akamai is generally known to have over 1000 as well [17]. While such large deployments are often the popular image of a CDN, they are exceptions. Ignoring the large Chinese deployments, the next largest CDNs are CDNetworks (161 locations) and SkyparkCDN (119 locations). The remaining 17 CDNs, including ChinaNetCenter’s and ChinaCache’s deployments outside of China, have between 17 (CDNify) and 62 (Level3) locations. In terms of number of locations and regional coverage, the Bing CDN is most similar to Level3 and MaxCDN. Well-known CDNs with smaller deployments include Amazon CloudFront (37 locations), CacheFly (41 locations), CloudFlare (43 locations), and EdgeCast (31 locations). CloudFlare, CacheFly, and EdgeCast are anycast CDNs.

To provide perspective on the density of front-end distribution, Figure 2 shows the distance from clients to the nearest front-ends, weighted by client Bing query volumes. The median distance to the nearest front-end is 280 km, to the second nearest is 700 km, and to the fourth nearest is 1300 km.

### 5. Anycast Performance

We use measurements to estimate the performance penalty anycast incurs in exchange for simple operation. Figure 3, based on millions of measurements collected over a few days, inspired us to take on this project.

As explained in § 3, each execution of the JavaScript beacon yields four measurements: one to the front-end selected by anycast and three to nearby unicast front-ends. For each request, we find the latency difference between anycast and the lowest-latency unicast front-end. Figure 3 shows the fraction of requests where anycast performance is slower than the best of the three unicast front-ends. Most of the time, in most regions, anycast performs as well as the best of the three nearby unicast front-ends. However, anycast is at least 25ms slower for 20% of requests, and just below 10% of anycast measurements are 100ms or more slower than the best unicast for the client.

This graph suggests potential benefits in using DNS-based redirection for some clients, with anycast for the rest. Note that this is not an upper bound; to derive that, we would need to poll all front-ends in each beacon execution, which is too much overhead. There is also no guarantee that a deployed DNS-based redirection system will achieve the performance improvement seen in Figure 3. Nonetheless, this result was sufficiently intriguing for us to study anycast performance in more detail and seek ways to improve it.

### Examples of Poor Anycast Routes

A challenge in understanding anycast performance is identifying why clients are directed to distant or poorly performing front-ends. To troubleshoot, we used the RIPE Atlas [2] testbed, a network of over 8000 probes predominantly hosted in home networks. We issued traceroutes from Atlas probes hosted within the same ISP-metro area pairs where we observed clients with poor performance. Our analysis revealed that many instances fall into one of two cases:
1. BGP's lack of insight into the underlying topology causes anycast to make suboptimal choices.
2. Intradomain routing policies of ISPs select remote peering points with our network.

In one example, a client was roughly the same distance from two border routers announcing the anycast route. Anycast chose to route towards router A. However, internally in our network, router B is very close to a front-end C, whereas router A has a longer intradomain route to the nearest front-end D. With anycast, there is no way to communicate this internal topology information in a BGP announcement.

Figure 5: Daily poor-path prevalence during April 2015, showing the fraction of client /24s that see different levels of latency improvement over anycast when directed to their best-performing unicast front-end.

### Is Anycast Performance Consistently Poor?

We first consider whether significant fractions of clients experience consistently poor performance with anycast. At the end of each day, we analyzed all collected client measurements to find prefixes with room for improvement over anycast performance. For each client /24, we calculated the median latency between the prefix and each measured unicast front-end and anycast.

Figure 5 shows the prevalence of poor anycast performance each day during April 2015. Each line specifies a particular minimum latency improvement, and the figure shows the fraction of client /24s each day for which some unicast front-end yields at least that improvement over anycast. On average, we find that 19% of prefixes see some performance benefit from going to a specific unicast front-end instead of using anycast. We see 12% of clients with 10ms or more improvement, but only 4% see 50ms or more.

Poor performance is not limited to a few days—it is a daily concern. We next examine whether the same client networks experience recurring poor performance. How long does poor performance persist? Are the problems seen in Figure 5 always due to the same problematic clients?

Figure 6 shows the duration of poor anycast performance during April 2015. For the majority of /24s categorized as having poor-performing paths, those poor-performing paths are short-lived. Around 60% appear for only one day over the month. Around 10% of /24s show poor performance for 5 days or more. These days are not necessarily consecutive. We see that only 5% of /24s see continuous poor performance over 5 days or more.

These results show that while there is a persistent amount of poor anycast performance over time, the majority of problems only last a short period.

### Does Anycast Direct Clients to Nearby Front-Ends?

In a large CDN with presence in major metro areas around the world, most ISPs will see BGP announcements for front-ends from a number of different locations. If peering among these points is uniform, then the ISP’s least cost path from a client to a front-end will often be the geographically closest. Since anycast is not load or latency aware, geographic proximity is a good indicator of expected performance.

Figure 4 shows the distribution of the distance from client to anycast front-end for all clients in one day of production Bing traffic. One line weights clients by query volume. Anycast is shown to perform 5-10% better at all percentiles when accounting for more active clients. We see that about 82% of clients are directed to a front-end within 2000 km, while 87% of client volume is within 2000 km.

The second pair of lines in Figure 4, labeled “Past Closest,” shows the distribution of the difference between the distance from a client to its closest front-end and the distance from the client to the front-end anycast directs to. About 55% of clients and weighted clients have a distance of 0, meaning they are directed to the nearest front-end. Further, 75% of clients are directed to a front-end within around 400 km, and 90% are within 1375 km of their closest. This supports the idea that, with a dense front-end deployment such as is achievable in North America and Europe, anycast directs most clients to a relatively nearby front-end that should be expected to deliver good performance, even if it is not the closest.

From a geographic view, we found that around 10-15% of /24s are directed to distant front-ends, a likely explanation for poor performance.

### 6. Addressing Poor Performance

The previous section showed that anycast often achieves good performance but sometimes suffers significantly compared to unicast beacon measurements. However, the ability for unicast to outperform anycast in a single measurement does not guarantee that this performance is predictable enough to be achievable if a system has to return a single unicast front-end to a DNS query.

If a particular front-end outperformed anycast in the past for a client, will it still do so if the system returns that front-end next time? Additionally, because of DNS's design, the system does not know which client it is responding to, and so its response applies either to all clients of an LDNS or all clients in a prefix (if using ECS). Can the system reliably determine front-ends that will perform well for the set of clients?

We evaluate to what degree schemes using DNS and ECS can improve performance for clients with poor anycast performance. We evaluate (in emulation based on our real user measurements) a prediction scheme that maps from a client group (clients of an LDNS or clients within an ECS prefix) to its predicted best front-end. It updates its mapping every prediction interval, set to one day in our experiment. The scheme chooses to map a client group to the lowest latency front-end across the measurements for that group, picking either the anycast address or one of the unicast front-ends. We evaluate two prediction metrics to determine the latency of a front-end: 25th percentile and median latency from that client group to that front-end. We choose lower percentiles, as analysis of client data showed that higher percentiles of latency distributions are very noisy (we omit detailed results due to lack of space). This noise makes prediction difficult, as it can result in overlapping performance between two front-ends. The 25th percentile and median have a lower coefficient of variation, indicating less variation and more stability. Our initial evaluation showed that both 25th percentile and median show very similar performance as prediction metrics, so we only present results for the 25th percentile.

We emulate the performance of such a prediction scheme using our existing beacon measurements. We base the predictions on one day’s beacon measurements. For a given client group, we select among the front-ends with 20+ measurements from the clients. We evaluate the performance of the prediction scheme by comparing against the performance observed in the next day’s beacon measurements. We compare the 50th and 75th percentile anycast performance for the group to the 50th and 75th percentile performance for the predicted front-end. The Bing team routinely uses 75th percentile latency as an internal metric.