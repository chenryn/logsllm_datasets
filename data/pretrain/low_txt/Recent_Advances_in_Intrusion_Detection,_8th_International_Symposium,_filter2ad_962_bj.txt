### References

1. Crispin Cowan, Calton Pu, and Heather Hinton, "Death, Taxes, and Imperfect Software: Surviving the Inevitable," The New Security Paradigms Workshop, 1998.
2. F. Buchholz, T. Daniels, J. Early, R. Gopalakrishna, R. Gorman, B. Kuperman, S. Nystrom, A. Schroll, and A. Smith, "Digging For Worms, Fishing For Answers," ACSAC 2002.
3. Sung-Bae Cho and Sang-Jun Han, "Two Sophisticated Techniques to Improve HMM-Based Intrusion Detection Systems," RAID 2003.
4. Scott Coull, Joel Branch, Boleslaw K. Szymanski, and Eric Breimer, "Intrusion Detection: A Bioinformatics Approach," ACSAC 2003.
5. Dorothy E. Denning, "An Intrusion Detection Model," IEEE Transactions on Software Engineering, Vol. 13, No. 2, pp. 222-232, February 1987.
6. Henry H. Feng, Oleg Kolesnikov, Prahlad Fogla, Wenke Lee, and Weibo Gong, "Anomaly Detection Using Call Stack Information," IEEE Symposium on Security and Privacy, 2003.
7. S. Forrest, S. Hofmeyr, A. Somayaji, and T. Longstaff, "A Sense of Self for UNIX Processes," IEEE Symposium on Security and Privacy, 1996.
8. S. Forrest, A. Somayaji, and D. Ackley, "Building Diverse Computer Systems," Proceedings of the 6th Workshop on Hot Topics in Operating Systems, IEEE Computer Society Press, pp. 67-72, 1997.
9. Tal Garfinkel, "Traps and Pitfalls: Practical Problems in System Call Interposition Based Security Tools," Network and Distributed Systems Security Symposium, February 2003.
10. Anup K. Ghosh, Christoph Michael, and Michael Schatz, "A Real-Time Intrusion Detection System Based on Learning Program Behavior," RAID 2000.
11. Jonathon T. Giffin, Somesh Jha, and Barton P. Miller, "Detecting Manipulated Remote Call Streams," 11th USENIX Security Symposium, 2002.
12. Jonathon T. Giffin, Somesh Jha, and Barton P. Miller, "Efficient Context-Sensitive Intrusion Detection," 11th Network and Distributed System Security Symposium, 2004.
13. S. A. Hofmeyr, A. Somayaji, and S. Forrest, "Intrusion Detection Using Sequences of System Calls," Journal of Computer Security, Vol. 6, pp. 151-180, 1998.
14. Ruiqi Hu and Aloysius K. Mok, "Detecting Unknown Massive Mailing Viruses Using Proactive Methods," RAID 2004.
15. A. Jones and S. Li, "Temporal Signatures of Intrusion Detection," ACSAC 2001.
16. Gaurav S. Kc, Angelos D. Keromytis, and Vassilis Prevelakis, "Countering Code-Injection Attacks with Instruction-Set Randomization," 10th ACM International Conference on Computer and Communications Security (CCS), pp. 272-280, October 2003.
17. V. Kiriansky, D. Bruening, and S. Amarasinghe, "Secure Execution via Program Shepherding," 11th USENIX Security Symposium, 2002.
18. C. Ko, "Logic Induction of Valid Behavior Specifications for Intrusion Detection," IEEE Symposium on Security and Privacy, 2000.
19. C. Kruegel, D. Mutz, F. Valeur, and G. Vigna, "On the Detection of Anomalous System Call Arguments," 8th European Symposium on Research in Computer Security (ESORICS), 2003.
20. Christopher Kruegel, Darren Mutz, William Robertson, and Fredrik Valeur, "Bayesian Event Classification for Intrusion Detection," ACSAC 2003.
21. Lap Chung Lam and Tzi-cker Chiueh, "Automatic Extraction of Accurate Application-Specific Sandboxing Policy," RAID 2004.
22. T. Lane and C. Brodley, "Temporal Sequence Learning and Data Reduction for Anomaly Detection," ACM Transactions on Information and System Security, 1999.
23. W. Lee and S. Stolfo, "Data Mining Approaches for Intrusion Detection," 7th USENIX Security Symposium, 1998.
24. Jamie Butler, "Bypassing 3rd Party Windows Buffer Overflow Protection," Phrack, Issue #62, July 10, 2004.
25. R. Sekar, M. Bendre, D. Dhurjati, and P. Bollineni, "A Fast Automaton-based Method for Detecting Anomalous Program Behaviors," Proceedings of the 2001 IEEE Symposium on Security and Privacy.
26. R. Sekar, A. Gupta, J. Frullo, T. Shanbhag, A. Tiwari, H. Yang, and S. Zhou, "Specification-Based Anomaly Detection: A New Approach for Detecting Network Intrusions," ACM Computer and Communication Security Conference, 2002.
27. skape, "Understanding Windows Shellcode," http://www.hick.org/code/skape/papers/win32-shellcode.pdf
28. A. Somayaji, S. Forrest, "Automated Response Using System-Call Delays," 9th USENIX Security Symposium, 2000.
29. Kymie M. C. Tan and Roy A. Maxion, "Why 6? Defining the Operational Limits of Stride, an Anomaly-Based Intrusion Detector," IEEE Symposium on Security and Privacy, 2002.
30. Kymie M. C. Tan, Kevin S. Killourhy, and Roy A. Maxion, "Undermining an Anomaly-Based Intrusion Detection System Using Common Exploits," RAID 2002.
31. Thomas Toth and Christopher Kruegel, "Accurate Buffer Overflow Detection via Abstract Payload Execution," RAID 2002.
32. P. Uppuluri and R. Sekar, "Experiences with Specification-Based Intrusion Detection," RAID 2001.
33. D. Wagner and P. Soto, "Mimicry Attacks on Host-Based Intrusion Detection Systems," ACM Conference on Computer and Communications Security, 2002.
34. D. Wagner and D. Dean, "Intrusion Detection via Static Analysis," IEEE Symposium on Security and Privacy, 2001.
35. Christina Warrender, Stephanie Forrest, and Barak Pearlmutter, "Detecting Intrusions Using System Calls: Alternative Data Models," IEEE Symposium on Security and Privacy, 1999.
36. A. Wespi, M. Dacier, and H. Debar, "Intrusion Detection Using Variable-Length Audit Trail Patterns," RAID, 2000.
37. Matthew M. Williamson, "Throttling Viruses: Restricting Propagation to Defeat Malicious Mobile Code," ACSAC 2002.
38. Haizhi Xu, Wenliang Du, and Steve J. Chapin, "Context-Sensitive Anomaly Monitoring of Process Control Flow to Detect Mimicry Attacks and Impossible Paths," RAID 2004.

### Environment-Sensitive Intrusion Detection

**Authors:**
- Jonathon T. Giffin, University of Wisconsin
- David Dagon, Georgia Institute of Technology
- Somesh Jha, University of Wisconsin
- Wenke Lee, Georgia Institute of Technology
- Barton P. Miller, University of Wisconsin

**Contact:**
- {giffin, jha, bart}@cs.wisc.edu
- {dagon, wenke}@cc.gatech.edu

**Abstract:**
We enhance host-based intrusion detection by constructing a model from a program's binary code and then restricting the program's execution based on this model. We improve the effectiveness of such model-based intrusion detection systems by incorporating knowledge of the environment in which the program runs and by increasing the accuracy of our models with a new data-flow analysis algorithm for context-sensitive recovery of static data.

The environment, including configuration files, command-line parameters, and environment variables, constrains acceptable process execution. By adding environment dependencies to the program model, we update the model to reflect the current environment at every program execution.

Our new static data-flow analysis associates a program’s data flows with specific calling contexts that use the data. This allows us to differentiate system-call arguments flowing from distinct call sites in the program.

Using a new average reachability measure suitable for evaluating call-stack-based program models, we demonstrate that our techniques improve the precision of several test programs’ models from 76% to 100%.

**Keywords:**
- Model-based anomaly detection
- Dyck model
- Static binary analysis
- Static data-flow analysis

### 1. Introduction

A host-based intrusion detection system (HIDS) monitors a process' execution to identify potentially malicious behavior. In a model-based anomaly HIDS or behavior-based HIDS, deviations from a precomputed model of expected behavior indicate possible intrusion attempts. The monitor verifies a stream of events, often system calls, generated by the executing process and rejects event streams deviating from the model. The system's ability to detect attacks with few or zero false alarms relies entirely on the precision of the model.

Static analysis builds an execution model by analyzing the source or binary code of the program. Traditionally, static analysis algorithms are conservative and produce models that overapproximate correct execution. Previous statically constructed models allowed execution behaviors possible in any execution environment. However, processes often read the environment—configuration files, command-line parameters, and environment variables known at process load time and fixed for the entire execution of the process. The environment can significantly constrain a process' execution, disabling entire blocks of functionality and restricting the process' access.

If the process can generate the language of event sequences \( L_e \) given the current environment \( e \), then previous program models constructed from static analysis accepted the language \( L_s = \cup_{i \in E} L_i \) for \( E \) the set of all possible environments. \( L_s \) is a superset of \( L_e \) and may contain system call sequences that cannot be generated by correct execution in environment \( e \).

These overly general models may fail to detect attacks. For example, versions of the OpenSSH secure-shell server prior to 3.0.2 had a design error that allowed users to alter the execution of the root-level login process. If the configuration file setting "uselogin" was disabled, then the ssh server disabled the vulnerable code. However, an attacker who has subverted the process can bypass the "uselogin" checks by directly executing the vulnerable code. Previous statically constructed models allowed all paths in the program, including the disabled path. By executing the disabled code, the attacker can undetectably execute root-level commands.

In this paper, we make statically constructed program models sensitive to the execution environment. An environment-sensitive program model restricts process execution behavior to only the behavior correct in the current environment. The model accepts a limited language of event sequences \( L_v \), where \( L_e \subseteq L_v \subseteq L_s \). Event sequences that could not be correctly generated in the current environment are detected as intrusive, even if those sequences are correct in some other environment. In the OpenSSH example, if "uselogin" was disabled, then the model disallows system calls and system-call arguments reachable only via the vulnerable code paths. The model detects an entire class of evasion attacks that manipulate environment data, as described in Section 7.4.

Environment dependencies characterize how execution behavior depends upon environment values. Similar to def-use relations in static data-flow analysis, an environment dependency relates values in the environment, such as "uselogin", to values of internal program variables. When an environment-sensitive HIDS loads a program model for execution enforcement, it customizes the model to the current environment based on these dependencies. In this paper, we manually identify dependencies. Our long-term goal is to automate this procedure, and in Section 5.3, we postulate that automated identification will not be an onerous task.

Environment sensitivity works best with system-call argument analysis. Our static analyzer includes powerful data-flow analysis to recover statically known system-call arguments. Different execution paths in a program may set a system-call argument differently. Our previous data-flow analysis recovered argument values without calling context, in that the analysis algorithm ignored the association between an argument value and the call site that set that value. In this work, we encode calling context with argument values to better model the correct execution behavior of a program. A system-call argument value observed at runtime must match the calling context leading up to the system call. Additionally, the data-flow analysis now crosses shared object boundaries, enabling static analysis of dynamically-linked executables.

Although environment-sensitive program modeling is the primary focus of our work, we make an additional contribution: a new evaluation metric. The existing standard metric measuring model precision, average branching factor, poorly evaluates models that monitor a program’s call stack in addition to the system-call stream. We instead use context-free language reachability to move forward through stack events to discover the next set of actual system calls reachable from the current program location. Our new average reachability measure fairly evaluates the precision of program models that include function call and return events. Using the average reachability measure, we demonstrate the value of whole-program data-flow analysis and environment-sensitive models. On four test programs, we improved the precision of context-sensitive models from 76% to 100%.

In summary, we believe that this paper makes the following contributions:
- Static model construction of dynamically-linked executables. In particular, the static analyzer continues data-flow analysis across shared-object boundaries by learning the API by which programs call library code, as described in Section 4.1.
- Context-sensitive encoding of recovered system-call arguments, detailed in Section 4.2. Combined with whole-program analysis, this technique improved argument recovery by 61% to 100% in our experiments.
- A formal definition of environment-sensitive program models and methods to encode environment dependencies into statically constructed program models. Environment sensitivity and static system-call argument recovery improved the precision of program models by 76% to 100%. Section 5 presents this work.
- An extension to the commonly-used average branching factor metric suitable for program models that require update events for function calls and returns (Section 6). The average reachability measure provides a fairer comparison of call-stack-based models and other models that do not monitor the call stack.

### 2. Related Work

In 1994, Fix and Schneider added execution environment information to a programming logic to make program specifications more precise. The logic better specified how a program would execute, allowing for more precise analysis of the program in a proof system. Their notion of environment was general, including properties such as scheduler behavior. We propose a similar idea: use environment information to more precisely characterize expected program behavior in a program model. As our models describe safety properties that must not be violated, we focus on environment aspects that can constrain the safety properties.

Chinchani et al. instrumented C source-code with security checks based on environment information. Their definition of environment primarily encompassed low-level properties of the physical machine on which a process executes. For example, knowing the number of bits per integer allowed the authors to insert code into a program to prevent integer overflows. This approach is specific to known exploit vectors and requires source-code editing, making it poorly suited for our environment-sensitive intrusion detection.

One aspect of our current work uses environment dependencies and static analysis to limit allowed values to system-call arguments. This specific problem has received prior attention.

Static analysis can identify constant, statically known arguments. While extracting execution models from C source code, Wagner and Dean identified arguments known statically. In earlier work, we used binary code analysis to recover arguments in SPARC executables. These efforts suffered from several problems:
- Earlier binary data-flow analysis required statically-linked executables. In this paper, we use data-flow analysis to learn the API for a shared object. When analyzing an executable, we continue data-flow analysis anywhere the library API is used.
- Values recovered were not sensitive to calling context. This forces two inaccuracies. First, the association between a system-call argument value and the execution path using that value is lost (Figure 1A). An attacker could undetectably use a value recovered on one execution path on any other execution path to the same system call. Second, if any execution path set an argument in a way not recoverable statically, all values recovered along all other execution paths must be discarded for the analysis to be safe (Figure 1B). Our current work avoids these two inaccuracies by encoding calling context with recovered values.
- Static analysis cannot recover values set dynamically. In this paper, we make a distinction between dynamic values set at load time and values set by arbitrary user input. Environment dependencies augment static analysis and describe how values set when the operating system loads a process flow to system-call arguments.

Dynamic analysis learns a program model by generalizing behavior observed during a training phase. Kruegel et al. and Sekar et al. used dynamic analysis to learn constraints for system-call arguments. These constraints will include values from the environment that are used as part of a system-call argument, which forces a tradeoff. The training phase could modify environment values to learn a general model, but such a model fails to constrain later execution to the specific environment. Conversely, training could use only the current environment. If the environment ever changes, however, then the model no longer characterizes correct execution and retraining becomes necessary. By including environment dependencies described in this paper, learning could be done only for arguments not dependent upon the environment. Environment dependencies would resolve the remaining arguments to the current environment every time the model was subsequently loaded.

Environment-sensitive models are well suited to the model-carrying code execution design. Sekar et al. proposed that unknown, untrusted executables can include models of their execution. A consumer of the executable can use a model checker to verify that the model does not violate their security policy and an execution monitor to limit the program’s execution to that allowed by the model. The code producer must build the program model, but they cannot know any consumer’s specific execution environment. To avoid false alarms, the model must be general to suit all possible environments. Such a general model may not satisfy a consumer’s security policy. If the code producer adds environment dependencies to the model shipped with the code, the model will automatically adapt to every consumer’s unique environment. With the environment constraints, the model is increasingly likely to satisfy a consumer’s security policy.

### 3. Overview

Model-based anomaly detection has two phases: construction of the program model and enforcement of the model during execution.