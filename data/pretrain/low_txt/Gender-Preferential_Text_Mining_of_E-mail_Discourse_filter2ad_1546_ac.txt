### 5.2. Attribute Scaling

To ensure that all attributes are treated equally in the classification process, a scaling factor, \( SFA_i \), is computed as follows:

\[ SFA_i = \frac{UBA_i - LBA_i}{A_{i,\text{max}} - A_{i,\text{min}}} \]

where:
- \( A_{i,\text{min}} \) and \( A_{i,\text{max}} \) are the minimum and maximum values of the attribute \( A_i \), respectively.
- \( LBA_i \) and \( UBA_i \) are the defined lower and upper bounds of the scaled attribute, respectively. In this study, we set \( LBA_i = 0.0 \) and \( UBA_i = 1.0 \).

### 5.3. Performance Evaluation Methodology

For the experiments, we utilized the SVMlight Support Vector Machine (SVM) classifier developed by T. Joachims from the University of Dortmund [19]. SVMlight is an implementation of Vapnik's Support Vector Machine [14], as described in Section 4. It scales well to a large number of sparse instance vectors and efficiently handles a large number of support vectors.

In our experiments, we explored several kernel functions for the SVM classifier, including linear, polynomial, radial basis, and sigmoid tanh functions. We achieved the highest F1 classification results on our dataset using a polynomial kernel of degree 3. The "LOQO" optimizer was used to maximize the margin.

The Support Vector Machine computes two-way categorization. Therefore, in our experiments on author gender categorization, only a single two-way classification model with a two-way confusion matrix was generated.

### Table 4: F1 Categorization Performance Results (in %) for Different E-mail Document Sizes and Cohort Sizes

| Number of E-mails per Gender Cohort Class | 50 | 100 | 200 | 300 | 400 | 1000 |
|-------------------------------------------|----|-----|-----|-----|-----|------|
| Minimum Word Count                        |    |     |     |     |     |      |
| 50                                        | 64.4 | 68.4 | 64.8 | 66.4 | 67.5 | 69.4 |
| 100                                       | 62.2 | 64.0 | 61.5 | 67.6 | 68.7 | 71.1 |
| 150                                       | 57.1 | 56.8 | 62.2 | 66.6 | 70.2 | -    |
| 200                                       | 59.8 | 65.0 | 63.8 | 67.3 | -    | -    |

Values indicated by “-” correspond to insufficient e-mail document size/word count population.

### Training-Testing Sampling Methodology

We used a 10-fold cross-validation of the entire e-mail document set to evaluate the categorization performance. To assess the performance, we calculated the accuracy, recall (R), precision (P), and combined F1 performance measures, which are commonly employed in information retrieval and text mining literature [20]. The F1 score is calculated as:

\[ F1 = \frac{2RP}{R + P} \]

### 6. Results and Discussion

We present the results of our author gender-preferential language attribution using the Support Vector Machine (SVM) classifier. The F1 statistic is reported for different e-mail document sizes (measured as the minimum word count) and for different e-mail author gender cohort sizes (number of e-mail documents per female and male author cohort). These results are displayed in Table 4.

As observed in Table 4, the F1 categorization performance results indicate that, in general, the SVM classifier combined with style markers, structural attributes, and gender-preferential language attributes can satisfactorily discriminate between the author gender cohorts. As expected, there is a general improvement, though not dramatic, in performance as the number of e-mails in each gender cohort class increases. However, the improvement in performance as a function of the minimum word count is not as consistent as the e-mail count performance results. A noticeable improvement is only achieved when the number of e-mails in each gender cohort class is not too small (> 300) and/or the number of authors in each author gender class increases. These results suggest that a small number of e-mails per author cohort class is generally sufficient for satisfactory gender classification, which aligns with similar observations in authorship attribution studies [10].

### Table 5: Effect of Attribute Type on F1 Categorization Performance Results

| Feature Set Type                | Operation | F1 (%) |
|---------------------------------|-----------|--------|
| Character-based attributes      | Removed   | 70.0   |
| Word-based attributes           | Removed   | 69.6   |
| Word length distribution        | Removed   | 67.4   |
| Structural attributes           | Removed   | 68.1   |
| Function words                  | Removed   | 64.0   |
| All baseline attributes         | -         | 70.1   |
| All attributes (baseline + gender-based) | - | 70.2 |

### Preliminary Analysis of Attribute Impact

A preliminary analysis of the impact of different types of attributes (stylistic, structural, gender-preferential) on the author gender categorization performance was conducted. Each type of attribute set was removed from the feature set, and the performance results were calculated. The results in Table 5 show that the full combination of attributes gives the best author gender categorization. Removal of any of the attributes leads to a reduced performance value, with some attributes more critical than others. Specifically, the set of function words (attributes A31 to A152) is seen to be an important gender discriminator, as expected, since function words have been shown to be a good author discriminator [11] and contain words that could belong to gender-preferential language (such as “so”, “very,” etc.). However, the gender-preferential attributes used in the experiment only provide a marginal improvement in categorization performance, indicating that the current set of gender-based attributes is insufficient. A more selective and/or more extensive set of gender-preferential attributes will be needed to achieve better categorization performance.

### 7. Conclusions

In this paper, we investigated the learning of author gender categories from e-mail documents. We used an extended set of predominantly content-free e-mail document features, such as style markers, structural characteristics, and gender-preferential language features, together with a Support Vector Machine learning algorithm. Experiments on a number of e-mail documents generated by over 800 authors of both genders yielded promising results for author gender categorization. We observed an improvement in performance with an increasing number of e-mails in both gender cohort classes.

The current approach has several limitations. Firstly, a larger set of gender-preferential language attributes needs to be used to improve the performance further. Secondly, more studies on the usefulness of specific style markers, such as N-grams, for author gender identification should be conducted, as it is conjectured that certain bi-grams incorporating punctuation could be effective discriminators [21]. Finally, the diversity in author characteristics in the author cohort e-mail database is currently quite small due to the type of organization where the e-mails were sourced. Although it is challenging to obtain a sufficiently large set of e-mails from authors with varying cohort characteristics (educational level, language background, etc.), we hope to build up a suitable forensic database and further test our approach.

### References

[1] C. Apte, F. Damerau, and S. Weiss, “Text Mining with Decision Rules and Decision Trees,” Workshop on Learning from Text and the Web, Conference on Automated Learning and Discovery, Pittsburgh, PA, 1998.

[2] Y. Yang and X. Liu, “A Re-examination of Text Categorization Methods,” Proc. 22nd Int. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR99), Berkeley, CA, 1999, pp. 67–73.

[3] T. Joachims, “Text Categorization with Support Vector Machines: Learning with Many Relevant Features,” Proc. European Conf. Machine Learning (ECML’98), Chemnitz, Germany, 1998, pp. 137–142.

[4] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz, “A Bayesian Approach to Filtering Junk E-mail,” Learning for Text Categorization Workshop: 15th National Conf. on AI. AAAI Technical Report (WS-98-05), Madison, WI, 1998, pp. 55–62.

[5] F. Mosteller and D. Wallace, Inference and Disputed Authorship: The Federalist, Addison-Wesley, Reading, MA, 1964.

[6] E. Elliot and R. Valenza, “Was the Earl of Oxford the True Shakespeare?” Notes and Queries, 38, 1991, pp. 501–506.

[7] F. Tweedie and R. Baayen, “How Variable May a Constant Be? Measures of Lexical Richness in Perspective,” Computers and the Humanities, 32(5), 1998, pp. 323–352.

[8] J. Farringdon, Analysing for Authorship: A Guide to the Cusum Technique, University of Wales Press, Cardiff, 1996.

[9] F. Tweedie, S. Singh, and D. Holmes, “Neural Network Applications in Stylometry: The Federalist Papers,” Computers and the Humanities, 30(1), 1996, pp. 1–10.

[10] M. Corney, A. Anderson, G. Mohay, and O. de Vel, “Identifying the Authors of Suspect E-mail,” Computers and Security, in press, 2001.

[11] O. de Vel, A. Anderson, M. Corney, and G. Mohay, “E-mail Authorship Attribution for Computer Forensics,” in D. Barbara and S. Jajodia, Data Mining for Security Applications, Kluwer Academic Publishers, Boston, MA, 2002.

[12] R. Thomson and T. Murachver, “Predicting Gender from Electronic Discourse,” British Journal of Social Psychology, 40, 2001, pp. 193–208.

[13] H. Schiffman, “Bibliography of Gender and Language,” July 2002. http://ccat.sas.upenn.edu/haroldfs/popcult/bibliogs/gender/genbib.htm, 2002.

[14] V. Vapnik, The Nature of Statistical Learning Theory, Springer-Verlag, New York, 1995.

[15] H. Druker, D. Wu, and V. Vapnik, “Support Vector Machines for Spam Categorization,” IEEE Trans. on Neural Networks, 10, 1999, pp. 1048–1054.

[16] J. Diederich, J. Kindermann, E. Leopold, and G. Paass, “Authorship Attribution with Support Vector Machines,” Applied Intelligence, 2000, submitted.

[17] O. de Vel, “Mining E-mail Authorship,” Proc. Workshop on Text Mining, ACM International Conference on Knowledge Discovery and Data Mining (KDD’2000), Boston, MA, 2000.

[18] Chaski, C., “A Daubert-Inspired Assessment of Current Techniques for Language-Based Author Identification,” US National Institute of Justice, 1998. http://www.ncjrs.org

[19] T. Joachims, “Making Large-Scale SVM Learning Practical,” in B. Schölkopf, C. Burges, and A. Smola, Advances in Kernel Methods - Support Vector Learning, MIT-Press, 1999.

[20] I. Witten and E. Frank, Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations, Morgan Kaufmann, San Francisco, CA, 2000.

[21] C. Chaski, “Empirical Evaluations of Language-Based Author Identification Techniques,” Forensic Linguistics, 8(1), 2001.