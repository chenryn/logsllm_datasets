### 6. A Specific All-Stages Truncated Model

The derivation of the all-stages truncated models in Section 4.2 is valid for any (non-defective) coverage function \( G(t) \). Therefore, \( G(t) \) and consequently the initial NHPP-I model have not been specified so far. In this section, we apply our approach to the well-known Goel-Okumoto model [5] and show how to estimate the parameters of the resulting all-stages truncated model. We then use this model to fit and predict a classic failure data set, comparing its performance to the original Goel-Okumoto model and the Musa-Okumoto model.

#### 6.1. The Truncated Goel-Okumoto Model

The mean value function and the failure intensity of the NHPP-I model introduced by Goel and Okumoto [5] are given by:
\[
\mu(t) = \nu(1 - \exp(-\phi t)) \quad \text{and} \quad \lambda(t) = \nu \phi \exp(-\phi t),
\]
respectively, implying the non-defective coverage function (7). By substituting equation (7) into equation (24), we obtain the mean value function of the all-stages truncated Goel-Okumoto model (referred to as the "truncated Goel-Okumoto model" for brevity):
\[
\mu(t) = \ln \left( \frac{\exp(\nu) - 1}{\exp[\nu \exp(-\phi t)] - 1} \right).
\]
The derivative with respect to time yields the failure intensity:
\[
\lambda(t) = \frac{\nu \phi \exp(-\phi t)}{1 - \exp[-\nu \exp(-\phi t)]}.
\]
From equations (4) and (26), the reliability in the interval \((t_{i-1}, t_{i-1} + x]\) is derived as:
\[
R(x | t_{i-1}, M(t_{i-1}) = i - 1) = \frac{\exp[\nu \exp(-\phi (t_{i-1} + x))] - 1}{\exp[\nu \exp(-\phi t_{i-1})] - 1},
\]
which approaches zero as \( x \to \infty \). Thus, all TTF distributions are non-defective. Additionally, it can be shown that all mean times to failure are finite. The mean time to the \( i \)-th failure implied by the truncated Goel-Okumoto model is:
\[
E(X_i) = \int_0^\infty R(x | t_{i-1}, M(t_{i-1}) = i - 1) \, dx = \frac{1}{\phi \left( \exp[\nu \exp(-\phi t_{i-1})] - 1 \right)} \sum_{j=1}^\infty \frac{(\nu \exp(-\phi t_{i-1}))^j}{j \cdot j!}.
\]
The sum converges to a finite value, as can be seen by comparing it to the Taylor series expansion of the exponential function. This means that for each failure \( i = 1, 2, \ldots \), the mean time to failure is finite. Since the summands vanish quickly, the mean time to failure can be easily calculated based on equation (28).

Maximum likelihood estimation (MLE) can be used to calculate point estimates of the two model parameters \( \nu \) and \( \phi \). Based on the observed failure times \( t_1, t_2, \ldots, t_{me} \) collected while testing the software from time 0 to \( t_e \) (where \( t_e \) may be identical to or larger than \( t_{me} \)), the log-likelihood function for NHPP models to be maximized with respect to the parameter vector \( \delta \) takes the general form [17, p. 324]:
\[
\ln L(\delta; t_1, \ldots, t_{me}, t_e) = \sum_{i=1}^{me} \ln \lambda(t_i) - \mu(t_e).
\]
With equations (26) and (27), the log-likelihood of the truncated Goel-Okumoto model becomes:
\[
\ln L(\nu, \phi; t_1, \ldots, t_{me}, t_e) = me \ln (\nu \phi) - \phi \sum_{i=1}^{me} t_i - \sum_{i=1}^{me} \ln [1 - \exp(-\nu \exp(-\phi t_i))] + \ln [\exp(\nu \exp(-\phi t_e)) - 1] - \ln [\exp(\nu) - 1].
\]

#### 6.2. Numerical Example

To illustrate the application of the truncated Goel-Okumoto model, we use one of the data sets collected by Musa and available at the Data & Analysis Center for Software [3]. The data are from the 1970s but were carefully controlled during collection to ensure high quality. They have been used before for validating new models and are well-studied. The "System 40" data set consists of the wall-clock times of 101 failures experienced during the system test phase of a military application containing about 180,000 delivered object code instructions.

Parameter estimation for the truncated Goel-Okumoto model is carried out according to the procedure described in the previous section. We also employ MLE to fit the Goel-Okumoto model and the Musa-Okumoto model to the data set. This is done by maximizing the log-likelihood derived from combining equations (25), (29), and (13), (29), respectively. Figure 6 shows the development of the cumulative number of failure occurrences over time for System 40, along with the mean value functions of the three models, with parameters estimated based on the complete data set. Clearly, the truncated Goel-Okumoto model provides the best fit to the actual data. This is supported by the log-likelihood values attained by the three models. The maximum log-likelihood value achieved by a model during MLE can be viewed as a measure of the likelihood that the data were generated by the respective model.

Since adding parameters to a model cannot worsen its fit, selecting the "best" model based on the log-likelihood value would generally favor overly complex models. Akaike's [1] information criterion, derived from the Kullback-Leibler distance, adjusts the log-likelihood value by penalizing for the number of model parameters. However, since all three models considered here contain two parameters, we can simply compare the log-likelihood values. For the Goel-Okumoto model, the truncated Goel-Okumoto model, and the Musa-Okumoto model, these values are -1282.362, -1239.508, and -1251.290, respectively. The model ranking implied by these numbers coincides with the visual impression given by Figure 6: The truncated Goel-Okumoto model attains the largest log-likelihood value and is therefore most capable of explaining the collected failure data, followed by the Musa-Okumoto model and the original Goel-Okumoto model.

As shown in the previous section, in the truncated Goel-Okumoto model, all mean times to failure are finite. For this data set, this is also the case for the Musa-Okumoto model because the estimate of the parameter \( \theta \) is smaller than one. We can therefore contrast the predicted mean times to failure according to both models with the failure data. For each model, we start with the first five data points, estimate the model parameters, and predict the time to the sixth failure based on the parameter estimates and the fifth failure time, using equations (14) and (28). This procedure is repeated, each time adding one data point, until the end of the data set is reached. The predicted mean times to the next failure and the actual times to failure are depicted in Figure 7.

**Figure 6.** (Expected) Cumulative number of failure occurrences
**Figure 7.** Observed times to failure and predicted mean times to failure

The development in the predicted \( E(X_i) \) values is quite similar for the two models. While the mean time to failure predictions of the truncated Goel-Okumoto model are slightly more optimistic, they seem to be less volatile than those of the Musa-Okumoto model. Moreover, the former model not only responds to the long inter-failure times by increasing the mean times to failure predictions (as the Musa-Okumoto model does) but also predicts this increasing trend before the first TTF exceeding 100 hours is observed.

### 7. Conclusions

Defective time-to-failure distributions are often unrealistic and entail infinite mean times to failure, making this metric useless. In the course of our investigations, we have answered the questions listed in the abstract: The \( i \)-th time-to-failure distribution is defective if the transition rate into state \( i \) decreases so quickly in time that the area below it is finite. While this can never happen for homogeneous CTMC models, it is possible for non-homogeneous ones. NHPP models are a special case of the latter, and due to the equality of all transition rates and the failure intensity, the areas below the transition rates are related to the mean value function. If this function is bounded as \( t \) approaches infinity, i.e., for NHPP-I models, all time-to-failure distributions are defective. However, there is a generic approach with which an NHPP-I model can be transformed into an NHPP-II model. Its application to the Goel-Okumoto model has proven both feasible and worthwhile, leading to a new SRGM with desirable properties, including all mean times to failure being finite.

### References

[1] H. Akaike. Information theory and an extension of the maximum likelihood principle. In S. Kotz and N. L. Johnson, editors, Breakthroughs in Statistics - Volume I, pages 611–624. Springer, New York, 1992. (Reprint of the original 1973 paper).

[2] Y. Chen and N. D. Singpurwalla. Unification of software reliability models by self-exciting point processes. Advances in Applied Probability, 29:337–352, 1997.

[3] Data & Analysis Center for Software. The software reliability dataset. Available at http://www.dacs.dtic.mil/databases/sled/swrel.shtml. (Link verified on 2004-11-16).

[4] O. Gaudoin. Outils statistiques pour l'évaluation de la fiabilité des logiciels. Thèse de doctorat, Université de Joseph Fourier - Grenoble 1, Grenoble, 1990.

[5] A. Goel and K. Okumoto. Time-dependent error-detection rate model for software reliability and other performance measures. IEEE Trans. Reliability, 28:206–211, 1979.

[6] S. S. Gokhale and K. S. Trivedi. A time/structure-based software reliability model. Annals of Software Engineering, 8:85–121, 1999.

[7] M. Grottke. Prognose von Softwarezuverlässigkeit, Softwareversagensfällen und Softwarefehlern. In P. Mertens and S. Rässler, editors, Prognoserechnung, pages 459–487. Physica, Heidelberg, 6th edition, 2005.

[8] C.-Y. Huang and S.-Y. Kuo. Analysis of incorporating logistic testing-effort function into software reliability modeling. IEEE Trans. Software Engineering, 51:261–270, 2002.

[9] C.-Y. Huang, S.-Y. Kuo, and I.-Y. Chen. Analysis of a software reliability growth model with logistic testing-effort function. In Proc. Eighth International Symposium on Software Reliability Engineering, pages 378–388, 1997.

[10] Z. Jelinski and P. Moranda. Software reliability research. In W. Freiberger, editor, Statistical Computer Performance Evaluation, pages 465–484. Academic Press, New York, 1972.

[11] S. Kotz, N. L. Johnson, and C. B. Read. Improper distributions. In S. Kotz, N. L. Johnson, and C. B. Read, editors, Encyclopedia of Statistics, volume 4, pages 25–26. John Wiley & Sons, New York, 1983.

[12] L. Kuo and T. Y. Yang. Bayesian computation for nonhomogeneous Poisson processes in software reliability. Journal of the American Statistical Association, 91:763–773, 1996.

[13] J. Ledoux. Software reliability modeling. In H. Pham, editor, Handbook of Reliability Engineering, pages 213–234. Springer, London, 2003.

[14] B. Littlewood. Stochastic reliability growth: A model for fault-removal in computer programs and hardware design. IEEE Trans. Reliability, 30:313–320, 1981.

[15] D. R. Miller. Exponential order statistic models for software reliability growth. IEEE Trans. Software Engineering, 12:12–24, 1986.

[16] P. B. Moranda. Event-altered reliability rate models for general reliability analysis. IEEE Trans. Reliability, 28:376–381, 1979.

[17] J. D. Musa, A. Iannino, and K. Okumoto. Software Reliability - Measurement, Prediction, Application. McGraw-Hill, New York, 1987.

[18] J. D. Musa and K. Okumoto. A logarithmic Poisson execution time model for software reliability measurement. In Proc. Seventh International Conference on Software Engineering, pages 230–238, 1984.

[19] H. Pham and X. Zhang. Software release policies with gain in reliability justifying the costs. Annals of Software Engineering, 8:147–166, 1999.

[20] N. D. Singpurwalla and S. P. Wilson. Statistical Methods in Software Engineering - Reliability and Risk. Springer Series in Statistics. Springer, New York, 1999.

[21] K. S. Trivedi. Probability and Statistics with Reliability, Queuing, and Computer Science Applications. John Wiley & Sons, New York, 2001.

[22] S. Yamada, J. Hishitani, and S. Osaki. Software-reliability growth with a Weibull test-effort. IEEE Trans. Reliability, 42:100–106, 1993.

[23] S. Yamada, H. Ohtera, and H. Narihisa. Software reliability growth models with testing-effort. IEEE Trans. Reliability, 35:19–23, 1986.