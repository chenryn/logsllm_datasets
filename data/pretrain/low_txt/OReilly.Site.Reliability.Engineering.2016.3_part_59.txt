### Service Level Objectives (SLOs) and Incident Resolution

- **Availability SLO**: 99.99%
- **Latency SLO**: 99th percentile < 100 ms
- **Incident Duration**: Over 30 minutes

### TODO List and Bugs Filed
- **Reindex Shakespeare Corpus**: MapReduce job completed.
- **Emergency Resource Allocation**: Additional capacity brought online.
- **Load Balancing with Flux Capacitor**: In progress (Bug 5554823).

### Incident Timeline (Most Recent First, UTC)
- **2015-10-21 15:28 UTC (Jennifer)**: Increased global serving capacity by 2x.
- **2015-10-21 15:21 UTC (Jennifer)**: Redirected all traffic to the USA-2 sacrificial cluster, allowing other clusters to recover from cascading failure while additional tasks were being spun up. The MapReduce index job was complete, and Bigtable replication to all clusters was in progress.
- **2015-10-21 15:10 UTC (Martym)**: Added a new sonnet to the Shakespeare corpus and initiated the indexing MapReduce job.
- **2015-10-21 15:04 UTC (Martym)**: Obtained the text of a newly discovered sonnet from the `shakespeare-discuss@` mailing list.
- **2015-10-21 15:01 UTC (Docbrown)**: Declared an incident due to a cascading failure.
- **2015-10-21 14:55 UTC (Docbrown)**: Received a pager storm, indicating a high number of HTTP 500 errors across all clusters.

### Postmortem: Shakespeare Sonnet++ Incident (Incident #465)

**Date**: 2015-10-21  
**Authors**: Jennifer, Martym, Agoogler  
**Status**: Complete, action items in progress  
**Summary**: The Shakespeare Search service was down for 66 minutes during a period of high interest due to the discovery of a new sonnet.  
**Impact**: Approximately 1.21 billion queries lost, with no revenue impact.  
**Root Causes**: A cascading failure caused by a combination of exceptionally high load and a resource leak when searches failed due to terms not being in the Shakespeare corpus. The newly discovered sonnet used a word that had never appeared in Shakespeare's works, leading to a high rate of task failures.  
**Trigger**: A latent bug triggered by a sudden increase in traffic.  
**Resolution**: Directed traffic to a sacrificial cluster and added 10x capacity to mitigate the cascading failure. Updated the index, resolving the interaction with the latent bug. Maintaining extra capacity until the surge in public interest passes. The resource leak has been identified and fixed.

### Detection
Borgmon detected a high level of HTTP 500 errors and paged the on-call team.

### Action Items
- **Update Playbook for Cascading Failure Response**: Completed (Owner: Jennifer).
- **Use Flux Capacitor for Load Balancing**: In progress (Owner: Martym, Bug 5554823).
- **Schedule Cascading Failure Test During Next DiRT Process**: In progress (Owner: Docbrown).
- **Investigate Continuous Indexing and Fusion**: In progress (Owner: Jennifer, Bug 5554824).
- **Fix File Descriptor Leak in Search Ranking Subsystem**: Completed (Owner: Agoogler, Bug 5554825).
- **Add Load Shedding Capabilities**: In progress (Owner: Agoogler, Bug 5554826).
- **Build Regression Tests for Query Handling**: In progress (Owner: Clarac, Bug 5554827).
- **Deploy Updated Search Ranking Subsystem to Production**: Completed (Owner: Jennifer).
- **Freeze Production Until 2015-11-20 or Seek Exception Due to Unusual Circumstances**: In progress (Owner: Docbrown).

### Lessons Learned
**What Went Well**
- Monitoring quickly alerted the team to the high rate of HTTP 500 errors.
- The updated Shakespeare corpus was rapidly distributed to all clusters.

**What Went Wrong**
- The team was out of practice in responding to cascading failures.
- The availability error budget was exceeded due to the exceptional surge in traffic, which resulted in a high rate of failures.

**Where We Got Lucky**
- The `shakespeare-discuss@` mailing list had a copy of the new sonnet.
- Server logs provided stack traces pointing to file descriptor exhaustion as the cause of the crash.
- The "query of death" was resolved by pushing a new index containing the popular search term.

### Timeline
- **14:51 UTC**: News reports of a new Shakespearean sonnet discovered in a DeLorean's glove compartment.
- **14:53 UTC**: Traffic to the Shakespeare search increased by 88x after a post to `/r/shakespeare` pointed to the search engine.
- **14:54 UTC**: Outage began as search backends started failing under the load.
- **14:55 UTC**: Pager storm, ManyHttp500s in all clusters.
- **14:57 UTC**: All traffic to the Shakespeare search was failing.
- **14:58 UTC**: Docbrown started investigating and found a very high backend crash rate.
- **15:01 UTC**: Incident declared, coordination on `#shakespeare`, Jennifer named incident commander.
- **15:02 UTC**: An email about the sonnet discovery was sent to `shakespeare-discuss@`.
- **15:03 UTC**: Jennifer notified the `shakespeare-incidents@` list.
- **15:04 UTC**: Martym tracked down the text of the new sonnet and looked for documentation on corpus updates.
- **15:06 UTC**: Docbrown found identical crash symptoms across all tasks and clusters, investigating based on application logs.
- **15:07 UTC**: Martym found documentation and started prep work for the corpus update.
- **15:10 UTC**: Martym added the sonnet to the known works and started the indexing job.
- **15:12 UTC**: Docbrown contacted Clarac and Agoogler to help examine the codebase for possible causes.
- **15:18 UTC**: Clarac found the smoking gun in the logs, confirming a file descriptor leak if a term not in the corpus was searched.
- **15:20 UTC**: Martym's indexing job completed.
- **15:21 UTC**: Jennifer and Docbrown decided to increase instance count to reduce load on instances.
- **15:23 UTC**: Docbrown load balanced all traffic to the USA-2 cluster, allowing instance count increases in other clusters.
- **15:25 UTC**: Martym started replicating the new index to all clusters.
- **15:28 UTC**: Docbrown started a 2x instance count increase.
- **15:32 UTC**: Jennifer changed load balancing to increase traffic to non-sacrificial clusters.
- **15:33 UTC**: Tasks in non-sacrificial clusters started failing with the same symptoms.
- **15:34 UTC**: Found an order-of-magnitude error in whiteboard calculations for the instance count increase.
- **15:36 UTC**: Jennifer reverted load balancing to resacrifice the USA-2 cluster, preparing for a 5x instance count increase.
- **15:36 UTC**: Outage mitigated, updated index replicated to all clusters.
- **15:39 UTC**: Docbrown started a second wave of instance count increase to 10x initial capacity.
- **15:41 UTC**: Jennifer reinstated load balancing across all clusters for 1% of traffic.
- **15:43 UTC**: Non-sacrificial clusters' HTTP 500 rates at nominal levels, task failures intermittent and low.
- **15:45 UTC**: Jennifer balanced 10% of traffic across non-sacrificial clusters.
- **15:47 UTC**: Non-sacrificial clusters' HTTP 500 rates within SLO, no task failures observed.
- **15:50 UTC**: 30% of traffic balanced across non-sacrificial clusters.
- **15:55 UTC**: 50% of traffic balanced across non-sacrificial clusters.
- **16:00 UTC**: Outage ended, all traffic balanced across all clusters.
- **16:30 UTC**: Incident ended, reached exit criterion of 30 minutes of nominal performance.

### Supporting Information
- **Monitoring Dashboard**: [http://monitor/shakespeare?end_time=20151021T160000&duration=7200](http://monitor/shakespeare?end_time=20151021T160000&duration=7200)

### Launch Coordination Checklist
**Architecture**
- Architecture sketch, types of servers, types of client requests.
- Programmatic client requests.

**Machines and Datacenters**
- Machines, bandwidth, datacenters, N+2 redundancy, network QoS.
- New domain names, DNS load balancing.

**Volume Estimates, Capacity, and Performance**
- HTTP traffic and bandwidth estimates, launch spike, traffic mix, 6 months out.
- Load test, end-to-end test, capacity per datacenter at max latency.
- Impact on other services, storage capacity.

**System Reliability and Failover**
- What happens when:
  - Machine, rack, or cluster fails.
  - Network fails between two datacenters.
- For each type of server that talks to other servers (backends):
  - How to detect and handle backend failures.
  - How to terminate or restart without affecting clients or users.
  - Load balancing, rate-limiting, timeout, retry, and error handling behavior.
- Data backup/restore, disaster recovery.

**Monitoring and Server Management**
- Monitoring internal state, end-to-end behavior, managing alerts.
- Monitoring the monitoring.
- Financially important alerts and logs.
- Tips for running servers within a cluster environment.
- Avoid crashing mail servers by sending email alerts in your own server code.

**Security**
- Security design review, security code audit, spam risk, authentication, SSL.
- Prelaunch visibility/access control, various types of blacklists.

**Automation and Manual Tasks**
- Methods and change control to update servers, data, and configs.
- Release process, repeatable builds, canaries under live traffic, staged rollouts.

**Growth Issues**
- Spare capacity, 10x growth, growth alerts.
- Scalability bottlenecks, linear scaling, scaling with hardware, changes needed.
- Caching, data sharding/resharding.

**External Dependencies**
- Third-party systems, monitoring, networking, traffic volume, launch spikes.
- Graceful degradation, avoiding overrunning third-party services.
- Playing nice with syndicated partners, mail systems, services within Google.

**Schedule and Rollout Planning**
- Hard deadlines, external events, Mondays or Fridays.
- Standard operating procedures for this and other services.

### Example Production Meeting Minutes
**Date**: 2015-10-23  
**Attendees**: Agoogler, Clarac, Docbrown, Jennifer, Martym

**Announcements**
- Major outage (#465), blew through error budget.

**Previous Action Item Review**
- **Certify Goat Teleporter for Use with Cattle (Bug 1011101)**: Nonlinearities in mass acceleration now predictable, should be able to target accurately in a few days.

**Outage Review**
- **New Sonnet (Outage 465)**: 1.21B queries lost due to a cascading failure after interaction between a latent bug (file descriptor leak on searches with no results) and the absence of the new sonnet in the corpus, combined with unprecedented and unexpected traffic volume.
  - **File Descriptor Leak Bug Fixed (Bug 5554825)**: Deployed to production.
  - **Using Flux Capacitor for Load Balancing (Bug 5554823)** and **Load Shedding (Bug 5554826)**: Investigating to prevent recurrence.
  - **Availability Error Budget Annihilated**: Pushes to production frozen for 1 month unless Docbrown can obtain an exception on grounds that the event was bizarre and unforeseeable (consensus is that the exception is unlikely).

**Paging Events**
- **AnnotationConsistencyTooEventual**: Paged 5 times this week, likely due to cross-regional replication delay between Bigtables.
  - **Investigation Ongoing**: See Bug 4821600.
  - **No Fix Expected Soon**: Will raise acceptable consistency threshold to reduce unactionable alerts.

**Nonpaging Events**
- None

**Monitoring Changes and/or Silences**
- **AnnotationConsistencyTooEventual**: Acceptable delay threshold raised from 60s to 180s, see Bug 4821600; TODO(Martym).

**Planned Production Changes**
- **USA-1 Cluster Maintenance**: Offline between 2015-10-29 and 2015-11-02.
  - No response required, traffic will automatically route to other clusters in the region.

**Resources**
- **Borrowed Resources for Sonnet++ Incident**: Will spin down additional server instances and return resources next week.
- **Utilization**: 60% CPU, 75% RAM, 44% disk (up from 40%, 70%, 40% last week).

**Key Service Metrics**
- **OK 99ile Latency**: 88 ms < 100 ms SLO target (trailing 30 days).
- **BAD Availability**: 86.95% < 99.99% SLO target (trailing 30 days).

**Discussion / Project Updates**
- **Project Molière Launching in Two Weeks**.

**New Action Items**
- **TODO(Martym)**: Raise AnnotationConsistencyTooEventual threshold.
- **TODO(Docbrown)**: Return instance count to normal and return resources.

### Bibliography
- [Ada15] Bram Adams, Stephany Bellomo, Christian Bird, Tamara Marshall-Keim, Foutse Khomh, and Kim Moir, “The Practice and Future of Release Engineering: A Roundtable with Three Release Engineers”, IEEE Software, vol. 32, no. 2 (March/April 2015), pp. 42–49.
- [Agu10] M. K. Aguilera, “Stumbling over Consensus Research: Misunderstandings and Issues”, in Replication, Lecture Notes in Computer Science 5959, 2010.
- [All10] J. Allspaw and J. Robbins, Web Operations: Keeping the Data on Time: O’Reilly, 2010.
- [All12] J. Allspaw, “Blameless PostMortems and a Just Culture”, blog post, 2012.
- [All15] J. Allspaw, “Trade-Offs Under Pressure: Heuristics and Observations of Teams