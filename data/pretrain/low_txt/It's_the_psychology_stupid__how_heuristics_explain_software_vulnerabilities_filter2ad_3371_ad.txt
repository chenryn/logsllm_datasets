### Optimized Text

#### Introduction
Development tools, such as integrated development environments (IDEs) and compilers, can provide security information to developers at the exact moment they need it: while coding. This information should be closely related to their current working scenario to increase the likelihood that security cues will be incorporated into the developer’s heuristics. Our insights aim to influence the next generation of developer tools and applications, ensuring that more secure software reaches the market. While this approach does not solve all the multifaceted challenges of cybersecurity, it can help illuminate developers' mental blind spots in vulnerabilities and secure programming.

#### 7. Related Work
The work presented in this paper intersects with the areas of vulnerability analysis, information security perception, and cognitive and human factors. This section discusses the related work in these areas.

##### 7.1 Vulnerability Studies
The first efforts to understand software vulnerabilities began in the 1970s with the RISOS Project, which investigated security flaws in operating systems [29]. Around the same time, the Protection Analysis study [30] focused on developing vulnerability detection tools to assist developers. Subsequent studies, such as the taxonomies by Landwehr et al. [31] and Aslam [32], furthered this research. In the 1990s, Bishop and Bailey [33] analyzed existing vulnerability taxonomies and concluded that they were imperfect, as a vulnerability could be classified in multiple ways depending on the layer of abstraction considered. More recently, Crandall and Oliveira [34] proposed a view of software vulnerabilities as fractures in the interpretation of information as it flows across abstraction boundaries.

Other studies have discussed theoretical and computational aspects of exploit techniques and input normalization. Bratus et al. [35] argued that the theoretical language aspects of computer science lie at the heart of practical security problems, particularly exploitable vulnerabilities. Samuel and Erlingsson [36] proposed input normalization via parsing as an effective way to prevent vulnerabilities that allow attackers to break out of data contexts. Garg and Camp [37] identified systematic errors by decision-makers using heuristics and suggested ways to improve security designs for risk-averse individuals.

Researchers have also studied vulnerability trends. Browne et al. [38] determined that the rates of incidents reported to CERT could be mathematically modeled. Gopalakrishna and Spafford [39] analyzed software vulnerabilities in five critical software artifacts using public vulnerability databases to predict trends. Alhazmi et al. [40] presented a vulnerability discovery model to predict short and long-term vulnerabilities for several major operating systems. Anbalagan and Vouk [41] analyzed and classified thousands of vulnerabilities from the OSVDB [42] and discovered a relationship between vulnerabilities and exploits. Wu et al. [43] performed an ontology-guided analysis of vulnerabilities and studied how semantic templates can be leveraged to identify further information and trends. Zhang et al. [44] used machine learning to analyze vulnerabilities from the NVD database but were unsuccessful in predicting the time to the next vulnerability for a given software application.

There are also studies on developer practices. Meneely and Williams [45] studied developers' collaboration and unfocused contributions, statistically correlating them with developer activity metrics. Schryen [46] analyzed the patching behavior of open-source and closed-source software vendors, finding that the policy of a particular vendor is the most influential factor on patching behavior.

However, none of these research efforts directly leverage the human factor to understand software vulnerabilities as proposed in this paper.

##### 7.2 Information Security Perception
Asghapours et al. [47] advocate the use of mental models of computer security risks to improve risk communication to naive end users. Risk communication involves messages from security experts to non-experts, and a mental model is a simplified internal concept of how something works in reality. Their study leveraged five conceptual models from the literature: Physical Safety, Medical Infections, Criminal Behavior, Warfare, and Economic Failures.

Huang et al. [48] studied ways to adjust people's perception of information security to increase their intention to adopt IT appliances and comply with security practices. Their user study involving e-banking and passwords showed that knowledge is a key factor influencing the gap between perceived and actual security.

Garg and Camp [49] used Fischhoff’s canonical nine-dimensional model of offline risk perception [50] to better understand online risk perceptions. They found that the results for online risks differed from those for offline risks, with the severity of a risk being the biggest factor in shaping risk perception.

Research has also been done on computer warnings for end users [51, 52]. In computer security, warnings are designed to protect people from becoming victims of attacks, such as phishing, malware installation, and email spam. Researchers have found that people tend to ignore messages that do not map well onto a clear course of action [51]. This supports our hypothesis that unless the cue is related to the working scenario, it will likely be overlooked by the decision-maker.

All these studies consider information security perception from the non-expert end-user viewpoint and not from the perspective of developers.

##### 7.3 Human Factors in Software Development
Using human factors in technology research is not a new concept. Curtis, Krasner, and Iscoe [53] studied software development processes by interviewing programmers from 17 large software development projects. They aimed to understand the effects of behavioral and cognitive processes on software productivity and believed that software quality could be improved by addressing the issues they discovered. They summarized the study by describing the implications of their interviews and observations on different aspects of the software development process, including team building, software tools, and development environment.

Other researchers have recognized the role of cognition in program representation and comprehension [54, 55], design strategies and patterns [56, 57], and software design [58, 59]. These studies show the evolution of design paradigms and development tools from task-centered to human-centered. Current software development tools are very good at pinpointing errors and making sensible suggestions to avoid problems later. New derivatives are created to assist programmers, helping to make the software development process less error-prone. These studies paved the way for secure software development from the human aspect.

We believe this paper builds on these studies and investigates deeper into human factor issues, particularly to understand the impact of cognitive processes on software vulnerabilities. Like previous studies that led to better software development tools, faster turnaround, and more robust software integration, the insights from this study can help the software security community gain insights, improve software security, and design more effective vulnerability blind spot tools.

#### 8. Conclusions
This paper investigated the hypothesis that software vulnerabilities are blind spots in developers' heuristics during their daily coding activities. Humans have evolved to adopt shortcuts and heuristics in decision-making due to limitations in their working memory. As vulnerabilities often lie in uncommon code paths, and the market generates perverse incentives for insecure software, security information is often left out of developers' coding strategies.

A study with 47 developers using psychological manipulation was conducted to validate this hypothesis. Each developer worked for approximately one hour on six programming scenarios that contained vulnerabilities. The developers were told that the study's goal was to understand their mental models while coding. The sessions progressed from providing no information about possible vulnerabilities, to priming developers about unexpected results, and explicitly mentioning the existence of vulnerabilities in the code. The results showed that developers generally changed their mindset towards security when primed about vulnerabilities on the spot. When not primed, even developers familiar with certain vulnerabilities failed to correlate them with their working scenario and fix them in the code when given a chance. Therefore, the assumption that developers should be educated about security and then apply what they learned while coding goes against the way the human brain behaves. This paper advocates that this assumption be reversed and that security information should reach developers when they need it, on the spot, and correlated to their tasks at hand. The authors hope that these insights can influence the next generation of tools interfacing with developers, such as IDEs, text editors, browsers, and compilers, so that more secure software reaches the market.

Plans for future work include investigating the best methodologies for cueing developers and analyzing the correlation between cueing effectiveness and previous security education.

#### Acknowledgments
We would like to thank the developers who participated in this study and the anonymous reviewers for valuable feedback. This research is funded by the National Science Foundation under grants CNS-1149730, CNS-1223588, and CNS-1205415.

#### References
[1] “Symantec Internet Security Threat Report 2013.” http://www.symantec.com/content/en/us/enterprise/other_resources/b-istr_main_report_v18_2012_21291018.en-us.pdf.
[2] B. K. Marshall, “PasswordResearch.com Authentication News: Passwords Found in the Wild for January 2013.” http://blog.passwordresearch.com/2013/02/passwords-found-in-wild-for-january-2013.html.
[3] “Seventeen steps to safer C code.” http://www.embedded.com/design/programming-languages-and-tools/4215552/Seventeen-steps-to-safer-C-code.
[4] D. Kahneman and A. Tversky, “On the reality of cognitive illusions,” Psychological Review, pp. 582–591, 1996.
[5] G. Gigerenzer, R. Hertwig, and T. Pachir, Heuristics: The Foundations of Adaptive Behavior. Oxford University Press, 2011.
[6] B. Schwartz, “The tyranny of choice,” Scientific American, pp. 71–75, 2004.
[7] S. Botti and S. S. Iyengar, “The dark side of choice: When choice impairs social welfare,” American Marketing Association, pp. 24–38, 2006.
[8] C. Kern, A. Kesavan, and N. Daswani, Foundations of security: what every programmer needs to know. Apress, 2007.
[9] E. Harmon-Jones, D. M. Amodio, and L. R. Zinner, Social psychological methods of emotion elicitation (Handbook of Emotion Elicitation and Assessment). Oxford University Press, 2007.
[10] W. Thorngate, “Efficient decision heuristics,” Behavioral Science, vol. 25, no. 3, pp. 219–225, 1980.
[11] K. V. Katsikopoulos, “Efficient decision heuristics,” Decision Analysis, vol. 8, no. 1, pp. 10–29, 2011.
[12] J. W. Payne, J. R. Bettman, and E. J. Johnson, The Adaptive Decision Maker. Cambridge University Press, 1993.
[13] G. K. Zipf, Human Behavior and The Principle of Least Effort. Addison-Wesley, 1949.
[14] J. Rieskamp and U. Hoffrage, Simple Heuristics that Make Us Smart. Oxford University Press, 1999.
[15] C. Cowan, C. Pu, D. Maier, J. Walpole, P. Bakke, S. Beattie, A. Grier, P. Wagle, Q. Zhang, and H. Hinton, “StackGuard: Automatic adaptive detection and prevention of buffer-overflow attacks,” in USENIX Security, pp. 63–78, Jan 1998.
[16] G. Wassermann and Z. Su, “Static Detection of Cross-site Scripting Vulnerabilities,” in 30th International Conference on Software Engineering, ICSE ’08, (New York, NY, USA), ACM, 2008.
[17] Z. Su and G. Wassermann, “The Essence of Command Injection Attacks in Web Applications,” in Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’06, (New York, NY, USA), pp. 372–382, ACM, 2006.
[18] “Urllib and validation of server certificate.” http://stackoverflow.com/questions/6648952/.
[44] S. Zhang, D. Caragea, and X. Ou, “An Empirical Study on using the National Vulnerability Database to Predict Software Vulnerabilities,” International Conference on Database and Expert Systems Applications (DEXA), 2011.
[45] A. Meneely and L. Williams, “Secure Open Source Collaboration: An Empirical Study of Linus’ Law,” ACM CCS, pp. 453–462, 2009.
[46] G. Schryen, “A comprehensive and comparative analysis of the patching behavior of open source and closed source software vendors,” IMF, 2009.
[47] F. Asgapour, D. Liu, and L. J. Camp, “Mental models of computer security risks,” Financial Cryptography and Data Security Lecture Notes in Computer Science, vol. 4886, pp. 367–377, 2007.
[48] D.-L. Huang, Pei-Luen, P. R. abd Gavriel Salvendya, F. Gaoa, and J. Zhoua, “Factors affecting perception of information security and their impacts on IT adoption and security practices,” International Journal of Human-Computer Studies, vol. 69, no. 12, 2011.
[49] V. Garg and L. J. Camp, “End user perception of online risk under uncertainty,” Hawaii International Conference On System Sciences, vol. 4886, 2012.
[50] B. Fischhoff, P. Slovic, S. Lichtenstein, and B. C. Stephen Read, “How safe is safe enough? A psychometric study of attitudes towards technological risks and benefits,” Policy Sciences, vol. 9, no. 2, 1978.
[51] C. Bravo-Lillo, L. Cranor, J. Downs, and S. Komanduri, “Bridging the gap in computer security warnings: A mental model approach,” IEEE Security and Privacy, vol. 9, no. 2, 2011.
[52] K. Witte, “Putting the fear back into fear appeals: The extended parallel process model,” Communication Monographs, vol. 59, no. 4, pp. 329–349, 1992.
[53] B. Curtis, H. Krasner, and N. Iscoe, “A field study of the software design process for large systems,” Communications of the ACM, vol. 31, no. 11, pp. 1268–1287, 1988.
[54] S. Letovsky, “Cognitive processes in program comprehension,” Journal of Systems and Software, vol. 7, no. 4, pp. 325–339, 1987.
[55] H. C. Purchase, L. Colpoys, M. McGill, D. Carrington, and C. Britton, “UML class diagram syntax: an empirical study of comprehension,” in Proceedings of the 2001 Asia-Pacific Symposium on Information Visualization-Volume 9, pp. 113–120, Australian Computer Society, Inc., 2001.
[56] A. Chatzigeorgiou, N. Tsantalis, and I. Deligiannis, “An empirical study on students ability to comprehend design patterns,” Computers & Education, vol. 51, no. 3, pp. 1007–1016, 2008.
[57] W. Visserl, J.-M. Hocz, and F. Chesnay, “Expert software design strategies,” 1990.
[58] R. Jeffries, A. A. Turner, P. G. Polson, and M. E. Atwood, “The processes involved in designing software,” Cognitive Skills and Their Acquisition, pp. 255–283, 1981.
[59] B. Adelson and E. Soloway, “A model of software design,” International Journal of Intelligent Systems, vol. 1, no. 3, pp. 195–213, 1986.
[19] W. S. McPhee, “Operating System Integrity in OS/VS2,” IBM Systems Journal, vol. 13, no. 3, pp. 230–252, 1974.
[20] A. Narayanan and V. Shmatikov, “Fast dictionary attacks on passwords using time-space tradeoff,” ACM CCS, 2005.
[21] R. E. Stake, Qualitative Research: Studying How Things Work. The Guilford Press, 2010.
[22] “Qualtrics (http://www.qualtrics.com/).”
[23] F. Gravetter and L. Wallnau, Statistics for the Behavioral Sciences. Wadsworth/Thomson Learning, 8th ed., 2009.
[24] R. S. Weiss, Learning from Strangers - The Art and Method of Qualitative Interview Studies. The Free Press, 1994.
[25] J. Saldana, The Coding Manual for Qualitative Researchers. SAGE Publications, 2012.
[26] A. Newell and H. Simon, Human Problem Solving. Prentice Hall, 1972.
[27] D. Denning, “A lattice model of secure information flow,” Communications of ACM, 1976.
[28] R. Anderson, “Why information security is hard - an economic perspective,” ACSAC, 2001.
[29] R. P. Abbot, J. S. Chin, J. E. Donnelley, W. L. Konigsford, and D. A. Webb, “Security Analysis and Enhancements of Computer Operating Systems,” NBSIR 76-1041, Institute for Computer Sciences and Technology, National Bureau of Standards, 1976.
[30] R. B. II and D. Hollingsworth, “Protection Analysis Project Final Report,” ISI/RR-78-13, DTIC AD A056816, USC/Information Sciences Institute, 1978.
[31] C. E. Landwehr, A. R. Bull, J. P. McDermott, and W. S. Choi, “A Taxonomy of Computer Program Security Flaws,” ACM Computing Surveys, vol. 26, no. 3, 1994.
[32] T. Aslam, “A Taxonomy of Security Faults in the UNIX Operating System,” 1995.
[33] M. Bishop and D. Bailey, “A Critical Analysis of Vulnerability Taxonomies,” Technical Report CSE-96-11, University of California at Davis, 1996.
[34] J. Crandall and D. Oliveira, “Holographic Vulnerability Studies: Vulnerabilities as Fractures in Interpretation as Information Flows Across Abstraction Boundaries,” New Security Paradigms Workshop (NSPW), 2012.
[35] S. Bratus, M. E. Locasto, M. L. Patterson, L. Sassaman, and A. Shubina, “Exploit Programming: From Buffer Overflows to ‘Weird Machines’ and Theory of Computation.” USENIX ;login, December 2011.
[36] M. Samuel and U. Erlingsson, “Let’s Parse to Prevent pwnage (invited position paper),” in Proceedings of the 5th USENIX conference on Large-Scale Exploits and Emergent Threats, LEET’12, (Berkeley, CA, USA), pp. 3–3, USENIX Association, 2012.
[37] V. Garg and L. J. Camp, “Heuristics and biases: Implications for security,” IEEE Technology & Society, March 2013.
[38] H. K. Browne, W. A. Arbaugh, J. McHugh, and W. L. Fithen, “A trend analysis of exploitations,” IEEE Symposium on Security and Privacy, 2001.
[39] R. Gopalakrishna and E. H. Spafford, “A Trend Analysis of Vulnerabilities,” CERIAS Tech Report 2005-05, 2005.
[40] O. H. Alhazmi and Y. K. Malaiya, “Prediction capabilities of vulnerability discovery models,” IEEE Reliability and Maintainability Symposium (RAMS), pp. 86–91, 2006.
[41] O. H. Alhazmi and Y. K. Malaiya, “Towards a unifying approach in understanding security problems,” IEEE International Conference on Software Reliability Engineering (ISSRE), pp. 136–145, 2009.
[42] “Open Source Vulnerability Database (http://www.osvdb.org/).”
[43] Y. Wu, R. A. Gandhi, and H. Siy, “Using Semantic Templates to Study Vulnerabilities Recorded in Large Software Repositories,” ICSE Workshop on Software Engineering for Secure Systems, 2010.