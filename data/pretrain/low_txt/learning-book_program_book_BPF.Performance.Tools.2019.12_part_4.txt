# Figure Captions and Copyrights

**Figure 17-10:** Screenshot of `grafana-pcp-live`, Copyright 2019 © Grafana Labs.

**Figures 17-11 to 17-14:** Screenshots of Grafana, Copyright 2019 © Grafana Labs.

# Supplementary Material and References

Readers are encouraged to visit the book's website:
<http://www.brendangregg.com/bpf-performance-tools-book.html>

All tools, errata, and reader feedback can be downloaded from this site. Many of the tools discussed in the book are also available in source code repositories, where they are maintained and enhanced. For the latest versions, refer to these repositories:

- <https://github.com/iovisor/bcc>
- <https://github.com/iovisor/bpftrace>

These repositories also contain detailed reference guides and tutorials, which I created and the BPF community maintains and updates.

---

## Page 33: Conventions Used in This Book

This book discusses various types of technology, and the way it presents material provides additional context.

- **Tool Output:** Bold text indicates the command that was executed or, in some cases, highlights something of interest.
- **Prompts:**
  - A hash prompt (`#`) signifies that the command or tool has been run as the root user (administrator).
  - A dollar prompt (`$`) signifies running the command or tool as a non-root user.
  - Some prompts include a directory name prefix to show the working directory.

- **Italic Text:** Italic is used to highlight new terms and sometimes to show placeholder text.

- **Root Access:** If you are not root, you can execute tools as root by prefixing them with `sudo` for the `sudo(8)` command (super-user do).

- **Shell Expansions:** Commands are sometimes executed in single quotation marks to prevent unnecessary (albeit unlikely) shell expansions. This is a good habit to form. For example:
  ```sh
  funccount 'vfs_*'
  ```

- **Linux Commands and System Calls:** A Linux command name or system call is followed by the man page chapter enclosed in parentheses. For example, the `ls(1)` command, the `read(2)` system call, and the `funccount(8)` system administration command. Empty parentheses signify function calls from a programming language, such as the `vfs_read()` kernel function.

- **Command Output Truncation:** Command output that is truncated includes an ellipsis in square brackets (`[...]`). A single line containing `^C` indicates that `Ctrl-C` was typed to terminate the program.

- **Bibliography References:** Website references are numbered, e.g., [123].

---

## Page 34: Acknowledgments

Many people have contributed to the development of the components necessary for the BPF tracing tools. Their work often goes unnoticed but is crucial for the BPF tools you will use. I would like to acknowledge and thank them, as well as others who contributed to this book.

- **eBPF:** Thanks to Alexei Starovoitov (Facebook; formerly PLUMgrid) and Daniel Borkmann (Isovalent; formerly Cisco, Red Hat) for creating and leading the development of eBPF. Thanks also to all other eBPF contributors, particularly David S. Miller (Red Hat) for supporting and improving the technology. As of writing, there are 249 different contributors to BPF kernel code with a total of 3,224 commits since 2014. Top contributors based on commit counts include Jakub Kicinski (Netronome), Yonghong Song (Facebook), Martin KaFai Lau (Facebook), John Fastabend (Isovalent; formerly Intel), Quentin Monnet (Netronome), Jesper Dangaard Brouer (Red Hat), Andrey Ignatov (Facebook), and Stanislav Fomichev (Google).

- **BCC:** Thanks to Brendan Blanco (VMware; formerly PLUMgrid) for creating and developing BCC. Major contributors include Sasha Goldshtein (Google; formerly SELA), Yonghong Song (Facebook; formerly PLUMgrid), Teng Qin (Facebook), Paul Chaignon (Orange), Vicent Marti (GitHub), Mark Drayton (Facebook), Allan McAleavy (Sky), and Gary Ching-Pang Lin (SUSE).

- **bpftrace:** Thanks to Alastair Robertson (Yellowbrick Data; formerly G-Research, Cisco) for creating bpftrace and insisting on quality code and extensive tests. Thanks to all other bpftrace contributors, especially Matheus Marchini (Netflix; formerly Shtima), Willian Gasper (Shtima), Dale Hamel (Shopify), Augusto Mecking Caringi (Red Hat), and Dan Xu (Facebook).

- **ply:** Thanks to Tobias Waldekranz for developing the first high-level tracer built upon BPF.

- **LLVM:** Thanks to Alexei Starovoitov, Chandler Carruth (Google), Yonghong Song, and others for their work on the BPF backend for LLVM, which BCC and bpftrace are built upon.

- **kprobes:** Thanks to all those who designed, developed, and worked on kernel dynamic instrumentation for Linux, including Richard Moore (IBM), Suparna Bhattacharya (IBM), Vamsi Krishna Sangavarapu (IBM), Prasanna S. Panchamukhi (IBM), Ananth N Mavinakayanahalli (IBM), James Keniston (IBM), Naveen N Rao (IBM), Hien Nguyen (IBM), Masami Hiramatsu (Linaro; formerly Hitachi), Rusty Lynch (Intel), Anil Keshavamurthy (Intel), Rusty Russell, Will Cohen (Red Hat), and David S. Miller (Red Hat).

- **uprobes:** Thanks to Srikar Dronamraju (IBM), Jim Keniston, and Oleg Nesterov (Red Hat) for developing user-level dynamic instrumentation for Linux, and Peter Zijlstra for technical review.

- **tracepoints:** Thanks to Mathieu Desnoyers (EfficiOS) for his contributions to Linux tracing, particularly for developing and driving static tracepoints to be accepted in the kernel, making it possible to build stable tracing tools and applications.

- **perf:** Thanks to Arnaldo Carvalho de Melo (Red Hat) for his work on the `perf(1)` utility, which added kernel capabilities that BPF tools make use of.

- **Ftrace:** Thanks to Steven Rostedt (VMware; formerly Red Hat) for Ftrace and his other contributions to tracing. Ftrace has aided BPF tracing development, and Tom Zanussi (Intel) has recently been contributing with Ftrace hist triggers.

- **(Classic) BPF:** Thanks to Van Jacobson and Steve McCanne.

- **Dynamic Instrumentation:** Thanks to Professor Barton Miller (University of Wisconsin-Madison) and his then-student Jeffrey Hollingsworth for founding the field of dynamic instrumentation in 1992 [Hollingsworth 94], which has been the key feature driving the adoption of DTrace, SystemTap, BCC, bpftrace, and other dynamic tracers.

- **LTT:** Thanks to Karim Yaghmour and Michel R. Dagenais for developing the first Linux tracer, LTT, in 1999. Also, thanks to Karim for his unrelenting push for tracing in the Linux community, building support for later tracers.

- **Dprobes:** Thanks to Richard J. Moore and his team at IBM for developing the first dynamic instrumentation technology for Linux, DProbes, in 2000, which led to the kprobes technology we use today.

- **SystemTap:** While SystemTap is not used in this book, the work by Frank Ch. Eigler (Red Hat) and others on SystemTap has greatly improved the field of Linux tracing.

- **ktap:** Thanks to Jovi Zhangwei for ktap, a high-level tracer that helped build support in Linux for VM-based tracers.

- **DTrace:** Thanks to Bryan Cantrill, Mike Shapiro, and Adam Leventhal, for their outstanding work in developing the first widely-used dynamic instrumentation technology: DTrace, launched in 2005. Thanks to Sun marketing, evangelists, sales, and many others inside and outside of Sun, for helping make DTrace known worldwide, driving demand for similar tracers in Linux.

Thanks to the many others not listed here who have also contributed to these technologies over the years.

Apart from creating these technologies, many of the same people have helped with this book. Daniel Borkmann provided amazing technical feedback and suggestions for several chapters, and Alexei Starovoitov also provided critical feedback and advice for the eBPF kernel content (as well as writing the Foreword). Alastair Robertson provided input on the bpftrace chapter, and Yonghong Song provided feedback for the BTF content while he was developing BTF.

Thanks to: Matheus Marchini (Netflix), Paul Chaignon (Orange), Dale Hamel (Shopify), Ames Ather (Netflix), Martin Spier (Netflix), Brian W. Kernighan (Google), Joel Fernandes (Google), Jesper Brouer (Red Hat), Greg Dunn (AWS), Julia Evans (Stripe), Toke Heiland-Jorgensen (Red Hat), Stanislav Kozina (Red Hat), Jiri Olsa (Red Hat), Jens Axboe (Facebook), Jon Haslam (Facebook), Andri Nakryiko (Facebook), Sargun Dhillon (Netflix), Alex Maestretti (Netflix), Joseph Lynch (Netflix), Richard Elling (Viking Enterprise Solutions), Bruce Curtis (Netflix), and Javier Honduvilla Coto (Facebook). Many sections have been rewritten, added, and improved thanks to their help. I also had some help on a couple of sections from Mathieu Desnoyers (EfficiOS) and Masami Hiramatsu (Linaro). Claire Black also provided a final check and feedback for many chapters.

My colleague Jason Koch wrote much of the "Other Tools" chapter and provided feedback on almost every chapter in the book (hand-annotated on a printed copy about two inches thick). The Linux kernel is complicated and ever-changing, and I appreciate the stellar work by Jonathan Corbet and Jake Edge of LWN.net for summarizing so many deep topics. Many of their articles are referenced in the Bibliography.

Myself and others have written thousands of lines of code to make the tools in this book possible. Special thanks to Matheus Marchini, Willian Gasper, Dale Hamel, Dan Xu, and Augusto Caringi for timely fixes.

Thanks to my current and former Netflix managers, Ed Hunter and Coburn Watson, for their support of my BPF work while at Netflix. Also, thanks to my colleagues on the OS team, Scott Emmons, Brian Moyles, and Gabrielle Munoz, for helping to get BCC and bpftrace installed on production servers at Netflix, from which I was able to fetch many example screenshots.

Thanks to Deirdre Straughan (AWS), now my wife, for her professional technical editing and suggestions, and general support of yet another book. My writing has greatly improved thanks to her help over the years. And thanks to my son Mitchell for support and sacrifices while I was busy with the book.

This book is inspired by the DTrace book written by myself and Jim Mauro. Jim's hard work to make the DTrace book a success, and our endless discussions on book structure and tool presentation, have contributed to the quality of this book. Jim has also made many direct contributions to this book. Thanks, Jim, for everything.

Special thanks to Senior Editor Greg Doench at Pearson for his help and enthusiasm for this project.

Working on this book has been an enormous privilege, providing me the opportunity to showcase BPF observability. Of the 156 tools in this book, I developed 135 of them, including 89 new tools for this book (there are over 100 new tools, counting variants, although it was never my intent to hit that milestone!). Creating these new tools required research, configuration of application environments and client workloads, experimentation, and testing. It has been exhausting at times, but it is satisfying to complete, knowing that these tools will be valuable to so many.

Brendan Gregg
San Jose, California (formerly Sydney, Australia)
November 2019

---

## Page 37: About the Author

Brendan Gregg, Netflix senior performance engineer, is a major contributor to BPF (eBPF) who has helped develop and maintain both main BPF front-ends, pioneered BPF's use for observability, and created dozens of BPF-based performance analysis tools. His books include the best-seller "Systems Performance: Enterprise and the Cloud."

---

## Page 38: Chapter Introduction

This chapter introduces the technologies and demonstrates some BPF performance tools. These technologies will be explained in more detail in the following chapters.

### 1.1 What Are BPF and eBPF?

BPF stands for Berkeley Packet Filter, a technology first developed in 1992 to improve the performance of packet capture tools [McCanne 92]. In 2013, Alexei Starovoitov proposed a major rewrite of BPF [2], which was further developed by Alexei and Daniel Borkmann and included in the Linux kernel in 2014 [3]. This turned BPF into a general-purpose execution engine that can be used for a variety of things, including the creation of advanced performance analysis tools.

BPF can be difficult to explain precisely because it can do so much. It provides a way to run mini programs on a wide variety of kernel and application events. If you are familiar with JavaScript, BPF allows the kernel to run mini programs on system and application events, such as disk I/O, thereby enabling new system technologies. It makes the kernel fully programmable, empowering users (including non-kernel developers) to customize and control their systems to solve real-world problems.

BPF is a flexible and efficient technology composed of an instruction set, storage objects, and helper functions. It can be considered a virtual machine due to its virtual instruction set specification. These instructions are executed by a Linux kernel BPF runtime, which includes an interpreter and a JIT compiler for turning BPF instructions into native instructions for execution. BPF instructions must first pass through a verifier that checks for safety, ensuring that the BPF program will not crash or corrupt the kernel (though it does not prevent the end user from writing illogical programs that may execute but not make sense). The components of BPF are explained in detail in Chapter 2.

So far, the three main uses of BPF are networking, observability, and security. This book focuses on observability (tracing).

---

## Page 39: Chapter 1 Introduction

Extended BPF is often abbreviated as eBPF, but the official abbreviation is still BPF without the "e," so throughout this book, I use BPF to refer to extended BPF. The kernel contains only one execution engine, BPF (extended BPF), which runs both extended BPF and "classic" BPF programs.1

### 1.2 What Are Tracing, Snooping, Sampling, Profiling, and Observability?

These are all terms used to classify analysis techniques and tools.

- **Tracing:** Event-based recording, the type of instrumentation that these BPF tools use. You can think of `strace(1)` as an example, which traces and prints system call events. There are many tools that do not trace events but instead measure events using fixed statistical counters and then print summaries, such as `top(1)` on Linux. A hallmark of a tracer is its ability to record raw events and event metadata. Such data can be voluminous and may need to be post-processed into summaries. Programmatic tracers, which BPF makes possible, can run small programs on the events to do custom on-the-fly statistical summaries or other actions, avoiding costly post-processing.

- **Snooping:** While `strace(1)` has "trace" in its name, not all tracers do. `tcpdump(8)`, for example, is another specialized tracer for network packets. (Perhaps it should have been named `tcptrace`?) The Solaris operating system had its own version of `tcpdump` called `snoop(1M)`, so named because it was used to snoop network packets. I was first to develop and publish many tracing tools and did so under the name "snoop." This is why we now have `execsnoop(8)`, `opensnoop(8)`, `biosnoop(8)`, etc. Snooping, event dumping, and tracing usually refer to the same thing. These tools are covered in later chapters.

- **Sampling:** Tools that take a subset of measurements to paint a coarse picture of the target; this is also known as creating a profile or profiling. There is a BPF tool called `profile(8)` that takes timer-based samples of running code. For example, it can sample every 10 milliseconds, or, put differently, it can sample one out of every 100 events. A disadvantage is that sampling provides only a rough picture and can miss events. However, the performance overhead can be lower than that of tracers, as they only measure one out of a much larger set of events.

- **Observability:** Refers to understanding a system through observation and classifies the tools that accomplish this. These tools include tracing tools, sampling tools, and tools based on fixed counters. It does not include benchmark tools, which modify the state of the system by performing a workload experiment. The BPF tools in this book are observability tools, and they use BPF for programmatic tracing.

1. Classic BPF programs, which refers to the original BPF (McCanne 92), are automatically migrated to the extended BPF engine by the kernel for execution. Classic BPF is also not being developed further.
2. For Solaris, section 1M of the man pages is for maintenance and administration commands (section 8 on Linux).

---

## Page 40: 1.3 What Are BCC, bpftrace, and IO Visor?

It is extremely tedious to code BPF instructions directly, so front-ends have been developed that provide higher-level languages; the main ones for tracing are BCC and bpftrace.

### 1.3 What Are BCC, bpftrace, and IO Visor?

**BCC (BPF Compiler Collection):** 
- BCC was the first higher-level tracing framework developed for BPF. It provides a C programming environment for writing kernel BPF code and other languages for the user-level interface: Python, Lua, and C++. It is also the origin of the `libbcc` and current `libbpf` libraries, which provide functions for instrumenting events with BPF programs. The BCC repository also contains more than 70 BPF tools for performance analysis and troubleshooting. You can install BCC on your system and then run the tools provided, without needing to write any BCC code yourself. This book will give you a tour of many of these tools.

**bpftrace:**
- bpftrace is a high-level tracing language for BPF. bpftrace code is so concise that tool source code is usually included in this book, to show what the tool is instrumenting and how it is processed. bpftrace is built upon the `libbcc` and `libbpf` libraries.

**IO Visor:**
- BCC and bpftrace do not live in the kernel code base but in a Linux Foundation project on GitHub called IO Visor. Their repositories are:
  - <https://github.com/iovisor/bcc>
  - <https://github.com/iovisor/bpftrace>

Throughout this book, I use the term "BPF tracing" to refer to both BCC and bpftrace tools.

### 1.4 A First Look at BCC: Quick Wins

Let's cut to the chase and look at some tool output for some quick wins. The following tool traces new processes and prints a one-line summary for each one as it begins. This particular tool, `execsnoop(8)` from BCC, works by tracing the `execve(2)` system call, which is an `exec(2)` variant (hence its name). Installation of BCC tools is covered in Chapter 4, and later chapters will introduce these tools in more detail.

```sh
execsnoop
TIME(s) COMM PID PPID RET ARGS
0.437 run 15524 4469 0 /run
0.438 bash 15524 4469 0 /bin/bash
0.440 svstat 15526 15525 0 /command/svstat /service/httpd
```

The output reveals which processes were executed while tracing: processes that may be so short-lived that they are invisible to other tools. There are many lines of output, showing standard Unix utilities: `ps(1)`, `grep(1)`, `sed(1)`, `cut(1)`, etc. What you can't see just from looking at this output is the time stamp column, which can be added with the `-t` option:

```sh
execsnoop -t
```

This will add a timestamp column to the output, making it easier to understand the sequence of events.