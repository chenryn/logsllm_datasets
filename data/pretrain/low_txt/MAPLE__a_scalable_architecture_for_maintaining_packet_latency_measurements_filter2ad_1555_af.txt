### Observations and Comparisons

The second key observation is that MAPLE, when using true latencies, achieves significantly higher accuracy (up to an order of magnitude) compared to RLI. For example, approximately 90% of flows have an absolute error of less than 1 µs with MAPLE, whereas only 50% of flows achieve this level of accuracy with RLI under moderate utilization conditions (this data is not shown for brevity). In other words, as illustrated in Figure 9, MAPLE reduces the median error by a factor of 5 compared to RLI. This suggests that if implemented, MAPLE could provide more accurate latency estimates than RLI, even with the approximations inherent in its storage data structure. It is worth noting that if actual packet latencies are available to RLI, interpolation would be unnecessary, and per-flow latencies could be obtained with 100% accuracy. However, we do not discuss this straightforward scenario further.

### Implementation

We envision that the streaming k-medians algorithm will be implemented in software. We assume there will be an additional processor or core dedicated to this task, which will perform the k-medians clustering on the sampled data. Prior efforts [29] have explored implementing k-medians directly in hardware, and these can be leveraged in our approach. In environments with limited processing capacity, we can use the static clustering method discussed in §3, although this will result in lower accuracy compared to the hybrid clustering approach.

The storage data structure (SVBF) outlined in Figure 2 will need to be implemented in high-speed SRAM. Bloom filters generally require simple hashing operations and bit map updates, making them suitable for high-speed implementations (see [38] for an example). For the hash functions, we can use H3 [34] or BOB [24], both of which are amenable to easy hardware implementations. Additionally, two extra counters per center will be maintained: one for tracking the number of packets and the other for summing the delays of all packets mapped to a given center. These counters will support the refined latency estimate heuristic (§3.3) and the tie-breaking heuristic (§3.3.4). The SVBF data structure must be flushed to off-chip storage (either DRAM or SSDs) at the end of each epoch, along with the associated k centers for that epoch. For smaller values of k, this requires only a small amount of additional storage. The epoch size can be determined based on technological constraints such as the amount of available high-speed memory and link speeds.

Assuming an OC-192 interface, we expect approximately 5 million packets per second, requiring about 60 Mbits of memory per second (assuming 12 bits per packet). This assumes the interface is running at full capacity, which is often not the case. Latency spikes typically occur under low average utilization due to microbursts [9]. If we assume 20% utilization, only 12 Mbits of memory per second would be needed to capture such latency spikes. A 16 GB DRAM (a commodity today) could store packet latencies for almost 3 hours. Flash memory densities are even higher; a 256 GB SSD could store packet latency state for 47 hours, providing ample time for network operators to debug and process the information. Even at 100% utilization, the DRAM and SSD can sustain for 36 minutes and 9.5 hours, respectively.

Queries will be handled in software. For each query, the appropriate SVBF (and its two neighboring epochs) will be queried by the processor or core, possibly shared with the k-medians implementation. Only the words corresponding to the hash indexes will be fetched from secondary memory (SSD or DRAM), which are then looked up according to the algorithm described in §3.3.3.

### Related Work

There is extensive research on measuring per-hop latencies, particularly in wide-area contexts where ISPs typically rely on injecting active probes and using tomographic approaches [14, 16, 39]. These methods do not meet our high-fidelity measurement requirements (§2.1), necessitating high-fidelity passive measurement mechanisms. In this regard, we have already discussed three relevant prior approaches: LDA [26], RLI [27], and Consistent NetFlow [28].

Storing packet-level information has been explored in other contexts, such as trajectory sampling for identifying packet trajectories [17] and SPIE for IP traceback [37]. Neither provides latency estimates, though trajectory sampling could be augmented with timestamps. However, only a small number of packets are sampled at each router (see [26, 27] for comparisons with trajectory sampling). SPIE, on the other hand, stores only packets without their associated timestamps, making a simple Bloom filter sufficient. In our setting, clustering and SVBF are necessary.

The concept of "in-band" diagnosis was proposed in NetReplay [8] and Orchid [33]. NetReplay suggests replaying packets to collect feedback from the network, while Orchid proposes in-band network troubleshooting, where packets collect feedback from routers along the path. Our approach, however, focuses on estimating, storing, and retrieving packet-level latency measurements, complementing these methods.

Song et al. propose a fast hash table [38] to provide constant lookup time by exploiting counting Bloom filters. While this does not address the large space requirement due to the extra counting Bloom filter and the need to store both packet digests and their delays, several data structures [11, 13, 20] have been proposed to address this issue. COMB [20] is a multi-group membership check data structure highly relevant to our work, and we discussed it in §3.3.

Our SVBF data structure shares similarities with the bit slicing idea proposed in the database community [35, 18]. Specifically, the concept of colocating bits corresponding to different centers that a packet may match is similar to laying out bits corresponding to records in a document on the disk, one of which a given keyword may match. The bit slicing idea could be applied to PBF to make the lookup time complexity close to SVBF's, but it requires uniform BF sizes. Normalizing the sizes of all BFs means each BF must be the size of the largest BF, which is all packets in the worst case, leading to significant wastage.

### Conclusion

This paper introduces MAPLE, a scalable and flexible measurement architecture. The core of the architecture consists of two novel mechanisms: a streaming clustering algorithm to group packet latencies into a small number of clusters in real-time, and a data structure called SVBF to efficiently store packet latencies in a router. Additionally, it provides a flexible query interface for network operators to retrieve the latency of individual packets. Together, the architecture offers both fine-grained and flexible latency measurements, helping network operators manage low-latency applications more effectively. Our evaluations using a software prototype indicate that the architecture can scale efficiently in terms of both storage needs and query bandwidth.

### Acknowledgments

The authors are grateful to Aditya Akella, our shepherd, and the anonymous reviewers for their comments on previous versions of this manuscript. This work was supported in part by NSF Awards CNS 0831647 and 1054788.

### References

[1] Cut-through and store-and-forward ethernet switching for low-latency environments.
http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-465436.html.

[2] Data Center Fabric with Nanosecond Accuracy - Use IEEE1588 PTP on Nexus 3000 Switches.
http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps11541/white_paper_c11-690975.html.

[3] FocalPoint TDM Support.
http://www.fulcrummicro.com/product_library/applications/TDM_App_Note.pdf.

[4] OpenRTB API Specification Version 2.0.
http://www.iab.net/media/file/OpenRTB_API_Specification_Version2.0_FINAL.PDF.

[5] The C Clustering Library.
http://bonsai.hgc.jp/~mdehoon/software/cluster/software.htm.

[6] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: Dynamic flow scheduling for data center networks. In USENIX/ACM NSDI, 2010.

[7] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data center TCP (DCTCP). In ACM SIGCOMM, 2010.

[8] A. Anand and A. Akella. NetReplay: a new network primitive. ACM SIGMETRICS Performance Evaluation Review, 37, 2010.

[9] T. Benson, A. Akella, and D. A. Maltz. Network traffic characteristics of data centers in the wild. In ACM/USENIX IMC, 2010.

[10] B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 1970.

[11] F. Chang, F. Chang, and W. chang Feng. Approximate Caches for Packet Classification. In IEEE INFOCOM, 2004.

[12] M. Charikar, L. O’Callaghan, and R. Panigrahy. Better Streaming Algorithms for Clustering Problems. In ACM STOC, 2003.

[13] B. Chazelle, J. Kilian, R. Rubinfeld, and A. Tal. The bloomier filter: an efficient data structure for static support lookup tables. In ACM SODA, 2004.

[14] Y. Chen, D. Bindel, H. Song, and R. H. Katz. An Algebraic Approach to Practical and Scalable Overlay Network Monitoring. In ACM SIGCOMM, 2004.

[15] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms. The MIT Press, 2nd edition, 2001.

[16] N. Duffield. Simple network performance tomography. In ACM/USENIX IMC, 2003.

[17] N. G. Duffield and M. Grossglauser. Trajectory sampling for direct traffic observation. In IEEE/ACM Transactions on Networking, 2000.

[18] C. Faloutsos and S. Christodoulakis. Signature Files: an Access Method for Documents and Its Analytical Performance Evaluation. ACM Transactions on Information Systems, 2(4):267–288, Oct. 1984.

[19] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: a scalable and flexible data center network. In ACM SIGCOMM, 2009.

[20] F. Hao, M. Kodialam, T. Lakshman, and H. Song. Fast multiset membership testing using combinatorial bloom filters. In IEEE Infocom, 2009.

[21] P. Indyk. A Sublinear Time Approximation Scheme for Clustering in Metric Spaces. In IEEE FOCS, 1999.

[22] P. Indyk. Sublinear Time Algorithms for Metric Space Problems. In ACM STOC, 1999.

[23] A. K. Jain and R. C. Dubes. Algorithms for clustering data: Prentice-Hall, 1981.

[24] B. Jenkins. Algorithm alley. Dr. Dobb’s Journal, September 1997.

[25] D. E. Knuth. The Art of Computer Programming, Volume II: Seminumerical Algorithms, 2nd Edition. Addison-Wesley, 1981.

[26] R. R. Kompella, K. Levchenko, A. C. Snoeren, and G. Varghese. Every MicroSecond Counts: Tracking Fine-grain Latencies Using Lossy Difference Aggregator. In ACM SIGCOMM, 2009.

[27] M. Lee, N. Duffield, and R. R. Kompella. Not All Microseconds are Equal: Fine-Grained Per-Flow Measurements with Reference Latency Interpolation. In ACM SIGCOMM, 2010.

[28] M. Lee, N. Duffield, and R. R. Kompella. Two Samples are Enough: Opportunistic Flow-level latency estimation using NetFlow. In IEEE Infocom, 2010.

[29] M. Leeser, J. Theiler, M. Estlick, and J. Szymanski. Design tradeoffs in a hardware implementation of the k-means clustering algorithm. In Sensor Array and Multichannel Signal Processing Workshop, 2000.

[30] S. Lloyd. Least squares quantization in PCM. Information Theory, IEEE Transactions on, 28(2):129–137, Mar. 1982.

[31] R. Martin. Wall street’s quest to process data at the speed of light.
http://www.informationweek.com/news/infrastructure/showArticle.jhtml?articleID=199200297.

[32] A. Meyerson. Online Facility Location. In IEEE FOCS, 2001.

[33] M. Motiwala, A. Bavier, and N. Feamster. Network troubleshooting: An in-band approach. In USENIX NSDI, 2007.

[34] M. Ramakrishna, E. Fu, and E. Bahcekapili. Efficient hardware hashing functions for high performance computers. IEEE Transactions on Computers, 46(12), Dec. 1997.

[35] C. S. Roberts. Partial-Match Retrieval via the Method of Superimposed Codes. Proceedings of the IEEE, 67(12):1624–1642, Dec. 1979.

[36] A. Shieh, S. Kandula, A. Greenberg, and C. Kim. Seawall: performance isolation for cloud datacenter networks. In USENIX HotCloud, 2010.

[37] A. C. Snoeren, C. Partridge, L. A. Sanchez, C. E. Jones, F. Tchakountio, B. Schwartz, S. T. Kent, and W. T. Strayer. Single-packet IP traceback. IEEE/ACM Transactions on Networking (ToN), 10, 2002.

[38] H. Song, S. Dharmapurikar, J. Turner, and J. Lockwood. Fast Hash Table Lookup Using Extended Bloom Filter: An Aid to Network Processing. In ACM SIGCOMM, 2005.

[39] Y. Zhao, Y. Chen, and D. Bindel. Towards unbiased end-to-end network diagnosis. In ACM SIGCOMM, 2006.