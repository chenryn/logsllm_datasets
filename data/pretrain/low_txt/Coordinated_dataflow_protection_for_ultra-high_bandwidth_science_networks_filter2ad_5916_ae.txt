### 5.1 Intra-Site Tag Assignment

To extend fine-grained traffic filtering capabilities beyond contemporary IP-based mechanisms, we have developed an efficient, context-aware, policy-based tagging mechanism called cTags. This mechanism enables:

- Logical grouping of traffic that spans across subnets, hosts, or geographic locations for policy enforcement.
- Dynamic steering, revocation, or forwarding of traffic across different Network Function Chains (NFCs) based on dynamic network and security conditions.

The tag assignment is carried out by the CNZ Controller at each site, but the actual tag is embedded into the flow by host DTNs (Data Transfer Nodes) for traffic generated from host applications, based on configured policies. The CNZ Coordinator provides conflict-free policies, which are reconciled with site-specific policies and translated into device-specific rules by the CNZ Controller and SDN (Software-Defined Networking) controller before being applied to host DTNs and SDN switches.

The set of rules supplied to each host, referred to as policy-to-tag mappings, includes the following details:
- `tagID{T1} => policyID{P1}:appID{A1}:userID{U1}:expID{E1}`
- `policyID{P1} => policySpec{...}`

These mappings carry the necessary details specific to each policy and the associated entities for enforcement. The SciMon module can dynamically change the flow tagID, even in the middle of flows, based on dynamic network conditions, by updating the policy-to-tag mapping entry.

Each SDMZ site must optimize the number of rules required to enforce policies, considering the availability of high-bandwidth switches and their switch TCAM (Ternary Content-Addressable Memory) space. Tagging facilitates rule space optimization by:
- Allowing a large number of hosts to be grouped into a common logical entity (beyond IP-tuple-based filtering).
- Efficiently assigning contiguous tags such that policies with the same action attributes can be grouped together using bit masking.

Each policy is associated with a unique tag after resolving conflicts among policies. From the composed graph, all nodes and edges associated with a policy are assigned the same tag. The number of tags required is approximately equal to the number of conflict-free policies. Further optimization of tag space utilization in collaborative SDMZ networks is discussed in Section 5.2.

### 5.2 Inter-Site Tag Allocation

In the SDMZ infrastructure, security and data analysis services provided by higher-tier sites (e.g., tier-0 or tier-1 DoE sites) are availed by lower-tier sites. For effective sharing of these services across sites, tags assigned by one site must be honored by other sites handling the same project.

To avoid conflicts in tag space utilization, we propose a unified tag space allocation mechanism that allocates the necessary tag space to each project, including additional slack tag space for future policies. Although the tag space allocation is carried out globally by the CNZ Coordinator, the tag assignment to each policy is performed locally within the site with the help of the CNZ Controller. We use IPv6 flow label bits as tagIDs. Since the 20 bits of the flow label header in IPv6 cannot effectively accommodate the tagging requirements of thousands of projects across hundreds of SDMZ sites, a centralized tag-allocation mechanism is needed to efficiently reuse tag bits across projects spanning multiple sites.

We assign a specific color to each project within a site and reuse the same color among other projects across other sites registered with the CNZ Coordinator, with the following design considerations:
- The tag space should never overlap with the tag space assigned to its immediate adjacent sites with which the current site has project associations.
- The tag size assigned to each project depends on the number of policies enforced by the project.

The key objective of the tag-space allocation mechanism, described in Algorithm 2, is to maximize the efficient reuse of tag space (i.e., colors) among cross-site projects while avoiding overlaps.

Algorithm 2 details the tag-space allocation mechanism used by the CNZ Coordinator to allocate a range of tags to each of its registered projects. The CNZ Coordinator traverses through the list of all SN sites associated with it in a breadth-first-search manner. For any chosen site \( S_i \), its adjacent sites are compared before allocating colors. We observe that for each site \( S_i \) and its adjacent sites, the complete list of available colors can be used in the assignment procedure.

Colors are assigned between \( S_i \) and \( S_{Ai} \). Each of the adjacent sites \( S_{Ai} \) of \( S_i \) (depending on the list of projects belonging to \( S_{Ai} \) that are associated with \( S_i \)) is assigned one color per project (depending on their policy size). Colors are assigned to each project in \( S_{Ai} \) that is associated with \( S_i \) such that:
- It satisfies the project’s tag space requirement.
- The color with the least size is considered for assignment.
- No other projects in \( S_{Ai} \) have the same color already assigned to them.

This procedure is carried out for all the adjacent sites of \( S_i \). When the list of adjacent sites of \( S_i \) is exhausted, the CNZ Coordinator picks the next site from \( S_{Ai} \) as the new \( S_i \), carrying out the aforementioned procedure until all sites in SN are iterated at least once. An example illustration of our algorithmic outcome is shown in Figure 8. The algorithm is quadratic in the number of sites in the worst-case scenario for a fully connected graph (i.e., all sites share all projects). As the number of sites does not change frequently, the overall complexity grows linearly with the number of projects. To further optimize tag space utilization and efficiently reuse the tag space, we propose a technique discussed in Appendix A.5.

### 6 System Evaluation

The CoordiNetZ evaluation platform was composed of Dell R720 servers with 72GB RAM, 24 cores (2.67GHz), and Ubuntu 4.4.0-97-generic kernel, used as DTNs, IDS hosts, and the CoordiNetZ controller (hosting the CNZ Controller and CNZ Coordinator). A quad-core Intel NUC server served as the SDN controller. Dell R710 servers with 48GB RAM, 16 cores (2.6GHz), and Ubuntu 4.4.0-97-generic kernel, integrated with DPDK-based OVS [30], acted as switches and nodes hosting security microservices. Host DTNs were interfaced inline with SDN switches via multiple Mellanox ConnectX-4 Lx 40GbE MT27500 Family 40 Gbps NICs. The server-based DPDK-enabled OVS switches [30] implemented tag-based forwarding and lightweight security services (e.g., rate limiting, spoofing protection, and connection tracking). The SDN controller and CNZ Controller were interfaced with host DTNs, OVS, and IDS service via the management network interface. The CNZ Coordinator and controller communicated via a separate management network.

#### Policy and Infrastructure Datasets

We evaluated our prototype using the following three different datasets:

1. **PS-1**: Policy sets from two different SDMZ network infrastructures [33, 45] with approximately 150 and 400 SDMZ policies (i.e., 5325 and 7987 enforceable rules, respectively) to benchmark the framework. Infrastructure abstraction trees required for these two SDMZ networks were constructed to drive the PS-1 policy configuration.

2. **PS-2**: Derived from PS-1, this is a large synthetic policy set of 20k policies for coordinator-scale experimentation, emulating 40 different SDMZ networks. Infrastructure abstraction trees were constructed using a scaled-up PS-1 configuration. Source and destination nodes for policies were chosen randomly by sampling technique, and dynamic states and conditions were added as edge properties.

3. **DS-1**: This dataset emulates a collaborative SDMZ network based on the “High Energy Physics - Theory collaboration network” dataset [15], which employs approximately 9.8k nodes with 25k edges.

#### Policy Composition

We evaluated the performance of the policy composition engine using the policy set PS-2. Figure 9a illustrates the latency incurred by the composition engine during pre-deployment. From the list of 20k policies, 1k to 20k policy sets were randomly selected. Their average composition times were computed over 10 rounds, which took approximately 49 seconds to compose 20k policies. To enhance composition performance, we employed a simple hashing technique to cache policies and policy attributes (see §4.3). Experiments were run to assess the impact of caching when an increasing number of abstraction trees are produced. We tested the composition latency for 20k policies built using 10, 30, 50, 70, and 100 abstraction trees (shown in Figure 9b). We find that increasing the number of abstraction trees produces more policy source and target nodes, thereby increasing the cost to create the composition graph. Caching the relations among the nodes reduces the composition latency by up to approximately 2.25× compared to composition without caching. Figure 9b illustrates that increasing the number of abstraction trees gradually diminishes the benefits of caching due to reduced likelihood of overlap in source-node, edge, and target-node pairings.

#### Tagging Efficiency

To evaluate the tag-based policy enforcement mechanism from §5.1, we used policy sets PS-1 and PS-2. We examined a policy set PS-1 from 2 SDMZ campus networks.