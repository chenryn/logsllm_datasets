### EDNS and Open Resolver

#### 5. Validation
In this section, we validate the methods used for frontend enumeration, geolocation, and clustering.

##### 5.1 Coverage of Frontend Enumeration
Using EDNS-client-subnet can improve coverage over previous methods that relied on fewer vantage points. We first quantify the benefits of using EDNS-client-subnet and then explore the sensitivity of our results to the choice of prefix length, as this can also affect frontend enumeration.

**Open Resolver vs. EDNS-client-subnet Coverage**
A common technique for enumerating frontends in a serving infrastructure is to issue DNS queries from multiple vantage points. Following previous work [12], we use open recursive DNS (rDNS) resolvers. Our list includes approximately 200,000 open resolvers, each acting as a distinct vantage point. These resolvers are distributed across 217 countries, 14,538 ASes, and 118,527 unique /24 prefixes. Enumerating Google's frontends via rDNS takes about 40 minutes. This dataset serves as a baseline to evaluate the coverage of the EDNS-client-subnet approach.

Table 1 shows the additional benefits of using EDNS-client-subnet for enumerating Google frontends. Our approach uncovers at least 29% more Google frontend IP addresses, prefixes, and ASes than were visible using rDNS. By allowing us to query Google on behalf of every client prefix, we gain visibility into locations that lack open recursive resolvers. In Section 6.1, we demonstrate the long-term benefits as Google evolves, and in Section 8, we discuss how our results can be used to calibrate the effectiveness of rDNS for other serving infrastructures that do not support EDNS-client-subnet.

**EDNS-client-subnet Prefix Length**
The choice of prefix length for EDNS-client-subnet can affect enumeration completeness. Prefix lengths smaller than /24 in BGP announcements are too coarse for effective enumeration. We observed cases where neighboring /24s within shorter BGP announcement prefixes are directed to different serving infrastructures. For example, an ISP announced a /18 with one of its /24 prefixes directed to Singapore while its neighboring prefix was directed to Hong Kong.

Our evaluations query using one IP address in each /24 block. If serving infrastructures perform redirections at finer granularities, some frontend IP addresses or serving sites might not be observed. The reply to the EDNS-client-subnet query returns the prefix length covering the response. If a query for an IP address in a /24 block returns a prefix length of /26, it means the corresponding redirection holds for all IP addresses in the /26 covering the query address, not just the /24. For nearly 75% of our /24 queries, the responses were for a /24 subnet, likely because it is the longest globally routable prefix. For most of the rest, we saw a /32 prefix length, indicating very fine-grained redirection. For each /24 subnet (about 500,000 subnets), we queried 6-8 other IP addresses within that prefix and discovered only 3 additional IP addresses. Thus, we believe our choice of /24 minimally affects completeness, but we plan to investigate the reasons for these fine-grain redirections in future work.

##### 5.2 Accuracy of Client-Centric Geolocation
Client-centric geolocation using EDNS-client-subnet shows substantial improvement over traditional ping-based techniques [10], undns [29], and geolocation databases [21].

**Dataset**
To validate our approach, we use a subset of Google frontends with hostnames containing airport codes, which hint at their locations. Although the airport location is not precise, it is reasonable to assume the actual frontend is within a few tens of kilometers. Using approximately 550 frontends with airport codes, we measure the error of our technique as the distance between our estimated location and the airport location.

**Accuracy**
Figure 1 shows the distribution of errors for client-centric geolocation (CCG) and three traditional techniques: constraint-based geolocation (CBG) [10], a traceroute-based technique [12], and the MaxMind GeoLite Free database [21]. CCG offers significant improvements. For example, the worst-case error for CCG is 409 km, whereas CBG, the traceroute-based technique, and MaxMind have errors of over 500 km for 17%, 24%, and 94% of frontends, respectively. CBG performs well when vantage points are close to the frontend but incurs large errors for frontends in remote regions. The traceroute-based technique fails to provide any location for 20% of frontends due to a lack of geographic hints in nearby router names. The MaxMind database performs poorly because it places most Google frontends in Mountain View, CA.

**Importance of Filtering**
Figure 2 demonstrates the need for the filters we apply in CCG. Without any filters, the median error is 556 km. Considering only client eyeball prefixes observed in the BitTorrent dataset reduces the median error to 484 km and increases the percentage of frontends located with an error less than 1000 km from 61% to 74%. Applying standard deviation filtering improves the median error to 305 km and the percentage of frontends with an error less than 1000 km to 86%. Using speed-of-light constraints measured from PlanetLab and M-Lab further reduces the median error to 26 km, with only 10% of frontends having an error greater than 1000 km. Our best results are achieved by simultaneously applying all three filters.

**Case Studies of Poor Geolocation**
CCG’s accuracy depends on its ability to draw tight speed-of-light constraints, which in turn depends on the deployment density of PlanetLab and M-Lab. We found one instance where sparse vantage point deployments affected CCG’s accuracy. In Stockholm, Sweden, a set of frontends with the arn airport code served a large group of clients throughout Northern Europe. However, our technique located the frontends 409 km southeast of Stockholm, influenced by a large number of clients in Oslo, Copenhagen, and northern Germany. Speed-of-light filtering usually eliminates distant clients, but in this case, PlanetLab sites in Sweden measured latencies to the Google frontends in the 24 ms range, yielding a feasible radius of 2400 km. This loose constraint resulted in poor geolocation for this set of frontends.

It is well-known that Google has a large data center in The Dalles, Oregon, but our map (Fig. 7) does not show any sites in Oregon. Instead, we place this site 240 km north, just south of Seattle, Washington. A disadvantage of our geolocation technique is that large data centers in remote locations are often pulled towards large population centers they serve, resulting in a "logical" serving center rather than the geographic location.

##### 5.3 Accuracy of Frontend Clustering
To validate the accuracy of our clustering method, we run clustering on three groups of nodes for which we have ground truth: 72 PlanetLab servers from 23 different sites around the world, 27 servers from 6 sites in California, USA, and 75 Google IP addresses with 9 different airport codes in their reverse DNS names. These sets vary in size and geographic scope, with the last set being the most representative.

**Metric for Clustering Accuracy**
We use the Rand Index [26] to measure clustering accuracy. The index is the ratio of the sum of true positives and negatives to the sum of these quantities and false positives and negatives. A Rand index of 1 indicates no false positives or negatives.

Table 2 shows the Rand index for the three node sets. In each case, the Rand index is upwards of 97%. This accuracy is due to two components of our clustering method: eliminating outliers for more accurate distance measures and dynamically selecting cluster boundaries using the OPTICS algorithm.

**False Positives and Negatives**
Our method has a small number of false positives and negatives. In the California nodeset, the method fails to separate some USC/ISI nodes from USC campus nodes, and in the PlanetLab nodeset, some clusters have low reachability distances that confuse our boundary detection method. The Google nodeset reveals one false negative, which we believe to be correct: the algorithm correctly identifies two distinct serving sites in mrs, as discussed below.

**Performance Analysis**
Figure 3 shows the output of the OPTICS algorithm on the Google nodeset. The x-axis represents the ordered output of the OPTICS algorithm, and the y-axis shows the reachability distance associated with each node. Impulses in the reachability distance depict cluster boundaries, and we have verified that the nodes within each cluster belong to the same airport code. The algorithm correctly identifies all 9 Google sites and even distinguishes two physically distinct serving sites within the mrs airport code.

Figure 4 shows the OPTICS output when using reverse-TTL (as proposed in [19]) instead of RTT for the metric embedding. This uses a slightly different set of Google servers to highlight the performance of reverse-TTL-based clustering. For this set, reverse-TTL-based embedding performs reasonably well but results in the OPTICS algorithm being unable to distinguish between serving sites in bom and del. RTT-based clustering can differentiate these serving sites (not shown). Moreover, although reverse-TTL suggests the possibility of two sites in mrs, it mis-identifies which servers belong to which site based on reverse DNS names.

### 6. Mapping Google’s Expansion
We present a longitudinal study of Google’s serving infrastructure. Our initial dataset is from late October to early November 2012, and our second dataset covers March and April 2013. We capture a substantial expansion of Google's infrastructure.

#### 6.1 Growth Over Time
Figure 5 shows the growth in the number of IP addresses, /24 prefixes, and ASes/countries observed to be serving Google's homepage over time. During our study, Google expanded rapidly at each of these granularities.