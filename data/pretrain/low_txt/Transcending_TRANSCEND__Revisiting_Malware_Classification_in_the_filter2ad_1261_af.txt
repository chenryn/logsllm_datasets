### Information Science and Statistics. Springer, 2007.

**References:**

13. Boshmaf, Y., Logothetis, D., Siganos, G., Lería, J., Lorenzo, J., Ripeanu, M., & Beznosov, K. (2015). Integro: Leveraging victim prediction for robust fake account detection in OSNs. *Network and Distributed System Security Symposium (NDSS)*.

14. Cao, Q., Sirivianos, M., Yang, X., & Pregueiro, T. (2012). Aiding the detection of fake accounts in large-scale social online services. *USENIX Symposium on Networked Systems Design and Implementation (NSDI)*.

15. Curtsinger, C., Livshits, B., Zorn, B. G., & Seifert, C. (2011). ZOZZLE: Fast and precise in-browser JavaScript malware detection. *USENIX Security Symposium*.

16. Dash, S. K., Suarez-Tangil, G., Khan, S. J., Tam, K., Ahmadi, M., Kinder, J., & Cavallaro, L. (2016). Droidscribe: Classifying Android malware based on runtime behavior. *IEEE Security and Privacy Workshops (SPW)*.

17. Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding back-translation at scale. *Conference on Empirical Methods in Natural Language Processing (EMNLP)*.

18. French, R. M., & Chater, N. (2002). Using noise to compute error surfaces in connectionist networks: A novel means of reducing catastrophic forgetting. *Neural Computation*.

19. Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. *Annals of Statistics*.

20. Jordaney, R., Sharad, K., Dash, S. K., Wang, Z., Papini, D., Nouretdinov, I., & Cavallaro, L. (2017). Transcend: Detecting concept drift in malware classification models. *USENIX Security Symposium*.

21. Kan, Z., Pendlebury, F., Pierazzi, F., & Cavallaro, L. (2021). Investigating labelless drift adaptation for malware detection. *ACM Workshop on Artificial Intelligence and Security (AISec)*.

22. Kantchelian, A., Tschantz, M. C., Afroz, S., Miller, B., Shankar, V., Bachwani, R., Joseph, A. D., & Tygar, J. D. (2015). Better malware ground truth: Techniques for weighting anti-virus vendor labels. *ACM Workshop on Artificial Intelligence and Security (AISec)*.

23. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2017). ImageNet classification with deep convolutional neural networks. *Communications of the ACM*.

24. Kullback, S., & Leibler, R. A. (1951). On information and sufficiency. *Annals of Mathematical Statistics*.

25. Lindorfer, M., Neugschwandtner, M., & Platzer, C. (2015). MARVIN: Efficient and comprehensive mobile app classification through static and dynamic analysis. *IEEE Annual Computer Software and Applications Conference (COMPSAC)*.

26. Linusson, H., Johansson, U., Boström, H., & Löfström, T. (2018). Classification with reject option using conformal prediction. *Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)*. Springer.

27. Miller, B., Kantchelian, A., Tschantz, M. C., Afroz, S., Bachwani, R., Faizullabhoy, R., Huang, L., Shankar, V., Wu, T., Yiu, G., Joseph, A. D., & Tygar, J. D. (2016). Reviewer integration and performance measurement for malware detection. *Conference on Detection of Intrusions and Malware & Vulnerability Assessment (DIMVA)*.

28. Moreno-Torres, J. G., Raeder, T., Aláíz-Rodríguez, R., Chawla, N. V., & Herrera, F. (2012). A unifying view on dataset shift in classification. *Pattern Recognition*.

29. Narayanan, A., Liu, Y., Chen, L., & Liu, J. (2016). Adaptive and scalable Android malware detection through online learning. *International Joint Conference on Neural Networks (IJCNN)*.

30. Narayanan, A., Chandramohan, M., Chen, L., & Liu, Y. (2017). Context-aware, adaptive, and scalable Android malware detection through online learning. *IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI)*.

31. Nilizadeh, S., Labreche, F., Sedighian, A., Zand, A., Fernandez, J. M., Kruegel, C., Stringhini, G., & Vigna, G. (2017). POISED: Spotting Twitter spam off the beaten paths. *ACM Conference on Computer and Communications Security (CCS)*.

32. Papadopoulos, H. (2008). Inductive conformal prediction: Theory and application to neural networks. *Tools in Artificial Intelligence*.

33. Papernot, N., & McDaniel, P. D. (2018). Deep k-nearest neighbors: Towards confident, interpretable, and robust deep learning. *CoRR, abs/1803.04765*.

34. Pendlebury, F. (2021). Machine Learning for Security in Hostile Environments. *PhD thesis, University of London*.

35. Pendlebury, F., Pierazzi, F., Jordaney, R., Kinder, J., & Cavallaro, L. (2019). TESSERACT: Eliminating experimental bias in malware classification across space and time. *USENIX Security Symposium*.

36. Pierazzi, F., Pendlebury, F., Cortellazzi, J., & Cavallaro, L. (2020). Intriguing properties of adversarial ML attacks in the problem space. *IEEE Symposium on Security and Privacy (S&P)*.

37. Plato. (c. 385–370 BC). The Symposium. *Penguin Classics edition published 1999, translated by Christopher Gill*.

38. Shafer, G., & Vovk, V. (2008). A tutorial on conformal prediction. *Journal of Machine Learning Research (JMLR)*.

39. Sotgiu, A., Demontis, A., Melis, M., Biggio, B., Fumera, G., Feng, X., & Roli, F. (2020). Deep neural rejection against adversarial examples. *EURASIP Journal on Information Security*.

40. Srndic, N., & Laskov, P. (2013). Detection of malicious PDF files based on hierarchical document structure. *Network and Distributed System Security Symposium (NDSS)*.

41. Srndic, N., & Laskov, P. (2016). Hidost: A static machine-learning-based detector of malicious files. *EURASIP Journal on Information Security*.

42. Suarez-Tangil, G., Tapiador, J. E., Peris-Lopez, P., & Alís, J. B. (2014). Dendroid: A text mining approach to analyzing and classifying code structures in Android malware families. *Expert Systems With Applications*.

43. Suarez-Tangil, G., Dash, S. K., Ahmadi, M., Kinder, J., Giacinto, G., & Cavallaro, L. (2017). Droidsieve: Fast and accurate classification of obfuscated Android malware. *ACM Conference on Data and Applications Security and Privacy (CODASPY)*.

44. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., & Fergus, R. (2014). Intriguing properties of neural networks. *ICLR (Poster)*.

45. Tong, L., Li, B., Hajaj, C., Xiao, C., Zhang, N., & Vorobeychik, Y. (2019). Improving robustness of ML classifiers against realizable evasion attacks using conserved features. *USENIX Security Symposium*.

46. Vovk, V. (2013). Conditional validity of inductive conformal predictors. *Journal of Machine Learning Research (JMLR)*.

47. Vovk, V., Gammerman, A., & Shafer, G. (2010). Algorithmic learning in a random world. *Springer-Verlag New York Inc.*.

48. Vovk, V., Nouretdinov, I., Manokhin, V., & Gammerman, A. (2018). Cross-conformal predictive distributions. *Workshop on Conformal Prediction and its Applications (COPA)*.

49. Xi, S., Yang, S., Xiao, X., Yao, Y., Xiong, Y., Xu, F., Wang, H., Gao, P., Liu, Z., Xu, F., & Lu, J. (2019). DeepIntent: Deep icon-behavior learning for detecting intention-behavior discrepancy in mobile apps. *ACM Conference on Computer and Communications Security (CCS)*.

50. Xu, K., Li, Y., Deng, R. H., Chen, K., & Xu, J. (2019). DroidEvolver: Self-evolving Android malware detection system. *IEEE European Symposium on Security and Privacy (EuroS&P)*.

51. Yang, L., Guo, W., Hao, Q., Ciptadi, A., Ahmadzadeh, A., Xing, X., & Wang, G. (2021). CADE: Detecting and explaining concept drift samples for security applications. *USENIX Security Symposium*.

52. Yang, W., Xiao, X., Andow, B., Li, S., Xie, T., & Enck, W. (2015). AppContext: Differentiating malicious and benign mobile app behaviors using context. *International Conference on Software Engineering (ICSE)*.

53. Zhang, X., Zhang, Y., Zhong, M., Ding, D., Cao, Y., Zhang, Y., Zhang, M., & Yang, M. (2020). Enhancing state-of-the-art classifiers with API semantics to detect evolved Android malware. *ACM Conference on Computer and Communications Security (CCS)*.

### Appendix

#### A. Symbol Table

Table IV reports the major symbols and abbreviations used throughout the paper.

| **Symbol** | **Description** |
|------------|-----------------|
| \( X \)    | Feature space \( X \subseteq \mathbb{R}^n \). |
| \( Y \)    | Label space. |
| \( z \)    | Example pair \( (x, y) \in X \times Y \). |
| \( z^* \)  | Previously unseen test example. |
| \( \hat{y} \) | Predicted class \( g(z^*) \). |
| \( a_z \)  | Nonconformity score output by an NCM for \( z \). |
| \( p_z \)  | Statistical p-value for \( z \). |
| \( p_y \)  | Statistical p-value for \( z \), calculated with respect to class \( y \in Y \) (used in label conditional calculations). |
| \( \tau_y \) | Rejection threshold \( \tau_y \in [0, 1] \) for class \( y \in Y \). |
| \( T \)    | The set of all per-class rejection thresholds \( \{ \tau_y \in [0, 1] \mid y \in Y \} \). |
| \( d \)    | Distance function \( d(z, z') \). |
| \( \hat{z}(B) \) | Point predictor. |
| \( A \)    | Nonconformity measure (NCM) usually composed of a distance function and point predictor. |
| \( S \)    | Collection of nonconformity scores computed in elements of \( B \), relative to other elements in \( B \). |
| \( g \)    | Classifier \( g : X \to Y \) that assigns object \( x \in X \) to class \( y \in Y \). Also known as the decision function. |
| \( \varepsilon \) | Significance level used in conformal prediction to define prediction region with confidence guarantees. |
| **NCM**   | Nonconformity measure. |
| **TCE**   | Transductive Conformal Evaluator. |
| **ICE**   | Inductive Conformal Evaluator. |
| **CCE**   | Cross-Conformal Evaluator. |
| \( S = \{ A(B \setminus \{ z \}, z) : z \in B \} \) | Bag of examples \( \{ z_1, z_2, \ldots, z_n \} \). |

#### B. Additional CP-Reject Results

In §VI-F, we demonstrate how TRANSCENDENT can apply to other classifiers and domains, comparing the performance of an ICE using credibility against using probabilities alone, for both PE and PDF malware (Figures 9 and 10). Here, we show additional results in Figure 11 to compare against the prior rejection approach CP-Reject (we exclude DroidEvolver as it is specific to Android malware). Similar to the results on the Android dataset (§VI-E), the overall ability for CP-Reject to distinguish between drifting and non-drifting points is poor on the PE malware dataset. For the PDF malware dataset, which exhibits much less drift, CP-Reject is significantly more effective, supporting the hypothesis that the violation of conformal prediction’s exchangeability assumption results in lower performance on the Android and PE datasets. Nevertheless, TRANSCENDENT with credibility (and even probabilities) outperforms CP-Reject in this setting as well (cf. Figure 10).

#### C. Full Vanilla TCE on EMBER Subset

A full-scale comparison to the original TCE is not possible due to its computational complexity—recall that one classifier must be trained for each example in the training set. However, it is informative to perform a small-scale experiment as there may be settings where the vanilla TCE is viable, and we wish to ensure that there is no significant performance difference between vanilla TCE and our novel conformal evaluators. We perform an experiment on the Windows PE malware dataset, where 10% of the training data is randomly sampled to use for training and calibrating the evaluators (this is the largest subsample we can take given our resource constraints). We choose the PE dataset over the Android dataset due to the high dimensionality of the Android feature space that may cause instability when the number of examples is very low, and over the PDF dataset which is relatively stationary and may make it harder to discern performance differences between the different evaluators. One caveat of this subsampling is the reduced performance of the baseline for the ICE, which is due to the reduced data available to the proper training set, although TRANSCENDENT appears unaffected by this.

Table V summarizes the F1 performance over the seven-month-long test periods using the area-under-time (AUT) metric [35]. The performance difference between TCE and our evaluators in terms of distinguishing between drifting and non-drifting examples is negligible, shown by the very high AUT of kept elements and very low AUT of rejected elements. That is, there is little to no performance sacrifice when using our evaluators over the vanilla TCE. The overall trends otherwise follow those in our main Android experiments (cf. Figure 7).

| **Metric** | **Baseline** | **Kept Elements** | **Rejected Elements** |
|------------|--------------|-------------------|-----------------------|
| **TCE**    | 0.68         | 0.97              | 0.00                  |
| **Approx-TCE** | 0.70      | 0.97              | 0.00                  |
| **ICE**    | 0.45         | 0.94              | 0.00                  |
| **CCE**    | 0.69         | 1.00              | 0.21                  |

#### D. Analysis of CCE Tuning

Here, we revisit the majority vote conditions for the CCE applied to the Android malware dataset in §VI-B. The size of the quorum for the CCE affects how conservative the CCE is in accepting test examples. Figure 13 shows the performance over time summarized using the AUT metric for F1 (a), Precision (b), and Recall (c). Note that Figure 13 omits the setting where the majority vote must be unanimous, as the CCE eventually rejects every example—causing F1, Precision, and Recall to be undefined for kept elements. As more folds of the CCE are required to agree with each other before a decision is accepted, the CCE will reject more elements. If fewer folds are required, more elements will be accepted. Similarly, the quality of the rejection lessens: more elements are rejected on which the underlying classifier would not have made a mistake. Tuning the majority vote conditions on the calibration set can help find the sweet spot between the performance of kept elements, and the quality—and volume—of rejections.

#### E. Guidance for Choosing Calibration Constraints

In §V-D, we formally describe the threshold calibration as an optimization problem in which one metric of interest is maximized or minimized given constraints on another metric. Throughout our evaluation, we focus on maximizing the F1 of kept elements while keeping a reasonably low rejection rate. We choose 15% after taking into account the size of our dataset and using guidance from Miller et al. [27] to estimate a reasonable labeling capacity. Recall that the calibration constraints are with respect to the calibration set, which ideally exhibits minimal drift. It is clear from our evaluation that as concept drift becomes more severe, the need for careful tuning of these constraints increases. Figure 12 shows the F1-Score of an ICE optimized to find calibration thresholds that minimize the rejection rate with an F1-Score no less than 0.8. These settings keep the rejection rate low (below 10%) while sacrificing the F1 performance on kept elements (cf. Figure 7b).

**Figure 12:** F1-Score of an ICE optimized to find calibration thresholds that minimize the rejection rate with F1-Score no less than 0.8. These settings keep the rejection rate low (below 10%) while sacrificing the F1 performance on kept elements (cf. Figure 7b).

**Figure 13:** Performance over time summarized using the AUT metric for F1 (a), Precision (b), and Recall (c) for different majority vote conditions in the CCE.