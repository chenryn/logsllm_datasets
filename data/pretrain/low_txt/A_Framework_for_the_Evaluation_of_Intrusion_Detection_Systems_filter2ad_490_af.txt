### Optimized Text

In this scenario, \( p \) is constrained in a manner that ensures the game's equilibrium is achieved through a pure strategy. Specifically, the optimal strategy for the intruder is to attack with a frequency of \( \hat{p} + \delta u \), while also generating missed detections with probability \( \beta \) and false alarms with probability \( \alpha \). Conversely, the optimal strategy for the Decision Maker (DM) is to identify the point on the ROC\(_{\alpha, \beta}\) curve that minimizes the expected cost, assuming the base rate is \( \hat{p} + \delta u \).

The optimal point on the ROC\(_{\alpha, \beta}\) curve corresponds to a threshold of 799, resulting in an expected cost \( E_{\delta, \alpha, \beta}[C(I, A)] = 5.19 \times 10^{-2} \). By adopting this optimal point instead of the original one, the expected operational cost during actual operation is reduced to \( E_{\text{operation}}[C(I, A)] = 2.73 \times 10^{-2} \). Thus, not only is the desired security level of \( 5.19 \times 10^{-2} \) maintained, but the new optimal point also outperforms the original one.

The evaluation depicted in Figure 10 directly relates to the problem introduced earlier, as it can be interpreted as the assessment of two Intrusion Detection Systems (IDSs). One IDS has a buffer threshold length of 399, while the other has a buffer threshold length of 773. Under ideal conditions, the IDS with a buffer threshold of 399 would be chosen due to its lower expected cost. However, after evaluating the worst-case behavior of the IDSs, the one with a buffer threshold of 773 is selected.

An alternative perspective can be gained through the use of IDOC curves. Figure 11(a) shows the original IDOC curve during the evaluation period, which may give a false sense of confidence in the IDS. Therefore, we examine the IDOC curves based on ROC\(_{\alpha, \beta}\) in Figure 11(b). As shown in Figure 11(c), the IDOC of the actual operating environment more closely follows the IDOC based on ROC\(_{\alpha, \beta}\) than the original one.

### Conclusions and Future Work

Empirical testing of IDSs faces two primary challenges. The first challenge is making inferences about the IDS system based solely on experimental data. For example, the low confidence in estimating the probability of detection in the ROC curve. While error bars are commonly used in other classification tasks to improve these estimates, their application to IDSs is limited by the small number of attacks and their variations, leading to insufficient data for accurate significance levels. Additionally, error bars and cross-validation techniques provide the average performance of the classifier, which brings us to the second challenge: since IDSs operate in adversarial environments, evaluating them based on average performance is insufficient. Our intruder model addresses these issues by providing a principled approach to determine the worst-case performance of the detector.

The extent to which the analysis with a (\(\delta, \alpha, \beta\))-intruder reflects the real operation of the IDS depends on the evaluator's understanding of the IDS and its environment. This includes factors such as the IDS's evasibility and the quality of the signatures (e.g., the likelihood of normal events triggering alarms). By assuming robust parameters, we adopt a pessimistic setting. If this pessimistic scenario does not occur, we might be operating at a suboptimal point, having been overly cautious in the evaluation.

Finally, IDOC curves are a general method applicable not only to IDSs but to any classification algorithm dealing with heavily imbalanced classes (very small or very large \( p \)). We plan to propose their use in other fields as an alternative to ROCs for such classification problems. Another option for the x-axis on an IDOC curve is to use \( 1 - \Pr[I = 1|A = 1] = \Pr[I = 0|A = 1] \), which intuitively represents the Bayesian false alarm rate. This would reflect the probability that the IDS operator will not find an intrusion when investigating an alarm report, representing the potential waste of time for the operator. The final choice of the x-axis will depend on the user's interpretation.

### References

[1] The MIT Lincoln Labs Evaluation Data Set, DARPA Intrusion Detection Evaluation. Available at: http://www.ll.mit.edu/IST/ideval/index.html.
[2] Software for Empirical Evaluation of IDSs. Available at: http://www.cshcn.umd.edu/research/IDSanalyzer.
[3] S. Axelsson. The Base-Rate Fallacy and Its Implications for the Difficulty of Intrusion Detection. In Proceedings of the 6th ACM Conference on Computer and Communications Security (CCS '99), pages 1–7, November 1999.
[4] S. Buchegger and J.-Y. Le Boudec. Nodes Bearing Grudges: Towards Routing Security, Fairness, and Robustness in Mobile Ad Hoc Networks. In Proceedings of Tenth Euromicro PDP (Parallel, Distributed and Network-based Processing), pages 403–410, Gran Canaria, January 2002.
[5] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, Inc, 1991.
[6] G. Di Crescenzo, A. Ghosh, and R. Talpade. Towards a Theory of Intrusion Detection. In ESORICS 2005, 10th European Symposium on Research in Computer Security, pages 267–286, Milan, Italy, September 12-14 2005. Lecture Notes in Computer Science 3679 Springer.
[7] E. Eskin, A. Arnold, M. Prerau, L. Portnoy, and S. Stolfo. A Geometric Framework for Unsupervised Anomaly Detection: Detecting Intrusions in Unlabeled Data. In D. Barbara and S. Jajodia, editors, Data Mining for Security Applications. Kluwer, 2002.
[8] S. Forrest, S. Hofmeyr, A. Somayaji, and T. A. Longstaff. A Sense of Self for Unix Processes. In Proceedings of the 1996 IEEE Symposium on Security & Privacy, pages 120–12, Oakland, CA, USA, 1996. IEEE Computer Society Press.
[9] J. E. Gaffney and J. W. Ulvila. Evaluation of Intrusion Detectors: A Decision Theory Approach. In Proceedings of the 2001 IEEE Symposium on Security and Privacy, pages 50–61, Oakland, CA, USA, 2001.
[10] G. Gu, P. Fogla, D. Dagon, W. Lee, and B. Skoric. Measuring Intrusion Detection Capability: An Information-Theoretic Approach. In Proceedings of ACM Symposium on Information, Computer and Communications Security (ASIACCS '06), Taipei, Taiwan, March 2006.
[11] H. Handley, C. Kreibich, and V. Paxson. Network Intrusion Detection: Evasion, Traffic Normalization, and End-to-End Protocol Semantics. In 10th USENIX Security Symposium, 2001.
[12] J. Jung, V. Paxson, A. Berger, and H. Balakrishnan. Fast Portscan Detection Using Sequential Hypothesis Testing. In IEEE Symposium on Security & Privacy, pages 211–225, Oakland, CA, USA, 2004.
[13] C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and G. Vigna. Automating Mimicry Attacks Using Static Binary Analysis. In Proceedings of the 2005 USENIX Security Symposium, pages 161–176, Baltimore, MD, August 2005.
[14] C. Kruegel, D. Mutz, W. Robertson, and F. Valeur. Bayesian Event Classification for Intrusion Detection. In Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC), pages 14–24, December 2003.
[15] C. Kruegel, D. Mutz, W. Robertson, G. Vigna, and R. Kemmerer. Reverse Engineering of Network Signatures. In Proceedings of the AusCERT Asia Pacific Information Technology Security Conference, Gold Coast, Australia, May 2005.
[16] W. Lee and S. J. Stolfo. Data Mining Approaches for Intrusion Detection. In Proceedings of the 7th USENIX Security Symposium, 1998.
[17] W. Lee, S. J. Stolfo, and K. Mok. A Data Mining Framework for Building Intrusion Detection Models. In Proceedings of the IEEE Symposium on Security & Privacy, pages 120–132, Oakland, CA, USA, 1999.
[18] R. P. Lippmann, D. J. Fried, I. Graf, J. W. Haines, K. R. Kendall, D. McClung, D. Weber, S. E. Webster, D. Wyschogrod, R. K. Cunningham, and M. A. Zissman. Evaluating Intrusion Detection Systems: The 1998 DARPA Off-Line Intrusion Detection Evaluation. In DARPA Information Survivability Conference and Exposition, volume 2, pages 12–26, January 2000.
[19] D. J. Marchette. A Statistical Method for Profiling Network Traffic. In USENIX Workshop on Intrusion Detection and Network Monitoring, pages 119–128, 1999.
[20] S. Marti, T. J. Giuli, K. Lai, and M. Baker. Mitigating Routing Misbehavior in Mobile Ad Hoc Networks. In Proceedings of the 6th Annual International Conference on Mobile Computing and Networking, pages 255–265. ACM Press, 2000.
[21] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki. The DET Curve in Assessment of Detection Task Performance. In Proceedings of the 5th European Conference on Speech Communication and Technology (Eurospeech'97), pages 1895–1898, Rhodes, Greece, 1997.
[22] J. McHugh. Testing Intrusion Detection Systems: A Critique of the 1998 and 1999 DARPA Intrusion Detection System Evaluations as Performed by the Lincoln Laboratory. ACM Transactions on Information and System Security (TISSEC), 3(4):262–294, November 2000.
[23] H. V. Poor. An Introduction to Signal Detection and Estimation. Springer-Verlag, 2nd edition, 1988.
[24] F. Provost and T. Fawcett. Robust Classification for Imprecise Environments. Machine Learning, 42(3):203–231, March 2001.
[25] T. H. Ptacek and T. N. Newsham. Insertion, Evasion and Denial of Service: Eluding Network Intrusion Detection. Technical Report, Secure Networks, Inc., January 1998.
[26] M. Schonlau, W. DuMouchel, W.-H. Ju, A. F. Karr, M. Theus, and Y. Vardi. Computer Intrusion: Detecting Masquerades. Technical Report 95, National Institute of Statistical Sciences, 1999.
[27] U. Shankar and V. Paxson. Active Mapping: Resisting NIDS Evasion Without Altering Traffic. In Proceedings of the 2003 IEEE Symposium on Security & Privacy, pages 44–61, Oakland, CA, USA, 2003.
[28] S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan. Cost-Based Modeling for Fraud and Intrusion Detection: Results from the JAM Project. In Proceedings of the 2000 DARPA Information Survivability Conference and Exposition, pages 130–144, January 2000.
[29] K. Tan, K. Killourchy, and R. Maxion. Undermining an Anomaly-Based Intrusion Detection System Using Common Exploits. In Proceedings of the 5th International Symposium on Recent Advances in Intrusion Detection (RAID 2002), pages 54–73, Zurich, Switzerland, October 2002.
[30] K. Tan, J. McHugh, and K. Killourhy. Hiding Intrusions: From the Abnormal to the Normal and Beyond. In Information Hiding: 5th International Workshop, pages 1–17, Noordwijkerhout, The Netherlands, October 2002.
[31] H. L. Van Trees. Detection, Estimation and Modulation Theory, Part I. Wiley, New York, 1968.
[32] G. Vigna, S. Gwalani, K. Srinivasan, E. Belding-Royer, and R. Kemmerer. An Intrusion Detection Tool for AODV-Based Ad Hoc Wireless Networks. In Proceedings of the Annual Computer Security Applications Conference (ACSAC), pages 16–27, Tucson, AZ, December 2004.
[33] G. Vigna, W. Robertson, and D. Balzarotti. Testing Network-Based Intrusion Detection Signatures Using Mutant Exploits. In Proceedings of the ACM Conference on Computer and Communication Security (ACM CCS), pages 21–30, Washington, DC, October 2004.
[34] D. Wagner and P. Soto. Mimicry Attacks on Host-Based Intrusion Detection Systems. In Proceedings of the 9th ACM Conference on Computer and Communications Security (CCS), pages 255–264, Washington D.C., USA, 2002.
[35] C. Warrender, S. Forrest, and B. Pearlmutter. Detecting Intrusions Using System Calls: Alternative Data Models. In Proceedings of the 1999 IEEE Symposium on Security & Privacy, pages 133–145, Oakland, CA, USA, May 1999.
[36] Y. Zhang, W. Lee, and Y. Huang. Intrusion Detection Techniques for Mobile Wireless Networks. ACM/Kluwer Mobile Networks and Applications (MONET), 9(5):545–556, September 2003.