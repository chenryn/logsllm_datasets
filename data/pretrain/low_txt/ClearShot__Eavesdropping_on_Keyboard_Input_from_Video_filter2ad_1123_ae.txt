### Introduction to Screen Emanation Reflections

Researchers have proposed using the reflections of a screen's optical emanations on common objects to read the screen’s contents from a distance [4]. This method involves employing a telescope to capture these reflections from objects such as plastic bottles and eyeglasses, which are often located near the screen. Standard image processing algorithms are used to enhance the readability of the recovered images, but no automatic analysis is performed to identify or reconstruct the eavesdropped content.

### Recent Research on Emanations

In recent years, researchers have explored emanations of various types, including acoustic ones. For instance, Asonov and Agrawal demonstrated that it is possible to differentiate the sounds produced by different key presses on a standard keyboard using a neural network [2]. Their system achieved a 50% probability of correctly identifying the pressed key from a set of four suggested keys in tests involving 10 clicks per key. However, their experiments did not address realistic typing patterns.

Zhuang et al. developed an attack that uses statistical constraints of the English language to reconstruct single words from 10-minute sound recordings without any labeled training data [48]. Similar to our work, they employed several error correction techniques to improve the performance of their keystroke classifier. They reported word recognition success rates between 47% and 74% in an unsupervised setting and higher in a supervised setting.

The most similar work to ours is that of Berger et al. [6], who analyzed audio recordings of single-word typing and computed spatial constraints (equal, adjacent, near, distant) for each pair of keystrokes. Using a dictionary, they identified words that satisfied the inferred constraints. Their experiments involved 27 words, each 7–13 characters long, using three different keyboards. They reported a 40% probability of finding the correct word in the top 10 proposed words, 60% in the top 25, and 70% in the top 50. Our approach, however, focuses on long text reconstruction and introduces several techniques to achieve this goal.

### Challenges in Video-Based Analysis

Reconstructing typed text from a video recording might seem simpler than performing a sound-based analysis, but it presents its own set of challenges. Specifically, extracting the timing information of key presses from a mute video is more difficult than from a sound recording. In a video-based analysis, basic information such as the number of characters in each word must be inferred. We anticipate that combining both techniques would result in very high detection rates.

### Traffic Analysis Techniques

Traffic analysis techniques have been used to eavesdrop on encrypted communications over a network. For example, Song et al. leveraged keystroke timing data observable in older SSH implementations to recover passwords typed in encrypted sessions [39]. In a test with four users typing five passwords of 6, 7, and 8 characters, they found that the correct password was in the top 0.1% to 62.3% of the strings proposed by their system. Shah et al. [38] used timing actively rather than passively, introducing JitterBugs, a class of in-line interception mechanisms that covertly transmit data by perturbing the timing of input events. Wright et al. analyzed Voice over IP (VoIP) communications, observing that different languages are encoded at different bit rates by Variable Bit Rate encoders. They used packet sizes to predict the bit rate and identify the language spoken in encrypted VoIP traffic [46].

### Conclusions

In this paper, we presented a novel approach to automatically extract information from a webcam video that records a user typing on a keyboard. The approach is based on several new techniques for movement tracking, sentence reconstruction, and error correction. It has been implemented in a tool called ClearShot, which can extract a substantial portion of the text being typed under certain assumptions.

Although the automatic recognition of key presses based on video information is a complex and challenging task, preventing such attacks is straightforward. A simple solution is to place a physical shield over the keyboard so that the keys can only be seen by the typist. This technique is sometimes used to protect keypads at ATMs and POS terminals, but it is not widely used for computer keyboards.

Future work will focus on improving the motion tracking algorithm to work reliably in various settings, such as different lighting conditions, camera angles, and camera types. Additionally, we plan to explore how the context of the extracted information (e.g., the use of specific keywords) can be leveraged to improve word selection among multiple alternatives.

To our knowledge, this is the first tool of its kind. We envision that it could be valuable in long-lasting surveillance operations. Furthermore, some of the techniques developed for extracting text from typing videos could be adapted for other fields, such as computer vision and augmented reality.

### Acknowledgments

This research was partially supported by the National Science Foundation under grants CCR-0238492, CCR-0524853, and CCR-0716095.

### References

[1] 3M Notebook Privacy Computer Filter PF15.2W. http://www.3m.com, 2007.
[2] D. Asonov and R. Agrawal. Keyboard Acoustic Emanations. In Proceedings of the IEEE Symposium on Security and Privacy, pages 3–11, 2004.
[3] K. Atkinson. GNU Aspell 0.50.5 Documentation. http://aspell.net/0.50-doc/man-html/manual.html, 2004.
[4] M. Backes, M. Dürmuth, and D. Unruh. Compromising Reflections - or - How to Read LCD Monitors Around the Corner. In Proceedings of the IEEE Symposium on Security and Privacy, 2008.
[5] J. Barron, D. Fleet, and S. Beauchemin. Performance of Optical Flow Techniques. International Journal of Computer Vision, 12(1):43–77, 1994.
[6] Y. Berger, A. Yeredor, and A. Wool. Dictionary Attacks Using Keyboard Acoustic Emanations. In Proceedings of the ACM Conference on Computer and Communications Security, pages 245–254, 2006.
[7] T. Brants and A. Franz. Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia, 2006.
[8] E. Brill and R. Moore. An Improved Error Model for Noisy Channel Spelling Correction. Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 286–293, 2000.
[9] Y. Cetin. Cable Free Keyboard Apparatus Based on Computer Vision. WIPO Patent WO/2002/027457, April 2000.
[10] K. Church and W. Gale. Probability Scoring for Spelling Correction. Statistics and Computing, 1(2):93–103, 1991.
[11] F. Damerau. A technique for computer detection and correction of spelling errors. Communications of the ACM, 7(3):171–176, 1964.
[12] S. Fels and G. Hinton. Glove-TalkII: an Adaptive Gesture-to-Formant Interface. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 456–463, 1995.
[13] S. Fogie. Axis communications 207w network camera web interface vulnerabilities. http://www.securityfocus.com/bid/25678, 2007.
[14] W. Freeman, K. Tanaka, J. Ohta, and K. Kyuma. Computer Vision for Computer Games. In Proceedings of the International Conference on Automatic Face and Gesture Recognition, pages 100–105, 1996.
[15] A. Golding and D. Roth. A Winnow-Based Approach to Context-Sensitive Spelling Correction. Machine Learning, 34(1):107–130, 1999.
[16] H. Highland. Electromagnetic Radiation Revisited. Computers & Security, 5(2):85–93, 1986.
[17] B. Horn and B. Schunck. Determining Optical Flow. Artificial Intelligence, 17:185–203, 1981.
[18] Intel. OpenCV: Open Source Computer Vision Library. http://www.intel.com/technology/computing/opencv/.
[19] Y. Ivanov and A. Bobick. Recognition of Multi-Agent Interaction in Video Surveillance. In Proceedings of the International Conference on Computer Vision, pages 169–176, 1999.
[20] D. Jurafsky and J. Martin. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. MIT Press, 2000.
[21] J. Kennedy. Inaugural Address, January 1961.
[22] M. Kuhn. Optical Time-Domain Eavesdropping Risks of CRT Displays. In Proceedings of the IEEE Symposium on Security and Privacy, pages 3–18, 2002.
[23] M. Kuhn. Electromagnetic Eavesdropping Risks of Flat-Panel Displays. In Proceedings of the International Workshop on Privacy Enhancing Technologies, pages 88–107, 2004.
[24] M. Kuhn and R. Anderson. Soft Tempest: Hidden Data Transmission Using Electromagnetic Emanations. In Proceedings of the International Workshop on Information Hiding, pages 124–142, 1998.
[25] K. Kukich. Techniques for Automatically Correcting Words in Text. ACM Computing Surveys, 24(4):377–439, 1992.
[26] J. LaViola. A Survey of Hand Posture and Gesture Recognition Techniques and Technology. Technical Report CS-99-11, Brown University, 1999.
[27] C. Lee and Y. Xu. Online, Interactive Learning of Gestures for Human/Robot Interfaces. In Proceedings of the IEEE International Conference on Robotics and Automation, pages 2982–2987, 1996.
[28] V. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Doklady Physics, 10(8):707–710, 1966.
[29] J. Loughry and D. A. Umphress. Information Leakage from Optical Emanations. ACM Transactions on Information and System Security, 5(3):262–289, 2002.
[30] T. Marrin and R. Picard. The ‘Conductor’s Jacket’: A Device for Recording Expressive Musical Gestures. In Proceedings of the International Computer Music Conference, 1998.
[31] E. Mays, F. Damerau, and R. Mercer. Context Based Spelling Correction. International Journal on Information Processing and Management, 27(5):517–522, 1991.
[32] M. McIlroy. Development of a Spelling List. IEEE Transactions on Communications, 30(1):91–99, 1982.
[33] Tempest Fundamentals. NACSIM 5000 NSA-82-89, National Security Agency, February 1982. Classified.
[34] S. Oualline. Vi IMproved – Vim. New Riders, 2001.
[35] N. Pal and S. Pal. A Review On Image Segmentation Techniques. Pattern Recognition, 26(9):1277–1294, 1993.
[36] A. Pastor. Owning big brother (or how to crack into axis ip cameras). www.procheckup.com, 2007.
[37] P. Robinson. Sneakers. Universal Pictures, 1992.
[38] G. Shah, A. Molina, and M. Blaze. Keyboards and Covert Channels. In Proceedings of the USENIX Security Symposium, pages 59–75, 2006.
[39] D. Song, D. Wagner, and X. Tian. Timing Analysis of Keystrokes and Timing Attacks on SSH. In Proceedings of the USENIX Security Symposium, 2001.
[40] T. Starner and A. Pentland. Real-time American Sign Language Recognition from Video Using Hidden Markov Models. In Proceedings of the International Symposium on Computer Vision, pages 265–270, 1995.
[41] S. Suzuki and K. Abe. Topological Structural Analysis of Digitized Binary Images by Border Following. Computer Vision, Graphics, and Image Processing, 30(1):32–46, 1985.
[42] T. Takahashi and F. Kishino. Hand Gesture Coding Based on Experiments Using a Hand Gesture Interface Device. ACM SIGCHI Bulletin, 23(2):67–74, 1991.
[43] C. Tappert, C. Suen, and T. Wakahara. The State of the Art in On-Line Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(8):787–808, 1990.
[44] W. van Eck. Electromagnetic Radiation from Video Display Units: An Eavesdropping Risk? Computers & Security, 4:269–286, 1985.
[45] A. Wexelblat. A Feature-Based Approach to Continuous-Gesture Analysis. Master’s thesis, Massachusetts Institute of Technology, 1994.
[46] C. Wright, L. Ballard, F. Monrose, and G. Masson. Language Identification of Encrypted VoIP Traffic: Alejandra y Roberto or Alice and Bob? In Proceedings of the USENIX Security Symposium, 2007.
[47] Y. Wu, Y. Shan, Z. Zhang, and S. Shafer. Visual Panel: From an Ordinary Paper to a Wireless and Mobile Input Device. Technical Report MSR-TR-2000-112, Microsoft Research, 2000.
[48] L. Zhuang, F. Zhou, and J. Tygar. Keyboard Acoustic Emanations Revisited. In Proceedings of the ACM Conference on Computer and Communications Security, pages 373–382, 2005.