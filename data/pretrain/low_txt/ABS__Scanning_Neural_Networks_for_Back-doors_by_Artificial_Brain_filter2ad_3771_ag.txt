### (b) Min_Diff

The training set is augmented with adversarial examples, which are difficult for human checks to recognize as malicious. However, these adversarial examples can alter the model's decision boundary, leading to performance degradation. The works in [19, 34] focus on hardware trojans that target the neural networks' underlying hardware by tampering with circuits to inject backdoors.

Several defense techniques have been proposed to counteract training data poisoning [16, 27, 44, 48]. Most of these techniques fall under the category of data sanitization, where they aim to remove poisoned data during the training process. In contrast, ABS (Adversarial Backdoor Scanner) provides a defense mechanism at a different stage of the model's lifecycle, specifically after the models have been trained.

ABS is also related to adversarial sample attacks (e.g., [13, 21, 45, 46, 52, 56, 62, 64]). Recent research has explored the construction of input-agnostic universal perturbations [43] and universal adversarial patches [15]. These approaches share similarities with Neural Cleanse [59], which we have compared thoroughly. The key difference is that ABS employs an analytic approach and leverages the internal structure of the model.

A number of defense techniques have been developed to detect adversarial examples. Some methods, such as [18, 40, 57, 63], can identify whether an input is an adversarial example and could potentially be used to detect pixel-space trojan triggers. However, these approaches primarily focus on detecting inputs rather than scanning the models themselves.

### 8. CONCLUSIONS

We have developed a novel analytic approach to scan neural network models for potential backdoors. This approach features a stimulation analysis that evaluates how providing different levels of stimulus to an inner neuron impacts the model's output activation. This analysis is then used to identify neurons that have been compromised by trojan attacks. Our experiments demonstrate that this technique significantly outperforms state-of-the-art methods and achieves a very high detection rate for trojaned models.

### ACKNOWLEDGMENTS

We thank the anonymous reviewers for their constructive comments. This research was supported, in part, by DARPA FA8650-15-C-7562, NSF 1748764, 1901242, and 1910300, ONR N000141410468 and N000141712947, and Sandia National Lab under award 1701331. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.

### REFERENCES

[1] BigML Machine Learning Repository. https://bigml.com/.

[2] Caffe Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo.

[3] Gradientzoo: pre-trained neural network models. https://www.gradientzoo.com/.

[4] BigML.com. https://bigml.com.

[5] bolunwang/backdoor. https://github.com/bolunwang/backdoor.

[6] BVLC/caffe. https://github.com/BVLC/caffe/wiki/Model-Zoo.

[7] DARPA Announces $2 Billion Campaign to Develop Next Wave of AI Technologies. https://www.darpa.mil/news-events/2018-09-07.

[8] Executive Order on Maintaining American Leadership in Artificial Intelligence. https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence.

[9] GitHub - BIGBALLON/cifar-10-cnn: Play deep learning with CIFAR datasets. https://github.com/BIGBALLON/cifar-10-cnn.

[10] onnx/models. https://github.com/onnx/models.

[11] Tensorpack - Models. http://models.tensorpack.com/.

[12] acoomans. 2013. https://github.com/acoomans/instagram-filters.

[13] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420.

[14] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. 2016. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316.

[15] Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. 2017. Adversarial patch. arXiv preprint arXiv:1712.09665.

[16] Yinzhi Cao, Alexander Fangxiao Yu, Andrew Aday, Eric Stahl, Jon Merwine, and Junfeng Yang. 2018. Efficient repair of polluted machine learning systems via causal unlearning. In Proceedings of the 2018 on Asia Conference on Computer and Communications Security. ACM, 735–747.

[17] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526.

[18] Edward Chou, Florian Tramèr, Giancarlo Pellegrino, and Dan Boneh. 2018. SentiNet: Detecting Physical Attacks Against Deep Learning Systems. arXiv preprint arXiv:1812.00292.

[19] Joseph Clements and Yingjie Lao. 2018. Hardware trojan attacks on neural networks. arXiv preprint arXiv:1806.05768.

[20] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.

[21] Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, and Jia Liu. 2018. Poisoning attacks to graph-based recommender systems. In Proceedings of the 34th Annual Computer Security Applications Conference. ACM, 381–392.

[22] Yarin Gal. 2016. Uncertainty in deep learning. Ph.D. Dissertation. PhD thesis, University of Cambridge.

[23] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. 2019. STRIP: A Defence Against Trojan Attacks on Deep Neural Networks. arXiv preprint arXiv:1902.06531.

[24] Ross Girshick. 2015. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision. 1440–1448.

[25] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733.

[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778.

[27] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. 2018. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 19–35.

[28] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2018. Model-reuse attacks on deep learning systems. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. ACM, 349–363.

[29] Yujie Ji, Xinyang Zhang, and Ting Wang. 2017. Backdoor attacks against learning systems. In 2017 IEEE Conference on Communications and Network Security (CNS).

[30] Melissa King. 2019. TrojAI. https://www.iarpa.gov/index.php?option=com_content&view=article&id=1142&Itemid=443.

[31] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features from tiny images. Technical Report. Citeseer.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. 1998. Gradient-based learning applied to document recognition. Proc. IEEE (1998).

[33] Gil Levi and Tal Hassner. 2015. Age and gender classification using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 34–42.

[34] Wenshuo Li, Jincheng Yu, Xuefei Ning, Pengjun Wang, Qi Wei, Yu Wang, and Huazhong Yang. 2018. Hu-fu: Hardware and software collaborative attack framework against neural networks. In ISVLSI.

[35] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. 2018. Backdoor embedding in convolutional neural network models via invisible perturbation. arXiv preprint arXiv:1808.10307.

[36] Min Lin, Qiang Chen, and Shuicheng Yan. 2013. Network in network. arXiv preprint arXiv:1312.4400.

[37] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273–294.

[38] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In 25nd Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-221, 2018. The Internet Society.

[39] Yuntao Liu, Yang Xie, and Ankur Srivastava. 2017. Neural trojans. In 2017 IEEE International Conference on Computer Design (ICCD). IEEE, 45–48.

[40] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang. 2019. NIC: Detecting Adversarial Samples with Neural Network Invariant Checking. In 26th Annual Network and Distributed System Security Symposium, NDSS.

[41] Wei Ma and Jun Lu. 2017. An Equivalence of Fully Connected Layer and Convolutional Layer. arXiv preprint arXiv:1712.01252.

[42] Andreas Møgelmose, Dongran Liu, and Mohan M Trivedi. 2014. Traffic sign detection for us roads: Remaining challenges and a case for tracking. In 17th International IEEE Conference on Intelligent Transportation Systems (ITSC).

[43] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1765–1773.

[44] Mehran Mozaffari-Kermani, Susmita Sur-Kolay, Anand Raghunathan, and Niraj K Jha. 2015. Systematic poisoning attacks on and defenses for machine learning in healthcare. IEEE journal of biomedical and health informatics.

[45] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security. ACM, 506–519.

[46] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. 2016. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P).

[47] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. 2015. Deep face recognition. In bmvc, Vol. 1. 6.

[48] Andrea Paudice, Luis Muñoz-González, and Emil C Lupu. 2018. Label sanitization against label flipping poisoning attacks. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 5–15.

[49] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition. 779–788.

[50] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision 115, 3, 211–252.

[51] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label poisoning attacks on neural networks. In NeuralIPS.

[52] Mahmood Sharif, Lujo Bauer, and Michael K Reiter. 2018. On the suitability of lp-norms for creating and preventing adversarial examples. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.

[53] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.

[54] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2011. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In IJCNN, Vol. 6. 7.

[55] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural networks 32, 323–332.

[56] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.

[57] Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang. 2018. Attacks meet interpretability: Attribute-steered detection of adversarial samples. In Advances in Neural Information Processing Systems. 7717–7728.

[58] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. 2018. Clean-Label Backdoor Attacks.

[59] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. IEEE, 0.

[60] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4, 600–612.

[61] Wikipedia. 2019. Electrical brain stimulation - Wikipedia. https://en.wikipedia.org/wiki/Electrical_brain_stimulation.

[62] Xi Wu, Uyeong Jang, Jiefeng Chen, Lingjiao Chen, and Somesh Jha. 2017. Reinforcing adversarial robustness using model confidence induced by adversarial training. arXiv preprint arXiv:1711.08001.

[63] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155.

[64] Wei Yang, Deguang Kong, Tao Xie, and Carl A Gunter. 2017. Malware detection in adversarial settings: Exploiting feature evolutions and confusions in android apps. In Proceedings of the 33rd Annual Computer Security Applications Conference.

[65] Minhui Zou, Yang Shi, Chengliang Wang, Fangyu Li, WenZhan Song, and Yu Wang. 2018. Potrojan: powerful neural-level trojan designs in deep learning models. arXiv preprint arXiv:1802.03043.

### Appendix A: EXAMPLE OF CONFOUNDING EFFECT

Figure 24 shows a real-world example of confounding using a trojaned model on MNIST. As shown in the figure, the NSFs (Neuron Sensitivity Functions) for a benign neuron and a compromised neuron (from the same trojaned model) are similar. Hence, both are candidates. When we run the model with all inputs, including the poisonous ones, we find that the input that induces the largest activation of the benign neuron is a benign input, with the largest activation being only 1.8. In contrast, a poisonous input can induce the compromised neuron to have a value of 10.4. From the NSF of the benign neuron, there is no elevation effect when the activation is 1.8. This example demonstrates that the elevation effect of a benign neuron usually does not originate from training and may likely be infeasible, whereas the elevation of a compromised neuron is induced by training.

**Figure 24: Example of confounding effect**

### Appendix B: MODEL TROJANING

In this section, we show the test accuracy and attack success rate of the models trojaned in this paper in Table 7. Columns 1 and 2 show the dataset and models. Column 3 shows the average accuracy of the benign models. The number 20 in the column label indicates that each reported value is an average over 20 models. Column 4 (Acc Dec) shows the accuracy decrease of models trojaned with trigger YS. Column 5 (ASR) shows the attack success rate of models trojaned with trigger YS. The accuracy decrease denotes the difference between the accuracy of a benign model and the accuracy of its trojaned version. Note that for CIAR-10 and GTSRB, we trojan 3 models with YS using 3 different combinations of benign and poisonous data, and hence the accuracy decrease and attack success rate are averaged across 3 models. For ImageNet, we trojan 1 model per trigger, and we directly report the accuracy decrease and attack success rate of each trojaned model in row 8. ImageNet is a challenging task, and top-5 accuracy is often used [50, 53]. Here, we report the top-5 accuracy decrease and the top-5 attack success rate of trojaned models. Columns 6-21 show the accuracy decrease and attack success rate for different kinds of attacks. In some cases, the trojaned models have higher accuracy than the original ones, indicated by negative values in the Acc Dec columns. Because the adversarial perturbation is very small, it is hard to trojan an adversarial perturbation model with a high attack success rate without degrading the performance on benign images. For adversarial perturbation attack on CIFAR-VGG, the accuracy drops by 10.3%. For all other types of attacks, the trojaned models have minor accuracy decreases. The downloaded (benign and trojaned) models have similar test accuracy differences and attack success rates, as shown in their papers [25, 38].

**Figure 25: Label Specific “Trigger”**

### Appendix C: DETECTION EFFECTIVENESS VS. TRIGGER SIZE

In this experiment, we evaluate how ABS’s detection rate changes with trigger size on CIFAR-10. In Table 8, the row TS means the size of the trigger, and column MMS denotes the max_mask_size in Algorithm 2. We vary the trigger size from 2% of the input to 25%, following a setting suggested in [30]. As shown in Table 8, as long as the max_mask_size is larger or equal to the trigger size, the REASR (Relative Error Area Similarity Ratio) of our approach is at least 85% (100% in most cases). This shows that our approach can successfully detect trojaned models within the trigger size budget. In all other experiments, we use max_mask_size = 6%.

### Appendix D: DETECTING LABEL SPECIFIC ATTACK

Although our goal is to defend against attacks that aim to persistently subvert any input (of any label) to the target label, we conduct an experiment to observe if ABS has potential in dealing with label-specific attacks, where the attacker aims to subvert inputs of a specific label (called the victim label) to the target label. We trojan the CIFAR-10 ResNet model with 10 triggers, and each trigger causes the images belonging to class k to be classified to k + 1. We poison 10% of the samples. The model accuracy changes by -0.3%, and the average attack success rate (on all labels) is 93.7%. Note that in this context, ABS can no longer use one input per label as stamping inputs of labels other than the victim with the trigger has no effect. Hence, we use 5 additional images for each label to validate if the generated trigger is effective. The REASR scores of benign models and trojaned models are shown in Table 9, with each column (except the first one) representing a label. For benign models, the label "deer" has a high REASR, and hence ABS fails. The reverse-engineered trigger is shown in Figure 25, which looks like antlers. In general, label-specific attacks incur more false positives for ABS because the reverse-engineered “trigger” only needs to subvert one class of inputs. For trojaned models, ABS can detect 9 out of 10. ABS fails to detect the trigger that misclassifies "Horse" to "Ship".

**Figure 25: Label Specific “Trigger”**

### Appendix E: PERTURBING PARAMETERS AND USING DATA AUGMENTATION IN NC

In this section, we try different parameter configurations and leverage data augmentation to see if NC’s performance can be improved. We perturb three main parameters used in NC optimization: the attack success rate threshold (THRESHOLD), cost multiplier (MULTIPLIER), and learning rate (LR). The first is the threshold at which NC stops the optimization and considers the reverse-engineered pattern a trigger. The second is used to control the weight between the optimization loss and the trigger size loss. The third is the learning rate for optimization. We experiment with different settings of the three parameters on CIFAR-10. Additionally, since NC’s performance is related to the number of samples used in optimization, we try to use data augmentation to increase the number of samples. We use the same data augmentation setting as normal CIFAR-10 training [9]. We test NC with one image per class, which is the same setting for ABS. The results are shown in Table 10. Column 1 shows the parameters, and column 2 shows the value settings. Columns 3-5 show the classification accuracy for pixel space trojaned models, feature space trojaned models, and benign models for the NiN structure. Columns 6-8 and columns 9-11 show the detection accuracy for VGG and ResNet models. By changing these parameters, the detection rate of pixel space attacks can increase to 60% on VGG models, but the accuracy on benign models also decreases to 72%. The detection rate of feature space attacks can increase to 67% on ResNet models, but the accuracy on benign models also decreases to 70%. As shown in row 12 in Table 10, using data augmentation increases the detection rate on pixel space attacks while simultaneously causing accuracy degradation on benign models. We speculate that data augmentation can only explore a small region around the input and cannot increase the diversity of data substantially, and does not help guide the optimization of NC. This shows that changing parameter settings or using data augmentation may improve NC’s performance, but the improvement is limited.

### Appendix F: OPTIMIZING MULTIPLE COMPROMISED NEURONS TOGETHER

There could be multiple compromised neurons in a trojaned model. In this section, we try to optimize multiple compromised neurons together to see if it can help produce triggers of better quality. For NiN models trojaned with pixel space patch triggers, we further reverse engineer triggers based on all the compromised neurons ABS finds. Figure 26 shows the results. Row 1 shows the original triggers. For each kind of trigger, we trojan the model with 3 different settings such that with the 5 different kinds, there are 15 different models in total. Row 2 shows the triggers reverse engineered by optimizing multiple compromised neurons together.