## IV. Methodology

### A. Experimental Setup
In the class-dependent noise experiments, for all combinations of \( p_{sbr} \in P \) and \( p_{nsbr} \in P \), the following steps are performed:
1. **Noise Generation**: Generate noise for the training and validation data sets.
2. **Model Training**: Train logistic regression, naive Bayes, and AdaBoost models using the noisy training data set.
3. **Model Tuning**: Tune the models using the noisy validation data set.
4. **Model Testing**: Test the models using a noiseless test data set.

## V. Experimental Results

In this section, we analyze the results of the experiments conducted according to the methodology described in Section IV.

### A. Model Performance without Noise in the Training Data Set
One of the contributions of this paper is the proposal of a machine learning model that identifies security bugs using only the title of the bug report. This approach allows for the training of machine learning models even when development teams do not wish to share full bug reports due to the presence of sensitive data. We compare the performance of three machine learning models (logistic regression, naive Bayes, and AdaBoost) when trained using only bug titles.

- **Logistic Regression**: The best-performing classifier with an AUC of 0.9826, recall of 0.9353, and FPR of 0.0735.
- **Naive Bayes**: Slightly lower performance than logistic regression, with an AUC of 0.9779, recall of 0.9189, and FPR of 0.0769.
- **AdaBoost**: Inferior performance compared to the other two classifiers, with an AUC of 0.9143, recall of 0.7018, and FPR of 0.0774.

The area under the ROC curve (AUC) is a good metric for comparing the performance of several models as it summarizes the TPR vs. FPR relation in a single value. In the subsequent analysis, we will focus on AUC values.

![Table I](./media/TableI.png)

### B. Class Noise: Single-Class
Consider a scenario where all bugs are assigned to the non-security bug (NSBR) class by default, and a bug is only assigned to the security bug (SBR) class if a security expert reviews the bug repository. This scenario is represented in the single-class experimental setting, where we assume \( p_{nsbr} = 0 \) and \( p_{sbr} = 0 \).

- For \( p_{sbr} = 0.25 \), the AUC differences from the noiseless model are 0.003 for logistic regression, 0.006 for naive Bayes, and 0.006 for AdaBoost.
- For \( p_{sbr} = 0.50 \), the AUC differences are 0.007 for logistic regression, 0.011 for naive Bayes, and 0.010 for AdaBoost.

The logistic regression classifier shows the smallest variation in its AUC metric, indicating more robust behavior compared to naive Bayes and AdaBoost.

### C. Class Noise: Class-Independent
We compare the performance of the three classifiers when the training set is corrupted by class-independent noise. The AUC for each model trained with different levels of \( p_{br} \) in the training data is measured.

![Table III](./media/TableIII.png)

- For \( p_{br} = 0.25 \), the AUC differences from the noiseless model are 0.011 for logistic regression, 0.008 for naive Bayes, and 0.0038 for AdaBoost.
- For \( p_{br} = 0.50 \), the AUC drops significantly, approaching 0.5, indicating random classification.

Label noise does not significantly impact the AUC of naive Bayes and AdaBoost classifiers when noise levels are below 40%. However, logistic regression experiences a significant impact in AUC for noise levels above 30%.

![AUC Variation Impact](./media/AUC.png)

### D. Class Noise: Class-Dependent
In the final set of experiments, we consider a scenario where different classes contain different noise levels, i.e., \( p_{sbr} \neq p_{nsbr} \). We systematically increment \( p_{sbr} \) and \( p_{nsbr} \) independently by 0.05 in the training data and observe the change in behavior of the three classifiers.

![Logistic Regression](./media/TableIV.png)
![Naive Bayes](./media/TableV.png)
![AdaBoost](./media/TableVI.png)

- **Logistic Regression**: Significant impact on AUC when both classes contain noise levels above 30%.
- **Naive Bayes**: Very small impact on AUC even when 50% of the positive class labels are flipped, provided the negative class contains 30% or less noisy labels.
- **AdaBoost**: Most robust behavior among the three classifiers. A significant change in AUC only occurs for noise levels greater than 45% in both classes.

### E. Presence of Residual Noise in the Original Data Set
Our data set was labeled by signature-based automated systems and human experts. While we expect minimal residual noise, its presence does not invalidate our conclusions. If the original data set were corrupted by class-independent noise, the performance of our classifiers would be even better with a completely noiseless data set. The existence of residual noise means that the resilience against noise of our classifiers is better than the results presented here.

## VI. Conclusions and Future Works

### A. Contributions
1. **Feasibility of Security Bug Report Classification**: We demonstrate the feasibility of security bug report classification based solely on the title of the bug report. This is particularly relevant in scenarios where full bug reports are not available due to privacy constraints. Our classification model, which uses a combination of TF-IDF and logistic regression, achieves an AUC of 0.9831.
2. **Robustness Analysis**: We analyze the effect of mislabeled training and validation data on three well-known machine learning classification techniques (naive Bayes, logistic regression, and AdaBoost). All three classifiers are robust to single-class noise, with a very small decrease in AUC (0.01) for a noise level of 50%. For class-independent noise, naive Bayes and AdaBoost show significant variations in AUC only when trained with noise levels greater than 40%. Class-dependent noise significantly impacts the AUC only when there is more than 35% noise in both classes, with AdaBoost showing the most robustness.

### B. Future Work
- **Severity Level Determination**: Examine the effect of noisy data sets in determining the severity level of a security bug.
- **Class Imbalance**: Understand the effect of class imbalance on the resilience of the trained models against noise.
- **Adversarial Noise**: Investigate the effect of adversarially introduced noise in the data set.

### References
[1] John Anvik, Lyndon Hiew, and Gail C Murphy. Who should fix this bug? _In Proceedings of the 28th international conference on Software engineering_, pages 361–370. ACM, 2006.
[2] Diksha Behl, Sahil Handa, and Anuja Arora. A bug mining tool to identify and analyze security bugs using naive bayes and tf-idf. In _Optimization, Reliabilty, and Information Technology (ICROIT)_, 2014 International Conference on, pages 294–299. IEEE, 2014.
[3] Nicolas Bettenburg, Rahul Premraj, Thomas Zimmermann, and Sunghun Kim. Duplicate bug reports considered harmful really? In _Software maintenance, 2008. ICSM 2008. IEEE international conference on_, pages 337–345. IEEE, 2008.
[4] Andres Folleco, Taghi M Khoshgoftaar, Jason Van Hulse, and Lofton Bullard. Identifying learners robust to low quality data. In _Information Reuse and Integration, 2008. IRI 2008. IEEE International Conference on_, pages 190–195. IEEE, 2008.
[5] Benoît Frenay. _Uncertainty and label noise in machine learning_. PhD thesis, Catholic University of Louvain, Louvain-la-Neuve, Belgium, 2013.
[6] Benoît Frenay and Michel Verleysen. Classification in the presence of label noise: a survey. _IEEE transactions on neural networks and learning systems_, 25(5):845–869, 2014.
[7] Michael Gegick, Pete Rotella, and Tao Xie. Identifying security bug reports via text mining: An industrial case study. In _Mining software repositories (MSR), 2010 7th IEEE working conference on_, pages 11–20. IEEE, 2010.
[8] Katerina Goseva-Popstojanova and Jacob Tyo. Identification of security related bug reports via text mining using supervised and unsupervised classification. In _2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)_, pages 344–355, 2018.
[9] Ahmed Lamkanfi, Serge Demeyer, Emanuel Giger, and Bart Goethals. Predicting the severity of a reported bug. In _Mining Software Repositories (MSR), 2010 7th IEEE Working Conference on_, pages 1–10. IEEE, 2010.
[10] Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. _IEEE transactions on cybernetics_, 43(3):1146–1151, 2013.
[11] G Murphy and D Cubranic. Automatic bug triage using text categorization. In _Proceedings of the Sixteenth International Conference on Software Engineering & Knowledge Engineering_. Citeseer, 2004.
[12] Mykola Pechenizkiy, Alexey Tsymbal, Seppo Puuronen, and Oleksandr Pechenizkiy. Class noise and supervised learning in medical domains: The effect of feature extraction. In _null_, pages 708–713. IEEE, 2006.
[13] Charlotte Pelletier, Silvia Valero, Jordi Inglada, Nicolas Champion, Claire Marais Sicre, and Gérard Dedieu. Effect of training class label noise on classification performances for land cover mapping with satellite image time series. _Remote Sensing_, 9(2):173, 2017.
[14] PS Sastry, GD Nagendra, and Naresh Manwani. A team of continuous-action learning automata for noise-tolerant learning of half-spaces. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_, 40(1):19–28, 2010.
[15] Choh-Man Teng. A comparison of noise handling techniques. In _FLAIRS Conference_, pages 269–273, 2001.
[16] Dumidu Wijayasekara, Milos Manic, and Miles McQueen. Vulnerability identification and classification via text mining bug databases. In _Industrial Electronics Society, IECON 2014-40th Annual Conference of the IEEE, pages 3612–3618. IEEE_, 2014.
[17] Xinli Yang, David Lo, Qiao Huang, Xin Xia, and Jianling Sun. Automated identification of high impact bug reports leveraging imbalanced learning strategies. In _Computer Software and Applications Conference (COMPSAC), 2016 IEEE 40th Annual_, volume 1, pages 227–232. IEEE, 2016.
[18] Deqing Zou, Zhijun Deng, Zhen Li, and Hai Jin. Automatically identifying security bug reports via multitype features analysis. In _Australasian Conference on Information Security and Privacy_, pages 619–633. Springer, 2018.