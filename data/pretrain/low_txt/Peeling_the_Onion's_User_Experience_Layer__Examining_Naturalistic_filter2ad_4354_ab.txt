### Script Logic for Selecting Questionnaires

- **If Tor Browser was running and no other browsers were:**
  - Launch the Tor Browser Questionnaire.
- **If Tor Browser was running along with another browser:**
  - Launch the Switched Browser Questionnaire.
- **If Tor Browser was not running and any other browser was running:**
  - Launch the Other Browser Questionnaire.
- **If no browser was closed within a 24-hour period:**
  - Launch the Tor Browser Questionnaire.

Figure 1 illustrates the script logic used to select the appropriate questionnaire. The source code for this script is available on GitHub [2] and was provided to study participants [3].

### Complementary Data Collection

To complement the insights gathered through the questionnaires, we obtained detailed qualitative data in two ways:
1. **Write-ups:**
   - At the end of the study, participants submitted 2-3 page write-ups reflecting on their experience using the Tor Browser for routine online tasks (see Appendix C for instructions).
2 - **Semi-structured Interviews:**
   - We conducted brief 10-minute semi-structured interviews to gather additional information about the user experience (UX) and challenges of using the Tor Browser (see Appendix B for the interview guide).

These write-ups and interviews provided context, added nuance, and corroborated the information gathered through the questionnaires.

### Study Procedures

The study was deployed as an assignment in an undergraduate course in the Department of Information and Library Science at Indiana University Bloomington. This sample, similar to those in previous studies [13, 32, 33], consisted of novice and non-expert users of the Tor Browser, a population whose adoption is crucial for making Tor more inclusive and diverse.

- **Grading and Research Separation:**
  - While the assignment counted toward 10% of the course grade, participation in the research component was optional and voluntary.
  - Grading and research aspects were kept completely separate, with the course instructor not involved in the research and researchers not involved in grading.
  - This separation ensured that there was no coercion for research participation and prevented undue influence of grade considerations on the collected data.

- **Anonymity:**
  - To maintain anonymity, each participant was assigned a unique identifier, such as "elegant eagle," to be used as the participant ID when providing responses.
  - Participants did not receive any compensation.

- **Consent and Instructions:**
  - Informed consent was obtained via a brief in-class presentation, followed by answering questions and providing clarifications.
  - Participants received detailed instructions to download and install the Tor Browser and our monitoring script.
  - After installation, participants filled out a brief pre-study questionnaire (see Appendix D).

- **Study Duration:**
  - The study lasted one week, from Monday to the following Sunday.
  - Participants were asked to use the Tor Browser for all online browsing activities.
  - Our script monitored browser processes and presented appropriate online questionnaires.
  - At the end of the study, participants were guided to uninstall the monitoring script and the Tor Browser if they desired.

- **Data Submission:**
  - Within a few days of study completion, participants submitted 2-3 page reports on their use of the Tor Browser.
  - Willing participants were interviewed, and the audio recordings were transcribed and then destroyed.

All study procedures were reviewed and approved by the Institutional Review Boards (IRBs) of Indiana University and New York University.

### Data Analysis

- **Participant Demographics:**
  - 19 students consented to participate (8 female, 7 male, 4 who did not provide demographic information), with ages ranging from 18 to 22 (average 20).
  - Of the 15 participants who provided demographic information, one was Hispanic, two were Asian, and the rest were Caucasian.
  - Only 3 participants had used the Tor Browser prior to the study.

- **Questionnaire Responses:**
  - We received 121 questionnaire responses (102 Tor Browser, 13 Switched Browser, and 6 Other Browser) from 13 of the 19 participants (mean: 9.3, median: 6, mode: 7 per participant across the 13 respondents).
  - All 19 participants provided thorough post-study write-ups, and 11 agreed to be interviewed.

### Issue Categorization

In addition to choosing from a provided list of issue categories, participants could enter open-ended text responses to describe encountered problems. These responses were categorized into 17 labels:

1. Broken Web site
2. Unresponsive Web site
3. Streaming Content
4. Reduced Productivity
5. Login
6. Browser Update Required
7. Browser Dependent Content
8. Shopping
9. Specific File Types
10. Latency
11. Inconvenience
12. Tor Traffic Block
13. CAPTCHAs
14. Geolocation
15. Browser Crash
16. Other
17. No Perceived Need for the Tor Browser

These labels were generated by analyzing all open-ended text responses and voluntarily provided URLs. In 83 cases, the selected categories matched the assigned labels. In 26 cases, more specific labels were assigned based on the open-ended responses and URLs. In the remaining 12 cases, the text responses did not match the selected categories, and we labeled the issues according to the open-ended text.

After assigning labels, some were combined to reflect higher-level issues:

1. **Broken Functionality:**
   - The website or some functionality within the website was not accessible via the Tor Browser.
2. **Latency:**
   - The Tor Browser was unacceptably slow.
3. **Differential Treatment:**
   - The website treated Tor traffic differently.
4. **Geolocation:**
   - The website provided content based on the locale of the Tor exit node, which did not match the participant’s locale.
5. **Crash:**
   - The Tor Browser crashed or encountered an error.
6. **Other:**
   - The participant reported an issue not specific to the Tor Browser.

For example, the first 9 labels were combined under the "Broken Functionality" category. Table 1 provides the full list of issues along with the respective underlying labels and counts.

### Qualitative Coding

Qualitative data from write-ups and interviews was analyzed using Grounded Theory approaches [15]. The first author began coding the qualitative data after the first interview and continued throughout the data collection process. The analysis utilized two stages of coding: open and axial.

- **Open Coding:**
  - Data was coded sentence-by-sentence without an initial hypothesis.
  - Each sentence was labeled with an underlying concept, with more attention given to UX-relevant codes.

- **Axial Coding:**
  - Codes were examined for similarities and connections and grouped into overarching categories.
  - These categories were used to generate insights into the UX problems faced by participants.

All coding and categorization were done by the first author and independently verified by the second author. RQDA [18] was used for the qualitative analyses.

### Findings

Table 1 provides quantitative details on the various issues reported in the online questionnaires, broken down into the types of problems falling under each issue. The following subsections detail these issues, integrating numeric counts with insights from the qualitative data.

#### 4.1 Broken Functionality and Latency

- **Broken Functionality:**
  - 54 out of 121 questionnaires reported some type of functional hindrance.
  - 9 out of 13 participants reported functionality breaks, ranging from completely inaccessible websites to partial access and specific operational issues.

- **Latency:**
  - 41 out of 121 questionnaires expressed frustration with latency.
  - 8 out of 13 participants reported slow speeds.

Participants experienced frustration due to the impact on productivity, such as issues with two-factor authentication, loading specific files, accessing translation services, and reading news.

- **Example Quotes:**
  - "Sometimes the Tor browser simply would fail to load the page or just continue to load, never reaching its goal of going to the page that I wanted to go to." – (P17, M, unspecified age, write-up)
  - "In some cases of using Tor, certain Web sites did not work at all." – (P3, F, 19, write-up)

Although slow speeds were annoying, many participants stated during interviews that they would be willing to deal with increased latency for the benefits of anonymity, especially if they understood the reasons behind it.

- **Example Quotes:**
  - "Yeah, definitely. I didn’t know it was that. I knew that Tor was a much more secure way to browse the Internet but I did not know that the slowness of it was part of how it did it. Now that I know that, if for whatever reason I wanted to make sure it was really secure, I would definitely use Tor even though it is slower. I did not know that was a thing!" – (P5, F, 19, interview)
  - "Because, I mean, some things are worth waiting for to make sure I can accomplish whatever I need to." – (P7, F, 21, interview)

#### 4.2 Inconvenience

- **Lack of Conveniences:**
  - Two participants noted the lack of mechanisms present in other browsers, such as easy access to bookmarks and password-saving capabilities.
  - These participants often switched to other browsers when they needed to access a bookmarked site or a saved password.

- **Example Quotes:**
  - "The Tor browser also does not provide a lot of the ease of access quirks that a traditional browser provides. For example, it does not save your passwords which forces you to put them in manually every time." – (P17, M, unspecified age, write-up)
  - "To elaborate on what I mean by ‘ease of access,’ because Google Chrome was my default browser of choice, none of my bookmarks or pre-saved information (i.e., passwords, payment information, etc.) were readily available to me while using the Tor Browser." – (P12, M, unspecified age, write-up)

While the lack of conveniences was a hindrance, many participants understood that including these features might compromise Tor’s anonymity goals.

- **Example Quotes:**
  - "I know that the goal of Tor is to allow for anonymity and privacy, so it does not store any information or have the capability to save passwords, but it was really inconvenient to have to log back into things whenever I opened the browser again." – (P5, F, 19, write-up)
  - "I think many of the things the average user would want in a browser to make usage more efficient would counteract the anonymity aspect of Tor — things like having a most visited sites page, having passwords saved for certain sites, and using bookmarks at the top of the page to make navigating faster." – (P14, F, 19, write-up)

#### 4.3 Differential Treatment

- **Differential Treatment:**
  - Two participants encountered websites that treated Tor traffic differently, including total blockage and CAPTCHA verification.
  - 5 questionnaire reports indicated differential treatment, which was lower than expected based on previous measurements [26].

- **Example Quote:**
  - "I was going to read articles on the online news site der-spiegel.de and I was trying to open articles, but it would not let me read them further." – (P14, F, 19, interview)

#### 4.4 Geolocation

- **Geolocation Issues:**
  - Only two participants reported issues due to geolocation, particularly problematic when accessing geographically restricted multimedia content or checking email.

- **Example Quote:**
  - "When I tried to get on the site, it told me that Pandora was not active in my country just yet, just the United States." – (P3, F, 19, write-up)

#### 4.5 Web Searching and Operational Messaging

- **Web Searching:**
  - The default search engine for the Tor Browser is DuckDuckGo, which may confuse users accustomed to Google.
  - Some participants noted undesirable characteristics, such as a lack of auto-complete and the inability to revisit past search results.

- **Example Quotes:**
  - "Someone who is using Tor and does not understand IP anonymity may be confused why when they search ‘Google’ in the search bar it turns into ‘DuckDuckGo’ which may lead users to believe they are doing something incorrect and feel lost." – (P18, M, 19, write-up)
  - "I personally did not care for DuckDuckGo at all. My one big complaint is that when I was searching something it would not autocomplete like Google does. That means I had to know what specifically I was looking for and how to spell it." – (P9,