# Optimized Text

## Figure 7: Percentage of User-Usable Memory Blocks After Certain Numbers of Writes

### Subfigure (a): Ocean Benchmark
- **X-Axis:** Number of writes (100, 200, 300, 400, 500, 600, 700, 800, 900)
- **Y-Axis:** Percentage of user-usable memory blocks
- **Legend:**
  - WL-Reviver with different percentages of pre-reserved space (0%, 5%, 10%, 15%)

### Subfigure (b): MG Benchmark
- **X-Axis:** Number of writes (100, 200, 300, 400, 500, 600, 700, 800, 900)
- **Y-Axis:** Percentage of user-usable memory blocks
- **Legend:**
  - WL-Reviver with different percentages of pre-reserved space (0%, 5%, 10%, 15%)

**Description:**
Figure 7 illustrates the percentage of user-usable memory blocks after a certain number of writes for two benchmarks: (a) Ocean and (b) MG. The experiments were conducted using ECP6 and Start-Gap. For the Ocean benchmark, which has a highly uniform write distribution, smaller pre-reservations (e.g., 5%) are more effective in postponing the first failure. In contrast, for the MG benchmark, which has a biased distribution, larger pre-reservations (e.g., 15%) are more effective. Pre-reservation provides free slots to hide failed blocks but reduces the available space for writes, potentially causing more failures. WL-Reviver consistently provides significantly more usable blocks, making 100% of the PCM space usable before the first failure.

## D. Comparison to LLS on Efficiency

### Figure 8: Reduction of Software-Usable Space with Ongoing Writes
- **X-Axis:** Number of writes (0, 100, 200, 300, 400, 500, 600, 700, 800, 900)
- **Y-Axis:** Percentage of software-usable space
- **Legend:**
  - WL-Reviver
  - LLS

**Description:**
Figure 8 shows the reduction of software-usable PCM space with ongoing writes. While LLS can prevent a rapid loss of usable space, it sustains fewer writes compared to WL-Reviver. The more uniform write distribution of the Ocean benchmark does not significantly benefit LLS due to its modification of the address randomization method, which restricts the spread of heavily written blocks. WL-Reviver, on the other hand, fully retains its randomization mapping, resulting in a longer lifetime.

### Table II: Average PCM Access Time and Software-Usable Space

| Failure Ratio | Avg. Access Time (mg) | Avg. Access Time (ocean) | Software-Usable Space (mg) | Software-Usable Space (ocean) |
|---------------|------------------------|--------------------------|-----------------------------|--------------------------------|
| 10%           | 1.001                  | 1.005                    | 84                          | 85                             |
| 20%           | 1.001                  | 1.004                    | 89                          | 89                             |
| 30%           | 1.001                  | 1.011                    | 73                          | 73                             |
| WL-Revival    | 1.003                  | 1.009                    | 79                          | 79                             |
| 10%           | 1.001                  | 1.020                    | 62                          | 63                             |
| 20%           | 1.004                  | 1.013                    | 68                          | 69                             |

**Description:**
Table II presents the average PCM access time for one software-issued request and the percentage of PCM capacity available for software use when different percentages of PCM space fail. WL-Reviver generally has a smaller average access time than LLS because it requires only two PCM accesses for data on a failed block, whereas LLS may need three. With a 32KB cache, both LLS and WL-Reviver achieve nearly optimal average access times. However, WL-Reviver makes almost all un-failed blocks usable, while LLS has less flexible and less efficient use of reserved space.

## V. Conclusion

We propose WL-Reviver, a framework that revives any PCM wear-leveling scheme that ceases to function once the first failed block appears. Given that wear leveling is an essential component for PCM devices, WL-Reviver efficiently hides failed blocks with shadow blocks, ensuring the mechanism remains operational. Designed as a framework without requiring adaptations of existing schemes, WL-Reviver minimally impacts PCM's average access time even with substantial failures. It leverages the well-accepted practice that the OS does not access a page after being notified of an access failure, thus requiring no additional OS support.

## VI. Acknowledgments

We thank the anonymous reviewers for their valuable comments and suggestions. This work is supported by the National Natural Science Foundation of China (Grant No. 61232003, 61327902, 60925006), the National High Technology Research and Development Program of China (Grant No. 2013AA013201), Shanghai Key Laboratory of Scalable Computing and Systems, Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology, Huawei Technologies Co. Ltd., and Tsinghua University Initiative Scientific Research Program.

## References

[1] Micron. "Memory Phase Change Package with Devices." Availability Mobile Announces for. July 2012.
[2] Samsung. "Multi-chip PRAM Chip Ships First for Handsets." April 2010.
[3] HP. "Technology Brief: Avoiding Server Downtime from Hardware Errors in System Memory with HP Memory Quarantine."
[4] Intel. "Pin - A Dynamic Binary Instrumentation Tool."
[5] Princeton University. "The PARSEC Benchmark Suite."
[6] NASA. "NAS Parallel Benchmarks (NPB)."
[7] University of Delaware. "SPLASH-2 Benchmarks Suite."
[8] R. Azevedo et al. "Zombie Memory: Extending Memory Lifetime by Reviving Dead Blocks." ISCA, June 2013.
[9] J. Condit et al. "Better I/O Through Byte-Addressable, Persistent Memory." SOSP, October 2009.
[10] ITRS. "Emerging Research Devices." 2011.
[11] E. Ipek et al. "Dynamically Replicated Memory: Building Reliable Systems from Nanoscale Resistive Memories." ASPLOS, 2010.
[12] L. Jiang et al. "Hardware-Assisted Cooperative Integration of Wear-Leveling and Salvaging for Phase Change Memory." ACM Transactions on Architecture and Code Optimization, Vol. 10 Issue 2, 2013.
[13] K. Kim. "Technology for sub-50nm DRAM and NAND flash manufacturing." International Electron Devices Meeting, 2005.
[14] B. Lee et al. "Architecting Phase-Change Memory as a Scalable DRAM Alternative." ISCA, June 2009.
[15] R. Melhem et al. "RDIS: A Recursively Defined Invertible Set Scheme to Tolerate Multiple Stuck-At Faults in Resistive Memory." DSN, June 2012.
[16] M. K. Qureshi et al. "Enhancing Lifetime and Security of Phase Change Memories via Start-Gap Wear Leveling." MICRO, 2009.
[17] M. K. Qureshi et al. "Scalable high performance main memory system using phase-change memory technology." ISCA, 2009.
[18] M. K. Qureshi. "Pay-As-You-Go: Low-Overhead Hard-Error Correction for Phase Change Memories." MICRO, December 2011.
[19] A. Seznec. "A Phase Change Memory as a Secure Main Memory." IEEE Computer Architecture Letters, Vol. 9, Issue 1, 2010.
[20] S. Schechter et al. "Use ECP, not ECC, for Hard Failures in Resistive Memories." ISCA, June 2010.
[21] N. H. Seong et al. "SAFER: Stuck-At-Fault Error Recovery for Memories." MICRO, 2010.
[22] N. H. Seong et al. "Security Refresh: Prevent Malicious Wear-out and Increase Durability for Phase-Change Memory with Dynamically Randomized Address Mapping." ISCA, 2010.
[23] D. H. Yoon et al. "FREE-p: Protecting Non-volatile Memory against both Hard and Soft Errors." HPCA, 2011.
[24] P. Zhou et al. "A Durable and Energy Efficient Main Memory Using Phase Change Memory Technology." ISCA, 2009.
[25] W. Zhang and T. Li. "Characterizing and mitigating the impact of process variations on phase change based memory systems." MICRO, 2009.