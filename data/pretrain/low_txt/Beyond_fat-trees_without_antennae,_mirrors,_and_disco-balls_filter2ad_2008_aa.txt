# Beyond Fat-Trees Without Antennae, Mirrors, and Disco-Balls

**Authors:**
- Simon Kassing, ETH Zürich
- Asaf Valadarsky, Hebrew University of Jerusalem
- Gal Shahaf, Hebrew University of Jerusalem
- Michael Schapira, Hebrew University of Jerusalem
- Ankit Singla, ETH Zürich

## Abstract
Recent studies have observed that large data center networks often exhibit a few communication hotspots while the majority of the network remains underutilized. Consequently, numerous data center network designs have explored the approach of identifying these hotspots in real-time and eliminating them by leveraging flexible optical or wireless connections to dynamically alter the network topology. These proposals are based on the premise that statically wired network topologies, which lack the opportunity for such online optimization, are fundamentally inefficient and must be built at uniform full capacity to handle unpredictably skewed traffic.

We show this assumption to be false. Our results establish that state-of-the-art static networks can achieve the performance benefits claimed by dynamic, reconfigurable designs of the same cost. For the skewed traffic workloads used to make the case for dynamic networks, the evaluated static networks can achieve performance matching full-bandwidth fat-trees at two-thirds of the cost. Surprisingly, this can be accomplished even without relying on any form of online optimization, including the optimization of routing configuration in response to traffic demands.

Our results substantially lower the barriers for improving today’s data centers by showing that a static, cabling-friendly topology built using commodity equipment yields superior performance when combined with well-understood routing methods.

## CCS Concepts
- **Networks → Data center networks**

## Keywords
- Data center
- Topology
- Routing

## ACM Reference Format
Simon Kassing, Asaf Valadarsky, Gal Shahaf, Michael Schapira, and Ankit Singla. 2017. Beyond fat-trees without antennae, mirrors, and disco-balls. In Proceedings of SIGCOMM ’17, Los Angeles, CA, USA, August 21–25, 2017, 14 pages. https://doi.org/10.1145/3098822.3098836

## Introduction
Virtually every popular web service today is supported by data center infrastructure. With the growth of these services, the supporting infrastructure has also scaled, with data centers being designed to accommodate as many as 100,000 servers [15]. For such large facilities, engineering full-bandwidth connectivity between all pairs of servers entails significant effort and expense. Furthermore, at any given time, only a fraction of servers may require high-bandwidth connectivity [13, 17], making such a design seem wasteful. Traditional topologies like the fat-tree [3] present network designers with only two choices: (a) build an expensive, rearrangeably non-blocking network, or (b) build a cheaper, oversubscribed network that does not provide high-bandwidth connectivity even to small (albeit arbitrary) subsets of the server pool, thereby necessitating aggressive workload management.

Over the past few years, numerous data center network designs have tackled this problem by adapting the topology itself to the traffic demands [10, 12–14, 17, 18, 24, 27, 35, 40]. The key insight in this literature is that if only a few network hotspots exist at any time, perhaps through online measurement, these hotspots can be identified, and the network topology can be optimized dynamically to alleviate them. Reconfigurable wireless and optical elements enable such online adjustments of connectivity. For many workloads, such a network can match the performance of a much more expensive interconnect that provides full bandwidth between all pairs of servers at all times. The ingenuity of the many proposals along these lines lies in the varied capabilities of the reconfigurable elements, how they are connected together (including movable wireless antennae [17], ceiling mirrors [18, 40], and disco-balls [13]), and the algorithms for online management of connectivity.

This broad approach has obvious, intuitive appeal, and indeed, some evaluations show performance similar to a full-bandwidth fat-tree at 25-40% lower cost for some workloads [13, 18]. Not only are such results impressive, but it is also unclear whether there are any other viable options besides full-bandwidth topologies and such dynamic topologies. Before fleshing out their reconfigurable optics-based architecture, Helios [12] summarized the (2010) state of data center network topology design as follows: "Unfortunately, given current data center network architectures, the only way to provision required bandwidth between dynamically changing sets of nodes is to build a non-blocking switch fabric at the scale of an entire data center, with potentially hundreds of thousands of ports."

More recently, the 3D-beamforming proposal [40] (2012) made a similar assessment, and FireFly [18] (2014) explicitly considered only two design possibilities: a full bisection-bandwidth network and topology dynamism. This literature has thus implied that static networks, which lack any opportunity for online topology optimization, are inherently inefficient for unpredictably skewed network traffic, and consequently, that dynamic topologies are the only alternative to expensive full-bandwidth fabrics.

The goals of our work are to critically examine this presumption, explore the utility and limits of topology dynamism, develop an understanding of the relative strength of static and dynamic networks, and outline directions for improving today’s data centers. Towards these goals, we make the following contributions:

1. **A Metric for Network Flexibility:** So far, there has been no established metric for the flexibility of topologies towards accommodating skewed traffic, with practitioners relying on specific workload instances for evaluations. We propose an intuitive metric capturing a topology’s performance under traffic skew.
   
2. **Comparison of Static and Dynamic Topologies:** We compare static and dynamic networks under the assumptions of optimal topology dynamics and traffic engineering. This comparison uses a linear program optimization of a fluid-flow model, thus abstracting out possible inefficiencies of routing, congestion control, and the traffic-estimation and optimization needed for dynamic networks. Our results show that at equal cost, recently proposed static networks outperform dynamic networks in this setting, with both achieving substantial performance benefits over fat-trees.

3. **Routing on Static Networks:** Translating performance in fluid-flow models to packet-level can be challenging, particularly with dynamic, unpredictable traffic. Surprisingly, we find that even simple, oblivious routing achieves high performance for the static networks we consider. Thus, even online optimization of routes is not essential for achieving efficiency substantially higher than fat-trees and comparable with dynamic networks. Over the same skewed workloads recently used to make the case for dynamic networks and heretofore considered challenging for static networks, static networks achieve performance gains over fat-trees similar to those claimed by dynamic networks at the same price point.

4. **Concrete, Deployable Alternatives to Today’s Data Centers:** Our results suggest that the least-resistance path beyond fat-trees may lie in transitioning to superior, cabling-friendly, static networks, such as Xpander [33], and that dynamic networks have not yet demonstrated an advantage over such static networks. We discuss the implications for future research on data center network design, especially for dynamic topologies, in §7.

To aid reproducibility of our results, help researchers and practitioners run further experiments, and provide an easy-to-use baseline for future research on dynamic networks to compare against, our simulation framework is available online [1].

## Network Flexibility
This section examines the instructive example of the inflexibility of oversubscribed fat-trees towards skewed traffic matrices (§2.1) and introduces a quantitative notion of the desired flexibility (§2.2).

### Figure 1: A k = 4 Fat-Tree
With more than 75% of the network’s capacity intact, if all 4 servers in one pod sent traffic to servers in another pod (thus involving 50% of all servers), each would get only 75% of the bandwidth.

### 2.1 Fat-Trees Are Inflexible
Oversubscribed fat-trees are fundamentally limited in their ability to support skewed traffic matrices. In the example shown in Fig. 1, the fat-tree is oversubscribed by removing one root switch. If each server in one pod is communicating with a (different) server in another pod, not all such connections can get full bandwidth. In this example, the network still has more than 75% of its original capacity, and yet cannot provide full bandwidth connectivity for a traffic matrix involving only 50% of the servers. This observation can be formalized as follows:

**Observation 1.** If a fat-tree built with k-port switches is oversubscribed to x fraction of its full capacity, then there exists a traffic matrix involving 2/k-fraction of the servers such that no more than x fraction of throughput per server is achievable.

**Proof.** The proof is constructive, i.e., we provide the specific traffic matrix which is bottlenecked to no more than x per-server throughput. If the oversubscription is at the top-of-rack (ToR) switch, with each ToR supporting s servers with only sx network ports, then trivially, any traffic matrix (with 2s servers) where each server under one ToR sends traffic to a unique server under another ToR, is bottlenecked at the sender’s ToR to x throughput. Note that nonuniform oversubscription only pushes throughput lower — in that case, some ToR uses even fewer connections to the network and achieves throughput lower than x.

Next, suppose that the aggregation layer (i.e., the middle layer of switches) is oversubscribed to an x fraction, i.e., each switch in this layer has a links connecting it to ToRs and ax links connecting it to the core layer (the upper layer of switches). Then, any traffic matrix where each server in a pod sends traffic to a unique server not within the same pod, is bottlenecked at the aggregation layer to x fraction of throughput. The same argument applies when the core layer is oversubscribed. In these cases, the number of servers in the difficult traffic matrices is two times the number of servers in a pod, i.e., 2/k of the servers in the network.

For a fat-tree built using 64-port switches, a 2/k fraction of the servers would be a mere 3%. If this fat-tree were built at, say 50% oversubscription, a pair of pods comprising only 3% of the network’s servers could still not achieve full throughput, even while the rest of the network idles. Only if 50% or less of the servers in each of the pods were involved, could such an oversubscribed fat-tree achieve full throughput for these servers. Of course, if it were known a priori which pods might need higher bandwidth, different pods may be oversubscribed differently, but this would be a strong assumption on the predictability and stability of traffic, and pod-pod traffic would need aggressive management in such scenarios. Other Clos-network-based designs suffer from similar problems.

Thus, as literature on topology adaptation rightly points out, oversubscribed fat-trees score low on any metric of network flexibility. But what precisely is the metric? So far, there has not been a clearly specified and easily replicable standard for evaluating the flexibility of a topology towards accommodating skewed traffic matrices. In the following, we offer a simple starting point.

### 2.2 Throughput Proportionality
We aim to build static networks that can move around their limited capacity to meet traffic demands in a targeted fashion. A network’s total capacity is fixed, and defined by the total link capacity of its edges, and for each server, the network expends this capacity on routing its flows. We may hope that as the number of servers participating in the traffic matrix (TM) decreases, we see a proportional increase in throughput. In the following, we describe a benchmark for network flexibility capturing this intuition.

**Modeling Throughput per Server:** Consider an arbitrary static network G with N servers. A traffic matrix M captures traffic demands between the N servers, with mij specifying the demand from server i to server j. We restrict traffic matrices to the hose model, i.e., the sum of demands from (to) each server is limited by the outgoing (incoming) capacity of its network interfaces.

G is said to support a TM M with the satisfaction matrix TM if each flow i → j in M achieves throughput TM(i, j) without violating link-capacity constraints. Throughput for server i is then ti = Σj TM(i, j). G is said to support M with throughput t if there is a satisfaction matrix where each server simultaneously achieves throughput at least t, i.e., ∃TM : ∀i ∈ [N] ti ≥ t. Defining throughput in this way, at a per-server granularity (rather than flow or server-pair granularity), allows us to capture the network's ability to handle skewed traffic effectively.