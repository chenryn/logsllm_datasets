### 4.3 Frequency of Use Does Not Predict Frequency of Crashes

Periodic retrieval of snapshots of processes running on users' computers would provide the best evaluation of application usage. However, due to stringent privacy concerns, it is difficult, if not impossible, to convince people to share such data. We conducted a survey targeting users whose machines generated crashes and received a high response rate (over 50%). The difficulty in objectively evaluating computer usage makes this data less accurate than automated monitoring. Nevertheless, the information highlights unusual occurrences in application crashes.

Figure 1 outlines the distribution of application usage. Comparing columns 3 and 4 of Figure 1, we observe that while web browsers cause a majority of crashes, they are not the most frequently used applications. We further analyzed web-browser crashes and usage by specific applications (see Figure 2). Internet Explorer is the most commonly used and the highest contributor to crashes among web browsers. Netscape and Firefox have approximately the same usage and crash rates. Mozilla also appears to be a popular browser but does not generate nearly as many crashes as other browsers. A possible explanation for Mozilla's robustness is its open-source nature, which benefits from user testing and evaluation.

On average, users reported more frequent usage of email and document preparation applications than web browsers, yet these applications caused a significant proportion of crashes. It is important to note that this data represents the Berkeley EECS department and not the entire Windows user population. Usage statistics underscore this fact, as code development and scientific computation are uncommon activities for most Windows users.

### 4.4 Applications Hang Frequently

Our data suggests that about half of the crashes are due to a user manually terminating an application, i.e., an application hang. Often, when an application does not respond in a timely manner, perhaps due to insufficient memory or an outdated .dll, users tend to terminate the process and retry. It is possible that such applications would eventually crash if the user avoided pre-termination during the "hang."

Figure 3 outlines the applications that commonly hang. Internet Explorer has the largest proportion of hangs; Netscape and Firefox are also among the top ten commonly hanging applications. A feasible explanation for this trend is that web browsers interact with numerous other applications such as Macromedia Flash, QuickTime, and Acrobat Reader. Perusing the contents of a website often requires downloading untrusted and unreliable code, leading to potential crashes. To resolve this problem, interaction must be restricted to trusted, safe plug-ins, avoiding potentially unsafe and malicious code.

In contrast, applications like MS Word, Outlook, and MATLAB can hang for different reasons, such as a corrupt file or insufficient computation memory. In some scenarios, a large file can cause problems at startup. A practical solution must reduce the workload or upgrade the software/machine.

### 4.5 .dll Files Are Not Robust Enough

Figure 4 lists the top ten .dll and executable files responsible for crashes. These components constitute a significant portion of non-application hang-induced crashes. A majority of problematic .dll files are invoked by multiple applications. Noteworthy examples include ntdll.dll and msvcrt.dll. In several scenarios, the same .dll can be blamed for a crash. For example, the caller of a .dll routine can pass invalid arguments to the callee, or a .dll’s callee routine can return a bad value. Moreover, a machine’s state can be corrupt at the time of .dll execution. Precise inter-.dll interface definition and sandboxing will help avoid the cascading effects of data corruption.

### 5. Future Directions: Open Source Data Collection via BOINC Crash Collector

To collect finer-grained usage information and study a broader population of Windows users, we have embarked on an effort to target public-resource computing volunteers. BOINC is a platform for pooling computer resources from volunteers to collect data and run distributed computations. A popular example of an application using this platform is SETI@home, which aggregates computing power to search for extraterrestrial intelligence. BOINC provides services to send and receive data from its users to a BOINC server via the HTTP protocol using XML formatted files. Each subscribed user’s machine, when idle, runs BOINC applications.

Taking advantage of these efforts, we have created a data collection application to run on this platform. BOINC offers a good opportunity to collect and aggregate data from users outside our department while addressing privacy concerns. We currently scrape crash dumps and, with user consent, usage information such as hardware/software profiles from users’ machines and send corresponding data to our BOINC server. The drawback of this mechanism is that we can only collect crash dumps stored in known locations on the user’s computer, excluding application crash dumps stored in unknown app-specific locations. Numerous people enthusiastically contribute data to projects on BOINC rather than to corporations, as they favor a research cause. Additionally, users appreciate the recognition as pioneering contributors to the project through statistics comparing their machine to an average BOINC user’s machine.

### 6. Conclusion

Our crash-data-related study has contributed several revelations about Windows. The most notable finding is that the Windows operating system is not responsible for a majority of PC crashes at Berkeley. Application software, especially browsers, is mostly responsible for these crashes. Users can alleviate computer frustration by better usage discipline and avoiding unsafe applications. With additional data collection and mining, we hope to make stronger claims about applications and extract safe product design and usage methodology that apply universally to all operating systems. Eventually, this research can gauge product and usage evolution.

Studying failure data is as important to the computing industry as it is to consumers. Product dependability evaluations, such as reports provided by J.D. Power and Associates, help evolve the industry by reducing quality differences between various products. Once product reliability data is publicized, users will use such information to guide their purchasing decisions and usage patterns. Product developers will react defensively, and resulting competition will improve quality control.

### References

[1] D. Anderson, “Public Computing: Reconnecting People to Science,” The Conference on Shared Knowledge and the Web, Residencia de Estudiantes, Madrid, Spain, Nov. 2003.
[2] A. Brown, L. Chung, and D. Patterson. “Including the Human Factor in Dependability Benchmarks,” In Proc. 2002 DSN Workshop on Dependability Benchmarking, Washington, D.C., June 2002.
[3] A. Brown and M. Seltzer. “Operating System Benchmarking in the Wake of Lmbench: A Case Study of the Performance of NetBSD on the Intel x86 Architecture,” In Proc. 1997 ACM SIGMETRICS Conference on the Measurement and Modeling of Computer Systems, Seattle, WA, June 1997.
[4] J. Forrester, B. Miller, “An Empirical Study of the Robustness of Windows NT Applications Using Random Testing,” In Proc. 4th USENIX Windows System Symposium, Seattle, WA, Aug. 2000.
[5] A. Ganapathi, Y. Wang, N. Lao and J. Wen. “Why PCs are Fragile and What We Can Do About It: A Study of Windows Registry Problems,” In Proc. International Conference on Dependable Systems and Networks (DSN-2004), Florence, Italy, June 2004.
[6] J. Gray. “Why Do Computers Stop and What Can Be Done About It?” Symp on Reliability in Distributed Software and Database Systems, pp 3–12, 1986.
[7] A. Kalakech, K. Kanoun, Y. Crouzet, J. Arlat, “Benchmarking the dependability of Windows NT4, 2000 and XP,” In Proc. International Conference on Dependable Systems and Networks (DSN-2004), Florence, Italy, June 2004.
[8] M. Kalyanakrishnam, “Analysis of Failures in Windows NT Systems,” Masters Thesis, Technical report CRHC 98-08, University of Illinois at Urbana-Champaign, 1998.
[9] P. Koopman, J. DeVale, “The Exception Handling Effectiveness of POSIX Operating Systems,” IEEE Trans. on Software Engineering, Vol 26, No 9, pp 837-848 Sept. 2000.
[10] I. Lee and R. Iyer, “Software Dependability in the Tandem GUARDIAN Operating System,” IEEE Trans. on Software Engineering, Vol 21, No 5, pp 455-467, May 1995.
[11] Y. Levendel, “Defects and Reliability Analysis of Large Software Systems: Field Experience,” Digest 19th Fault-Tolerant Computing Symposium, pp 238-243, June 1989.
[12] B. Murphy, “Automating Software Failure Reporting,” ACM Queue Vol 2, No 8, Nov. 2004.
[13] D. Oppenheimer, A. Brown, J. Traupman, P. Broadwell, and D. Patterson. “Practical issues in dependability benchmarking,” Workshop on Evaluating and Architecting System dependabilitY (EASY ’02), San Jose, CA, Oct. 2002.
[14] C. Shelton, P. Koopman, K. DeVale, “Robustness Testing of the Microsoft Win32 API,” In Proc. International Conference on Dependable Systems and Networks (DSN-2000), New York, June 2000.
[15] C. Simache, M. Kaaniche, A. Saidane, “Event log based dependability analysis of Windows NT and 2K systems,” In Proc. 2002 Pacific Rim International Symposium on Dependable Computing (PRDC'02), pp 311-315, Tsukuba, Japan, Dec. 2002.
[16] D. Tang and R. Iyer, “Analysis of the VAX/VMS Error Logs in Multicomputer Environments – A Case Study of Software Dependability,” International Symposium on Software Reliability Engineering, Research Triangle Park, North Carolina, Oct 1992.
[17] A. Thakur, R. Iyer, L. Young, I. Lee, “Analysis of Failures in the Tandem NonStop-UX Operating System,” International Symposium on Software Reliability Engineering, Oct 1995.
[18] D. Wilson, B. Murphy, L. Spainhower, “Progress on Defining Standardized Classes for Comparing the Dependability of Computer Systems,” In Proc. DSN 2002 Workshop on Dependability Benchmarking, Washington, D.C., June 2002.