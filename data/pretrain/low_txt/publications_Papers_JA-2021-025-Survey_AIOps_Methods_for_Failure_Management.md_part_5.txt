### Reinforcement Learning and Q-Learning for Optimal Checkpointing

Reinforcement learning, particularly Q-learning, is employed to determine the optimal checkpointing strategy by analyzing the asymptotic behavior of policies. This approach is especially valuable in scenarios with unknown failure characteristics.

Moody et al. [95] describe a multi-level checkpointing system designed for large high-performance computing (HPC) systems. To estimate execution times under different configurations, the multi-level system is modeled using a joint Stochastic Markov Model. This model includes definitions for computation and recovery states and incorporates assumptions such as the cost of checkpointing and the system's failure rate. The resulting checkpointing schemes are claimed to achieve high recovery efficiencies (85%) even at a 50× increased failure rate.

Jangjaimon et al. [62] present an incremental checkpointing strategy for multi-threaded applications in cloud environments. Their system uses an Adjusted Markov Model to predict program execution turnaround times, accounting for both hardware failures and potential resource revocation events. The model also considers monetary aspects, including costs and unavailability. Experimental results indicate that an adaptive multi-level checkpointing scheme is beneficial in cloud paradigms with resource revocation, reducing checkpoint sizes, shortening execution times (up to −25%), and lowering costs (up to −20%).

### Online Failure Prediction

The fundamental idea behind failure avoidance is the anticipation of errors. Predicting failures is therefore essential for proactive management. The previous section illustrated how estimating the distribution of future failures is valuable for preemptive action. However, previous prediction methods were closely tied to specific prevention tasks and dependent on available preventive actions (e.g., software rejuvenation). Additionally, most predictions operated using offline-derived assumptions, estimating the probability of failures rather than the time to the next failure. On-the-fly failure prediction allows awareness of future failures and provides advance notice for timely deployment of recovery and failover mechanisms.

This section discusses the second category of proactive approaches: online failure prediction, which identifies future runtime errors by assessing the current system state [122]. The lead time of the predictor, the time between the prediction and the actual failure, and the prediction time, the length of the time window where the failure may occur, determine how far into the future these errors can be predicted. We also introduce warning time, a metric describing the time required to perform inference and signal a future failure. A prediction is useful only if it warns operators sufficiently in advance of the failure (i.e., \( t_{\text{warn}} < t_{\text{lead}} \)). From a functional perspective, an online failure predictor is comparable to a detector and is evaluated in terms of accuracy, precision, recall, and other associated measures. We discuss online failure prediction approaches in two subcategories: hardware failure prediction and system failure prediction.

#### 4.2.1 Hardware Failure Prediction

In large-scale computing infrastructures, hardware reliability is crucial for achieving service availability goals in distributed services. Due to the magnifying effect of large numbers and the commercial necessity to deploy commodity components in data centers, hardware represents the most vulnerable aspect from a failure perspective. According to Vishwanath et al. [138], 8 out of 100 servers are expected to encounter at least one hardware error per year of operation. Moreover, machines affected by errors often require more than one repair per year (with an average of two repairs), showing a successive correlation pattern between failures. Hardware repairs for a 100k-scale data center can amount to millions of dollars. Therefore, it is crucial from an industrial perspective to investigate factors influencing hardware faults to improve design choices and deploy failure predictors.

Hard drives are the most frequently replaced components in large cloud computing systems (78%) and a dominant reason for server failures [138]. Machines with a higher number of disks are more prone to additional faults within a fixed time period. This has led hard drive manufacturers to adopt self-monitoring technologies, such as Self-Monitoring, Analysis, and Reporting Technology (SMART), since 1994 [100]. Consequently, hard drive failure prediction is a well-investigated target.

Several studies have investigated the failure characteristics of hard drives to identify common patterns. For instance, Pinheiro et al. [118] conducted a large-scale study on over a hundred thousand disk drives used in production by Google, varying in storage size, speed, and manufacturer. The study could not identify any consistent correlation between failure rates and high temperatures or high utilization levels. However, some SMART features correlated well with higher failure rates. SMART metrics alone, however, were insufficient for single-disk predictions, as most failed drives did not manifest SMART errors before faults. A predictor based solely on SMART attributes is likely to have good specificity but low sensitivity unless additional features are introduced. SMART data is still useful for evaluating reliability and risk trends within a disk drive population.

Vishwanath et al. [138] investigated different attributes from a population of disks in Microsoft servers. By constructing decision trees for failure prediction, they identified two discrimination factors: data center and manufacturer. Other factors, such as workload type, position in rack, server age, and configuration, were found to be insignificant for predicting future drive faults.

Hamerly et al. [54] presented two different Bayesian learning approaches for disk failure prediction using SMART attributes collected from a real-world dataset of 1936 drives. The first method works as an unsupervised anomaly detector, training a mixture of Naïve Bayes discrete submodels to estimate the posterior probability of observations, and using a hard-set threshold to separate anomalous events from normal behavior. The second method consists of a supervised Naïve Bayes classifier with binned continuous variables. The approach achieves a recall of 0.33 with a false-positive rate of 0.0067. Both methods predict failures at the snapshot level, without considering temporal evolution.

Zhao et al. [164] treated SMART data as a time series, emphasizing the importance of temporal information. Their approach employs Hidden Markov Models (HMMs) and Semi-Markov Models (HSMMs) to estimate the sequence of likely events from disk metric observations, obtained from a dataset of approximately 300 disks (two-thirds healthy). One model is trained from healthy disk sequences, and another from faulty disk sequences. At test time, the two models are used to estimate the sequence log-likelihood, and the corresponding class is selected based on the highest score. By combining the HMM approach with an SVM, they claim to obtain a recall of 0.52 at a 0 false-alarm rate.

Murray et al. [101] tested the applicability of several machine learning methods using a sliding window approach, where the last n samples constitute the observation for predicting an imminent failure. Naïve Bayes, kernel SVMs, and Naïve Bayes Expectation-Maximization, as proposed by Reference [54], were implemented and compared. Features were selected from SMART data using statistical relevance tests. SVMs achieved their highest performance with a 50.6% recall and 100% precision. The approach was tested on a dataset composed of 369 drives (approximately 50/50 split), which was also used in a work by Wang et al. [143] where an online, similarity-based detection algorithm is presented. Relevant SMART features were selected via Minimum Redundancy Maximum Relevance (mRMR), and the input data was projected into a Mahalanobis space constructed from the healthy disk population, so that faulty disks deviate more from the distribution. Faulty disks were recognized with a sliding window approach, raising an alarm when the mean deviation inside a window appeared anomalous. An anomaly was identified with four different statistical tests. This approach improved the detection rate up to 67% at a 0 false-positive rate, with less computational effort. Moreover, it was shown that for 56% of the faulty cases, it was possible to intervene with an advance of at least 20 hours before the failure.

Comparable results were obtained by Zhu et al. [167], who constructed and trained multilayer perceptron and SVM models on an in-house (Baidu) dataset of SMART data comprising 23,395 drives (433 faulty). Assuming a 12-hour recovery window, their SVM model obtained a failure detection rate of 68.5% with a 0.03% false-alarm rate. The neural network method achieved much higher detection rates (94.62–100%) but at the expense of a higher false-alarm rate (0.48–2.26%). It is therefore indicated for monitoring with the highest reliability requirements.

Xu et al. [149] introduced Recurrent Neural Networks (RNNs) to hard drive failure prediction. Similar to Reference [164], the method can analyze sequences directly and model long-term relationships, taking advantage of the temporal dimension of the problem. Unlike previous approaches that applied binary classification, the model is trained to predict the health status of the disk, providing additional information on the remaining useful life of disks. On the failure prediction task, the approach still outperformed other evaluated models in terms of detection rate (96.08–97.78%) and false-alarm rate (0.004–0.03%).

In a related work [78], two new evaluation metrics (migration and mismigration rate) were introduced to measure the efficiency of data migration concerning forecasted faults. RNNs and Gradient-boosted Decision Trees (GBRT) were implemented to accomplish three tasks: predict faulty disks, measure the rate of wrongful and missed migrations using the classifier's information, and estimate the residual life of the disks by predicting the risk level of each disk (1 to 5 for faulty disks, 6 for healthy ones). For residual life prediction, results were compared using the newly defined metrics at variable migration rates. The RNN approach showed higher faulty-level prediction accuracy (27.02–39.90%), and the GBRT model provided better protection from data loss with a higher successful migration rate (84.91–87.54%).

Mahdisoltani et al. [89] tackled failure prediction in storage media at the sector level. Their method employed a few SMART features as target prediction variables rather than explanatory variables. They experimented with both HDD and SSD data using five different machine learning approaches. For HDD data, the analysis illustrated detection rates of sector errors similar to traditional disk failure prediction (70–90% at 2% false-positive rate). In SSDs, where random forests obtained the best comparative results, the prediction was less promising (50–60% detection rate at 2%). SSD failure characterization was also the focus of Narayanan et al.'s work [104], a large-scale study conducted on 2.5 years of production data covering over 500,000 solid-state disks from different Microsoft data centers. SSD failures were correlated with data center, workload, and device-level features via statistical learning. The best and only reported prediction model (random forest) obtained a precision of 87% and a recall of 71%. Features directly representing underlying problems (such as count of data errors and reallocation sectors), number of NAND writes, and workload factors were reported as the most important discriminators of failures. According to the same authors, the classification rules obtained by the analysis enable the identification of likely-to-fail devices with sufficient advance to take preventive actions, especially in the case of issues heavily dependent on the workload.

All previous approaches are used in an online setting after an initial setup or training step is performed offline. This poses the problem of how to integrate additional data, especially in cases where the scarcity of positive training examples imposes an online learning approach, able to update the characteristics of faulty disks as soon as new failures appear. To this end, Xiao et al. [148] proposed the use of Online Random Forests, a model able to evolve and adapt to changes in data distribution via online labeling. The approach was tested on a dataset covering over 10,000 disks, where the results showed that the algorithm could increase performance over 20 months, reaching detection rates of 93% and more with reasonably low alarm rates (0.73–0.76% in the offline setting). The prediction performances were comparable to a Random Forest and outperformed other tested approaches (SVMs and decision trees).

Although most scientific interest is concentrated around disk failures, a minor group of contributions focuses on failure prediction for other components. Ma et al. [88] elevated the discussion on storage reliability to the level of RAID groups. They modeled the failure probability of a vulnerable RAID group using a Naïve Bayes assumption, where the failure of whole disks inside the group is independent of others and estimated from metrics such as reallocated sector counts. A statistical model was built from data collected on 5,000 RAID groups. Results showed that the present in-place mechanisms were able to prevent a vast majority of consecutive failures (98% for triple failures).

Costa et al. [33] investigated the occurrence of main-memory errors for HPC applications. They proposed and evaluated methods for temporal and spatial correlation among memory failures. Temporal correlation analyzes logs to estimate the prior rate of errors, integrated with timing information, allowing the estimation of the remaining number of errors at the job level. Spatial correlation measures the probability of experiencing a failure after observing errors in adjacent bits. In the same paper, a memory migration approach based on available runtime information was presented and evaluated. The results showed that 63% of memory-induced failures could be avoided thanks to the prediction and migration mechanisms deployed.

Davis et al. [37] presented FailureSim, a Cloudsim-based [51] simulator for status assessment of hardware in cloud data centers using deep learning. Multilayer Perceptrons and RNNs were used to assess 13 different host failing states, each associated with a specific component (CPU, memory, I/O, etc.). The system, tested by assigning a variable workload to a scalable number of VMs, could accurately identify 50% of failing hosts and predict 89% of future failures before their occurrence.

Zhang et al. [161] dealt with the problems of failure prediction and diagnosis in network switches. Their method, based on system log history, proposed extracting templates from logs and correlating them with faulty behavior. Their extraction method and similar approaches were compared on the extraction tasks, and the templates obtained were used to train a Hidden semi-Markov Model.