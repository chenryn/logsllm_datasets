# 10. References

[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center network architecture. In SIGCOMM, pages 63–74, 2008.

[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: Dynamic flow scheduling for data center networks. In Proceedings of NSDI 2010, San Jose, CA, USA, April 2010.

[3] T. Benson, A. Anand, A. Akella, and M. Zhang. Understanding Data Center Traffic Characteristics. In Proceedings of Sigcomm Workshop: Research on Enterprise Networks, 2009.

[4] T. Benson, A. Anand, A. Akella, and M. Zhang. The case for fine-grained traffic engineering in data centers. In Proceedings of INM/WREN '10, San Jose, CA, USA, April 2010.

[5] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown, and S. Shenker. Ethane: Taking control of the enterprise. In SIGCOMM, 2007.

[6] J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. In Communications of the ACM, volume 51, pages 107–113, New York, NY, USA, 2008. ACM.

[7] A. B. Downey. Evidence for long-tailed distributions in the Internet. In Proceedings of ACM SIGCOMM Internet Measurement Workshop, pages 229–241, 2001. ACM Press.

[8] M. Fomenkov, K. Keys, D. Moore, and K. Claffy. Longitudinal study of Internet traffic in 1998-2003. In WISICT '04: Proceedings of the Winter International Symposium on Information and Communication Technologies, pages 1–6, Trinity College Dublin, 2004.

[9] H. J. Fowler, W. E. Leland, and B. Bellcore. Local area network traffic characteristics, with implications for broadband network congestion management. IEEE Journal on Selected Areas in Communications, 9(11):1139–1149, 1991.

[10] C. Fraleigh, S. Moon, B. Lyles, C. Cotton, M. Khan, D. Moll, R. Rockell, T. Seely, and C. Diot. Packet-level traffic measurements from the Sprint IP backbone. IEEE Network, 17(1):6–16, 2003.

[11] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A scalable and flexible data center network. In SIGCOMM, 2009.

[12] A. Greenberg, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. Towards a next generation data center architecture: Scalability and commoditization. In PRESTO '08: Proceedings of the ACM workshop on Programmable routers for extensible services of tomorrow, pages 57–62, New York, NY, USA, 2008. ACM.

[13] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and S. Lu. BCube: A high performance, server-centric network architecture for modular data centers. In Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication, Barcelona, Spain, August 17-21, 2009.

[14] B. Heller, S. Seetharaman, P. Mahadevan, Y. Yiakoumis, P. Sharma, S. Banerjee, and N. McKeown. ElasticTree: Saving energy in data center networks. In Proceedings of NSDI 2010, San Jose, CA, USA, April 2010.

[15] NOX: An OpenFlow Controller. http://noxrepo.org/wp/.

[16] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu. DCell: A scalable and fault-tolerant network structure for data centers. In SIGCOMM '08: Proceedings of the ACM SIGCOMM 2008 conference on Data communication, pages 75–86, New York, NY, USA, 2008. ACM.

[17] W. John and S. Tafvelin. Analysis of Internet backbone traffic and header anomalies observed. In IMC '07: Proceedings of the 7th ACM SIGCOMM conference on Internet measurement, pages 111–116, New York, NY, USA, 2007. ACM.

[18] S. Kandula, J. Padhye, and P. Bahl. Flyways to de-congest data center networks. In Proc. ACM Hotnets-VIII, New York City, NY, USA, Oct. 2009.

[19] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and R. Chaiken. The Nature of Data Center Traffic: Measurements and Analysis. In IMC, 2009.

[20] W. E. Leland, M. S. Taqqu, W. Willinger, and D. V. Wilson. On the self-similar nature of Ethernet traffic. In SIGCOMM '93: Conference proceedings on Communications architectures, protocols and applications, pages 183–193, New York, NY, USA, 1993. ACM.

[21] J. Mudigonda, P. Yalagandula, M. Al-Fares, and J. C. Mogul. SPAIN: COTS data-center Ethernet for multipathing over arbitrary topologies. In Proceedings of NSDI 2010, San Jose, CA, USA, April 2010.

[22] R. Niranjan Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat. Portland: A scalable fault-tolerant layer 2 data center network fabric. In SIGCOMM, 2009.

[23] The OpenFlow Switch Consortium. http://www.openflowswitch.org/.

[24] V. Paxson. Empirically-Derived Analytic Models of Wide-Area TCP Connections. IEEE/ACM Transactions on Networking, 2(4):316–336, Aug. 1994.

[25] V. Paxson. Measurements and analysis of end-to-end Internet dynamics. Technical report, 1997.

[26] V. Paxson. Bro: A system for detecting network intruders in real-time. In SSYM'98: Proceedings of the 7th conference on USENIX Security Symposium, pages 3–3, Berkeley, CA, USA, 1998. USENIX Association.

[27] V. Paxson and S. Floyd. Wide area traffic: The failure of Poisson modeling. IEEE/ACM Transactions on Networking, 3(3):226–244, 1995.

[28] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker. Applying NOX to the datacenter. In Proc. of workshop on Hot Topics in Networks (HotNets-VIII), 2009.

[29] G. Wang, D. G. Andersen, M. Kaminsky, M. Kozuch, T. S. E. Ng, K. Papagiannaki, M. Glick, and L. Mummert. Your data center is a router: The case for reconfigurable optical circuit switched paths. In Proc. ACM Hotnets-VIII, New York City, NY, USA, Oct. 2009.

---

# 7. Implications for Data Center Design

## 7.1 Role of Bisection Bandwidth

Several proposals [1, 22, 11, 2] for new data center network architectures aim to maximize the network bisection bandwidth. These approaches are well-suited for data centers running applications that stress the network fabric with all-to-all traffic. However, they may be unnecessary in data centers where the bisection bandwidth is not heavily utilized by the applications. In this section, we re-evaluate the SNMP and topology data captured from 10 data centers to determine if the prevalent traffic patterns are likely to stress the existing bisection bandwidth. We also examine how much of the existing bisection bandwidth is needed at any given time to support these traffic patterns.

Before delving into the analysis, we provide some definitions. For a tiered data center, the bisection links are the set of links at the top-most tier of the data center's tree architecture; these core links make up the bisection links. The bisection capacity is the aggregate capacity of these links. The full bisection capacity is the capacity required to support servers communicating at full link speeds with arbitrary traffic matrices and no oversubscription. The full bisection capacity can be computed as the aggregate capacity of the server NICs.

To address the questions posed earlier, we use SNMP data to compute the following:
1. The ratio of the current aggregate server-generated traffic to the current bisection capacity.
2. The ratio of the current traffic to the full bisection capacity.

We assume that the bisection links can be treated as a single pool of capacity from which all offered traffic can draw. While this may not be true in all current networks, it allows us to determine whether more capacity is needed or if better use of existing capacity is required (e.g., by improving routing, topology, or the migration of application servers within the data center).

In Figure 16, we present these two ratios for each of the data centers studied. Recall from Table 2 that all data centers are oversubscribed, meaning that if all servers sent data as fast as possible and all traffic left the racks, the bisection links would be fully congested (utilization ratios over 100%). However, Figure 16 shows that the prevalent traffic patterns are such that, even in the worst case where all server-generated traffic is assumed to leave the rack hosting the server, the aggregate output from servers is smaller than the network’s current bisection capacity. This means that even if the applications were moved around and the traffic matrix changed, the current bisection would still be more than sufficient, and no more than 25% of it would be utilized across all data centers, including the MapReduce data centers. Finally, we note that the aggregate output from servers is a negligible fraction of the ideal bisection capacity in all cases. This implies that if these data centers were equipped with a network that provides full bisection bandwidth, at least 95% of this capacity would go unused and be wasted by today’s traffic patterns.

Thus, the prevalent traffic patterns in the data centers can be supported by the existing bisection capacity, even if applications were placed in such a way that there was more inter-rack traffic than exists today. This analysis assumes that the aggregate capacity of the bisection links forms a shared resource pool from which all offered traffic can draw. If the topology prevents some offered traffic from reaching some links, then some links can experience high utilization while others see low utilization. Even in this situation, however, the issue is one of changing the topology and selecting a routing algorithm that allows offered traffic to draw effectively from the existing capacity, rather than a question of adding more capacity. Centralized routing, discussed next, could help in constructing the requisite network paths.

## 7.2 Centralized Controllers in Data Centers

The architectures for several proposals [1, 22, 12, 2, 14, 21, 4, 18, 29] rely in some form or another on a centralized controller for configuring routes or disseminating routing information to end hosts. A centralized controller is only practical if it can scale up to meet the demands of the traffic characteristics within the data centers. In this section, we examine this issue in the context of the flow properties analyzed in Section 5.

In particular, we focus on the proposals (Hedera [2], MicroTE [4], and ElasticTree [14]) that rely on OpenFlow and NOX [15, 23]. In an OpenFlow architecture, the first packet of a flow, when encountered at a switch, can be forwarded to a central controller that determines the route the packet should follow to meet some network-wide objective. Alternatively, to eliminate the setup delay, the central controller can precompute a set of network paths that meet network-wide objectives and install them into the network at startup time.

Our empirical observations in Section 5 have important implications for such centralized approaches. First, the fact that the number of active flows is small (see Figure 4(a)) implies that switches enabled with OpenFlow can make do with a small flow table, which is a constrained resource on switches today.

Second, flow inter-arrival times have important implications for the scalability of the controller. As we observed in Section 5, a significant number of new flows (2–20%) can arrive at a given switch within 10µs of each other. The switch must forward the first packets of these flows to the controller for processing. Even if the data center has as few as 100 edge switches, in the worst case, a controller can see 10 new flows per µs or 10 million flows per second. Depending on the complexity of the objective implemented at the controller, computing a route for each of these flows could be expensive. For example, prior work [5] showed a commodity machine computing a simple shortest path for only 50K flow arrivals per second. Thus, to scale the throughput of a centralized control framework while supporting complex routing objectives, we must employ parallelism (i.e., use multiple CPUs per controller and multiple controllers) and/or use faster but less optimal heuristics to compute routes. Prior work [28] has shown, through parallelism, the ability of a central controller to scale to 20 million flows per second.

Finally, the flow duration and size also have implications for the centralized controller. The lengths of flows determine the relative impact of the latency imposed by a controller on a new flow. Recall that we found that most flows last less than 100ms. Prior work [5] showed that reactive controllers, which make decisions at flow startup time, take approximately 10ms to install flow entries for new flows. Given our results, this imposes a 10% delay overhead on most flows. Additional processing delay may be acceptable for some traffic but might be unacceptable for other kinds. For the class of workloads that find such a delay unacceptable, OpenFlow provides a proactive mechanism that allows the controllers, at switch startup time, to install flow entries in the switches. This proactive mechanism eliminates the 10ms delay but limits the controller to proactive algorithms.

In summary, it appears that the number and inter-arrival time of data center flows can be handled by a sufficiently parallelized implementation of the centralized controller. However, the overhead of reactively computing flow placements is a reasonable fraction of the length of the typical flow.

# 8. Summary

In this paper, we conducted an empirical study of the network traffic of 10 data centers spanning three very different categories: university campus, private enterprise data centers, and cloud data centers running web services, customer-facing applications, and intensive Map-Reduce jobs. To the best of our knowledge, this is the broadest-ever large-scale measurement study of data centers.

We started our study by examining the applications run within the various data centers. We found that a variety of applications are deployed and that they are placed non-uniformly across racks. Next, we studied the transmission properties of the applications in terms of the flow and packet arrival processes at the edge switches. We discovered that the arrival process at the edge switches is ON/OFF in nature, where the ON/OFF durations can be characterized by heavy-tailed distributions. In analyzing the flows that constitute these arrival processes, we observed that flows within the data centers studied are generally small in size, and many of these flows last only a few milliseconds.

We studied the implications of the deployed data center applications and their transmission properties on the data center network and its links. We found that most of the server-generated traffic in cloud data centers stays within a rack, while the opposite is true for campus data centers. We found that at the edge and aggregation layers, link utilizations are fairly low and show little variation. In contrast, link utilizations at the core are high with significant variations over the course of a day. In some data centers, a small but significant fraction of core links appear to be persistently congested, but there is enough spare capacity in the core to alleviate congestion. We observed losses on the links that are lightly utilized on average and argued that these losses can be attributed to the bursty nature of the underlying applications run within the data centers.

On the whole, our empirical observations can help inform data center traffic engineering and QoS approaches, as well as recent techniques for managing other resources, such as data center network energy consumption. To further highlight the implications of our study, we re-examined recent data center proposals and architectures in light of our results. In particular, we determined that full bisection bandwidth is not essential for supporting current applications. We also highlighted practical issues in successfully employing centralized routing mechanisms in data centers.

Our empirical study is by no means all-encompassing. We recognize that there may be other data centers in the wild that may or may not share all the properties that we have observed. Our work points out that it is worth closely examining the different design and usage patterns, as there are important differences and commonalities.

# 9. Acknowledgments

We would like to thank the operators at the various universities, online service providers, and private enterprises for the time and data they provided us. We would also like to thank the anonymous reviewers for their insightful feedback.

This work is supported in part by an NSF FIND grant (CNS-0626889), an NSF CAREER Award (CNS-0746531), an NSF NetSE grant (CNS-0905134), and by grants from the University of Wisconsin-Madison Graduate School. Theophilus Benson is supported by an IBM PhD Fellowship.