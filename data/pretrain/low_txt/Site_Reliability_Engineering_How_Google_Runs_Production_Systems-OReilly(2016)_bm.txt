### Timeline 5
**Date: 2015-10-21 (All times UTC)**

- **14:51**: News reports that a new Shakespearean sonnet has been discovered in a DeLorean’s glove compartment.
- **14:53**: Traffic to the Shakespeare search engine increases by 88x after a post on /r/shakespeare directs users to the search engine. However, the new sonnet is not yet available.
- **14:54**: **OUTAGE BEGINS** - Search backends start failing under the increased load.
- **14:55**: `docbrown` receives a pager storm, with many HTTP 500 errors reported across all clusters.
- **14:57**: All traffic to the Shakespeare search engine is failing.
- **14:58**: `docbrown` starts investigating and finds that the backend crash rate is very high.
- **15:01**: **INCIDENT BEGINS** - `docbrown` declares incident #465 due to cascading failures. Coordination begins on the `#shakespeare` channel, and `jennifer` is named the incident commander.
- **15:02**: Someone coincidentally sends an email to `shakespeare-discuss@` regarding the sonnet discovery, which appears at the top of `martym`'s inbox.
- **15:03**: `jennifer` notifies the `shakespeare-incidents@` list about the ongoing incident.
- **15:04**: `martym` tracks down the text of the new sonnet and looks for documentation on corpus updates.
- **15:06**: `docbrown` discovers that the crash symptoms are identical across all tasks in all clusters and continues to investigate based on application logs.
- **15:07**: `martym` finds the necessary documentation and begins prep work for the corpus update.
- **15:10**: `martym` adds the new sonnet to Shakespeare’s known works and starts the indexing job.
- **15:12**: `docbrown` contacts `clarac` and `agoogler` from the Shakespeare development team to help examine the codebase for possible causes.
- **15:18**: `clarac` identifies the root cause in the logs, pointing to file descriptor exhaustion. This is confirmed against the code, indicating a leak if a term not in the corpus is searched for.
- **15:20**: `martym`’s index MapReduce job completes.
- **15:21**: `jennifer` and `docbrown` decide to increase the instance count to reduce the load on individual instances, allowing them to perform more work before failing and being restarted.
- **15:23**: `docbrown` load balances all traffic to the USA-2 cluster, allowing for an instance count increase in other clusters without immediate server failure.
- **15:25**: `martym` starts replicating the new index to all clusters.
- **15:28**: `docbrown` initiates a 2x instance count increase.
- **15:32**: `jennifer` changes the load balancing to increase traffic to non-sacrificial clusters.
- **15:33**: Tasks in non-sacrificial clusters start failing, showing the same symptoms as before.
- **15:34**: An order-of-magnitude error is found in the whiteboard calculations for the instance count increase.
- **15:36**: `jennifer` reverts the load balancing to resacrifice the USA-2 cluster in preparation for an additional global 5x instance count increase (to a total of 10x initial capacity).
- **15:36**: **OUTAGE MITIGATED** - The updated index is replicated to all clusters.
- **15:39**: `docbrown` starts the second wave of the instance count increase to 10x initial capacity.
- **15:41**: `jennifer` reinstates load balancing across all clusters for 1% of traffic.
- **15:43**: Non-sacrificial clusters’ HTTP 500 rates return to nominal levels, with task failures occurring intermittently at low levels.
- **15:45**: `jennifer` balances 10% of traffic across non-sacrificial clusters.
- **15:47**: Non-sacrificial clusters’ HTTP 500 rates remain within SLO, with no task failures observed.
- **15:50**: 30% of traffic is balanced across non-sacrificial clusters.
- **15:55**: 50% of traffic is balanced across non-sacrificial clusters.
- **16:00**: **OUTAGE ENDS** - All traffic is balanced across all clusters.
- **16:30**: **INCIDENT ENDS** - The exit criterion of 30 minutes of nominal performance is met.

### Supporting Information
- **Monitoring Dashboard**
- Useful information, links, logs, screenshots, graphs, IRC logs, IM logs, etc.

---

### Appendix E: Launch Coordination Checklist
**Architecture**
- Architecture sketch, types of servers, types of requests from clients
- Programmatic client requests

**Machines and Datacenters**
- Machines and bandwidth, datacenters, N+2 redundancy, network QoS
- New domain names, DNS load balancing

**Volume Estimates, Capacity, and Performance**
- HTTP traffic and bandwidth estimates, launch “spike,” traffic mix, 6 months out
- Load test, end-to-end test, capacity per datacenter at max latency
- Impact on other services we care most about
- Storage capacity

**System Reliability and Failover**
- What happens when:
  - Machine dies, rack fails, or cluster goes offline
  - Network fails between two datacenters
- For each type of server that talks to other servers (its backends):
  - How to detect when backends die, and what to do when they die
  - How to terminate or restart without affecting clients or users
  - Load balancing, rate-limiting, timeout, retry, and error handling behavior
- Data backup/restore, disaster recovery

**Monitoring and Server Management**
- Monitoring internal state, monitoring end-to-end behavior, managing alerts
- Monitoring the monitoring
- Financially important alerts and logs
- Tips for running servers within a cluster environment
- Don’t crash mail servers by sending yourself email alerts in your own server code

**Security**
- Security design review, security code audit, spam risk, authentication, SSL
- Prelaunch visibility/access control, various types of blacklists

**Automation and Manual Tasks**
- Methods and change control to update servers, data, and configs
- Release process, repeatable builds, canaries under live traffic, staged rollouts

**Growth Issues**
- Spare capacity, 10x growth, growth alerts
- Scalability bottlenecks, linear scaling, scaling with hardware, changes needed
- Caching, data sharding/resharding

**External Dependencies**
- Third-party systems, monitoring, networking, traffic volume, launch spikes
- Graceful degradation, how to avoid accidentally overrunning third-party services
- Playing nice with syndicated partners, mail systems, services within Google

**Schedule and Rollout Planning**
- Hard deadlines, external events, Mondays or Fridays
- Standard operating procedures for this service, for other services

---

### Appendix F: Example Production Meeting Minutes
**Date: 2015-10-23**

**Attendees:**
- `agoogler`
- `clarac`
- `docbrown`
- `jennifer`
- `martym`

**Announcements:**
- Major outage (#465), blew through error budget

**Previous Action Item Review:**
- Certify Goat Teleporter for use with cattle (bug 1011101)
  - Nonlinearities in mass acceleration now predictable, should be able to target accurately in a few days.

**Outage Review:**
- **New Sonnet (outage 465)**
  - 1.21B queries lost due to cascading failure after interaction between a latent bug (leaked file descriptor on searches with no results) + not having the new sonnet in the corpus + unprecedented and unexpected traffic volume.
  - File descriptor leak bug fixed (bug 5554825) and deployed to production.
  - Looking into using flux capacitor for load balancing (bug 5554823) and using load shedding (bug 5554826) to prevent recurrence.
  - Availability error budget was severely impacted; pushes to production are frozen for 1 month unless `docbrown` can obtain an exception on grounds that the event was bizarre and unforeseeable (but consensus is that an exception is unlikely).

**Paging Events:**
- [Details to be filled in]