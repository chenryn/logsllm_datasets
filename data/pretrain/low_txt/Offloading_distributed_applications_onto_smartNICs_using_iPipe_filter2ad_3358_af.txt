### References

1. **Daniel Firestone, Andrew Putnam, Sambhrama Mundkur, Derek Chiou, Alireza Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian Caulfield, Eric Chung, Harish Kumar Chandrappa, Somesh Chaturmohta, Matt Humphrey, Jack Lavier, Norman Lam, Fengfen Liu, Kalin Ovtcharov, Jitu Padhye, Gautham Popuri, Shachar Raindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Madhan Sivakumar, Nisheeth Srivastava, Anshuman Verma, Qasim Zuhair, Deepak Bansal, Doug Burger, Kushagra Vaid, David A. Maltz, and Albert Greenberg. 2018. Azure Accelerated Networking: SmartNICs in the Public Cloud. In 15th USENIX Symposium on Networked Systems Design and Implementation.**

2. **Alex Goldhammer and John Ayer Jr. 2008. Understanding Performance of PCI Express Systems. Xilinx WP350, Sept 4 (2008).**

3. **Troy D. Hanson. 2017. Uthash Hashtable. https://troydhanson.github.io/uthash/.**

4. **Carl Hewitt, Peter Bishop, and Richard Steiger. 1973. A Universal Modular ACTOR Formalism for Artificial Intelligence. In Proceedings of the 3rd International Joint Conference on Artificial Intelligence (IJCAI’73).**

5. **Huawei. 2018. Huawei IN550 SmartNIC. https://e.huawei.com/us/news/it/201810171443. (2018).**

6. **Xin Jin, Xiaozhou Li, Haoyu Zhang, Robert Soulé, Jeongkeun Lee, Nate Foster, Changhoon Kim, and Ion Stoica. 2017. NetCache: Balancing Key-Value Stores with Fast In-Network Caching. In Proceedings of the 26th Symposium on Operating Systems Principles.**

7. **Kostis Kaffes, Timothy Chong, Jack Tigar Humphries, Adam Belay, David Mazières, and Christos Kozyrakis. 2019. Shinjuku: Preemptive Scheduling for µsecond-scale Tail Latency. In 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19).**

8. **Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2014. Using RDMA Efficiently for Key-Value Services. In ACM SIGCOMM Computer Communication Review, Vol. 44. 295–306.**

9. **Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2016. Design Guidelines for High-Performance RDMA Systems. In 2016 USENIX Annual Technical Conference.**

10. **Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2016. FaSST: Fast, Scalable, and Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16).**

11. **Antoine Kaufmann, Simon Peter, Naveen Kr. Sharma, Thomas Anderson, and Arvind Krishnamurthy. 2016. High-Performance Packet Processing with FlexNIC. In Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems.**

12. **Daehyeok Kim, Amirsaman Memaripour, Anirudh Badam, Yibo Zhu, Hongqiang Harry Liu, Jitu Padhye, Shachar Raindel, Steven Swanson, Vyas Sekar, and Srinivasan Seshan. 2018. Hyperloop: Group-based NIC-offloading to Accelerate Replicated Transactions in Multi-tenant Storage Systems. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication.**

13. **Joongi Kim, Keon Jang, Keunhong Lee, Sangwook Ma, Junhyun Shim, and Sue Moon. 2015. NBA (Network Balancing Act): A High-Performance Packet Processing Framework for Heterogeneous Processors. In Proceedings of the Tenth European Conference on Computer Systems.**

14. **Eddie Kohler, Robert Morris, Benjie Chen, John Jannotti, and M. Frans Kaashoek. 2000. The Click Modular Router. ACM Transactions on Computer Systems (TOCS) 18, 3 (2000), 263–297.**

15. **Leslie Lamport. 2001. Paxos Made Simple. ACM Sigact News 32, 4 (2001), 18–25.**

16. **Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network Dataset Collection. http://snap.stanford.edu/data. (June 2014).**

17. **LevelDB. 2017. LevelDB Key-Value Store. http://leveldb.org. (2017).**

18. **Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu, Yongqiang Xiong, Andrew Putnam, Enhong Chen, and Lintao Zhang. 2017. KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC. In Proceedings of the 26th Symposium on Operating Systems Principles.**

19. **Bojie Li, Kun Tan, Layong Larry Luo, Yanqing Peng, Renqian Luo, Ningyi Xu, Yongqiang Xiong, Peng Cheng, and Enhong Chen. 2016. ClickNP: Highly Flexible and High-Performance Network Processing with Reconfigurable Hardware. In Proceedings of the 2016 ACM SIGCOMM Conference.**

20. **Hyeontaek Lim, Dongsu Han, David G. Andersen, and Michael Kaminsky. 2014. MICA: A Holistic Approach to Fast In-Memory Key-Value Storage. In Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation.**

21. **Jianxiao Liu, Zonglin Tian, Panbiao Liu, Jiawei Jiang, and Zhao Li. 2016. An Approach of Semantic Web Service Classification Based on Naive Bayes. In Services Computing (SCC), 2016 IEEE International Conference on.**

22. **Ming Liu, Liang Luo, Jacob Nelson, Luis Ceze, Arvind Krishnamurthy, and Kishore Atreya. 2017. IncBricks: Toward In-Network Computation with an In-Network Cache. In Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems.**

23. **Marvell. 2018. Marvell LiquidIO SmartNICs. https://www.marvell.com/documents/08icqisgkbtn6kstgzh4/. (2018).**

24. **Mellanox. 2018. Mellanox BlueField SmartNIC. http://www.mellanox.com/page/products_dyn?product_family=275&mtag=bluefield_smart_nic. (2018).**

25. **Mellanox. 2019. Accelerated Switch and Packet Processing (ASAP2). http://www.mellanox.com/page/asap2?mtag=asap2. (2019).**

26. **Christopher Mitchell, Yifeng Geng, and Jinyang Li. 2013. Using One-Sided RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store. In USENIX Annual Technical Conference.**

27. **Jacob Nelson, Brandon Holt, Brandon Myers, Preston Briggs, Luis Ceze, Simon Kahan, and Mark Oskin. 2015. Latency-Tolerant Software Distributed Shared Memory. In USENIX Annual Technical Conference.**

28. **Netronome. 2018. Netronome Agilio SmartNIC. https://www.netronome.com/products/agilio-cx/. (2018).**

29. **Rolf Neugebauer, Gianni Antichi, José Fernando Zazo, Yury Audzevich, Sergio López-Buedo, and Andrew W. Moore. 2018. Understanding PCIe Performance for End Host Networking. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication.**

30. **Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc Kwiatkowski, Herman Lee, Harry C. Li, Ryan McElroy, Mike Paleczny, Daniel Peek, Paul Saab, David Stafford, Tony Tung, and Venkateshwaran Venkataramani. 2013. Scaling Memcache at Facebook. In Presented as part of the 10th USENIX Symposium on Networked Systems Design and Implementation.**

31. **OFED. 2019. Infiniband Verbs Performance Tests. https://github.com/linux-rdma/perftest. (2019).**

32. **Amy Ousterhout, Joshua Fried, Jonathan Behrens, Adam Belay, and Hari Balakrishnan. 2019. Shenango: Achieving High CPU Efficiency for Latency-Sensitive Datacenter Workloads. In 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19).**

33. **Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno Rajahalme, Jesse Gross, Alex Wang, Joe Stringer, Pravin Shelar, Keith Amidon, and Martin Casado. 2015. The Design and Implementation of Open vSwitch. In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 15).**

34. **Phitchaya Mangpo Phothilimthana, Ming Liu, Antoine Kaufmann, Simon Peter, Rastislav Bodik, and Thomas Anderson. 2018. Floem: A Programming System for NIC-Accelerated Network Applications. In 13th USENIX Symposium on Operating Systems Design and Implementation.**

35. **George Prekas, Marios Kogias, and Edouard Bugnion. 2017. ZygOS: Achieving Low Tail Latency for Microsecond-Scale Networked Tasks. In Proceedings of the 26th Symposium on Operating Systems Principles.**

36. **Amedeo Sapio, Ibrahim Abdelaziz, Abdulla Aldilaijan, Marco Canini, and Panos Kalnis. 2017. In-Network Computation is a Dumb Idea Whose Time Has Come. In Proceedings of the 16th ACM Workshop on Hot Topics in Networks.**

37. **Linus Schrage. 1968. Letter to the Editor—A Proof of the Optimality of the Shortest Remaining Processing Time Discipline. Operations Research 16, 3 (1968), 687–690.**

38. **Naveen Kr. Sharma, Ming Liu, Kishore Atreya, and Arvind Krishnamurthy. 2018. Approximating Fair Queueing on Reconfigurable Switches. In 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18).**

39. **Madhavapeddi Shreedhar and George Varghese. 1996. Efficient Fair Queuing Using Deficit Round-Robin. IEEE/ACM Transactions on Networking 4, 3 (1996), 375–385.**

40. **Sriram Srinivasan and Alan Mycroft. 2008. Kilim: Isolation-Typed Actors for Java. In European Conference on Object-Oriented Programming.**

41. **James W. Stamos and Flaviu Cristian. 1993. Coordinator Log Transaction Execution Protocol. Distributed and Parallel Databases 1, 4 (1993), 383–408.**

42. **Alexander L. Stolyar and Kavita Ramanan. 2001. Largest Weighted Delay First Scheduling: Large Deviations and Optimality. Annals of Applied Probability (2001), 1–48.**

43. **SmartNIC Vendors. 2019. Marvell, Private Communications. Unpublished. (2019).**

44. **Xingda Wei, Jiaxin Shi, Yanzhe Chen, Rong Chen, and Haibo Chen. 2015. Fast In-Memory Transaction Processing Using RDMA and HTM. In Proceedings of the 25th Symposium on Operating Systems Principles.**

45. **Adam Wierman and Bert Zwart. 2012. Is Tail-Optimal Scheduling Possible? Operations Research 60, 5 (2012), 1249–1257.**

46. **Irene Zhang, Naveen Kr. Sharma, Adriana Szekeres, Arvind Krishnamurthy, and Dan R. K. Ports. 2015. Building Consistent Transactions with Inconsistent Replication. In Proceedings of the 25th Symposium on Operating Systems Principles.**

### iPipe FCFS Scheduler Algorithm

```python
while True:
    # On each FCFS core
    wqe = iPipe_nstack_rec()
    actor = iPipe_dispatcher(wqe)
    
    if actor.is_DRR:
        actor.mailbox_push(wqe)
        continue
    
    actor.actor_exe(wqe)
    actor.bookkeeping()
    
    if T_tail > Tail_thresh:
        actor.is_DRR = 1
        DRR_queue.push(actor)
    
    if T_mean > Mean_thresh:
        iPipe_actor_migrate(actor_chosen)
    
    if core_id == 0:
        if T_mean < (1 - alpha) * Mean_thresh:
            iPipe_actor_pull()
```

### iPipe Actor Migration Evaluation

When migrating an actor to the host, our runtime performs the following steps:

1. Collects all objects that belong to the actor.
2. Sends the object data to the host side using messages and DMA primitives.
3. Creates new objects on the host side and then inserts entries into the host-side object table.
4. Deletes related entries from the NIC-side object table upon deleting the actor.

The host-side DMO works similarly, except that it uses the glibc memory allocator.

We estimate the migration cost (SmartNIC-pushed) by breaking down the time elapsed in four phases. We choose 8 actors from three applications. Our experiments are conducted under 90% networking load, and we force the actor migration after a warm-up period of 5 seconds. Figure 18 presents our results. 

- **Phase 3** dominates the migration cost (i.e., 67.8% on average of 8 actors) since it requires moving the distributed objects to the host side. For example, the LSM memtable actor has around 32MB of objects and consumes 35.8ms.
- **Phase 4** ranks second (i.e., 27.2%) as it pushes buffered requests to the host. This phase varies based on the networking load.
- **Phase 1** and **Phase 2** are lightweight parts because they only introduce the iPipe runtime locking/unlocking and state manipulation overheads.

### Appendix A: SmartNIC Computing Unit Characterization

Table 3 summarizes the microarchitecture results for the LiquidIOII CN2350 multicore processor and accelerators.

### Appendix B: More Details in the iPipe Framework

#### B.1 iPipe Runtime APIs

Table 4 presents the major APIs. Specifically, the actor management APIs are used by our runtime. We provide five calls for managing DMOs. When creating an object on the NIC, iPipe first allocates a local memory region using the dlmalloc2 allocator and then inserts an entry (i.e., object ID, actor ID, start address, size) into the NIC object table. Upon `dmo_free`, iPipe frees the space allocated for the object and deletes the entry from the object table. `dmo_memset`, `dmo_memcpy`, and `dmo_memmove` resemble `memset`, `memcpy`, and `memmove` APIs in glibc, except that they use the object ID instead of a pointer.

For the networking stack, iPipe takes advantage of packet processing accelerators (if the SmartNIC has them) to build a customized networking stack for the SmartNIC. This stack performs simple Layer 2/Layer 3 protocol processing, such as packet encapsulating/decapsulation, checksum verification, etc. When building a packet, it uses the DMA scatter-gather technique to combine the header and payload if they are not colocated. This helps improve bandwidth utilization, as shown in our characterization (Section 2.2.5).

#### B.2 iPipe Actor Scheduling Algorithm

Algorithms 1 and 2 show the details of our iPipe hybrid scheduler.

#### B.3 iPipe Actor Migration Evaluation

When migrating an actor to the host, as shown in Figure 12, our runtime (1) collects all objects that belong to the actor; (2) sends the object data to the host side using messages and DMA primitives; (3) creates new objects on the host side and then inserts entries into the host-side object table; (4) deletes related entries from the NIC-side object table upon deleting the actor. The host-side DMO works similarly, except that it uses the glibc memory allocator.

We estimate the migration cost (SmartNIC-pushed) by breaking down the time elapsed in four phases. We choose 8 actors from three applications. Our experiments are conducted under 90% networking load, and we force the actor migration after a warm-up period of 5 seconds. Figure 18 presents our results. Phase 3 dominates the migration cost (i.e., 67.8% on average of 8 actors) since it requires moving the distributed objects to the host side. For example, the LSM memtable actor has around 32MB of objects and consumes 35.8ms. Phase 4 ranks second (i.e., 27.2%) as it pushes buffered requests to the host. This phase varies based on the networking load. Phase 1 and Phase 2 are lightweight parts because they only introduce the iPipe runtime locking/unlocking and state manipulation overheads.

### Figure 18: Migration Elapsed Time Breakdown

- **Applications**: Filter, Count, Rank, Coord, Parti, Consensus, LSM mem
- **Elapsed Time (ms)**: Phases 1, 2, 3, 4
- **Computation**
- **DS**
- **Exe. Lat. (us)**

This figure shows the breakdown of the migration elapsed time for 8 actors from three applications evaluated with 10GbE CN2350 cards.