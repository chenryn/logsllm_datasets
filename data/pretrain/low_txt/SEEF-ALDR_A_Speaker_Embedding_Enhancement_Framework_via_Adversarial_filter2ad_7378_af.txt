### Intelligent Monitoring Systems and Sound Event Detection

Intelligent monitoring systems, such as those used in healthcare and home security, can benefit significantly from sound event detection. For example, in the context of home security, sound event detection can analyze audio signals to detect potential break-ins, thereby compensating for the blind spots of surveillance cameras.

However, real-world audio signals often contain multiple simultaneous sound sources and are distorted by environmental noise. This makes it challenging to accurately and efficiently extract the relevant sound events from the rich audio information. Similar to speaker verification, existing works on sound event detection focus on optimizing machine learning models to improve detection performance rather than decoupling event-related and event-unrelated features [26, 29].

### SEEF-ALDR Framework for Sound Event Detection

The SEEF-ALDR (Speaker Embedding Enhancement Framework via Adversarial Learning based Disentangled Representation) framework can be extended to enhance the performance of sound event detection tasks. Specifically, the eliminating encoder can be adapted to remove event-related features through adversarial learning, while the purifying encoder retains most of the event-related information for classification. The decoder then reconstructs the spectrogram using outputs from both encoders, aiming to closely resemble the original input. We plan to extend SEEF-ALDR to the domain of sound event detection, making it a general approach for any task that benefits from feature decoupling of the original input.

### Novel Identity Impersonation Attack Based on SEEF-ALDR

Interestingly, SEEF-ALDR can also be used to create a novel attack against speaker identity verification through voice conversion. As discussed, the speaker identity feature from the purifying encoder determines the speaker's identity, while the identity-unrelated feature from the eliminating encoder contains other information.

### Conclusion

Speaker verification is widely used in various user authentication scenarios, making its reliability crucial for privacy and system security. In this paper, we propose SEEF-ALDR, a novel speaker embedding enhancement framework that uses adversarial learning to disentangle speaker identity features from identity-unrelated ones in the original speech. SEEF-ALDR is built on twin networks with an autoencoder-like architecture and an adversarial learning approach. One network, the speaker eliminating encoder, erases speaker identity features, while the other, the speaker purifying encoder, extracts these features. By combining adversarial and reconstruction supervision signals, our framework learns complementary feature representations, ensuring that the fusion of decoupled features closely matches the original input. Experimental results show that SEEF-ALDR can construct more accurate speaker embeddings, improving the performance of "in-the-wild" speaker verification with minimal effort.

### Acknowledgments

This work was partially supported by the National Key Research and Development Program of China (2016QY04W0903), Beijing Municipal Science and Technology Project (Z191100007119010), Strategic Priority Research Program of Chinese Academy of Sciences (No. XDC02010900), and the National Natural Science Foundation of China (NO.61772078). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.

### References

[1] Gautam Bhattacharya, Md Jahangir Alam, and Patrick Kenny. 2017. Deep Speaker Embeddings for Short-Duration Speaker Verification. In Interspeech. 1517–1521.
[2] Gautam Bhattacharya, Joao Monteiro, Jahangir Alam, and Patrick Kenny. 2019. Generative adversarial speaker embedding networks for domain robust end-to-end speaker verification. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6226–6230.
[3] Weicheng Cai, Jinkun Chen, and Ming Li. 2018. Analysis of length normalization in end-to-end speaker verification system. arXiv preprint arXiv:1806.03209 (2018).
[4] Weicheng Cai, Jinkun Chen, and Ming Li. 2018. Exploring the Encoding Layer and Loss Function in End-to-End Speaker and Language Recognition System. In Odyssey 2018 The Speaker and Language Recognition Workshop. 74–81.
[5] Si Chen, Kui Ren, Sixu Piao, Cong Wang, Qian Wang, Jian Weng, Lu Su, and Aziz Mohaisen. 2017. You Can Hear But You Cannot Steal: Defending Against Voice Impersonation Attacks on Smartphones. In 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS). 183–195.
[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018. VoxCeleb2: Deep Speaker Recognition. In Interspeech 2018. 1086–1090.
[7] J. Daugman. 2004. How iris recognition works. IEEE Transactions on Circuits and Systems for Video Technology 14, 1 (2004), 21–30.
[8] Peter French. 2013. An overview of forensic phonetics with particular reference to speaker identification. International Journal of Speech Language and The Law 1, 2 (2013), 169–181.
[9] Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang, et al. 2018. FD-GAN: Pose-guided feature distilling GAN for robust person re-identification. In Advances in Neural Information Processing Systems. 1222–1233.
[10] Erica Gold and Peter French. 2011. International practices in forensic speaker comparison. International Journal of Speech Language and The Law 18, 2 (2011), 293–307.
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Advances in Neural Information Processing Systems 27. 2672–2680.
[12] Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. 6645–6649.
[13] Craig S. Greenberg, Vincent M. Stanford, Alvin F. Martin, Meghana Yadagiri, George R. Doddington, John J. Godfrey, and Jaime Hernandez-Cordero. 2013. The 2012 NIST speaker recognition evaluation. In INTERSPEECH. 1971–1975.
[14] Mahdi Hajibabaei and Dengxin Dai. 2018. Unified Hypersphere Embedding for Speaker Recognition. arXiv preprint arXiv:1807.08312 (2018).
[15] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng. 2014. Deep Speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567 (2014).
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778.
[17] John R. Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe. 2016. Deep clustering: Discriminative embeddings for segmentation and separation. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 31–35.
[18] G. Hinton, Li Deng, Dong Yu, G. E. Dahl, A. Mohamed, N. Jaitly, Andrew Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. 2012. Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. IEEE Signal Processing Magazine 29, 6 (2012), 82–97.
[19] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. science 313, 5786 (2006), 504–507.
[20] Geoffrey E Hinton and Richard S Zemel. 1994. Autoencoders, minimum description length and Helmholtz free energy. In Advances in neural information processing systems. 3–10.
[21] I Hsu, Ayush Jaiswal, Premkumar Natarajan, et al. 2019. NIESR: Nuisance invariant end-to-end speech recognition. arXiv preprint arXiv:1907.03233 (2019).
[22] Wei-Ning Hsu, Yu Zhang, and James Glass. 2017. Unsupervised learning of disentangled and interpretable representations from sequential data. In Advances in neural information processing systems. 1878–1889.
[23] Rui Huang, Shu Zhang, Tianyu Li, and Ran He. 2017. Beyond face rotation: Global and local perception GAN for photorealistic and identity preserving frontal view synthesis. In Proceedings of the IEEE International Conference on Computer Vision. 2439–2448.
[24] Xuedong Huang, Fileno Alleva, Hsiao Hon, Mei Hwang, and Ronald Rosenfeld. 1992. The SPHINX-II Speech Recognition System: An Overview. Computer Speech & Language 7, 2 (1992), 137–148.
[25] Arindam Jati, Raghuveer Peri, Monisankha Pal, Tae Jin Park, Naveen Kumar, Ruchir Travadi, Panayiotis Georgiou, and Shrikanth Narayanan. 2019. Multi-task Discriminative Training of Hybrid DNN-TVM Model for Speaker Verification with Noisy and Far-Field Speech. In proceedings of Proceedings of Interspeech (2019).
[26] Chieh-Chi Kao, Weiran Wang, Ming Sun, and Chao Wang. 2018. R-CRNN: Region-based Convolutional Recurrent Neural Network for Audio Event Detection. In Interspeech 2018. 1358–1362.
[27] Insoo Kim, Kyuhong Kim, Jiwhan Kim, and Changkyu Choi. 2019. Deep Speaker Representation Using Orthogonal Decomposition and Recombination for Speaker Verification. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6126–6130.
[28] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).
[29] Anurag Kumar and Bhiksha Raj. 2016. Audio Event Detection using Weakly Labeled Data. In Proceedings of the 24th ACM international conference on Multimedia. 1038–1047.
[30] Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer, and Marc’Aurelio Ranzato. 2017. Fader networks: Manipulating images by sliding attributes. In Advances in Neural Information Processing Systems. 5967–5976.
[31] Hung-Shin Lee, Yu-Ding Lu, Chin-Cheng Hsu, Yu Tsao, Hsin-Min Wang, and Shyh-Kang Jeng. 2017. Discriminative autoencoders for speaker verification. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 5375–5379.
[32] Chao Li, Xiaokong Ma, Bing Jiang, Xiangang Li, Xuewei Zhang, Xiao Liu, Ying Cao, Ajay Kannan, and Zhenyao Zhu. 2017. Deep Speaker: an End-to-End Neural Speaker Embedding System. arXiv preprint arXiv:1705.02304 (2017).
[33] Mu Li, Wangmeng Zuo, and David Zhang. 2016. Deep identity-aware transfer of facial attributes. arXiv preprint arXiv:1610.05586 (2016).
[34] LinWei-wei, MakMan-Wai, and ChienJen-Tzung. 2018. Multisource I-Vectors Domain Adaptation Using Maximum Mean Discrepancy Based Autoencoders. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) (2018).
[35] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. 2017. SphereFace: Deep Hypersphere Embedding for Face Recognition. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 6738–6746.
[36] Yi Liu, Liang He, and Jia Liu. 2019. Large margin softmax loss for speaker verification. arXiv preprint arXiv:1904.03479 (2019).
[37] Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang. 2018. Exploring disentangled feature representation beyond face identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2080–2089.
[38] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. 2015. The variational fair autoencoder. arXiv preprint arXiv:1511.00830 (2015).
[39] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, Nov (2008), 2579–2605.
[40] Davide Maltoni, Dario Maio, Anil K. Jain, and Salil Prabhakar. 2005. Handbook of Fingerprint Recognition.
[41] Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann LeCun. 2016. Disentangling factors of variation in deep representation using adversarial training. In Advances in neural information processing systems. 5040–5048.
[42] Zhong Meng, Yong Zhao, Jinyu Li, and Yifan Gong. 2019. Adversarial speaker verification. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6216–6220.
[43] M.G. [n. d.]. Transparency Market Research. https://www.researchmoz.us/publisher..
[44] Lindasalwa Muda, Mumtaj Begam, and I. Elamvazuthi. 2010. Voice Recognition Algorithms using Mel Frequency Cepstral Coefficient (MFCC) and Dynamic Time Warping (DTW) Techniques. arXiv preprint arXiv:1003.4083 (2010).
[45] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman. 2020. Voxceleb: Large-scale speaker verification in the wild. Computer Speech & Language 60 (2020), 101027.
[46] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2017. VoxCeleb: A Large-Scale Speaker Identification Dataset. In Interspeech 2017. 2616–2620.
[47] Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda. 2018. Attentive statistics pooling for deep speaker embedding. arXiv preprint arXiv:1803.10963 (2018).
[48] Thomas Pellegrini and Leo Cances. 2019. Cosine-similarity penalty to discriminate sound classes in weakly-supervised sound event detection. In 2019 International Joint Conference on Neural Networks (IJCNN). 1–8.
[49] Raghuveer Peri, Monisankha Pal, Arindam Jati, Krishna Somandepalli, and Shrikanth Narayanan. 2019. Robust speaker recognition using unsupervised adversarial invariance. arXiv preprint arXiv:1911.00940 (2019).
[50] P.J. Phillips, Hyeonjoon Moon, S.A. Rizvi, and P.J. Rauss. 2000. The FERET evaluation methodology for face-recognition algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence 22, 10 (2000), 1090–1104.
[51] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recognition Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding.
[52] Simon JD Prince and James H Elder. 2007. Probabilistic linear discriminant analysis for inferences about identity. In 2007 IEEE 11th International Conference on Computer Vision. IEEE, 1–8.
[53] Alec Radford, Luke Metz, and Soumith Chintala. 2016. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. In ICLR 2016 : International Conference on Learning Representations 2016.
[54] FA Rezaur rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, and Li Wan. 2018. Attention-based models for text-dependent speaker verification. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 5359–5363.
[55] Desh Raj, David Snyder, Daniel Povey, and Sanjeev Khudanpur. 2019. Probing the Information Encoded in x-vectors. arXiv preprint arXiv:1909.06351 (2019).
[56] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. 2011. Contractive auto-encoders: Explicit invariance during feature extraction. (2011).
[57] Suwon Shon, Hao Tang, and James Glass. 2018. Frame-level speaker embeddings for text-independent speaker recognition and analysis of end-to-end model. In 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 1007–1013.
[58] David Snyder, Daniel Garcia-Romero, Daniel Povey, and Sanjeev Khudanpur. 2017. Deep Neural Network Embeddings for Text-Independent Speaker Verification. In Interspeech. 999–1003.
[59] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. 2018. X-vectors: Robust DNN embeddings for speaker recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 5329–5333.
[60] T.B. [n. d.]. Voiceprint. https://www.tdbank.com/bank/tdvoiceprint.html..
[61] Kar-Ann Toh, Jaihie Kim, and Sangyoun Lee. 2008. Equal Error Rate Minimization for Biometrics Fusion. ICEIC : International Conference on Electronics, Informations and Communications (2008), 513–516.
[62] Luan Tran, Xi Yin, and Xiaoming Liu. 2017. Disentangled representation learning GAN for pose-invariant face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1415–1424.
[63] Youzhi Tu, Man-Wai Mak, and Jen-Tzung Chien. 2019. Variational domain adversarial learning for speaker verification. Proc. Interspeech 2019 (2019), 4315–4319.
[64] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning. 1096–1103.
[65] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. 2018. Generalized end-to-end loss for speaker verification. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 4879–4883.
[66] Matt Warman. 2013. Say goodbye to the pin: voice recognition takes over at Barclays Wealth. https://www.telegraph.co.uk/technology/news/10044493/Say-goodbye-to-the-pin-voice-recognition-takes-over-at-Barclays-Wealth.html.