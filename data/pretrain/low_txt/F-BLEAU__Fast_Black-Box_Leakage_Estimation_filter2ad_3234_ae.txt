# Optimized Text

## Data Points
- 1.479
- 2.310
- 2.746
- 1.862
- 2.591
- 2.983
- 1.501
- 2.304
- 2.738
- 1.988
- 2.638
- 2.996
- F-BLEAU: ME
- True ME
- 1.802
- 2.550
- 2.970
- 1.987
- 2.631
- 3.003
- L.E.: Leakage Evidence Test

## Gowalla Dataset Analysis
We compare F-BLEAU and leakiEst on the location privacy mechanisms (Section VII): Blahut-Arimoto, planar Geometric, and planar Laplacian. The primary objective is to determine whether the advantage of F-BLEAU over the frequentist approach, observed in large output spaces, also extends to an advantage over leakiEst. For the first two mechanisms, we also compare the Mutual Information (MI) estimates. For the Laplacian case (continuous), we only use leakiEst's leakage evidence test.

### Experimental Setup
F-BLEAU and leakiEst were run on the datasets as described in Section VII. The results, presented in Table XIII, show that for the planar Geometric and Laplacian distributions, leakiEst fails to detect any leakage, reporting "Too small sample size." Additionally, the MI estimates provided by leakiEst for the planar Geometric distribution are significantly off from their true values. In contrast, F-BLEAU produces more reliable estimates.

For the Blahut-Arimoto mechanism, both F-BLEAU and leakiEst perform equally well in terms of MI estimates due to the small number of actual outputs. However, leakiEst's leakage evidence test still reports "Too small sample size." We hypothesize that this issue arises because leakiEst considers the declared size of the output space rather than the effective number of observed individual outputs. This problem can be easily resolved by inferring the output size from the examples (indicated by the "*" in Table XIII).

## Conclusion and Future Work
We have demonstrated that black-box leakage of a system, traditionally measured using classical statistical paradigms (frequentist approach), can be effectively estimated using machine learning (ML) techniques. Our methods, based on the nearest neighbor principle, have been thoroughly evaluated on both synthetic and real-world data. This allows us to address problems that were previously impractical and sets a new paradigm in black-box security, leveraging the equivalence between ML and black-box leakage estimation.

Empirical evidence shows that our nearest neighbor techniques excel when there is a metric in the output space, allowing them to use information from neighboring observations. For irregular output distributions, they are equivalent to the frequentist approach but can be misled in maliciously crafted systems. Nevertheless, they are asymptotically equivalent to the frequentist approach due to their universal consistency property.

Given the No Free Lunch (NFL) theorem in ML, no single estimate can guarantee optimal convergence. Therefore, we propose F-BLEAU, which combines frequentist and nearest neighbor rules, running all these techniques and selecting the estimate that converges faster. We anticipate that this work will inspire researchers to explore new leakage estimators from the ML literature, particularly those that provide universal consistency, such as Support Vector Machines (SVMs) and neural networks. Future work may also extend this to systems with continuous secret spaces, formalized as regression in ML terms.

A current limitation of our methods is the lack of confidence intervals. While this is a challenge for continuous systems under weak assumptions, it applies to any leakage estimation method. Despite this, ML methods offer a significant advantage by enabling the measurement of security in systems with a strongly reduced number of examples.

## Acknowledgments
Giovanni Cherubin was partially supported by an EcoCloud grant. Konstantinos Chatzikokolakis and Catuscia Palamidessi were partially supported by the ANR project REPAS. We are grateful to Marco Stronati, Tom Chothia, and Yusuke Kawamoto for their contributions and discussions. The name F-BLEAU honors the unfulfilled dream of climbing in Fontainebleau, France, which was a secondary goal of the research visit that initiated this collaboration.

## References
[1] D. Clark, S. Hunt, and P. Malacaria, “Quantitative information flow, relations and polymorphic types,” J. of Logic and Computation, vol. 18, no. 2, pp. 181–199, 2005.
[2] G. Smith, “On the foundations of quantitative information flow,” in Proceedings of the 12th International Conference on Foundations of Software Science and Computation Structures (FOSSACS 2009), ser. LNCS, L. de Alfaro, Ed., vol. 5504. York, UK: Springer, 2009, pp. 288–302.
[3] M. S. Alvim, K. Chatzikokolakis, C. Palamidessi, and G. Smith, “Measuring information leakage using generalized gain functions,” in Proceedings of the 25th IEEE Computer Security Foundations Symposium (CSF), 2012, pp. 265–279.
[Online]. Available: http://hal.inria.fr/hal-00734044/en
[4] C. Braun, K. Chatzikokolakis, and C. Palamidessi, “Quantitative notions of leakage for one-try attacks,” in Proceedings of the 25th Conf. on Mathematical Foundations of Programming Semantics, ser. Electronic Notes in Theoretical Computer Science, vol. 249. Elsevier B.V., 2009, pp. 75–91. [Online]. Available: http://hal.archives-ouvertes.fr/inria-00424852/en/
[5] T. Chothia, Y. Kawamoto, and C. Novakovic, “LeakWatch: Estimating information leakage from java programs,” in Proc. of ESORICS 2014 Part II, 2014, pp. 219–236.
[6] ——, “A tool for estimating information leakage,” in International Conference on Computer Aided Verification (CAV). Springer, 2013, pp. 690–695.
[7] M. E. Andrés, N. E. Bordenabe, K. Chatzikokolakis, and C. Palamidessi, “Geo-indistinguishability: Differential privacy for location-based systems,” in Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security. ACM, 2013, pp. 901–914.
[8] S. Oya, C. Troncoso, and F. Pérez-González, “Back to the drawing board: Revisiting the design of optimal location privacy-preserving mechanisms,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS ’17. ACM, 2017, pp. 1959–1972. [Online]. Available: http://doi.acm.org/10.1145/3133956.3134004
[9] T. Chothia and V. Smirnov, “A traceability attack against e-passports,” in International Conference on Financial Cryptography and Data Security. Springer, 2010, pp. 20–34.
[10] D. H. Wolpert, “The lack of a priori distinctions between learning algorithms,” Neural computation, vol. 8, no. 7, pp. 1341–1390, 1996.
[11] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that exploit confidence information and basic countermeasures,” in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. ACM, 2015, pp. 1322–1333.
[12] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference attacks against machine learning models,” in Security and Privacy (SP), 2017 IEEE Symposium on. IEEE, 2017, pp. 3–18.
[13] K. Chatzikokolakis, T. Chothia, and A. Guha, “Statistical measurement of information leakage,” Tools and Algorithms for the Construction and Analysis of Systems, pp. 390–404, 2010.
[14] M. Boreale and M. Paolini, “On formally bounding information leakage by statistical estimation,” in International Conference on Information Security. Springer, 2014, pp. 216–236.
[15] T. Chothia and A. Guha, “A statistical test for information leaks using continuous mutual information,” in Proceedings of the 24th IEEE Computer Security Foundations Symposium, CSF 2011, Cernay-la-Ville, France, 27-29 June, 2011. IEEE Computer Society, 2011, pp. 177–190. [Online]. Available: https://doi.org/10.1109/CSF.2011.19
[16] T. Chothia, Y. Kawamoto, C. Novakovic, and D. Parker, “Probabilistic point-to-point information leakage,” in Computer Security Foundations Symposium (CSF), 2013 IEEE 26th. IEEE, 2013, pp. 193–205.
[17] G. Cherubin, “Bayes, not naïve: Security bounds on website fingerprinting defenses,” Proceedings on Privacy Enhancing Technologies, vol. 2017, no. 4, pp. 215–231, 2017.
[18] N. Santhi and A. Vardy, “On an improvement over Rényi’s equivocation bound,” 2006, presented at the 44-th Annual Allerton Conference on Communication, Control, and Computing, September 2006. Available at http://arxiv.org/abs/cs/0608087.
[19] I. Belghazi, S. Rajeswar, A. Baratin, R. D. Hjelm, and A. Courville, “MINE: Mutual Information Neural Estimation,” arXiv preprint arXiv:1801.04062, 2018.
[20] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, “InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets,” in Advances in neural information processing systems, 2016, pp. 2172–2180.
[21] J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan, “Learning to explain: An information-theoretic perspective on model interpretation,” arXiv preprint arXiv:1802.07814, 2018.
[22] V. Vapnik, The nature of statistical learning theory. Springer science & business media, 2013.
[23] L. Devroye, L. Györfi, and G. Lugosi, A probabilistic theory of pattern recognition. Springer Science & Business Media, 2013, vol. 31.
[24] C. J. Stone, “Consistent nonparametric regression,” The annals of statistics, pp. 595–620, 1977.
[25] C. Dwork, “Differential privacy,” in 33rd International Colloquium on Automata, Languages and Programming (ICALP 2006), ser. Lecture Notes in Computer Science, M. Bugliesi, B. Preneel, V. Sassone, and I. Wegener, Eds., vol. 4052. Springer, 2006, pp. 1–12. [Online]. Available: http://dx.doi.org/10.1007/11787006_1
[26] R. Shokri, G. Theodorakopoulos, C. Troncoso, J.-P. Hubaux, and J.-Y. L. Boudec, “Protecting location privacy: optimal strategy against localization attacks,” in Proceedings of the 19th ACM Conference on Computer and Communications Security (CCS 2012), T. Yu, G. Danezis, and V. D. Gligor, Eds. ACM, 2012, pp. 617–627.
[27] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed. John Wiley & Sons, Inc., 2006.
[28] “The Gowalla dataset.” [Online]. Available: https://snap.stanford.edu/data/loc-gowalla.html
[29] E. Cho, S. A. Myers, and J. Leskovec, “Friendship and mobility: User movement in location-based social networks,” in Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD ’11. New York, NY, USA: ACM, 2011, pp. 1082–1090. [Online]. Available: http://doi.acm.org/10.1145/2020408.2020579
[30] Example: e-passport traceability. School of Computer Science - [Online]. Available: www.cs.bham.ac.uk/research/projects/leakiEst/infotools/leakiest/examples/epassports.php
[31] I. Steinwart, “Support vector machines are universally consistent,” Journal of Complexity, vol. 18, no. 3, pp. 768–791, 2002.
[32] T. Glasmachers, “Universal consistency of multi-class support vector classification,” in Advances in Neural Information Processing Systems, 2010, pp. 739–747.
[33] T. Cover and P. Hart, “Nearest neighbor pattern classification,” IEEE transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.
[34] K. Fukunaga and D. M. Hummels, “Bayes error estimation using Parzen and k-NN procedures,” IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 5, pp. 634–643, 1987.
[35] M. Backes and B. Köpf, “Formally bounding the side-channel leakage in unknown-message attacks,” in European Symposium on Research in Computer Security. Springer, 2008, pp. 517–532.
[36] P. C. Kocher, “Timing attacks on implementations of Diffie-Hellman, RSA, DSS, and other systems,” in Annual International Cryptology Conference. Springer, 1996, pp. 104–113.
[37] B. Köpf and D. Basin, “Timing-sensitive information flow analysis for synchronous systems,” in European Symposium on Research in Computer Security. Springer, 2006, pp. 243–262.

## Appendix A: Additional Tools from ML
The ML literature offers several tools for black-box security. We list additional universally consistent (UC) rules, a lower bound of the Bayes risk, and a general way to obtain estimates that converge from below.

The family of UC rules is extensive. Devroye et al. [23] provide an overview, including nearest neighbor methods, histogram rules, and certain types of neural networks, which are UC under specific parameter requirements. Steinwart [31] proved that Support Vector Machines (SVMs) are UC for some parameter choices when |S| = 2; however, attempts to construct UC SVMs for |S| > 2 have been unsuccessful [32].

In applications with strict security requirements, a pessimistic lower bound of the Bayes risk may be desirable. From a result by Cover and Hart [33], a lower bound on the Bayes risk based on the nearest neighbor (NN) error, \( R_{NN} \), can be derived:
\[
\lim_{n \to \infty} \left( \frac{|S| - 1}{|S|} \left( 1 - \frac{|S| - 1}{|S|} R_{NN} \right) \right) \leq R^*,
\]
where \( R^* \) is the Bayes risk. This was used to measure the black-box leakage of website fingerprinting defenses [17].

Finally, one can obtain estimators that converge to the Bayes risk in expectation from below by estimating the error of a k-NN rule on its training set [17, 34].

## Appendix B: Description of Synthetic Systems
### Geometric System
Geometric systems are common in differential privacy and involve adding negative exponential noise to the result of a query. The noise is added to ensure that the probability of a reported answer is within a factor of the probability of its immediate neighbor. Here, we provide an abstract definition of a geometric system in terms of secrets (e.g., query result/real location) and observables (e.g., reported answer/reported location).

Let \( S \) and \( O \) be sets of consecutive natural numbers with a standard notion of distance. Two numbers \( s, s' \in S \) are adjacent if \( s = s' + 1 \) or \( s' = s + 1 \). Let \( \nu \) be a real non-negative number and consider a function \( g: S \to O \). After adding negative exponential noise to the output of \( g \), the resulting geometric system is described by the channel matrix:
\[
C_{s,o} = P(o | s) = \lambda \exp(-\nu |g(s) - o|),
\]
where \( \lambda \) is a normalizing factor. The privacy level is defined by \( \nu / \Delta_g \), where \( \Delta_g \) is the sensitivity of \( g \):
\[
\Delta_g = \max_{s_1 \sim s_2 \in S} (g(s_1) - g(s_2)),
\]
and \( s_1 \sim s_2 \) means \( s_1 \) and \( s_2 \) are adjacent. Let \( S = \{1, \ldots, w\} \) and \( O = \{1, \ldots, w'\} \). We define:
\[
\lambda =
\begin{cases}
\frac{e^\nu}{e^\nu + 1} & \text{if } o = 1 \text{ or } o = w', \\
\frac{e^\nu - 1}{e^\nu + 1} & \text{otherwise},
\end{cases}
\]
to truncate the distribution at its boundaries. This definition prohibits the case \( |S| > |O| \). To handle such cases, we generate a repeated geometric channel matrix:
\[
C'_{s,o} = C_{s \mod |O|, o}.
\]

### Multimodal Geometric System
We construct a multimodal distribution as the weighted sum of two geometric distributions, shifted by a shift parameter. Let \( C_{s,o} \) be a geometric channel matrix. The multimodal geometric channel, for shift parameter \( \sigma \), is:
\[
C^M_{s,o} = w_1 C_{s,o} + w_2 C_{s+2\sigma, o}.
\]
In experiments, we used \( \sigma = 5 \) and weights \( w_1 = w_2 = 0.5 \).

### Spiky System
Consider an observation space \( O = \{0, \ldots, q-1\} \) for some even positive integer \( q \), and a secret space \( |S| = 2 \). Assume \( O \) is a ring with operations \( + \) and \( - \) defined as the sum and difference modulo \( q \), respectively, and a distance on \( O \) defined as \( d(i, j) = |i - j| \). The Spiky system has a uniform prior and a channel matrix constructed as follows:
\[
C_{s,o} =
\begin{cases}
\frac{2}{q} & \text{if } s = o \text{ or } s = o + \frac{q}{2}, \\
0 & \text{otherwise}.
\end{cases}
\]

---

This optimized text provides a clear, coherent, and professional presentation of the original content.