# Comparison of Exact and Approximate Profiling for Transient Faults

## Relative Outcomes Due to Permanent Faults

| Outcome | Percentage |
|---------|------------|
| 100%    |            |
| 90%     |            |
| 80%     |            |
| 70%     |            |
| 60%     |            |
| 50%     |            |
| 40%     |            |
| 30%     |            |
| 20%     |            |
| 10%     |            |
| 0%      |            |

### Figure 2: Comparison of Exact and Approximate Profiling for Transient Faults
This figure compares the outcomes of exact and approximate profiling methods for transient faults.

### Figure 3: Relative Outcomes for Permanent Faults
This figure illustrates the relative occurrence of SDC (Silent Data Corruption), DUE (Detectable Unrecoverable Error), and masked outcomes for permanent faults. For each program, 171 runs were conducted with one opcode out of the possible 171 opcodes selected for injection in each run. The outcome of each run is weighted based on the relative number of dynamic instructions for that opcode.

For example, if injections into the FADD instruction result in an SDC and account for 10% of all program instructions, and FMUL results in a DUE and accounts for 20% of all program instructions, then the DUE outcome would be weighted twice that of the SDC outcome. This weighting reflects the greater likelihood that the FMUL instruction is executed and activates the permanent fault.

Compared to transient faults, the permanent faults for our benchmark programs result in more SDC and DUE outcomes. Masked outcomes constitute 57.6% for transient faults but only 17.4% for permanent faults. This result is intuitive as permanent faults are activated multiple times and result in greater error propagation, leading to more SDCs and DUEs.

## Normalized NVBitFI Overheads Relative to No-FI

### Figure 4: Execution Overheads
This figure shows the overheads for profiling and injection, relative to the runtime of an uninstrumented program. Profiling need only be performed once per application to characterize the set of injection sites. The overhead for exact profiling can be quite large (as much as 558× for 350.md), especially if the instrumentation causes GPU registers to be spilled to memory. On average, our programs demonstrate an exact profiling overhead that is 28× more than approximate profiling. If the exact profiling time is unacceptable, approximate profiling offers an attractive alternative.

In contrast to profiling, injection experiments must be performed many times to obtain statistical confidence. For these programs, injection times can vary depending on the amount of instrumentation added to a program and the number of times that instrumentation is executed. The injection times in Figure 4 are the median from the set of 100 injection experiments for each program and fault type. For our programs, transient fault injection slows down program execution by about 2.9×, while permanent fault injection slows down program execution by about 4.8× on average.

## Total Campaign Times for Transient and Permanent Faults

### Figure 5: Total Campaign Times (Assuming 100 Transient Faults)
This figure illustrates aggregate campaign times for transient campaigns with 100 faults and permanent campaigns that leverage a profile to identify unused opcodes. The number of executed opcodes for our programs ranges from 16 to 41 opcodes per program (out of the total possible 171 opcodes). For these applications, the transient campaigns typically take about twice the time as the permanent campaigns, although the transient campaign can take as much as 5× longer or be slightly faster.

## Summary and Future Directions

We have presented the NVBitFI tool for dynamic fault injection into GPUs. The tool is built using the NVBit dynamic binary instrumentation framework and therefore offers a convenient way to conduct a fault injection campaign into GPUs without having to know many details about the GPU. Furthermore, no source code for the target program is required, which not only simplifies the user’s job but also allows faults to be injected into dynamic libraries for which no source code is available. Porting the tool to non-Nvidia GPUs would require (a) a binary instrumentation tool on the GPU and (b) porting the handlers from CUDA to something like OpenCL or employing a translator (e.g., HIP [26]) to perform the translation.

Using the NVBit framework, NVBitFI can limit instrumentation needed for fault injection to the dynamic instance of the target kernel. Non-target instances of the same static kernel execute unmodified, thus minimizing the performance overhead of the injection code.

NVBitFI currently supports a transient and a simple permanent fault model. We are considering extensions of the fault model, including:

- **Intermittent Faults:** The permanent fault model corrupts the destination register of every dynamic instruction of a particular opcode. An intermittent fault model would inject into only a subset of those instructions. The subset can be specified as a random, bursty process.
- **More Complex Fault Models:** Our current fault models can be extended to provide additional flexibility in specifying fault parameters, including (1) corrupting multiple registers, (2) supporting corruption functions beyond the current set of XOR, random, and zero functions, (3) conditioning error effects on the specific opcode, and (4) allowing a permanent fault to affect multiple opcodes.
- **Fault Dictionary:** A fault dictionary based on microarchitectural simulation or an analytical model is a specific example of a more complex fault model. A fault dictionary might be useful when a complex fault model is not easily characterized by a set of parameters.

## References

[1] “TOP500 LIST - NOVEMBER 2020,” https://www.top500.org/lists/top500/list/2020/11/, accessed: 2021-03-27.
[2] “GREEN500 LIST - NOVEMBER 2020,” https://www.top500.org/lists/green500/list/2020/11/, accessed: 2021-03-27.
[3] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt, and T. Austin, “A Systematic Methodology to Compute the Architectural Vulnerability Factors for a High-Performance Microprocessor,” in International Symposium on Microarchitecture (MICRO), 2003, pp. 29–40.
[4] S. K. S. Hari, T. Tsai, M. Stephenson, S. W. Keckler, and J. Emer, “SASSIFI: An Architecture-level Fault Injection Tool for GPU Application Resilience Evaluation,” in International Symposium on Performance Analysis of Systems and Software (ISPASS), 2017.
[5] G. Li, K. Pattabiraman, C.-Y. Cher, and P. Bose, “Understanding Error Propagation in GPGPU Applications,” in International Conference on High Performance Computing, Networking, Storage and Analysis (SC), 2016.
[6] B. Fang, K. Pattabiraman, M. Ripeanu, and S. Gurumurthi, “GPU-Qin: A Methodology for Evaluating the Error Resilience of GPGPU Applications,” in International Symposium on Performance Analysis of Systems and Software (ISPASS), 2014.
[7] K. S. Yim, C. Pham, M. Saleheen, Z. Kalbarczyk, and R. Iyer, “Hauberk: Lightweight Silent Data Corruption Error Detector for GPGPU,” in International Symposium on Parallel and Distributed Processing (IPDPS), 2011.
[8] “NVIDIA CUDA Binary Utilities,” https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html, accessed: 2021-03-27.
[9] O. Villa, M. Stephenson, D. Nellans, and S. W. Keckler, “NVBit: A Dynamic Binary Instrumentation Framework for NVIDIA GPUs,” in International Symposium on Microarchitecture (MICRO), 2019.
[10] “Parallel Thread Execution ISA Version 7.1,” https://docs.nvidia.com/cuda/parallel-thread-execution/, accessed: 2021-03-27.
[11] M. Stephenson, S. K. S. Hari, Y. Lee, E. Ebrahimi, D. R. Johnson, D. Nellans, M. O’Conner, and S. W. Keckler, “Flexible Software Profiling of GPU Architectures,” in International Symposium on Computer Architecture (ISCA), 2015.
[12] “CUDA-GDB,” https://developer.nvidia.com/cuda-gdb, accessed: 2021-03-27.
[13] A. Vallero, D. Gizopoulos, and S. D. Carlo, “SIFI: AMD Southern Islands GPU Microarchitectural Level Fault Injector,” in International Symposium on On-Line Testing and Robust System Design (IOLTS), 2017.
[14] R. Ubal, B. Jang, P. Mistry, D. Schaa, and D. Kaeli, “Multi2Sim: A Simulation Framework for CPU-GPU Computing,” in International Conference on Parallel Architectures and Compilation Techniques (PACT), 2012.
[15] S. Tselonis and D. Gizopoulos, “GUFI: A Framework for GPUs Reliability Assessment,” in International Symposium on Performance Analysis of Systems and Software (ISPASS), 2016.
[16] J. Tan, N. Goswami, T. Li, and X. Fu, “Analyzing Soft-error Vulnerability on GPGPU Microarchitecture,” in International Symposium on Workload Characterization (IISWC), 2011.
[17] “CUDA Parallel Computing Platform,” https://developer.nvidia.com/cuda-zone, accessed: 2021-03-27.
[18] A. Bakhoda, G. L. Yuan, W. W. L. Fung, H. Wong, and T. M. Aamodt, “Analyzing CUDA Workloads Using a Detailed GPU Simulator,” in International Symposium on Performance Analysis of Systems and Software (ISPASS), 2009.
[19] M. Wilkening, V. Sridharan, S. Li, F. Previlon, S. Gurumurthi, and D. R. Kaeli, “Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults,” in International Symposium on Microarchitecture (MICRO), 2014.
[20] H. Jeon, M. Wilkening, V. Sridharan, S. Gurumurthi, and G. H. Loh, “Architectural Vulnerability Modeling and Analysis of Integrated Graphics Processors,” in Workshop on Silicon Errors in Logic–System Effects (SELSE), 2013.
[21] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, R. Sen, K. L. Sewell, A. Muahammad, N. Vaish, M. D. Hill, and D. A. Wood, “The gem5 Simulator,” ACM SIGARCH Computer Architecture News, vol. 39, no. 2, pp. 1–7, 2011.
[22] S. Jha, S. Banerjee, T. Tsai, S. K. S. Hari, M. B. Sullivan, Z. T. Kalbarczyk, S. W. Keckler, and R. K. Iyer, “ML-Based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection,” in Dependable Systems and Networks (DSN), 2019.
[23] G. Juckeland, W. C. Brantley, S. Chandrasekaran, B. M. Chapman, S. Che, M. E. Colgrove, H. Feng, A. Grund, R. Henschel, W. mei W. Hwu, H. Li, M. S. M¨uller, W. E. Nagel, M. Perminov, P. Shelepugin, K. Skadron, J. A. Stratton, A. Titov, K. Wang, G. M. van Waveren, B. Whitney, S. Wienke, R. Xu, and K. Kumaran, “SpecACCEL: A Standard Application Suite for Measuring Hardware Accelerator Performance,” in International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS), 2014.
[24] B. Nie, L. Yang, A. Jog, and E. Smirni, “Fault Site Pruning for Practical Reliability Analysis of GPGPU Applications,” in International Symposium on Microarchitecture (MICRO), 2018.
[25] B. Nie, A. Jog, and E. Smirni, “Characterizing Accuracy-Aware Resilience of GPGPU Applications,” in IEEE/ACM International Symposium on Cluster Computing and the Grid (CCGRID), 2020.
[26] “Whitepaper: Introducing AMD CDNA Architecture - The All-New AMD GPU Architecture for the Modern Era of HPC & AI,” https://www.amd.com/system/files/documents/amd-cdna-architecture-whitepaper.pdf, accessed: 2021-03-27.