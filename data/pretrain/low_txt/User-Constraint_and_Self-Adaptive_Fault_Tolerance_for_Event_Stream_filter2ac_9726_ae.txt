# 优化后的文本

## 资源开销与恢复时间阈值的关系

### 图5: 使用All Costs成本模型时，不同恢复时间阈值下的资源开销
- 横轴: 恢复时间 (秒)
- 纵轴: 状态大小 (MB)

### 图6: 使用Amazon EC2成本模型时，不同恢复时间阈值下的资源开销
- 横轴: 恢复时间 (秒)
- 纵轴: 状态大小 (MB)

### 图7: 不同状态大小下的最低恢复时间阈值
- 横轴: 状态大小 (MB)
- 纵轴: 最低恢复时间阈值 (秒)

当用户选择零秒的恢复时间时，系统完全处于主动复制模式下运行，导致CPU、内存、网络和基础设施的利用率高达100%。然而，随着恢复时间阈值的增加，可以节省资源。例如，当恢复时间阈值设置为四秒或更长时，CPU开销降至50%，而网络利用率上升至300%。这是由于系统主要使用热备模式，其中CPU周期因暂停的次级节点而得以节省，但代价是定期状态同步机制消耗了更多的网络带宽。实际上，300%的开销表明状态同步机制消耗的网络带宽比事件分发本身还要多。通过降低状态同步频率，可以减少这种开销。

根据应用程序的性质，这样的CPU资源节省可能是显著的。例如，对于由于某些非常昂贵的操作而导致CPU受限的应用程序，使用自适应方案可以通过牺牲网络资源来节省CPU资源，同时仍然提供与主动复制相同的恢复时间和语义保证。

### 成本模型

在之前的实验基础上，我们稍微修改了成本权重向量，以匹配Amazon EC2的成本模型。在Amazon EC2中，用户是基于虚拟机使用的小时数收费，而不是实际的CPU周期或网络带宽。因此，即使持续满载使用CPU和内部网络也不会产生额外费用。为了匹配这一特性，我们将CPU、内存和网络的权重设为零，从而在评估不同方法时不会考虑这些资源。换句话说，那些使用最少虚拟机小时数的方法将被优先选择。

图6展示了这一修改的结果。比较图5和图6中的基础设施利用率，可以看到，在恢复时间阈值为5.5秒时，使用修改后的成本权重向量的开销（即所需节点数量）下降得更快，这证实了对基础设施成本而非单个资源成本的重视。另一方面，对于小于5.5秒的恢复时间阈值，CPU、内存和网络资源的开销保持不变。这是因为控制器倾向于使用主动复制，因为它提供了比其他类似资源消耗的方法更快的恢复速度。

### 状态大小与资源节省的关系

如前所述，状态大小对恢复时间有显著影响。较大的状态需要更多的时间从磁盘加载并在内存中重建。因此，在接下来的实验中，我们在情感分析应用中添加了一个附加数据字段，并改变了事件的大小，以研究其对资源节省的影响。预计具有较小状态的应用程序在容错方面可能实现更多的资源节省。

图7显示了最低界限，即用户必须选择的最低恢复时间阈值，以便实现资源节省。结果表明，无论状态大小如何，超过四秒的恢复时间阈值已经允许节省资源，因为系统可以透明地切换到被动备用模式，从而减少CPU开销。然而，需要注意的是，随着状态大小的增加，状态同步可以进行得更少，从而增加了上游日志中的事件数量。

为了节省内存资源，用户需要提供较大的恢复时间阈值才能开始节省资源。这是因为在默认成本权重向量下，像主动复制和主动备用这样占用大量CPU的方案会被替换为占用更多内存的状态，如被动备用。对于网络和基础设施利用率，我们可以观察到类似的趋势：随着状态大小的增加，需要提供更高的恢复时间阈值才能受益于资源节省。

注意，对于500 MB及更大的状态大小，需要超过20秒的恢复时间。这种情况没有直接在图表中显示，而是通过平台表示出来。由于StreamMine3G支持细粒度的状态分区，每个分区的状态通常较小，很少会超过100 MB，前提是工作负载均衡。即便如此，类似于Hadoop中的滞后者问题，拆分阶段通常是减少分区状态量的一种可行方法。因此，即使在较短的恢复时间阈值下，用户也能受益于资源节省。

### 成本模型与容错方案的使用关系

在下一个实验中，我们针对不同的成本模型调整了恢复时间阈值，以了解系统在每种方案上花费的时间。与之前的实验一样，我们可以识别出所选成本模型与所用容错方案之间的明确关联。使用默认成本权重向量时，可以看到系统在恢复时间阈值低于三秒时停留在主动复制模式。随着阈值的增加，系统转为主动备用，然后是被动备用等方案。由于系统总是从主动复制开始，因此无论指定的恢复时间阈值如何，总会有一部分时间与主动复制相关联。

对于Amazon EC2成本模型，可以看到系统主要选择了两种状态：主动复制和被动复制。被动复制更受欢迎，因为它在部署较少副本时能减少更多成本，因为节点可以被释放。另一方面，如果不能使用被动复制，则使用主动复制，因为它提供了最快的恢复时间，并且仍然消耗相当多的资源，而这些资源无论如何都是由Amazon EC2客户支付的。

### 恢复保证与容错方案的使用关系

类似于之前的实验，我们调整了恢复时间阈值，但现在是针对不同的恢复保证。仅要求间隙恢复会导致比精确恢复更低的恢复时间，因为可以省略事件重放。如图9所示，要求精确恢复会使系统更多时间停留在主动备用模式，而选择间隙恢复的用户则可以在指定18秒或更长时间的恢复阈值时让系统停留在被动复制模式。

### CPU和网络资源消耗的权衡

在最后一个实验中，我们研究了在改变状态同步和检查点间隔时，节省的CPU资源和网络资源之间的权衡。较短的间隔会导致更及时的状态更新和更短的进出队列，从而提高恢复时间，但代价是每次状态同步都会给网络带来额外的带宽开销。图10展示了不同状态同步间隔下的CPU和网络利用率开销。如果状态同步连续进行，不中断，可能会遇到高达600%的峰值开销，具体取决于应用程序和状态的性质。例如，平均状态大小为10 MB的应用程序可能会出现高达200%的开销。然而，关于CPU的节省是微不足道的，因此，状态同步不应频繁于每两秒一次。

## 相关工作

在本节中，我们将简要概述ESP系统中使用的容错技术。受数据库系统中使用的方案启发，提出了几种基于检查点和日志的方法，例如[10]和[15]。虽然Hwang等人[10]提出了一种上游备份方法，其中事件在上游节点上记录以供恢复，Gu等人[15]结合了日志和检查点，引入了扫描检查点算法。我们采用了第二种方法的改进版本，使检查点间隔可调，从而能够直接控制恢复时间。

一种不需要检查点的最新方法是Koldehofe等人[18]的工作，该方法使用安全点并通过依赖图跟踪状态修改。尽管这种方法可以节省创建和维护检查点的开销，但它不适合我们的当前操作符模型和API，因为它需要一个通知机制来跟踪与传入事件相关的状态修改。

与StreamMine3G类似的系统是由Castro Fernandez等人[19]提出的，该系统带有显式状态管理支持，既用于弹性也用于回滚恢复。尽管两个系统（SEEP和StreamMine3G）有很多相似之处，但SEEP仅支持单一容错方案，而我们的系统涵盖了多种成熟的方案。

Balazinska等人[20]提出了一种类似于我们的方法，保证用户指定的恢复时间。然而，他们不是通过切换适当的方案并保持一致性保证，而是通过临时引入不一致来转发部分结果，因为上游操作符分区不可用。

最后，已经提出了几种结合多个容错方案的方法。然而，它们的目标各不相同：[21]和[9]的作者使用主动复制和被动备用的组合。然而，Martin等人[9]在正常操作期间运行主动复制，利用已付费的云资源，而Zhang等人[21]使用被动备用，仅在故障情况下切换到主动复制。在我们的方法中，我们在一个系统中结合了多种方法，并根据用户指定的恢复时间在方案之间切换。使用成本权重向量，我们的方法也可以利用免费资源来进行容错，如Martin等人[9]所述。

最接近我们自适应机制的方法是由Upadhyaya等人[22]提出的。作者提出了一种优化算法，专门针对特定操作符而不是整个查询，以保证用户指定的恢复时间。然而，与我们的工作相比，该方法不是自适应的，也没有考虑资源开销。

## 结论

本文介绍了StreamMine3G，据我们所知，它是第一个在一个系统中结合多种容错方案的弹性ESP系统。为了减轻用户选择最合适方案的负担，系统配备了容错控制器，用户只需指定恢复时间阈值、恢复保证（精确或间隙恢复），以及可选的成本权重向量，用于资源和成本优化。使用提供的输入，系统将在运行时自适应，选择确保用户指定恢复时间阈值且资源消耗成本最低的容错方案。系统采用基于执行过程中收集的性能指标的估计方法进行自适应。

## 致谢

这项研究得到了欧盟第七框架计划（FP7/2012-2015）资助协议编号318809（LEADS）、CAPES、DAAD和GIZ通过NoPa项目（TruEGrid项目，2011-2013）以及德国卓越倡议中心——德累斯顿先进电子中心（cfAED）的支持。

## 参考文献

[1] J. Dean and S. Ghemawat, “MapReduce: Simplified data processing on large clusters,” Commun. ACM, vol. 51, no. 1, pp. 107–113, Jan. 2008.
[2] “Hadoop open source mapreduce implementation,” http://hadoop.apache.org/, 2014.
[3] “Google adwords,” https://www.google.com/adwords/, 2014.
[4] “Apache samza - distributed stream processing framework,” http://samza.incubator.apache.org/, 2014.
[5] “Apache storm - distributed and fault-tolerant realtime computation,” https://storm.incubator.apache.org/, 2014.
[6] L. Neumeyer, B. Robbins, A. Nair, and A. Kesari, “S4: Distributed stream computing platform,” in Proceedings of the 2010 IEEE International Conference on Data Mining Workshops, ser. ICDMW ’10. Washington, DC, USA: IEEE Computer Society, 2010, pp. 170–177.
[7] M. A. Shah, J. M. Hellerstein, and E. Brewer, “Highly available, fault-tolerant, parallel dataflows,” in Proceedings of the 2004 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD ’04. New York, NY, USA: ACM, 2004, pp. 827–838.
[8] J.-H. Hwang, U. Cetintemel, and S. Zdonik, “Fast and reliable stream processing over wide area networks,” in Proceedings of the 2007 IEEE 23rd International Conference on Data Engineering Workshop, ser. ICDEW ’07. Washington, DC, USA: IEEE Computer Society, 2007, pp. 604–613.
[9] A. Martin, C. Fetzer, and A. Brito, “Active replication at (almost) no cost,” in Proceedings of the 2011 IEEE 30th International Symposium on Reliable Distributed Systems, ser. SRDS ’11. Washington, DC, USA: IEEE Computer Society, 2011, pp. 21–30.
[10] J.-H. Hwang, M. Balazinska, A. Rasin, U. Cetintemel, M. Stonebraker, and S. Zdonik, “High-availability algorithms for distributed stream processing,” in Proceedings of the 21st International Conference on Data Engineering, ser. ICDE ’05. Washington, DC, USA: IEEE Computer Society, 2005, pp. 779–790.
[11] X. Défago, A. Schiper, and P. Urbán, “Total order broadcast and multicast algorithms: Taxonomy and survey,” ACM Comput. Surv., vol. 36, no. 4, pp. 372–421, Dec. 2004.
[12] M. K. Aguilera and R. E. Strom, “Efficient atomic broadcast using deterministic merge,” in Proceedings of the Nineteenth Annual ACM Symposium on Principles of Distributed Computing, ser. PODC ’00. New York, NY, USA: ACM, 2000, pp. 209–218.
[13] C. Fetzer, U. Schiffel, and M. Süßraut, “An-encoding compiler: Building safety-critical systems with commodity hardware,” in Proceedings of the 28th International Conference on Computer Safety, Reliability, and Security, ser. SAFECOMP ’09. Berlin, Heidelberg: Springer-Verlag, 2009, pp. 283–296.
[14] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed, “Zookeeper: Wait-free coordination for internet-scale systems,” in Proceedings of the 2010 USENIX Conference on USENIX Annual Technical Conference, ser. USENIXATC’10. Berkeley, CA, USA: USENIX Association, 2010, pp. 11–11. [Online]. Available: http://dl.acm.org/citation.cfm?id=1855840.1855851
[15] Y. Gu, Z. Zhang, F. Ye, H. Yang, M. Kim, H. Lei, and Z. Liu, “An empirical study of high availability in stream processing systems,” in Proceedings of the 10th ACM/IFIP/USENIX International Conference on Middleware, ser. Middleware ’09. New York, NY, USA: Springer-Verlag New York, Inc., 2009, pp. 23:1–23:9. [Online]. Available: http://dl.acm.org/citation.cfm?id=1656980.1657012
[16] R. E. Kalman, “A new approach to linear filtering and prediction problems,” Transactions of the ASME–Journal of Basic Engineering, vol. 82, no. Series D, pp. 35–45, 1960.
[17] A. Martin, R. Marinho, A. Brito, and C. Fetzer, “Predicting energy consumption with streammine3g,” in Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems, ser. DEBS ’14. New York, NY, USA: ACM, 2014, pp. 270–275.
[18] B. Koldehofe, R. Mayer, U. Ramachandran, K. Rothermel, and M. Völz, “Rollback-recovery without checkpoints in distributed event processing systems,” in Proceedings of the 7th ACM International Conference on Distributed Event-based Systems, ser. DEBS ’13. New York, NY, USA: ACM, 2013, pp. 27–38.
[19] R. Castro Fernandez, M. Migliavacca, E. Kalyvianaki, and P. Pietzuch, “Integrating scale out and fault tolerance in stream processing using operator state management,” in Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD ’13. New York, NY, USA: ACM, 2013, pp. 725–736.
[20] M. Balazinska, H. Balakrishnan, S. Madden, and M. Stonebraker, “Fault-tolerance in the borealis distributed stream processing system,” in Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD ’05. New York, NY, USA: ACM, 2005, pp. 13–24.
[21] Z. Zhang, Y. Gu, F. Ye, H. Yang, M. Kim, H. Lei, and Z. Liu, “A hybrid approach to high availability in stream processing systems,” in Proceedings of the 2010 IEEE 30th International Conference on Distributed Computing Systems, ser. ICDCS ’10. Washington, DC, USA: IEEE Computer Society, 2010, pp. 138–148.
[22] P. Upadhyaya, Y. Kwon, and M. Balazinska, “A latency and fault-tolerance optimizer for online parallel query plans,” in Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD ’11. New York, NY, USA: ACM, 2011, pp. 241–252.