### DHT-Based Implementation of Place Lab: Lessons Learned

Distributed Hash Tables (DHTs) have demonstrated their capability to support applications with more complex semantics than simple put/get operations. This paper summarizes the key lessons learned from implementing Place Lab on top of a third-party DHT, OpenDHT.

#### 6.1 Simplicity of Implementation

The integration of Place Lab with OpenDHT required only 2,100 lines of Java code, compared to over 14,000 lines for the entire OpenDHT implementation. A custom, non-layered approach would have necessitated re-implementing the scalable routing, robustness, and management features provided by OpenDHT. 

Several characteristics of Place Lab made it well-suited for this layered implementation:
- **Link-Free Data Structures**: Each node in Place Lab is largely independent, facilitating easy distribution across the DHT.
- **Independent Beacons**: Information for each beacon is mostly self-contained, allowing for straightforward data decomposition across servers.
- **Redundant Mapping Data**: The significant redundancy in Place Lab's mapping data enables effective masking of transient failures.
- **Self-Healing Capabilities**: The data structures can refresh themselves and recover from failures, making them suitable for deployment on an uncontrolled infrastructure.

#### 6.2 Ease of Deployment

We initially questioned whether building Place Lab’s mapping service on top of OpenDHT simplifies its deployment. This question has two aspects: long-term service deployment and experimental deployment for performance testing.

**Long-Term Deployment**: By leveraging OpenDHT, Place Lab offloads much of the management overhead to the DHT. Each mapping server in Place Lab operates independently, and the third-party DHT provider handles the underlying infrastructure. This reduces the management burden on Place Lab participants, who only need to manage their individual mapping servers and their connection to the DHT.

**Experimental Deployment**: During performance testing, we had to set up our own OpenDHT infrastructure due to the shared nature of the existing service. The maintainers were unwilling to disrupt the service for our experiments. Additionally, we extended the OpenDHT APIs, which would have caused disruptions if integrated into the shared deployment. This is similar to experimenting with Internet protocols, where direct manipulation of the deployed infrastructure is not feasible.

#### 6.3 Flexibility of APIs

Place Lab was built using a narrow set of application-independent APIs. While put/get/remove were the primary interfaces, additional auxiliary APIs were necessary to support the distributed data structures effectively.

Typically, DHTs are designed for "best-effort" performance, lacking concurrency primitives and atomicity guarantees. This suffices for simple rendezvous and storage applications but complicates the construction of more complex data structures. PHTs (Prefix Hash Trees) overcome these limitations by using TTLs and periodic refreshes to handle concurrency issues. Even a simple test-and-set operation (e.g., our `putConditional()` extension) significantly improves PHT performance.

#### 6.4 Performance

While using DHTs simplified the implementation and deployment of Place Lab, it came at the cost of performance. Queries take an average of 2-4 seconds, depending on the input data size. A centralized implementation would eliminate the multiple round trips that contribute to this overhead. Similarly, modifying the underlying DHT routing (e.g., using Mercury) could offer optimization opportunities. This trade-off is inherent in any layered versus monolithic implementation.

Aggressive caching can significantly improve Place Lab’s performance. For infrequently modified PHT data structures, caching the current tree shape can reduce the number of round trips. Applications that can leverage such caching will achieve better performance. Ultimately, whether the performance trade-off is acceptable depends on the application's requirements and user expectations.

### 7. Conclusion

This paper explores the viability of using a DHT service as a general-purpose building block for Place Lab, an end-user positioning system. We investigated the feasibility of layering Place Lab on top of a third-party DHT to minimize deployment and management overhead. Place Lab requires stronger semantics than simple put/get operations, specifically two-dimensional geographic range queries. We designed and evaluated Prefix Hash Trees (PHTs), a multi-dimensional range query data structure layered on top of OpenDHT.

The layered approach allowed us to inherit the robustness, availability, and scalable routing properties of the DHT. Although this simplified the implementation, it came at the expense of performance. Customized DHTs could provide optimizations, but the ease of deployment and management offered by the layered approach is a significant advantage.

### 8. References

[1] Aberer, K. P-Grid: A self-organizing access structure for P2P information systems. In Proc. CoopIS (2001).

[2] Anonymous, et al. OpenDHT: A public DHT service and its uses. Under submission to SIGCOMM 2005.

[3] Aspnes, J., Kirsch, J., and Krishnamurthy, A. Load balancing and locality in range-queriable data structures. In Proceedings of the Twenty-Third ACM Symposium on Principles of Distributed Computing (July 2004).

[4] Aspnes, J., and Shah, G. Skip graphs. In Proc. ACM-SIAM Symposium on Discrete Algorithms (SODA) (2003).

[5] Awerbuch, B., and Scheideler, C. Peer-to-peer systems for prefix search. In Proc. ACM Symposium on Principles of Distributed Computing (PODC) (2003).

[6] Berchtold, S., Ohm, C. B., and Kriegel, H.-P. The Pyramid-Technique: Towards Breaking the Curse of Dimensionality. In Proceedings of International Conference on Management of Data (SIGMOD ’98) (June 1998).

[7] Bhagwan, R., Varghese, G., and Voelker, G. Cone: Augmenting DHTs to support distributed resource discovery. Tech. Rep. CS2003-0755, UC, SanDiego, Jul 2003.

[8] Bharambe, A. R., Agrawal, M., and Seshan, S. Mercury: Supporting scalable multi-attribute range queries. In Proc. SIGCOMM (2004).

[9] Cheng, Y.-C., Chawathe, Y., LaMarca, A., and Krumm, J. Accuracy characterization for metropolitan-scale Wi-Fi localization. In Proceedings of MobiSys ’05 (Seattle, WA, June 2005).

[10] Crainiceanu, A., Linga, P., Gehrke, J., and Shanmugasundaram, J. Querying Peer-to-Peer Networks Using P-Trees. In Proc. WebDB Workshop (2004).

[11] Dabek, F., Kaashoek, M. F., Karger, D., Morris, R., and Stoica, I. Wide-area Cooperative Storage with CFS. In Proceedings of the 18th ACM Symposium on Operating Systems Principles (SOSP 2001) (Lake Louise, AB, Canada, October 2001).

[12] Druschel, P., and Rowstron, A. Storage Management and Caching in PAST, a Large-scale, Persistent Peer-to-peer Storage Utility. In Proceedings of the 18th ACM Symposium on Operating Systems Principles (SOSP 2001) (Lake Louise, AB, Canada, October 2001).

[13] FIPS PUB 180-1. Secure Hash Standard, April 1995.

[14] Freedman, M. J., Freudenthal, E., and Mazières, D. Democratizing Content Publication with Coral. In Proceedings of the 1st Symposium on Networked Systems Design and Implementation (NSDI 2004) (San Francisco, Mar. 2004).

[15] Ganesan, P., Bawa, M., and Garcia-Molina, H. Online balancing of range-partitioned data with applications to peer-to-peer systems. In Proc. VLDB (2004).

[16] Gupta, A., Agrawal, D., and Abbad, A. E. Approximate range selection queries in peer-to-peer systems. In Proc. Conference on Innovative Data Systems Research (CIDR) (2003).

[17] Huebsch, R., Chun, B., Hellerstein, J. M., Loo, B. T., Maniatis, P., Roscoe, T., Shenker, S., Stoica, I., and Yumerefendi, A. R. The Architecture of PIER: An Internet-Scale Query Processor. In Proc. Conference on Innovative Data Systems Research (CIDR) (Jan. 2005).

[18] Jagadish, H. V. Linear clustering of objects with multiple attributes. In Proceedings of ACM SIGMOD International Conference on Management of Data (SIGMOD’90) (May 1990), pp. 332-342.

[19] Karger, D. R., and Ruhl, M. Simple efficient load balancing algorithms for peer-to-peer systems. In Proc. SPAA (2004).

[20] Kubiatowicz, J. Oceanstore: An Architecture for Global-Scalable Persistent Storage. In Proceedings of the ASPLOS 2000 (Cambridge, MA, USA, November 2000).

[21] LaMarca, A., et al. Place lab: Device positioning using radio beacons in the wild. In Proceedings of International Conference on Pervasive Computing (Pervasive) (June 2005).

[22] Lamport, L. The Part-Time Parliament. ACM Transactions on Computer Systems 16, 2 (1998), 133-169.

[23] Mislove, A., Post, A., Reis, C., Willmann, P., Druschel, P., Wallach, D. S., Bonnaire, X., Sens, P., Busca, J.-M., and Arantes-Bezerra, L. POST: A secure, resilient, cooperative messaging system. In Proceedings of the 9th Workshop on Hot Topics in Operating Systems (Lihue, HI, May 2003).

[24] Muthitacharoen, A., Gilbert, S., and Morris, R. Etna: A fault-tolerant algorithm for atomic mutable DHT data. Technical report, Massachusetts Institute of Technology, June 2004.

[25] Muthitacharoen, A., Morris, R., Gil, T., and Chen, B. Ivy: A read/write peer-to-peer file system. In Proc. OSDI (2002).

[26] Oppenheimer, D., Albrecht, J., Vahdat, A., and Patterson, D. Design and Implementation Trade-offs for Wide-area Resource Discovery. In Proceedings of 14th IEEE Symposium on High Performance Distributed Computing (HPDC-14) (Research Triangle Park, NC, July 2005).

[27] Peterson, L., Anderson, T., Culler, D., and Roscoe, T. A Blueprint for Introducing Disruptive Technology into the Internet. In Proceedings of the ACM HotNets-I Workshop (Princeton, NJ, Oct. 2002). See also http://www.planet-lab.org/.

[28] Rowstron, A., Kermarrec, A.-M., Castro, M., and Druschel, P. Scribe: The design of a large-scale event notification infrastructure. In Networked Group Communication, Third International COST264 Workshop (NGC’2001) (Nov. 2001), J. Crowcroft and M. Hofmann, Eds., vol. 2233 of Lecture Notes in Computer Science, pp. 30-43.

[29] Sit, E., Dabek, F., and Robertson, J. UsenetDHT: A low overhead Usenet server. In Proc. of the 3rd IPTPS (Feb. 2004).

[30] Stoica, I., Adkins, D., Zhuang, S., Shenker, S., and Surana, S. Internet Indirection Infrastructure. In Proceedings of the ACM SIGCOMM 2002 (Pittsburgh, PA, USA, August 2002).

[31] Stoica, I., Morris, R., Karger, D., Kaashoek, F., and Balakrishnan, H. Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications. In Proceedings of the ACM SIGCOMM 2001 (San Diego, CA, USA, August 2001).

[32] Tang, C., Xu, Z., and Mahalingam, M. pSearch: Information Retrieval in Structured Overlays. SIGCOMM Comput. Commun. Rev. 33, 1 (2003), 89-94.

[33] Yalagandula, P., and Browne, J. Solving range queries in a distributed system. Tech. Rep. TR-04-18, UT CS, 2004.

[34] Yalagandula, P., and Dahlin, M. A scalable distributed information management system. In Proc. SIGCOMM (2004).