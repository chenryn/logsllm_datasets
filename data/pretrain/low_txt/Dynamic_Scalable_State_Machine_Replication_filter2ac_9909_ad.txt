### Network and Node Configuration
Each node is equipped with two network interfaces. The switches interconnect via a 20 Gbps link. The nodes run CentOS Linux 6.2 (64-bit) with kernel version 2.6.32. Clients are deployed on Dell nodes, while Paxos acceptors and servers are deployed on HP nodes.

### Implementation
We use a B+-tree service to evaluate and compare the SMR, P-SMR, and opt-PSMR techniques. Each entry in the B+-tree consists of an 8-byte integer key, used as the tree index, and an 8-byte value. The service supports all the commands described in Section IV-A. There are two replicas, and each replica is initialized with 10 million keys.

### Performance Analysis
**Figure 4:**
The impact of dependent commands on the performance of SMR, P-SMR, and opt-PSMR. The x-axis shows the percentage of dependent commands in the workload. The following metrics are displayed:
- Maximum throughput in Kilo commands per second (Kcps) (left graph)
- Average latency in milliseconds (bottom-right graph)
- Percentage of failed commands (top-right graph)

### Atomic Multicast Implementation
We implemented atomic multicast using Multi-Ring Paxos [18]. In Multi-Ring Paxos, multicast groups are mapped to one or more Ring Paxos instances [19]. A message can be addressed to a single group only, not to multiple groups. For P-SMR and opt-PSMR, each server thread \( t_i \) in our prototypes belongs to two groups:
- One unique group \( g_i \) that no other thread in the server belongs to.
- One shared group \( g_{\text{all}} \) that every thread in each server belongs to.

#### Safety Check for B+-tree
The safety check function for opt-PSMR in the B+-tree is implemented as follows:
- The key space is range-partitioned among the threads.
- When a thread \( t_i \) receives an `insert(k)` or `delete(k)` operation that is optimistically multicast, it first locates the leaf node \( \alpha \) in the B+-tree where the key will be inserted or deleted.
- Node \( \alpha \)'s parent in the tree points to \( \alpha \), ensuring that any key within the range \( \beta_1 \) to \( \beta_2 \) will be directed to \( \alpha \).
- The safety check passes if:
  1. The insertion or deletion of the key does not cause structural changes in the tree (e.g., the leaf node has space for the insert or will not result in a merge in the case of a delete).
  2. The largest key in the partition assigned to thread \( t_{(i-1)} \) is smaller than \( \beta_1 \).
  3. The smallest key in the partition assigned to thread \( t_{(i+1)} \) is greater than \( \beta_2 \).

Conditions 2 and 3 ensure that no thread other than \( t_i \) will access node \( \alpha \) during the insertion or deletion.

### Experimental Setup
In all experiments, clients select keys uniformly. Each experiment runs for 60 seconds, with the first and last 5 seconds discarded. We perform three sets of experiments:
1. **Cost of Failed Commands in opt-PSMR:**
   - This experiment measures the cost of failed commands in opt-PSMR (see Section V-D). Failed commands pass through the agreement layer twice, potentially increasing latency.
   
2. **Performance with Dependent Commands:**
   - The initial objective behind opt-PSMR is to overcome the inefficiency of P-SMR in executing dependent commands. This experiment varies the percentage of dependent commands in the workload to see if opt-PSMR achieves its goal (see Section V-E).
   
3. **Scalability with Dependent Commands:**
   - With a workload composed of dependent commands only, as the number of threads in P-SMR increases, performance reduces. This experiment compares the performance of opt-PSMR versus P-SMR while varying the number of threads (see Section V-F).

### Impact of Failed Commands on Performance
**Figure 3:**
Shows the effect of failed commands on the latency of opt-PSMR. Each replica has 8 threads, and the workload consists of only insert and delete operations, which are optimistically multicast. In our implementation, when a command fails, a replica notifies the corresponding client to resubmit the command, causing failed commands to traverse the path between the client and the server twice.

- **Top Left Graph:** Fail rate versus throughput. As throughput increases, the fail rate decreases because the growth in the number of failed commands (bottom left graph) is not proportional to the increase in throughput.
- **Right Graph:** Three curves for latency: average latency for all commands, average latency for failed commands, and average latency for passed commands. As expected, the latency of failed commands is approximately twice that of passed commands. Since the number of failed commands is much lower than the number of passed commands, the impact of fails on the average latency is negligible.

### Impact of Dependent Commands on Performance
**Figure 4:**
Shows the maximum performance of SMR, P-SMR, and opt-PSMR with a workload composed of read, insert, and delete operations. Replicas of P-SMR and opt-PSMR contain 8 threads each. In P-SMR, insert and delete operations are multicast to all groups and delivered by all worker threads. In opt-PSMR, these operations are optimistically multicast based on the keys they access.

- **Left Graph:** As the percentage of dependent commands in the workload increases, the throughput of P-SMR decreases and falls below SMR's throughput at about 50%. However, opt-PSMR's throughput remains above SMR's even with 100% dependent commands.
- **Right Top Graph:** Shows the fail rate in opt-PSMR. The low fail rate is the reason opt-PSMR outperforms SMR and P-SMR, regardless of the percentage of dependent commands in the workload.
- **Right Bottom Graph:** Latency of opt-PSMR is slightly higher than that of SMR and P-SMR, mainly due to the higher throughput achieved by opt-PSMR.

### Impact of the Number of Threads on Performance
**Figure 5:**
Shows the scalability of opt-PSMR and P-SMR with a workload composed of 100% dependent commands.

- **Top Left Graph:** Adding more threads to the replicas increases the throughput of opt-PSMR but decreases the throughput of P-SMR. This is because all threads in P-SMR must deliver these commands and synchronize, leading to increased overhead as more threads are added.
- **Bottom Left Graph:** Normalized per-thread throughput. Although opt-PSMR does not achieve perfect scalability due to failed commands, its scalability is better than P-SMR's.
- **Top Right Graph:** Latency values show that opt-PSMR's gain in throughput does not incur high costs on latency.
- **Bottom Right Graph:** CPU usage. opt-PSMR has higher CPU consumption due to the higher number of requests executed.

### Related Work
In Section III, we provided a thorough discussion on parallel state-machine replication and reviewed related work. Here, we review general-purpose approaches for implementing parallel replicas and briefly overview optimistic approaches applied to replication techniques.

#### General-Purpose Approaches
Allowing multiple threads to execute commands concurrently can result in state and output inconsistencies if dependent commands are scheduled differently in two or more replicas. Various approaches have been proposed to enforce deterministic multithreaded execution of commands, such as those in [20], [21], [22], and [23]. These solutions impose performance overheads and may require re-development of the service using new abstractions. Another solution is to allow one of the multithreaded replicas to execute commands non-deterministically and log the execution path, which will be later replayed by the rest of the replicas. Logging and replaying have been mainly developed for debugging and security rather than fault tolerance [24], [25], [26], [27], [28], [29], [30]. These approaches typically have high overhead due to logging and may suffer from inaccurate replay, leading to differences among original and secondary copies.

#### Optimistic Techniques
Optimistic or speculative execution has been suggested as a mechanism to reduce the latency of agreement problems. For example, in [31] and [32], clients are included in the execution of the protocol to reduce the latency of Byzantine fault-tolerant agreement. In [9] and [10], the authors introduce atomic broadcast with optimistic delivery in the context of replicated databases. Similar to [11], the motivation is to overlap the execution of transactions or commands with the ordering protocol, optimistically assuming that the outcome of the agreement layer will comply with the execution order. Our optimistic strategy differs from these approaches in that it only involves clients and replicas and not the agreement layer. Moreover, for some applications, a safety check is sufficient to avoid the need for execution rollbacks, as demonstrated with a B+-tree example.

### Conclusion
State-machine replication is a well-established replication technique extensively discussed in the literature. In this paper, we focused on works that adapt state-machine replication to parallel services. We reviewed existing proposals and compared their architectures. Our comparison showed that among existing techniques, P-SMR has a more scalable architecture, as its design model does not include centralized components. We built on the scalable design of P-SMR and identified its shortcomings, proposing a novel optimistic strategy that significantly boosts its performance.

### References
[1] L. Lamport, “Time, clocks, and the ordering of events in a distributed system,” Communications of the ACM, vol. 21, no. 7, pp. 558–565, 1978.
[2] F. B. Schneider, “Implementing fault-tolerant services using the state machine approach: A tutorial,” ACM Computing Surveys, vol. 22, no. 4, pp. 299–319, 1990.
[3] R. Kotla and M. Dahlin, “High throughput byzantine fault tolerance,” in DSN, 2004.
[4] M. Kapritsos, Y. Wang, V. Quema, A. Clement, L. Alvisi, and M. Dahlin, “Eve: Execute-verify replication for multi-core servers,” in OSDI, 2012.
[5] P. J. Marandi, C. E. Bezerra, and F. Pedone, “Rethinking state-machine replication for parallelism,” ICDCS, 2014.
[6] F. Pedone and A. Schiper, “Optimistic atomic broadcast,” in DISC, 1998.
[7] L. Lamport, “Fast Paxos,” Distributed Computing, vol. 19, no. 2, pp. 79–103, 2006.
[8] A. L. P. F. de Sousa, J. O. Pereira, F. Moura, and R. C. Oliveira, “Optimistic total order in wide area networks,” in SRDS, 2002.
[9] R. Jiménez-Peris, M. Patiño Martínez, B. Kemme, and G. Alonso, “Improving the scalability of fault-tolerant database clusters,” ICDCS, 2002.
[10] B. Kemme, F. Pedone, G. Alonso, and A. Schiper, “Processing transactions over optimistic atomic broadcast protocols,” ICDCS, 1999.
[11] P. J. Marandi, M. Primi, and F. Pedone, “High performance state-machine replication,” DSN, 2011.
[12] T. D. Chandra and S. Toueg, “Unreliable failure detectors for reliable distributed systems,” J. ACM, vol. 43, no. 2, pp. 225–267, 1996.
[13] L. Lamport, “The part-time parliament,” ACM Transactions on Computer Systems, vol. 16, pp. 133–169, May 1998.
[14] H. Attiya and J. Welch, Distributed Computing: Fundamentals, Simulations, and Advanced Topics. Wiley-Interscience, 2004.
[15] A. D. Birrell and B. J. Nelson, “Implementing remote procedure calls,” ACM Transactions on Computer Systems, vol. 2, no. 1, pp. 39–59, 1984.
[16] A. S. Tanenbaum, Distributed operating systems. Pearson Education India, 1995.
[17] N. Santos and A. Schiper, “Achieving high-throughput state machine replication in multi-core systems,” in ICDCS, 2013.
[18] P. J. Marandi, M. Primi, and F. Pedone, “Multi-Ring Paxos,” in DSN, 2012.
[19] P. J. Marandi, M. Primi, N. Schiper, and F. Pedone, “Ring Paxos: A high-throughput atomic broadcast protocol,” in DSN, 2010.
[20] A. Aviram, S.-C. Weng, S. Hu, and B. Ford, “Efficient system-enforced deterministic parallelism,” in OSDI, 2010.
[21] T. Bergan, N. Hunt, L. Ceze, and S. D. Gribble, “Deterministic process groups in DOS,” in OSDI, 2010.
[22] J. Devietti, B. Lucia, L. Ceze, and M. Oskin, “DMP: Deterministic shared memory multiprocessing,” in ASPLOS, 2009.
[23] A. Thomson and D. J. Abadi, “The case for determinism in database systems,” Proc. VLDB Endow., vol. 3, pp. 70–80, Sept. 2010.
[24] G. Altekar and I. Stoica, “ODR: Output-deterministic replay for multi-core debugging,” in SOSP, 2009.
[25] G. W. Dunlap, D. G. Lucchetti, M. A. Fetterman, and P. M. Chen, “Execution replay of multiprocessor virtual machines,” in VEE, 2008.
[26] P. Montesinos, L. Ceze, and J. Torrellas, “Delorean: Recording and deterministically replaying shared-memory multiprocessor execution efficiently,” in ISCA, 2008.
[27] S. Park, Y. Zhou, W. Xiong, Z. Yin, R. Kaushik, K. H. Lee, and S. Lu, “PRES: Probabilistic replay with execution sketching on multiprocessors,” in SOSP, 2009.
[28] M. Ronsse and K. De Bosschere, “Recplay: A fully integrated practical record/replay system,” ACM Trans. Comput. Syst., vol. 17, pp. 133–152, May 1999.
[29] K. Veeraraghavan, D. Lee, B. Wester, J. Ouyang, P. M. Chen, J. Flinn, and S. Narayanasamy, “DoublePlay: Parallelizing sequential logging and replay,” SIGPLAN Not., vol. 47, pp. 15–26, Mar. 2011.
[30] M. Xu, R. Bodik, and M. D. Hill, “A ‘flight data recorder’ for enabling full-system multiprocessor deterministic replay,” in ISCA, 2003.
[31] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong, “Zyzzyva: Speculative Byzantine fault tolerance,” SOSP, 2007.
[32] B. Wester, J. Cowling, E. B. Nightingale, P. M. Chen, J. Flinn, and B. Liskov, “Tolerating latency in replicated state machines through client speculation,” in NSDI, 2009.