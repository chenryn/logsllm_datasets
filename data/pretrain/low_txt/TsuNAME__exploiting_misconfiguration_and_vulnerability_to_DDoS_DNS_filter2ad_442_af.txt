# Figure 15: Google (AS15169) Resolvers on 2020-02-06 During .nz TsuNAME Event
Figure 15 illustrates the time intervals between AAAA queries for Google (AS15169) resolvers during the .nz TsuNAME event.

## References
[46] RIPE NCC. 2021. RIPE Atlas Measurement IDs. https://atlas.ripe.net/measurements/ID, where ID is the experiment ID: New Domain: 25666966, Recurrent: 25683316, One-off-AfterGoogle: 29078085, RecurrentAfterGoogle: 29099244, probe52196: 29491104, TripleDep: 29559226, CNAME: 29560025.

[47] RIPE NCC Staff. 2015. RIPE Atlas: A Global Internet Measurement Network. *Internet Protocol Journal (IPJ)* 18, 3 (Sep 2015), 2-26.

[48] RIPE Network Coordination Centre. 2020. RIPE Atlas. https://atlas.ripe.net.

[49] RIPE Network Coordination Centre. 2020. RIPE Atlas - Raw Data Structure Documentation. https://atlas.ripe.net/docs/data_struct/.

[50] Root Server Operators. 2015. Events of 2015-11-30. http://root-servers.org/news/events-of-20151130.txt.

[51] Root Server Operators. 2020. Root DNS. http://root-servers.org/.

[52] Root Zone File. 2020. Root. http://www.internic.net/domain/root.zone.

[53] Kyle Schomp, Tom Callahan, Michael Rabinovich, and Mark Allman. 2013. On Measuring the Client-Side DNS Infrastructure. In *Proceedings of the 2015 ACM Conference on Internet Measurement Conference*. ACM, 77-90.

[54] SIDN Labs. 2020. ENTRADA - DNS Big Data Analytics. https://entrada.sidnlabs.nl/.

[55] Raffaele Sommese, Leandro Bertholdo, Gautam Akiwate, Mattijs Jonker, van Rijswijk-Deij, Roland, Alberto Dainotti, KC Claffy, and Anna Sperotto. 2020. MAnycast2—Using Anycast to Measure Anycast. In *Proceedings of the ACM Internet Measurement Conference*. ACM, Pittsburgh, PA, USA. https://doi.org/10.1145/3419394.3423646

[56] Suzanne Goldlust. 2018. Using the Response Rate Limiting Feature. https://kb.isc.org/docs/aa-00994.

[57] S. Thomson, C. Huitema, V. Ksinant, and M. Souissi. 2003. DNS Extensions to Support IP Version 6. RFC 3596. IETF. http://tools.ietf.org/rfc/rfc3596.txt

[58] Sipat Triukose, Zakaria Al-Qudah, and Michael Rabinovich. 2009. Content Delivery Networks: Protection or Threat?. In *Computer Security — ESORICS 2009*, Michael Backes and Peng Ning (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 371-389.

[59] Roland van Rijswijk-Deij, Anna Sperotto, and Aiko Pras. 2014. DNSSEC and Its Potential for DDoS Attacks: A Comprehensive Measurement Study. In *Proceedings of the 2014 ACM Conference on Internet Measurement Conference (IMC)*. ACM, 449-460.

[60] Duane Wessels and Marina Fomenkov. 2003. Wow, That’s a Lot of Packets. In *Proceedings of the Passive and Active Measurement Workshop*. https://www.caida.org/publications/papers/2003/dnspackets/wessels-pam2003.pdf

[61] Chris Williams. 2019. Bezos DDoS’d: Amazon Web Services’ DNS Systems Knackered by Hours-Long Cyber-Attack. https://www.theregister.co.uk/2019/10/22/aws_dns_ddos/.

[62] D. Wing and A. Yourtchenko. 2012. Happy Eyeballs: Success with Dual-Stack Hosts. RFC 6555. IETF. http://tools.ietf.org/rfc/rfc6555.txt

[63] S. Woolf and D. Conrad. 2007. Requirements for a Mechanism Identifying a Name Server Instance. RFC 4892. IETF. http://tools.ietf.org/rfc/rfc4892.txt

[64] Maarten Wullink, Giovane CM Moura, Moritz Müller, and Cristian Hesselman. 2016. ENTRADA: A High-Performance Network Traffic Data Streaming Warehouse. In *Network Operations and Management Symposium (NOMS), 2016 IEEE/IFIP*. IEEE, 913-918.

## Additional Tables and Figures
### Figure 15
Figure 15 shows the AAAA queries for Google during the .nz event.

### Figure 16
Figure 16 shows a timeseries of daily queries per AS during the .nz event.

### Influence of Recurrent Queries
In Section 4.1, we established the new domain for problematic resolvers using a one-off configuration. We observed that the volume of queries is time-dependent, and approximately 574 resolvers (out of 11k, or 5.1%) are problematic.

To determine the influence of recurrent queries, similar to what happened with TsuNAME, we set up an experiment where VPs repeat queries every 10 minutes, as shown in Table 3 (recurrent column). To avoid warming up caches, we configured Atlas probes to query unique query names with random values (R in qname).

On the client side, we saw 13k VPs in Table 3, which issued approximately 727k queries for this measurement. Most of these were answered as SERVFAIL, similar to the one-off measurement.

On the authoritative server side (ns1 and ns2 in Table 3), we observed that the servers received approximately 70M queries over the period, resulting in an amplification factor of 99x compared to the queries sent by Atlas VPs.

**Figure 18** shows the timeseries of both queries and unique resolvers reaching our authoritative servers. We see a large oscillation during the period in which Atlas is active, with anywhere from 1k to 8k resolvers active at any moment (Figure 18b). The reasons for this are twofold: some resolvers are indeed in a loop, and our setup sends new queries every 30 minutes (Table 3). Similar to Section 4.1, once Atlas stops sending queries, we still see a portion of resolvers staying in the loop. For this measurement, we found 1423 resolvers from 192 ASes (Table 4) that are in the loop mode.

**Figure 19** shows the top 10 ASes sending queries to our authoritative servers when Atlas stopped, i.e., they should not have sent any queries. We see that Google did roughly 60% of the queries again, but other ASes have the same issue.

**Figure 20** shows the IQR and queries for the top 50 resolvers in terms of query volume, that send A records queries for a server for a qname in cyclic dependency. Compared with the one-off case (Figure 7), we see a very similar pattern, except for the volume of queries, which is larger due to the longer measurement duration.

The conclusion we can draw from this experiment is that more client incoming queries will further amplify the number of queries experienced by authoritative servers.

### Stopping the Sinkhole Experiment
We stop the sinkhole experiment in two steps:
1. We return these domains to their original, sinkholed NS records, as in the Pre phase. We do this by changing the NS records in the parent DNS zone (.nl). In theory, this should redirect all clients to the newly configured NS records. However, we still receive queries on the "old" server (AWS Route 53) — referred to as child-centric resolvers [55], as they trust the information of the child zone delegation over the parent.
2. We fully stop the experiment at 20:18 UTC by removing the zones from AWS Route 53 (Delegation removed phase). After that point, all clients of this domain query the NS records of the Pre phase, which we do not monitor.

### Figure 16
Figure 16 shows the .nz event: queries per AS.

### Figure 17
Figure 17 shows the resolvers with at least 100x traffic growth for AAAA queries in the sinkhole experiment.

### Table 11
Table 11 lists the top ASes per volume of queries during experiments and the .nz event.

### Longer and CNAME Cycles
#### Triple Cyclic Dependency
In the TripleDep measurement (Table 3), we configured a triple cyclic dependency (Table 12) to determine if resolvers would also be vulnerable and what the impact would be compared to regular cyclic dependencies.

Compared with the New Domain experiment (Figure 5), we see that the query rates reduce very little after Atlas stops sending queries (> 8:45). However, only a fraction of resolvers remain active after Atlas stops sending queries (Figure 21b).

**Figure 22** shows the top 10 ASes for this experiment. It is similar to the ASes from the New Domain Experiment (Figure 6), except that GDNS (AS15169) has been fixed, reducing the number of queries.

**Figure 21** shows the timeseries results. We see that, differently from normal cyclic dependent domains, a triple cyclic dependency query volume does not reduce as fast as the previous ones. Thus, making longer cycles will make the problem even worse.

#### CNAME Cycles
We also ran an experiment with loops done with CNAME records, which are like 'aliases' for a domain. We configured the CNAME experiment in Table 3, where we set up cyclic CNAMES:

### Impact of Google Public DNS Mitigation
We worked together with Google to understand the CycleHunter vulnerability. We determined that Google would not loop by itself, but its client population would. Given that Google did not cache cyclic dependent records, queries from looping clients were amplified and sent to authoritative servers.

On February 3rd, 2021, Google mitigated this vulnerability on their Public DNS services by implementing a cyclic dependent detector and caching such records, so once it was cached, it would not pass along any client queries, blocking the effects of looping downstream resolvers.

#### Repeating Lower-Bound Experiment
In Section 4.1, we configured approximately 10k Atlas probes to send 1 query each to their local resolvers to measure the lower-bound of amplification. We repeated this experiment to determine how much of a problem remains after Google's mitigation.

Table 13 shows the cyclic dependency we configured — third-level domains not used before. We deleted the record on Wednesday, February 10th, 2021, at 08:30 UTC, after keeping the cyclic dependency active for 12 hours and 30 minutes.

**Figure 24** shows the results. We see that, altogether, after no more user queries, the authoritative servers received approximately 88k queries/5min combined (both zones, in Figure 24a). Previously, this value in Figure 5 was around 135k queries/5min, a 35% reduction in the total query volume. We also found 1560 problematic resolvers (Figure 24b), which are unique IP addresses sending queries after the initial round of queries from the Atlas probes. This number is larger than the one from Figure 5, but they generate fewer queries.

Table 14 shows the details of this measurement. We see that 18.5k VPs sent 18.6k queries, resulting in 12.3M queries at the authoritative server over the 12.5h measurement duration.

**Figure 25** shows the top 10 ASes for this experiment. Compared with before the mitigation (Figure 6), Google significantly reduced its volume of queries, from approximately 4.5M to 400k (90%), even though this measurement lasted for 12.5h instead of 6.

#### Repeating Recurrent Queries Measurement
Next, we repeated the measurement with recurrent queries from Appendix B, after Google's mitigation. Table 15 shows the cyclically dependent zones we configured.

**Figure 23** shows the timeseries of queries. We see that most resolvers detect the cycle and do not begin to loop.

### Figures and Tables
- **Figure 15**: Time intervals between AAAA queries for Google (AS15169) resolvers during the .nz TsuNAME event.
- **Figure 16**: Timeseries of daily queries per AS during the .nz event.
- **Figure 17**: Resolvers with at least 100x traffic growth for AAAA queries in the sinkhole experiment.
- **Table 11**: List of top ASes per volume of queries during experiments and the .nz event.
- **Figure 21**: Timeseries of queries and unique resolvers querying authoritative servers (5min bins).
- **Figure 18**: Recurrent: queries and unique resolvers timeseries (5min bins).
- **Figure 19**: Recurrent experiment: queries per AS with problematic resolvers.
- **Figure 20**: Recurrent: IQR and queries for A records of ns.vuur.cachetest.net.
- **Figure 22**: Top 10 ASes for the TripleDep experiment.
- **Figure 23**: CNAME measurement: queries and unique resolvers querying authoritative servers (5min bins).
- **Figure 24**: RIPE Atlas: queries and unique resolvers querying authoritative servers (5min bins).
- **Figure 25**: One-off-AfterGoogle: top 10 ASes by query volume with problematic resolvers.
- **Table 13**: Cyclic dependency for new one-off measurement.
- **Table 15**: Cyclic dependency for new one-off measurement.