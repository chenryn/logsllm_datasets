### Producing Trusted Systems: Evaluation Against Established Standards

Beginning in the late 1970s, several funded and commercial efforts were initiated to produce trusted systems or tools for the construction and verification of such systems. Consequently, the Trusted Computer System Evaluation Criteria (TCSEC) or a similar standard needed to be published, ideally no later than the 1980s.

Unfortunately, there was a shortage of adequately educated and experienced developers of trusted operating systems. While several laboratory prototypes existed, only Multics (AIM) had a developed user community, and it was the only robust security product on the open market. Multics provided both multilevel security capabilities and a structured set of advanced integrity controls. However, it was not widely available, its hardware base was less popular than the more cost-effective IBM or DEC mainframes, and its user interface was not as user-friendly as the increasingly popular, but vulnerable, UNIX.

Another unfortunate consequence was the lack of experienced trusted system developers who were willing and able to serve as evaluators. Many preferred to create products rather than "look over someone else’s shoulder." The lengthy and overly cautious evaluation and interpretation process ultimately discouraged vendor participation in trusted product development, largely due to the uncertainty of the costs and time associated with getting a product evaluated.

We failed to consult experienced procurement officers to review our wording, and no one aggressively ensured that we had written a sufficient and complete glossary of technical terms and concepts. Indeed, the TCSEC’s glossary was something of an afterthought and did not receive the careful attention given to the main body of the text. This oversight significantly contributed to the lengthy interpretation process.

Another significant problem was our neglecting to document what we considered obvious: that not all features and assurances were created equal. For example, if we had stated that individual accountability under Discretionary Access Control (DAC) is less significant than assured individual accountability under Mandatory Access Control (MAC), many bitter and divisive debates could have been avoided, and possibly more A1 products would have been produced.

I do not question the wisdom of our decision to limit the TCSEC to its seven all-or-nothing classes rather than adopting the "Chinese-menu" approach that was advocated at the time. I believe this was the right decision. In that sense, I consider the TCSEC to be an improvement over the criteria created afterward, particularly the swollen and confusing Common Criteria with its extensible myriad of Protection Profiles. While this puts the interpretation in front of the evaluation, it also has the capacity to produce a large number of slightly different policies or assurances, which are difficult for sophisticated consumers to compare or accurately comprehend.

### Current State of the Industry

A generation after the debut of the DoD Computer Security Initiative and the publication of the TCSEC, there are essentially no commercially available trusted systems in use offering protection equivalent to a B2 Multics or the A1 M-component GEMSOS. Instead, there are bloated, untested, feature-laden, interoperating, untrustworthy, less-than-C2 products that are self-penetrating. Their alleged kernels consist of millions of lines of highly privileged code written by teams of people who have never met their coding counterparts. The illusion of system security is provided by software encryption algorithms that can often be coaxed to reveal their keys to a skilled interloper. Add-on security gadgets, such as pattern-matching virus scanners and restrictive firewalls, belie the vendors' claims of mature security architectures. Periodic announcements of urgent multi-megabyte security patches only emphasize the tawdry state of today’s commercial offerings.

Never has compromising a system been easier! Never have so many effective penetration tools been provided off-the-shelf by the vendor to the would-be interloper!

### Electronic Voting Systems

One cannot help but comment adversely on the current issue of electronic touch-screen voting systems. In at least one state, Maryland, the only legal way to vote is on a system that uses cryptography for some aspects of secrecy, but which is implemented on a version of Windows CE—a foundation that would not meet the unexacting standards of the TCSEC C1 class. Attacks against Windows operating system variants are commonplace, and the vendor's flagship C2 systems (NT and 2000) require regular security patching due to Internet malware, with no one questioning the presence of its gaping Active Desktop and other inviting security vulnerabilities. Several security studies have identified voting system security flaws, and some could be exploited through a prepared attack. The fact that there is no permanent and immutable audit trail and recovery system has been discussed and dismissed by the manufacturer and the state election board. Most recently, the Maryland court system has dismissed concerns over the machines' security on the grounds that the system is not going to be connected to hackers and need not withstand "military-style attacks," thus questioning the existence of a security threat.

### Future Directions

The TCSEC was written and emended by skilled computer security practitioners of the late 1970s and early 1980s. The derivative criteria, though written by large committees of skilled personnel, reflect the fact that they were written by committee, with the goal of harmonizing protection philosophies rather than establishing more focused requirements and guidelines. It is doubtful that any vendor will produce a completely new operating system for the current internetworked environment. For commercial viability, operating systems need to accommodate everything from real-time wireless gaming to play-on-demand multimedia presentations. With technology moving computer usage away from previous trends (i.e., computation and data processing), a new paradigm is needed for security engineering in today’s environment. Returning to basics no longer seems practicable.

One can legitimately ask whether there is yet a perceived, validated security requirement.

### Acknowledgements

Many people encouraged and helped with the writing of this paper. I would like to thank Dan Thomsen, LouAnna Notargiacomo, Steve Greenwald, and Ken Olthoff for their continuing encouragement and critiques. In particular, I am especially indebted to LouAnna, who took extraordinary steps to ensure the paper’s timely completion. I received valuable assistance in reconstructing the past from Rich Graubart, Ronda Henning, Paul Karger, Ted Lee, Peter Neumann, Roger Schell, and Tom van Vleck. Thank you, dear friends!

### References

[1] Abbott, Bob, J. Chin, J. Donnelley, W. Konigsford, S. Tokubo, and D. Webb, “Security Analysis and Enhancements of Computer Operating Systems,” Technical Report NBSIR 76-1041, ICET, National Bureau of Standards, 1976.

[2] Anderson, James P., Computer Security Planning Study, Electronic Systems Division, USAF Report ESD-TR-73-51 in two volumes.

[3] Bell, D. Elliott, and L. J. LaPadula, “Secure Computer System: Unified Exposition and Multics Interpretation,” Tech. Report MTR-2997 Rev 1, MITRE Corp., March 1975.

[4] Boebert, Earl, “On the Inability of an Unmodified Capability Machine to Enforce the *-Property,” Proc. 7th DOD/NBS Computer Security Conf., 1984.

[5] Brand, Sheila, ed., Trusted Computer System Evaluation Criteria, Final Draft, 27 January 1983, 109 pp. as C1-FEB-83-S3-25366. DoD Computer Security Center.

[6] Corbató, F. J., and V. A. Vyssotsky, “Introduction and Overview of the Multics System,” 1965 Fall Joint Computer Conference.

[7] Department of Defense, Trusted Computer System Evaluation Criteria, DoD 5200.28-STD, 26 December 1985.

[8] DoD Computer Security Center, Trusted Computer System Evaluation Criteria, Draft, 24 May 1982, 43 pp.

[9] DoD Computer Security Center, Trusted Computer System Evaluation Criteria, 15 August 1983, 117 pp, as CSC-STD-001-83.

[10] Harrison, M., W. Ruzzo, and J. Ullman, “Protection in Operating Systems,” Comm. ACM, vol. 19, no. 8, 1977.

[11] Demillo, R.A., R. J. Lipton, A. J. Perlis, “Social Processes and Proofs of Theorems and Programs,” Comm. ACM, Vol. 22, No. 5, 1979.

[12] Frantz, Bill, Norm Hardy, Jay Jonekait, Charlie Landau, GNOSIS: A Prototype Operating System for the 1990’s, Tymshare, Inc., 1979.

[13] Graham, G.S. and P.J. Denning, “Protection – Principles and Practice,” Spring Joint Computer Conference, AFIPS Conf. Proc., 1972.

[14] Jelen, George F., Information Security: an Elusive Goal, Program on Information Resources Policy, Harvard University Center for Information Policy Research, April 1984.

[15] Lee, Theodore M. P., “Processors, Operating Systems and Nearby Peripherals: A Consensus Report,” appearing as Section 8 of Ruthberg, op. cit., 1980.

[16] Lipner, Stephen B., A Comment on the Confinement Problem, Proc. 6th Symp. Operating Systems Principles, 1975.

[17] McLean, John, “Reasoning About Security Models,” Proc 1987 IEEE Symp. Security and Privacy, Apr. 1987.

[18] Millen, Jonathan K., “Security Kernel Validation in Practice,” Comm. ACM, vol. 19, no. 5 (May 1976), pp. 243-250.

[19] Neumann, Peter, Larry Robinson, Karl Levitt, R.S. Boyer, and A.R. Saxena, “A Provably Secure Operating System: Final Report,” Stanford Research Institute Report, June 1975.

[20] Nibaldi, Grace H[ammond], Proposed Technical Evaluation Criteria for Trusted Computer Systems, MITRE Report, M-79-225, 25 October 1979.

[21] Ruthberg, Zella, Audit and Evaluation of Computer Security II: System Vulnerabilities and Controls, NBS Special Publication No. 500-57, MD78733, April 1980.

[22] Schaefer, Marvin., “Symbol Security Condition Considered Harmful,” Proceedings 1989 IEEE Computer Society Symposium on Security and Privacy, pp. 20-46, May 1-3, 1989.

[23] Schaefer, Marvin, W. C. Barker, C. P. Pfleeger, “Tea and I: an Allergy,” Proceedings 1989 IEEE Computer Society Symposium on Security and Privacy, pp. 178-182, May 1-3, 1989.

[24] Schaefer, Marvin, R. R. Linde, et al., “Program Confinement in KVM/370,” in Proc. ACM National Conference, Seattle, October, 1997.

[25] Vyssotsky, V.A., F. J. Corbató, and R.M. Graham, “Structure of the Multics Supervisor,” AFIPS Conf Proc., vol. 27, part I, 1965.

[26] Walter, K. G, W. Ogden, F. Bradshaw, S. Ames, and D. Shumway, “Primitive Models for Computer Security, ESD-TR-74-117, Air Force ESD, Hanscom AFB, Mass, 1974.

[27] Ware, Willis H., ed. Security Controls for Computer Systems: Report of Defense Science Board Task Force on Computer Security, R-609-1, reissued by the RAND Corporation, 1979.

[28] Weissman, Clark. Security Controls in the ADEPT-50 Time Sharing System. In AFIPS Conference Proceedings, volume 35, New Jersey, 1969.

[29] Karger, P.A. and A.J. Herbert. “An Augmented Capability Architecture to Support Lattice Security and Traceability of Access.” in Proceedings of the 1984 Symposium on Security and Privacy, pp. 2-12, 29 April - 2 May 1984.

[30] National Security Agency Trusted Product Evaluation Report, Gemini Trusted Network Processor (GTNP), available at http://www.radium.ncsc.mil/tpep/epl/entries/CSC-EPL-94-008.html.