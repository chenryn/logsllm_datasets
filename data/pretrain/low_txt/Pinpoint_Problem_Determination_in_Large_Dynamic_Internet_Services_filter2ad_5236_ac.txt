# Dependency and Detection Analysis

## Overview
This section provides an in-depth analysis of the performance of three techniques: Dependency Analysis, Detection Analysis, and Pinpoint Cluster Analysis. The figures presented below illustrate the accuracy and false positive rates for these techniques under different fault conditions.

### Figures
- **Figure 3**: Accuracy vs. False Positive Rate for Single-Component Faults
- **Figure 5**: Dependency’s Accuracy vs. False Positive Rate for Interacting Component Faults
- **Figure 6**: Detection’s Accuracy vs. False Positive Rate for Interacting Component Faults

### Performance Metrics
- **False Positive Rate (1 - Precision)**
- **Accuracy**

| Technique | False Positive Rate | Accuracy |
|-----------|---------------------|----------|
| Dependency Analysis | 0.2, 0.4, 0.6, 0.8, 1 | 0.2, 0.4, 0.6, 0.8, 1 |
| Detection Analysis | 0.2, 0.4, 0.6, 0.8, 1 | 0.2, 0.4, 0.6, 0.8, 1 |
| Pinpoint Cluster Analysis | 0.2, 0.4, 0.6, 0.8, 1 | 0.2, 0.4, 0.6, 0.8, 1 |

### Analysis of Latent Faults
To better understand how the three techniques perform under latent faults—faults that occur but are not manifested as failures until a later component is used—we show their ROC curves as the "fault length" increases. Fault length is defined as the number of components interacting to cause a failure.

- **Pinpoint Cluster Analysis**:
  - For single-component faults, Pinpoint has very high accuracy and precision.
  - As latent faults are introduced, Pinpoint's ROC curve deteriorates, but it still outperforms Detection and Dependency Analysis.

- **Dependency Analysis**:
  - Results are not significantly affected by fault length.
  - Consistently high accuracy (up to 100%), but low precision (about 15%).

- **Detection Analysis**:
  - Heavily affected by fault length.
  - High precision (about 30%) but variable accuracy, ranging from 50% for single-component faults to 0% for three or more component faults.

## Performance Impact
We compared the throughput of the PetStore application hosted on an unmodified J2EE server with our version with logging enabled. We measured a cold server with a warm file cache for three 5-minute runs. The online overhead of Pinpoint was found to be 8.4%. The uncompressed trace files generated by Pinpoint average about 2.5k per request, but when compressed, they average only 100 bytes per request.

## Discussion

### Pinpoint Limitations
- **Tightly Coupled Components**:
  - Pinpoint cannot distinguish between sets of tightly coupled components that are always used together. This results in reporting a superset of the actual faults.
  - Potential solution: Create synthetic requests to exercise components in different combinations, similar to achieving good code coverage in test cases.

- **State Corruption**:
  - Pinpoint does not handle faults that corrupt state and affect subsequent requests.
  - Potential solution: Extend tracing to include shared state, such as database tables, to identify which sets of components share state.

- **Deterministic Failures**:
  - Pinpoint cannot differentiate deterministic failures due to pathological inputs from other failures.
  - Potential solution: Record the requests themselves to use as another factor in differentiating failed requests from successful ones.

- **Fail-Stutter Faults**:
  - Pinpoint does not capture "fail-stutter" faults where components mask internal faults and exhibit only a decrease in performance.
  - Potential solution: Use timing information to detect fail-stutter faults and perform problem determination.

### Application Observations
- In the J2EE PetStore application, the average number of application components used in requests for static pages is 3.
- For dynamic pages, the average is 14.2, with a median of 14 and a maximum of 23.
- The large number of components used in requests motivates the need for monitoring at the middleware layer and the importance of automated problem determination techniques.

### Related Work
- **Event Correlation Systems**:
  - Extensive literature exists on event correlation systems, primarily in the context of network management.
  - Commercial service management systems, such as HP’s OpenView, IBM’s Tivoli, and Altaworks’ Panorama, use expert systems with rules or dependency models.
  - Recent research focuses on automatically generating dependency models, but these approaches often require extensive knowledge and can be intrusive.

- **Future Work**:
  - Investigate additional factors and trade-offs affecting accuracy and precision.
  - Explore ways to loosen the assumption of request independence by tracking state sharing.
  - Address scaling issues and extend tracing across machine boundaries.
  - Automate statistical analysis and integrate with an alert system for online analysis of live systems.
  - Integrate Pinpoint with recovery-oriented computing techniques to reduce mean time to recovery (MTTR).

## Conclusions
This paper presents a new problem determination framework, Pinpoint, which provides high accuracy in identifying faults and produces relatively few false positives. Pinpoint requires no application-level knowledge, making it suitable for large and dynamic systems. It traces requests, detects component failures, and performs data clustering analysis to determine the likely causes of failures. This approach is a significant improvement over existing fault management techniques that require extensive system knowledge.

## Acknowledgements
We are grateful to Aaron Brown, George Candea, Kim Keeton, Dave Patterson, and the anonymous reviewers for their valuable suggestions.

## References
- [1] Network Packet Capture Facility for Java. http://jpcap.sourceforge.net/.
- [2] TPC-W Benchmark Specification. http://www.tpc.org/wspec.html.
- [3] Altaworks. Panorama. http://www.altaworks.com/product/panorama.htm.
- [4] A. Bouloutas, S. Calo, and A. Finkel. Alarm Correlation and Fault Identification in Communication Networks. IEEE Transactions on Communication, 42(2/3/4), 1994.
- [5] A. Brown and D. Patterson. An Active Approach to Characterizing Dynamic Dependencies for Problem Determination in a Distributed Environment. In Seventh IFIP/IEEE International Symposium on Integrated Network Management, Seattle, WA, May 2001.
- [6] J. Choi, M. Choi, and S. Lee. An Alarm Correlation and Fault Identification Scheme Based on OSI Managed Object Classes. In IEEE International Conference on Communications, Vancouver, BC, Canada, 1999.
- [7] G. Corporation. Google. http://www.google.com/.
- [8] H. Corporation. HotMail. http://www.hotmail.com/.
- [9] H. P. Corporation. HP Openview. http://www.hp.com/openview/index.html.
- [10] David Patterson et al. Recovery Oriented Computing (ROC): Motivation, Definition, Techniques, and Case Studies. Technical Report CSD-02-1175, UC Berkeley Computer Science, 2002.
- [11] J. Gray. Dependability in the Internet Era. http://research.microsoft.com/~gray/talks/InternetAvailability.ppt.
- [12] A. Group. Log4j Project, 2001. http://jakarta.apache.org/log4j.
- [13] B. Gruschke. A New Approach for Event Correlation based on Dependency Graphs. In 5th Workshop of the OpenView University Association: OVUA’98, Rennes, France, April 1998.
- [14] J. L. Hennessy and D. A. Patterson. Computer Architecture: A Quantitative Approach. Morgan Kaufmann, third edition, 2002. Chapter 8.12.
- [15] HP. e-Speak, 2001. http://www.e-speak.hp.com/.
- [16] IBM. Tivoli Business Systems Manager, 2001. http://www.tivoli.com.
- [17] V. Jacobson, C. Leres, and S. McCanne. tcpdump, 1989. ftp://ftp.ee.lbl.gov/.
- [18] A. K. Jain and R. C. Dubes. Algorithms for Clustering Data. Prentice-Hall, 1988.
- [19] M. J. Katchabaw, S. L. Howard, H. L. Lutfiyya, A. D. Marshall, and M. A. Bauer. Making distributed applications manageable through instrumentation. The Journal of Systems and Software, 45(2):81–97, 1999.
- [20] Microsoft. .NET, 2001. http://www.microsoft.com/net/.
- [21] S. Microsystems. Java Pet Store 1.1.2 Blueprint Application, 2001. http://developer.java.sun.com/developer/sampsource/petstore/petstore1_1%_2.html.
- [22] D. Oppenheimer and D. A. Patterson. Architecture operation and dependability of large-scale Internet services. Submission to IEEE Internet Computing, 2002.
- [23] H. C. Romesburg. Cluster Analysis for Researchers. Lifetime Learning Publications, 1984.
- [24] I. Rouvellou and G. W. Hart. Automatic Alarm Correlation for Fault Identification. In INFOCOM, pages 553–561, 1995.