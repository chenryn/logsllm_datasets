# Examining Failures and Repairs on Supercomputers with Multi-GPU Compute Nodes

**Authors:** Amir Taherin, Tirthak Patel, Giorgis Georgakoudis, Ignacio Laguna, Devesh Tiwari

**Conference:** 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)

**DOI:** 10.1109/DSN51789.2021.0000000

**Copyright:** © 2021 IEEE

## Abstract
Understanding the reliability characteristics of supercomputers has been a key focus of the HPC and dependability communities. However, there is no current study that analyzes both the failure and recovery characteristics over multiple generations of a GPU-based supercomputer with multiple GPUs on the same node. This paper bridges that gap and reveals surprising insights based on monitoring and analyzing the failures and repairs on the Tsubame-2 and Tsubame-3 supercomputers.

## I. Introduction
High-performance computing (HPC) system reliability has been a major area of research for several decades. The primary driving factor has been the need to provide sustained reliability for long-running applications executing on multiple nodes. This line of research has resulted in making CPUs more reliable over time, and now GPUs as well, as they have become mainstream for supercomputing. While there have been multiple field studies about GPU and CPU errors, they are largely focused on a single production-scale supercomputer. There is no existing study that shares the experience and lessons learned from GPU-accelerated supercomputers over multiple generations. Furthermore, previous studies on GPU-accelerated supercomputers have included only one GPU per node and are limited to NVIDIA K80 or older GPUs.

In this study, we examine two generations of Tsubame supercomputers (employing NVIDIA K20X and P100 GPUs). Importantly, each node has multiple GPU cards, which results in previously unobserved failure characteristics and creates opportunities for further innovation. Additionally, this study highlights the need for optimizing the time to recovery from failure—an aspect that has not received sufficient discussion and attention from previous field studies. We show that the time to recovery is now becoming an important concern and a key metric for system operations. Innovative solutions are needed to reduce the time to recovery and, in turn, minimize the impact of failures on system operations.

Overall, our major findings and implications include:
- **GPU Reliability:** As expected, GPUs are one of the most critical components in these GPU-accelerated supercomputers from a reliability perspective. Contrary to other GPU deployments, we find that the hardware reliability of NVIDIA GPUs has improved remarkably over the generations (up to 4× improvement in overall system MTBF). However, GPU-related software and firmware failures (e.g., GPU driver issues) remain a concern and require further research investment.
- **Performance-Error-Proportionality:** We introduce a new term, "performance-error-proportionality," to encourage the systems community to jointly capture the effects of raw computing power and failure rate for benchmarking: "useful work done per failure-free period" (e.g., total FLOP per MTBF).
- **Software Failures:** We found that software failures are becoming the dominant type of failure in supercomputers. Alarmingly, the cause or type of a large fraction of these software failures is not known and is difficult to reproduce.
- **Multi-Accelerator Per Node:** As we move toward multi-accelerator-per-node supercomputers, system operators need to be wary of multiple GPUs failing simultaneously, and the failure distribution within a node being non-uniform and temporally correlated.
- **Mean Time to Recovery:** While the mean time between failures has improved drastically over the generations, we find that the mean time to recovery remains largely similar, i.e., the time to quickly heal from a failure is not improving at all. Each failure disrupts the system for roughly the same amount of time. Our failure type and seasonal analysis shows that the time to recovery trends vary across failure types and are not necessarily strongly correlated by the failure density in a particular time frame.

Our analysis tool and failure logs are available open-source at: [http://doi.org/10.5281/zenodo.4606221](http://doi.org/10.5281/zenodo.4606221).

## II. Tsubame Supercomputer Background and Analysis Methodology
Tsubame is a series of large-scale computing facilities housed at the Global Scientific Information and Computing Center (GSIC) at Tokyo Institute of Technology. Tsubame-1 was announced in 2006 as the then most powerful supercomputer in Japan, leveraging specific accelerators from ClearSpeed. Tsubame-2 was introduced in 2010 with 1408 nodes, reaching a theoretical peak (Rpeak) of 2.3 PFlop/s and power consumption of 1.4 MW. In 2017, Tsubame-3 was announced for Artificial Intelligence applications, reaching a theoretical peak (Rpeak) of 12.1 PFlop/s with power consumption of 792 kW.