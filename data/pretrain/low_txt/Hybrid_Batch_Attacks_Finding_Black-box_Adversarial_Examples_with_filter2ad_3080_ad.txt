### Universal Local Ensemble

The results above validate our hypothesis that the distinct attack surfaces of robust and normal models contribute to the ineffectiveness of attacks against the robust CIFAR10 model, as shown in Table 3. To achieve better performance, the attacker should selectively choose local models based on the target model type. However, in practice, attackers may not know whether the target model is robustly trained, making it difficult to predetermine the best local models.

We did not repeat the experiments with robust MNIST local models because, without separately training robust local models, we can significantly improve the attack performance by tuning the local models during the hybrid attack process (see Table 6 in Section 4.6). The tuning process transforms the normal local models into more robust ones (details in Section 4.6).

**Figure 1: Transfer rates of different local ensembles.**
- **Normal-3 Ensemble:** Composed of three normal models (NA, NB, NC).
- **Robust-2 Ensemble:** Composed of two robust models (R-DenseNet and R-ResNet).
- **All-5 Ensemble:** Composed of all five local models.
- **Transfer rate:** Measured on independently sampled test images and averaged over five runs.

Next, we explore whether a universal local model ensemble exists that works well for both normal and robust target models.

To find the best local ensemble, we tested all 31 different combinations of the five local models (three normal and two robust) and measured their direct transfer rates against both normal and robust target models. Figure 1 reports the transfer rates for each local ensemble against both types of target models. For clarity, we only include results for the five individual models and four representative ensembles:
- **NA-NB Ensemble:** Represents a mediocre case.
- **Robust-2 Ensemble:** Composed of the two robust models (R-DenseNet and R-ResNet).
- **Normal-3 Ensemble:** Composed of three normal models (NA, NB, NC).
- **All-5 Ensemble:** Composed of all five local models.

None of the ensembles we tested had high direct transfer rates against both normal and robust target models. Ensembles with good performance against robust targets performed poorly against normal targets (e.g., Robust-2 has a 37.8% transfer rate to robust targets but only 18.1% to normal targets), and vice versa (e.g., Normal-3 has a 65.6% transfer rate to normal targets but only 9.4% to robust targets). Some ensembles, like NA-NB, performed moderately against both types of targets.

One possible reason for the failure of ensembles to apply to both types of targets is that when white-box attacks are applied to mixed ensembles, the attacks still "focus" on the normal models, which are easier to attack. This bias towards normal models makes the candidate adversarial examples less likely to transfer to robust target models. This conjecture is supported by the observation that although mixtures of normal and robust models mostly fail against robust target models, they still have reasonable transfer rates to normal target models (e.g., the ensemble of five local models has a 63.5% transfer rate to normal CIFAR10 target models but only 9.5% to robust target models). It might be interesting to explore if one can explicitly enforce the attack to focus more on the robust model when attacking the mixture of normal and robust models.

In practice, attackers can dynamically adapt their local ensemble based on observed results, trying different local ensembles against a particular target for the first set of attempts and measuring their transfer rates, then selecting the one that worked best for future attacks. This simulation process adds overhead and complexity to the attack but may still be worthwhile given the significant variations in transfer success rates for different local ensembles.

For our subsequent experiments on CIFAR10 models, we use the Normal-3 and Robust-2 ensembles, as these give the highest transfer rates to normal and robust target models, respectively.

### 4.6 Local Model Tuning

To test the hypothesis that labels learned from optimization attacks can be used to tune local models, we measure the impact of tuning on the local models' transfer rate.

During black-box gradient attacks, two types of input-label pairs are generated:
1. **Random Noise Pairs:** Produced by adding small magnitudes of random noise to the current image to estimate target model gradients.
2. **Perturbed Image Pairs:** Generated by perturbing the current image in the direction of estimated gradients.

We use only the latter type of pairs, as they contain richer information about the target model boundary. These by-products of the black-box attack search can be used to retrain the local models (line 15 in Algorithm 1). The newly generated image and label pairs are added to the original training set to form the new training set, and the local models are fine-tuned on the new training set. As more images are attacked, the training set size can quickly grow. To avoid this, when the size of the new training set exceeds a certain threshold \( c \), we randomly sample \( c \) of the training data and conduct fine-tuning using the sampled training set. For MNIST and CIFAR10, we set the threshold \( c \) as the standard training data size (60,000 for MNIST and 50,000 for CIFAR10). At the beginning of the hybrid attack, the training set consists of the original seeds available to the attacker with their ground-truth labels (i.e., 1,000 seeds for MNIST and CIFAR10, as shown in Section 4.2).

Algorithm 1 shows the local model being updated after every seed, but considering the computational cost required for tuning, we only update the model periodically. For MNIST, we update the model after every 50 seeds; for CIFAR10, we update after 100 seeds. We were not able to conduct the tuning experiments for the ImageNet models due to the high cost of each attack and retraining. To check the transferability of the tuned local models, we independently sample 100 unseen images from each of the 10 classes, use the local model ensemble to find candidate adversarial examples, and test the candidate adversarial examples on the black-box target model to measure the transfer rate.

We first test whether the local model can be fine-tuned by the label by-products of baseline gradient attacks (Baseline attack + H2) by checking the transfer rate of local models before and after the fine-tuning process. We then test whether the attack efficiency of hybrid attacks can be boosted by fine-tuning local models during the attack process (Baseline attack + H1 + H2) by reporting their average query cost and attack success rate. The first experiment helps us check the applicability of H2 without worrying about possible interactions between H2 and other hypotheses. The second experiment evaluates how much attackers can benefit from fine-tuning the local models in combination with hybrid attacks.

**Table 5: Impact of tuning local models on transfer rates (Baseline + Hypothesis 2)**
- **MNIST (N, t):** Targeted attacks on normal models.
- **MNIST (R, u):** Untargeted attacks on robust models.
- **CIFAR10 (N, t):** Targeted attacks on normal models.
- **CIFAR10 (R, u):** Untargeted attacks on robust models.
- **Transfer rate:** Measured on independently sampled test images and averaged over five runs.

For the MNIST model, we observe increases in the transfer rate of local models by fine-tuning using the byproducts of both attack methods. The transfer rate increases from 60.6% to 77.9% for NES and from 60.6% to 64.4% for AutoZOOM. Even against the robust MNIST models, the transfer rate improves from the initial value of 3.4% to 4.3% (AutoZOOM) and 4.5% (NES). However, for the CIFAR10 dataset, we observe a significant decrease in transfer rate. For the normal CIFAR10 target model, the original transfer rate is as high as 65.6%, but with fine-tuning, the transfer rate decreases significantly (decreased to 8.6% and 33.4% for AutoZOOM and NES, respectively). A similar trend is also observed for the robust CIFAR10 target model. These results suggest that the examples used in the attacks are less useful as training examples for the CIFAR10 model than the original training set.

**Table 6: Impact of tuning local models (averaged 5 times)**
- **Transfer rate:** Measured on independently sampled test images.

Our second experiment, reported in Table 6, combines model tuning with the hybrid attack. Through our experiments, we observe that for MNIST models, the transfer rate also increases significantly by fine-tuning the local models. For the MNIST normal models, the (targeted) transfer rate increases from the original 60.6% to 74.7% and 76.9% for AutoZOOM and NES, respectively. The improved transfer rate is also higher than the results reported in the first experiment. For the AutoZOOM attack, in the first experiment, the transfer rate can only be improved from 60.6% to 64.4% while in the second experiment, it is improved from 60.6% to 76.9%. Therefore, there might be some boosting effects by taking local AEs as starting points for gradient attacks. For the Madry robust model on MNIST, the low (untargeted) transfer rate improves by a relatively large amount, from the original 3.4% to 5.1% for AutoZOOM and 4.8% for NES (still a low transfer rate, but a 41% relative improvement over the original local model). The local models become more robust during the fine-tuning process. For example, with the NES attack, the local model attack success rate (attack success is defined as compromising all the local models) decreases significantly from the original 96.6% to 25.2%, indicating that the tuned local models are more resistant to the PGD attack. The improvements in transferability, obtained as a free by-product of the gradient attack, also lead to substantial cost reductions for the attack on MNIST, as seen in Table 6. For example, for the AutoZOOM attack on the MNIST normal model, the mean query cost is reduced by 31%, from 282 to 194, and the attack success rate is also increased slightly, from 98.9% for static local models to 99.5% for tuned local models. We observe similar patterns for the robust MNIST model and demonstrate that Hypothesis 2 also holds on the MNIST dataset.

However, for CIFAR10, we still find no benefits from the tuning. Indeed, the transfer rate decreases, reducing both the attack success rate and increasing its mean query cost (Table 6). We do not have a clear understanding of the reasons for the CIFAR10 tuning failure, but speculate it is related to the difficulty of training CIFAR10 models. The results returned from gradient-based attacks are highly similar to a particular seed and may not be diverse enough to train effective local models. This is consistent with Carlini et al.'s findings that MNIST models tend to learn well from outliers (e.g., unnatural images) whereas more realistic datasets like CIFAR10 tend to learn well from more prototypical (e.g., natural) examples [7]. Therefore, fine-tuning CIFAR10 models using label by-products, which are more likely to be outliers, may diminish learning effectiveness. Potential solutions to this problem include tuning the local model with a mixture of normal seeds and attack by-products. One may also consider keeping some fraction of model ensembles fixed during the fine-tuning process such that when by-products mislead the tuning process, these fixed models can mitigate the problem. We leave further exploration of this for future work.

### 5 Batch Attacks

Section 4 evaluates attacks assuming an attacker wants to attack every seed from some fixed set of initial seeds. In more realistic attack scenarios, each query to the model has some cost or risk to the attacker, and the attacker's goal is to find as many adversarial examples as possible using a limited total number of queries. Carlini et al. show that defenders can identify purposeful queries for adversarial examples based on past queries, and therefore, detection risk will increase significantly when many queries are made [11]. We call these attack scenarios batch attacks. To be efficient in these resource-limited settings, attackers should prioritize "easy-to-attack" seeds.

A seed prioritization strategy can easily be incorporated into the hybrid attack algorithm by defining the `selectSeed` function used in step 6 of Algorithm 1 to return the most promising seed:

\[ \text{argmin}_{x \in X} \text{EstimatedAttackCost}(x, F) \]