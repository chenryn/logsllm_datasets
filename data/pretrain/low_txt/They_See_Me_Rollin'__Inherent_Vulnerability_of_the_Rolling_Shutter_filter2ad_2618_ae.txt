以下是优化后的文本，使其更加清晰、连贯和专业：

### 参考文献

1. Koltun, V., et al. (2017). CARLA: An Open Urban Driving Simulator. In *Proceedings of the 1st Annual Conference on Robot Learning*, pp. 1-16.
2. Durini, D. (2019). High Performance Silicon Imaging: Fundamentals and Applications of CMOS and CCD Sensors. *Elsevier*. https://doi.org/10.1016/C2017-0-01564-1
3. EVERGUARD.ai (2020). Sentri360. Retrieved from https://everguard.ai/sentri360.php
4. Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Tramer, F., Prakash, A., Kohno, T., & Song, D. (2018). Physical Adversarial Examples for Object Detectors. In *12th USENIX Workshop on Offensive Technologies (WOOT 18)*.
5. Giechaskiel, I., & Rasmussen, K. (2019). Taxonomy and Challenges of Out-of-Band Signal Injection Attacks and Defenses. *IEEE Communications Surveys & Tutorials*, 22(1), 645-670.
6. Gog, I., Kalra, S., Schafhalter, P., Wright, M. A., Gonzalez, J. E., & Stoica, I. (2021). Pylot: A Modular Platform for Exploring Latency-Accuracy Tradeoffs in Autonomous Vehicles. *arXiv preprint arXiv:2104.07830*.
7. Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and Harnessing Adversarial Examples. *arXiv preprint arXiv:1412.6572*.
8. Google Nest (2020). Nest Cam. Retrieved from https://store.google.com/us/product/nest_cam_outdoor?hl=en-US
9. He, K., Gkioxari, G., Dollár, P., & Girshick, R. (2017). Mask R-CNN. In *Proceedings of the IEEE International Conference on Computer Vision*, pp. 2961-2969.
10. Kinugasa, T., Noda, M., Imaide, T., Aizawa, I., Todaka, Y., & Ozawa, M. (1987). An Electronic Variable-Shutter System in Video Camera Use. *IEEE Transactions on Consumer Electronics*, 33(3), 249-255.
11. Knapp, E. D., & Langill, J. T. (2014). Industrial Network Security: Securing Critical Infrastructure Networks for Smart Grid, SCADA, and Other Industrial Control Systems. *Syngress*.
12. Kuroda, T. (2017). Essential Principles of Image Sensors. *CRC Press*. https://doi.org/10.1201/b17411
13. Li, H., Wang, Y., Xie, X., Liu, Y., Wang, S., Wan, R., Chau, L.-P., & Kot, A. C. (2020). Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems. *arXiv preprint arXiv:2009.06996*.
14. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., & Berg, A. C. (2016). SSD: Single Shot MultiBox Detector. In *European Conference on Computer Vision*, Springer, pp. 21-37.
15. Lovisotto, G., Turner, H., Sluganovic, I., Strohmeier, M., & Martinovic, I. (2021). SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations. In *30th USENIX Security Symposium (USENIX Security 21)*.
16. Man, Y., Li, M., & Gerdes, R. (2020). GhostImage: Remote Perception Attacks against Camera-based Image Classification Systems. In *23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2020)*, USENIX Association, San Sebastian, pp. 317-332. https://www.usenix.org/conference/raid2020/presentation/man
17. Nakamura, J. (2006). Image Sensors and Signal Processing for Digital Still Cameras. *CRC Press*. https://doi.org/10.1201/9781420026856
18. Nassi, B., Nassi, D., Ben-Netanel, R., Mirsky, Y., Drokin, O., & Elovici, Y. (2020). Phantom of the ADAS: Phantom Attacks on Driver-Assistance Systems.
19. Nassi, D., Ben-Netanel, R., Elovici, Y., & Nassi, B. (2019). MobilBye: Attacking ADAS with Camera Spoofing. *arXiv preprint arXiv:1906.09765*.
20. Oh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee, J. T., Mukherjee, S., Aggarwal, J. K., Lee, H., Davis, L., et al. (2011). A Large-Scale Benchmark Dataset for Event Recognition in Surveillance Video. In *CVPR 2011*, IEEE, pp. 3153-3160.
21. BBC Online (2020). Met Police to Deploy Facial Recognition Cameras. Retrieved from https://www.bbc.co.uk/news/uk-51237665
22. Park, Y., Son, Y., Shin, H., Kim, D., & Kim, Y. (2016). This Ain’t Your Dose: Sensor Spoofing Attack on Medical Infusion Pump. In *10th USENIX Workshop on Offensive Technologies (WOOT 16)*.
23. Petit, J., Stottelaar, B., Feiri, M., & Kargl, F. (2015). Remote Attacks on Automated Vehicles Sensors: Experiments on Camera and Lidar. *Black Hat Europe 2015*.
24. QImaging (2014). Rolling Shutter vs. Global Shutter. Retrieved from https://tecnicaenlaboratorios.com/Qimaging/brochures/RollingvsGlobalShutter.pdf
25. Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In *Advances in Neural Information Processing Systems*, pp. 91-99.
26. Ring (2020). Ring Camera. Retrieved from https://ring.com
27. Sayles, A., Hooda, A., Gupta, M., Chatterjee, R., & Fernandes, E. (2021). Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 14666-14675.
28. Blue Iris Security (2020). Video Security. Retrieved from https://blueirissoftware.com
29. Sharif, M., Bauer, L., & Reiter, M. K. (2018). On the Suitability of Lp-Norms for Creating and Preventing Adversarial Examples. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*, pp. 1605-1613.
30. Köhler, S., Lovisotto, G., Birnbach, S., Baker, R., & Martinovic, I. (2021). Using Rectification Attacks. In *Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security*, pp. 2301-2315.
31. Wang, H., Wu, X., Huang, Z., & Xing, E. P. (2020). High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 8684-8694.
32. Wany, M., & Israel, G. P. (2003). CMOS Image Sensor with NMOS-Only Global Shutter and Enhanced Responsivity. *IEEE Transactions on Electron Devices*, 50(1), 57-62.
33. Xu, B., Wang, N., Chen, T., & Li, M. (2015). Empirical Evaluation of Rectified Activations in Convolutional Network. *arXiv preprint arXiv:1505.00853*.
34. Xu, W., Yan, C., Jia, W., Ji, X., & Liu, J. (2018). Analyzing and Enhancing the Security of Ultrasonic Sensors for Autonomous Vehicles. *IEEE Internet of Things Journal*, 5(6), 5015-5029.
35. Yan, C., Xu, W., & Liu, J. (2016). Can You Trust Autonomous Vehicles: Contactless Attacks Against Sensors of Self-Driving Vehicle. *DEF CON 24*, 8, 109.
36. Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., & Darrell, T. (2020). BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 2636-2645.
37. Zhang, G., Yan, C., Ji, X., Zhang, T., Zhang, T., & Xu, W. (2017). DolphinAttack: Inaudible Voice Commands. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security*, pp. 103-117.
38. Zhou, Z., Pain, B., & Fossum, E. R. (1997). Frame-Transfer CMOS Active Pixel Sensor with Pixel Binning. *IEEE Transactions on Electron Devices*. https://doi.org/10.1109/16.628834
39. Zhou, Z., Tang, D., Wang, X., Han, W., Liu, X., & Zhang, K. (2018). Invisible Mask: Practical Attacks on Face Recognition with Infrared. *arXiv preprint arXiv:1803.04683*.

### 附录

#### A. 曝光时间估计

基于Axis M3045-V摄像头，我们展示了攻击者如何估计曝光时间 \( t_{\text{exp}} \)。此外，通过比较自动曝光和手动设置曝光的帧，我们验证了该方法的准确性。

**方法：**
1. 从数据手册中提取 \( H_v = 0.25 \, \text{lx} \)。
2. 收集多张自动曝光机制下的帧。
3. 关闭自动曝光机制，使用照度计测量当前环境光照水平 \( E_v \)，并利用公式7估计 \( t_{\text{exp}} \)。
4. 在不同的光照条件下（如日落、阴天和晴天）重复上述步骤。

**估计准确性：**
初步观察发现，自动曝光和手动设置曝光的帧在视觉上没有明显差异。为了分析准确性，我们计算了每张帧的颜色通道（R、G和B）的直方图，并计算了自动曝光帧与手动设置曝光帧的颜色直方图之间的互相关。表4显示，自动曝光和手动设置曝光的帧之间只有微小差异。为了进一步对比，我们还收集了曝光时间为 \( t_{\text{exp}} + 60 \, \mu \text{s} \) 的帧。选择60 μs是因为这是亮度变化明显的第一个值。表4表明，60 μs的偏移也导致所有三个颜色通道的互相关出现显著差异，这表明 \( t_{\text{exp}} \) 的估计是准确的。

#### B. 错误的 \( t_{\text{exp}} \) - 额外结果

如第6.2节所述，错误的曝光时间估计会导致失真大小偏离预期。这里报告了我们在数据收集中使用的参数的最佳和最差偏差情况，这些偏差取决于 \( t_{\text{on}} \)。较大的 \( t_{\text{on}} \) 通常会导致较小的偏差，反之亦然。最短的 \( t_{\text{on}} \) 如图12所示，最长的 \( t_{\text{on}} \) 如图13所示。图中显示，对于较大的 \( t_{\text{on}} \)（通常在我们的评估中产生更好的攻击效果，见第7节），错误估计倾向于导致预期与实际失真大小之间的偏差减小。对于Logitech，最坏情况下的失真大小增加为1.6（从最短 \( t_{\text{on}} \) 设置中的6减少），而对于Axis，最坏情况下的失真大小增加从最短 \( t_{\text{on}} \) 设置中的11减少到3。

#### C. 对自动驾驶的影响

我们使用开源模拟器CARLA [11] 和自驾车平台Pylot [16] 来说明滚动快门攻击对自主系统的影响。默认情况下，Pylot代理配备了四个传感器：（i）用于车道检测和目标检测/跟踪的广角RGB摄像头，（ii）用于交通灯检测的长焦摄像头，（iii）用于定位的LiDAR传感器，以及（iv）用于路线规划的GPS。我们专门针对广角RGB摄像头进行攻击，以测量整个系统受到的影响。

**CARLA设置：**
我们使用了CARLA场景运行器中的两个预定义场景：场景1涉及行人，场景2涉及自行车手。我们通过分别模拟每个场景50次来收集Pylot的基本性能。然后在存在滚动快门攻击的情况下重复模拟。我们截取了广角摄像头捕获的帧，并在其转发给Pylot之前叠加了滚动快门图案（如第7.1.1节所述）。攻击参数如下：\( f = 750 \, \text{Hz} \)，\( t_{\text{exp}} = 200 \, \mu \text{s} \)，\( t_{\text{on}} = 0.53 \, \text{ms} \)。与之前的评估一致，我们再次使用了Pylot中预先实现的两种目标检测器FRCNN和SSD。

**结果：**
第7节中的大多数关键发现可以转移到CARLA模拟中。特别是，SSD和FRCNN之间的性能差异尤为突出。在正常操作下，两种目标检测器在场景1中的碰撞概率均为5%，但在滚动快门攻击下，SSD的碰撞概率为67%，而FRCNN的碰撞概率为47%。类似的行为可以在场景2的基线中观察到。FRCNN的碰撞概率为0%，而使用SSD时则增加到15%。然而，有趣的是，在攻击下，FRCNN的性能急剧下降，落后于SSD。安全违规的概率增加到97%，而SSD为65%。

图14展示了两个场景和目标检测器在正常操作（基线）和滚动快门攻击下的速度曲线。图中显示，在基线中，汽车一旦非玩家角色开始移动就立即减速。相比之下，在攻击期间，可以看到制动延迟。

### 图像

图15展示了物理实验的结果。我们在这些图像中打印了图片，将其放置在摄像头前，并使用激光进行了攻击（参见图5）。第一列报告了FRCNN的目标检测结果，第二列报告了SSD的目标检测结果。

(a) Logitech C922
(b) Logitech C922
(c) Axis M3045-V
(d) Axis M3045-V
(e) YI Home Camera 1080p
(f) YI Home Camera 1080p
(g) V380 Camera 720p
(h) V380 Camera 720p
(i) Netgear Arlo
(j) Netgear Arlo
(k) Apple iPhone 7
(l) Apple iPhone 7