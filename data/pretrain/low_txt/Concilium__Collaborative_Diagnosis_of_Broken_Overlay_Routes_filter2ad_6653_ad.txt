### Optimized Text

For node B to be in A's routing table and for C to be in B's routing table, their probe results must be considered. In the second scenario, when a non-faulty node is being evaluated, malicious peers will always claim that their probed links are up (increasing the false positive rate). Conversely, when a malicious peer is being evaluated, other malicious peers will always claim that their probed links are down (increasing the false negative rate).

Comparing Figure 5(a) with Figure 5(b), we observe that incorporating erroneous probe results into Equation 2 leads to more blame being assigned to non-faulty nodes and less blame to faulty ones. However, Concilium can still make accurate fault accusations using a thresholding scheme that produces binary verdicts. For instance, if a node receives less than 40% of the blame for any message drop, it is deemed innocent; otherwise, it receives a guilty verdict. If all peers report their probe results faithfully, innocent peers will receive guilty verdicts 1.8% of the time, while faulty peers will receive guilty verdicts 93.8% of the time. If 20% of peers collude and provide malicious probe results, then innocent peers will receive guilty verdicts 8.4% of the time, and faulty peers will receive guilty verdicts 71.3% of the time.

A host formally accuses a peer if that peer accumulates at least \( m \) guilty verdicts for the \( w \) most recent message drops. To determine the false positive and false negative rates of formal accusations, let \( p_{\text{good}} \) be the probability that a non-faulty node receives a guilty verdict for a message drop, and \( p_{\text{faulty}} \) be the probability that a faulty node receives a guilty verdict. These probabilities are derived from the blame probability density functions (PDFs) and thresholds as described in the previous paragraph. Let \( W \) be a random variable describing the number of guilty verdicts in a \( w \)-slot window. \( W \) is a binomial random variable, meaning that the error rates can be described as follows:

\[ \text{Pr}(\text{false positive}) = \text{Pr}(W \geq m) = \sum_{k=m}^{w} \binom{w}{k} p_{\text{good}}^k (1 - p_{\text{good}})^{w-k} \]

\[ \text{Pr}(\text{false negative}) = \text{Pr}(W < m) = \sum_{k=0}^{m-1} \binom{w}{k} p_{\text{faulty}}^k (1 - p_{\text{faulty}})^{w-k} \]

Figure 6 depicts the error rates with a blame PDF threshold of 40% and a sliding window size of 100. If all nodes faithfully report probe results, both error rates can be driven below 1% with an \( m \) of 6. If 20% of hosts maliciously invert their probe results, equivalent error rates can be achieved with an \( m \) of 16.

### Bandwidth Requirements

Concilium has two primary sources of network overhead: peers must exchange signed, timestamped copies of their routing state, and they must perform tomographic probing. We expect local routing state to reference \( \mu \phi + 16 \) peers, where 16 is the number of leaf nodes. Each routing entry contains a 16-byte node identifier and a 4-byte freshness timestamp. Using PSS-R [4] with 1024-bit public keys, both quantities plus a signature consume 144 bytes. The exchanged routing state also includes tomographic probe results for the IP path to each routing peer. As explained in Section 3.2, the results for each path can be encoded in a few bits. Assuming 1 byte for each path summary and a 100,000-node overlay, an entire advertised routing table is about 11.5 kilobytes. This overhead can be reduced by sending diffs for updated entries instead of entire tables.

In the absence of forwarding faults, lightweight tomography requires no additional bandwidth beyond that already required for availability probing. The outgoing bandwidth required for heavyweight striped probing of a tree is given by:

\[ \frac{\left| \text{leaves} \in T_H \right|}{2} \times (\text{stripes per pair}) \times (\text{stripe size}) \times (\text{packet size}) \]

In a 100,000-node overlay, the average node has 77 entries in its local routing state. Suppose each node sends 100 stripes to each ordered pair of peers, each stripe contains two UDP probes, and each probe is 30 bytes long (28 bytes for IP+UDP headers and 16 bits for a nonce). Probing an entire tree will require 16.7 MB of outgoing network traffic. Incoming probes will require no more than this amount and less if there are legitimately lossy network links.

The probing cost can be reduced in several ways. If IP multicast were widely deployed, we could reduce the probe traffic sent from the root of a tree to its leaf nodes. Additionally, as described in Section 3.7, cooperative hosts on the same stub network can share probe results, reducing the probing bandwidth for the collective.

### Related Work

Packet obituary systems [2] allow end hosts to determine the autonomous system (AS) which dropped a particular packet. Each AS deploys an "accountability box" at each border link. When an incoming packet hits a box, the box records the next AS that the packet will traverse. Boxes periodically push these records along the reverse box paths, allowing each packet source to determine the last AS that successfully received their datagrams. Concilium differs from obituary systems in three ways: First, Concilium does not require the modification of core Internet routers. Second, Concilium protects and validates its network data using various cryptographic and statistical techniques. Finally, obituary systems cannot arbitrate between two adjacent ASes when the first claims that the second dropped its packet, and the second claims that the first never sent the packet. Concilium resolves such disputes using reputation systems.

### Conclusions

In this paper, we introduce Concilium, a distributed diagnostic protocol for overlay networks. By aggregating peer-advertised routing state, Concilium determines forwarding paths at the overlay level. Using collaborative network tomography, Concilium discovers the IP links that comprise these paths and the quality of these links. By combining topological and tomographic data with application-level message acknowledgments, Concilium judges whether dropped overlay messages are due to failures in the core Internet or failures in overlay forwarders. Concilium’s fault accusations are self-verifying and robust to tampering, but they may place blame on nodes that are victims of misbehavior further downstream in their routes. Thus, Concilium provides mechanisms to revise such incorrect accusations and methods for detecting peers that publish faulty routing state or tomographic data.

### References

[1] D. Andersen, H. Balakrishnan, M. F. Kaashoek, and R. Morris. Resilient Overlay Networks. In Proceedings of SOSP, pages 131–145, Banff, Canada, October 2001.
[2] K. Argyraki, P. Maniatis, D. Cheriton, and S. Shenker. Providing Packet Obituaries. In Proceedings of ACM SIGCOMM HotNets, San Diego, CA, November 2004.
[3] V. Arya, T. Turletti, and C. Hoffmann. Feedback Verification for Trustworthy Tomography. In Proceedings of IPS-MoMe, Warsaw, Poland, March 2005.
[4] M. Bellare and P. Rogaway. The Exact Security of Digital Signatures – How to Sign with RSA and Rabin. Advances in Cryptology–EUROCRYPT ’96, 1070:399–416, 1996.
[5] R. Bellman and M. Giertz. On the analytic formalism of the theory of fuzzy sets. Information Sciences, 5:149–156, 1973.
[6] M. Castro, M. Costa, and A. Rowstron. Performance and dependability of structured peer-to-peer overlays. In Proceedings of DSN, Florence, Italy, June 2004.
[7] M. Castro, P. Druschel, A. Ganesh, A. Rowstron, and D. S. Wallach. Secure routing for structured peer-to-peer overlay networks. In Proceedings of OSDI, pages 299–314, Boston, MA, December 2002.
[8] M. Castro, P. Druschel, Y. C. Hu, and A. Rowstron. Proximity neighbor selection in tree-based structured peer-to-peer overlays. Technical Report MSR-TR-2003-52, Microsoft Research, 2003.
[9] Y. Chen, D. Bindel, H. Song, and R. Katz. An Algebraic Approach to Practical and Scalable Overlay Network Monitoring. In Proceedings of ACM SIGCOMM, pages 55–66, Portland, OR, September 2004.
[10] N. Duffield, F. L. Presti, V. Paxson, and D. Towsley. Inferring Link Loss Using Striped Unicast Probes. In Proceedings of IEEE INFOCOM, pages 915–923, Anchorage, AK, April 2001.
[11] R. Govindan and H. Tangmunarunkit. Heuristics for Internet Map Discovery. In Proceedings of IEEE INFOCOM, pages 1371–1380, Tel Aviv, Israel, March 2000.
[12] R. Jurgelenaite, P. Lucas, and T. Heskes. Exploring the noisy threshold function in designing Bayesian networks. In Proceedings of SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence, pages 133–146, Cambridge, UK, December 2005.
[13] R. Mahajan, M. Castro, and A. Rowstron. Controlling the cost of reliability in peer-to-peer overlays. In Proceedings of the 2nd IPTPS, Berkeley, CA, February 2003.
[14] R. Mahajan, N. Spring, D. Wetherall, and T. Anderson. User-level Internet Path Diagnosis. In Proceedings of SOSP, pages 106–119, Lake George, NY, October 2003.
[15] A. Mizrak, Y.-C. Cheng, K. Marzullo, and S. Savage. Fatih: Detecting and Isolating Malicious Routers. In Proceedings of DSN, pages 538–547, Yokohama, Japan, June 2005.
[16] D. Oppenheimer, A. Ganapathi, and D. A. Patterson. Why do Internet services fail, and what can be done about it? In Proceedings of USITS, March 2003.
[17] A. Rowstron and P. Druschel. Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems. In Proceedings of the IFIP/ACM International Conference on Distributed Systems Platforms (Middleware), Heidelberg, Germany, November 2001.
[18] N. Spring, R. Mahajan, and D. Wetherall. Measuring ISP topologies with Rocketfuel. In Proceedings of ACM SIGCOMM, pages 133–145, Pittsburgh, PA, August 2002.
[19] I. Stoica, R. Morris, D. Karger, M. Kaashoek, and H. Balakrishnan. Chord: A scalable peer-to-peer lookup service for Internet applications. In Proceedings of ACM SIGCOMM, pages 149–160, San Diego, CA, August 2001.
[20] K. Walsh and E. G. Sirer. Experience with an Object Reputation System for Peer-to-Peer Filesharing. In Proceedings of NSDI, pages 1–14, San Jose, CA, May 2006.
[21] Y. Zhang, V. Paxson, and S. Shenker. The Stationarity of Internet Path Properties: Routing, Loss, and Throughput. Technical Report, AT&T Center for Internet Research at ICSI, May 2000.