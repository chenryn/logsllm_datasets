### References

1. **Internet Network Management**. Pages 305–306, 2007.
2. **Gill, P., Jain, N., and Nagappan, N.** "Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications." In *SIGCOMM*, 2011.
3. **Hollander, M. and Wolfe, D.** *Nonparametric Statistical Methods*. Wiley, 1973.
4. **Jain, M. and Dovrolis, C.** "End-to-End Available Bandwidth: Measurement Methodology, Dynamics, and Relation with TCP Throughput." In *SIGCOMM*, 2002.
5. **Jaquith, A.** *Security Metrics: Replacing Fear, Uncertainty, and Doubt*. Addison-Wesley, 2007.
6. **Jensen, D. D., Fast, A. S., Taylor, B. J., and Maier, M. E.** "Automatic Identification of Quasi-Experimental Designs for Discovering Causal Knowledge." In *KDD*, 2008.
7. **Khoshgoftaar, T. M., Golawala, M., and Van Hulse, J.** "An Empirical Study of Learning from Imbalanced Data Using Random Forest." In *International Conference on Tools with Artificial Intelligence (ICTAI)*, 2007.
8. **Kim, H., Benson, T., Akella, A., and Feamster, N.** "The Evolution of Network Configuration: A Tale of Two Campuses." In *IMC*, 2011.
9. **Krishnan, S. S. and Sitaraman, R. K.** "Video Stream Quality Impacts Viewer Behavior: Inferring Causality Using Quasi-Experimental Designs." In *IMC*, 2012.
10. **Krishnan, S. S. and Sitaraman, R. K.** "Understanding the Effectiveness of Video Ads: A Measurement Study." In *IMC*, 2013.
11. **Krothapalli, S. D., Sun, X., Sung, Y.-W. E., Yeo, S. A., and Rao, S. G.** "A Toolkit for Automating and Visualizing VLAN Configuration." In *SafeConfig*, 2009.
12. **Mahajan, R., Spring, N., Wetherall, D., and Anderson, T.** "User-Level Internet Path Diagnosis." In *SOSP*, 2003.
13. **Potharaju, R. and Jain, N.** "Demystifying the Dark Side of the Middle: A Field Study of Middlebox Failures in Datacenters." In *IMC*, 2013.
14. **Potharaju, R., Jain, N., and Nita-Rotaru, C.** "Juggling the Jigsaw: Towards Automated Problem Inference from Network Trouble Tickets." In *NSDI*, 2013.
15. **Quinlan, J. R.** *C4.5: Programs for Machine Learning*. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1993.
16. **RANCID (Really Awesome New Cisco Config Differ)**. <http://shrubbery.net/rancid>.
17. **Rubin, D. B.** "Using Multivariate Matched Sampling and Regression Adjustment to Control Bias in Observational Studies." *Journal of the American Statistical Association*, 74:318–328, 1979.
18. **Shadish, W., Cook, T., and Campbell, D.** *Experimental and Quasi-Experimental Designs for Generalized Causal Inference*. Houghton Mifflin, 2002.
19. **Spring, N., Mahajan, R., Wetherall, D., and Anderson, T.** "Measuring ISP Topologies with Rocketfuel." *IEEE/ACM Transactions on Networking (ToN)*, 2004.
20. **Stuart, E. A.** "Matching Methods for Causal Inference: A Review and a Look Forward." *Statistical Science*, 25, 2010.
21. **Stuart, E. A. and Rubin, D. B.** "Best Practices in Quasi-Experimental Designs: Matching Methods for Causal Inference." In *Best Practices in Quantitative Methods*, pages 155–176. Sage, 2008.
22. **Sung, Y., Rao, S., Sen, S., and Leggett, S.** "Extracting Network-Wide Correlated Changes from Longitudinal Configuration Data." In *PAM*, 2009.
23. **Traceroute.org**. <http://www.traceroute.org>.
24. **HP OpenView TrueControl Software**. <http://support.openview.hp.com>.
25. **Turner, D., Levchenko, K., Mogul, J. C., Savage, S., and Snoeren, A. C.** "On Failure in Managed Enterprise Networks." Technical Report HPL-2012-101, HP.
26. **Turner, D., Levchenko, K., Savage, S., and Snoeren, A. C.** "A Comparison of Syslog and IS-IS for Network Failure Analysis." In *IMC*, 2013.
27. **Turner, D., Levchenko, K., Snoeren, A. C., and Savage, S.** "California Fault Lines: Understanding the Causes and Impact of Network Failures." In *SIGCOMM*, 2010.

### Appendix

#### A. Characterization of Management Practices

We provide a detailed characterization of the management practices used at a large online service provider (OSP). This offers a unique and rich view into the practices used in a modern, professionally-managed infrastructure. We are not claiming that this view is representative. For brevity, we quantify a subset of the practice metrics in Table 1. We find significant diversity in the design and operational practices employed across the OSP's networks.

##### A.1 Design Practices

We start by examining the OSP's networks in terms of their network composition, structure, and purpose.

- **Workload Homogeneity**: The majority (81%) of networks host only one workload—networks are quite homogeneous in this respect. A handful of networks do not host any workloads; they only connect networks to each other or the external world.
- **Device Roles**: The networks contain a mix of device roles, including routers, switches, firewalls, application delivery controllers (ADCs), and load balancers. Most networks (86%) have devices in multiple roles—although no single device has more than one role—and 71% of networks contain at least one middlebox (firewall, ADC, or load balancer).
- **Vendor and Model Diversity**: Over 81% of networks contain devices from more than one vendor, with a maximum of 6 vendors. Over 96% of networks contain more than one device model, with a maximum of 25 models. Thus, some networks must use more than one device model for the same role. The hardware entropy of the networks (solid line in Figure 11(a)) shows that only 4% of networks have just one model and one role; the remaining (96%) networks have varying degrees of heterogeneity, up to a maximum entropy metric value of 0.82. The extent of firmware heterogeneity is similar (dashed line in Figure 11(a)).
- **Logical Composition and Structure**: All networks use at least two layer-2 protocols (VLAN, spanning tree, link aggregation, unidirectional link detection (UDLD), DHCP relay, etc.), and 89% of networks use at least one routing protocol (BGP and/or OSPF). Furthermore, 10% of networks use 8 different protocols. There is significant diversity in the combination of protocols used. Similarly, there is significant diversity in the number of instances of each protocol. Less than 5 VLANs are configured in 5% of networks, but over 100 VLANs are configured in 9% of networks (Figure 11(c)). 86% of networks use BGP for layer-3 routing, with just one BGP instance in 39% of networks and more than 20 instances in 8% of networks (Figure 11(e)). In contrast, only 31% of networks use OSPF for layer-3 routing, with just one or two OSPF instances used in these networks.
- **Configuration Complexity**: To characterize configuration complexity, Figure 11(d) shows a CDF of intra- and inter-device referential complexity. Some networks' configuration is extremely complex (based on Benson et al.'s metrics [5]): in 20% of networks, the mean intra- and inter-device reference counts are higher than 100. However, it is worth noting that: (i) the range in complexity is rather large, and (ii) most networks have significantly lower configuration complexity metrics than the worst 10%.

##### A.2 Operational Practices

We now characterize the frequency, type, and modality of configuration changes, as well as those of change events.

- **Frequency of Changes**: The average number of configuration changes per month is correlated with network size (Figure 12(a); Pearson correlation coefficient of 0.64). However, several large networks have relatively fewer changes per month: e.g., one network has over 300 devices but fewer than 150 changes per month. Likewise, there are several small networks with a disproportionately high change rate. Furthermore, not every device is changed every month—in 77% of networks, less than half of a network’s devices are changed in a given month—but most devices are changed at least once per year—in 80% of networks, more than three-quarters of the devices are changed in a year (Figure 12(b)). Thus, changes occur frequently, and to different sets of devices in different months.
- **Types of Changes**: Across our entire dataset, there are approximately 480 different types of changes. Figure 12(c) shows CDFs of the fraction of changes in which at least one stanza of a given type is changed. On a per-network basis, interface changes are the most common, followed by pool (used on load balancers), ACL, user, and router.
- **Automation of Changes**: Among the most frequently changed types, pool changes are also the most frequently automated—more than half of all pool changes are automated in 77% of networks—followed by ACL and interface changes. More than half (quarter) of the changes each month are automated in 41% (81%) of networks. The extent of automation ranges between 10% and 70%. The fraction of automated changes is not strongly correlated with the number of changes (Pearson correlation coefficient is 0.23). The types of changes that are automated most frequently—sFlow and QoS—are not the most frequent types of changes.

**Note**: ADCs perform TCP and SSL offload, HTTP compression and caching, content-aware load balancing, etc. There are no pool changes in 63% of networks because these networks do not contain load balancers.