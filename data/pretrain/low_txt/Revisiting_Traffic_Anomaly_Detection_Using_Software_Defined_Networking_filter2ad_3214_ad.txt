### E. Effectiveness of Multi-Threading

**Figure 10: Controller Throughput**

Figure 10 illustrates the throughput scalability of Maestro with varying numbers of worker threads. The optimal throughput is achieved with 7 worker threads on an 8-core server, so we only present results up to 7 threads. The two "W/O core binding" cases in the figure highlight the importance of binding a worker thread to a specific core. Without this feature, the overhead of moving running code and associated data between cores significantly impacts performance, limiting the system's scalability. When core binding is enabled, as shown by the upper lines, throughput scales almost linearly with the number of worker threads. Additionally, enabling the feature to bind requests to specific worker threads further enhances throughput, as it reduces cache synchronization overhead by keeping the entire request processing within a single core.

**Figure 11: Average Delay under Maximum Request Input Rate**

Figure 11 shows the average delay experienced with different numbers of worker threads. With both core and thread binding features enabled, Maestro achieves the best performance in reducing the average delay. Note that the minimum delay of 163ms in the figure is for the maximum input rate; the delay would be much lower under normal load conditions.

To evaluate the overhead of sharing a single raw-packet task queue due to the pull-style work distribution, we measured the time spent waiting to acquire the lock on the task queue and the time spent running tasks at maximum throughput. The time spent waiting for the lock was only 0.098% of the time spent running tasks, confirming that the overhead of sharing the raw-packet task queue is negligible.

### F. Effectiveness of Output Batching

**Figure 12: Performance Comparison with and without Output Batching**

Figure 12 compares the performance of Maestro with and without the output batching feature, under the maximum input request rate. As shown, enabling output batching significantly improves both throughput and average delay. This is due to the reduction in overhead from sending out messages. Therefore, output batching is crucial for enhancing Maestro's performance.

### G. Discussion

In this section, we demonstrate that by fully exploiting parallelism within a single server machine and employing additional optimization techniques such as minimizing cross-core overhead and batching, Maestro can achieve near-linear scalability in throughput for processing flow requests on an eight-core server. Although the maximum throughput (600,000 rps) is still far from the requirements of large-scale data centers (over 10 million rps), we expect that Maestro can be distributed, similar to NOX, to scale to tens of millions of flow requests per second. Moreover, the scalability of Maestro within a single multi-core server can reduce the number of required distributed controllers by at least tenfold compared to NOX.

### IV. Related Work

**Ethane [4]**
Ethane, the predecessor of NOX and OpenFlow, is an early flow-based networking technology designed for secure enterprise networks. It enforces strong security policies by restricting network reachability before identity authentication. OpenFlow generalizes Ethane, and NOX provides a more full-featured and programmable controller design. However, neither Ethane nor NOX considers parallelism in their designs.

**HyperFlow [18]**
Similar to Maestro, HyperFlow aims to improve the performance of the OpenFlow control plane. However, it takes a different approach by extending NOX to a distributed control plane. By synchronizing network-wide state among distributed controllers using a distributed file system, HyperFlow localizes the processing of flow requests to individual controllers. The techniques used by HyperFlow are orthogonal to the controller platform design, allowing Maestro to also become fully distributed for higher scalability.

**DIFANE [20]**
DIFANE enables efficient rule-based policy enforcement by performing policy rules matching at the switches themselves. While DIFANE offloads policy rules to switches, OpenFlow is more flexible, as its control logic can implement behaviors not easily achieved by static policy rules. The techniques proposed by DIFANE and our techniques to enhance controller performance are complementary: functionalities that can be offloaded to switches can be handled by DIFANE, while those requiring central processing can be efficiently managed by Maestro.

### V. Summary

OpenFlow's flexibility and direct control make it a popular choice for various networking scenarios. However, the performance of the OpenFlow controller can be a bottleneck in larger networks. Maestro is the first OpenFlow controller system to exploit parallelism, achieving near-linear performance scalability on multi-core processors. Programmers can change the control plane functionality by writing simple single-threaded programs. Maestro incorporates designs and techniques that address specific OpenFlow requirements, exploit parallelism, and deliver significant performance improvements over existing solutions. This performance enhancement will have a substantial positive impact on many deployed and future OpenFlow networks.

### References

[1] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: Dynamic flow scheduling for data center networks. In USENIX NSDI, 2010.
[2] J.R. Ballard, I. Rae, and A. Akella. Extensible and Scalable Network Monitoring Using OpenSAFE. apr 2010.
[3] Theophilus Benson, Aditya Akella, and David A. Maltz. Network Traffic Characteristics of Data Centers in the Wild. In IMC, November 2010.
[4] Martin Casado, Michael J. Freedman, Justin Pettit, Jianying Luo, Nick McKeown, and Scott Shenker. Ethane: taking control of the enterprise. In SIGCOMM ’07, 2007.
[5] N. Feamster, A. Nayak, H. Kim, R. Clark, Y. Mundada, A. Ramachandran, and M. bin Tariq. Decoupling Policy from Configuration in Campus and Enterprise Networks. 2010.
[6] Albert Greenberg, Gisli Hjalmtysson, David A. Maltz, Andy Myers, Jennifer Rexford, Geoffrey Xie, Hong Yan, Jibin Zhan, and Hui Zhang. A clean slate 4D approach to network control and management. ACM Computer Communication Review, October 2005.
[7] Natasha Gude, Teemu Koponen, Justin Pettit, Ben Pfaff, Martn Casado, Nick McKeown, and Scott Shenker. Nox: Towards an operating system for networks. ACM Computer Communication Review, July 2008.
[8] S.W. Han, N. Kim, and J.W. Kim. Designing a virtualized testbed for dynamic multimedia service composition. In Proceedings of the 4th International Conference on Future Internet Technologies, 2009.
[9] Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar, Larry Peterson, Jennifer Rexford, Scott Shenker, and Jonathan Turner. OpenFlow: enabling innovation in campus networks. ACM Computer Communication Review, April 2009.
[10] J. Naous, R. Stutsman, D. Mazières, N. McKeown, and N. Zeldovich. Delegating network security with more information. In Proceedings of the 1st ACM workshop on Research on enterprise networking, 2009.
[11] A.K. Nayak, A. Reimers, N. Feamster, and R. Clark. Resonance: dynamic access control for enterprise networks. In Proceedings of the 1st ACM workshop on Research on enterprise networking, 2009.
[12] R. Niranjan Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat. PortLand: a scalable fault-tolerant layer 2 data center network fabric. ACM SIGCOMM Computer Communication Review, 2009.
[13] R. Sherwood, M. Chan, A. Covington, G. Gibb, M. Flajslik, N. Handigol, T.Y. Huang, P. Kazemian, M. Kobayashi, J. Naous, et al. Carving research slices out of your production networks with OpenFlow. ACM SIGCOMM Computer Communication Review, 2010.
[14] Neil Spring, Ratul Mahajan, and David Wetheral. Measuring ISP topologies with RocketFuel. In Proc. ACM SIGCOMM, August 2002.
[15] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker. Applying NOX to the Datacenter. In Eighth ACM Workshop on Hot Topics in Networks (HotNets-VIII), 2009.
[16] Arsalan Tavakoli, Martin Casado, Teemu Koponen, and Scott Shenker. Applying nox to the datacenter. In Proc. HotNets, October 2009.
[17] A. Tootoonchian, M. Ghobadi, and Y. Ganjali. OpenTM: Traffic Matrix Estimator for OpenFlow Networks. In Passive and Active Measurement, 2010.
[18] Amin Tootoonchian and Yashar Ganjali. HyperFlow: A distributed control plane for OpenFlow. In INM/WREN, 2010.
[19] K.K. Yap, M. Kobayashi, D. Underhill, S. Seetharaman, P. Kazemian, and N. McKeown. The Stanford OpenRoads deployment. In Proceedings of the 4th ACM international workshop on Experimental evaluation and characterization, 2009.
[20] M. Yu, J. Rexford, M.J. Freedman, and J. Wang. Scalable flow-based networking with DIFANE. In Proc. ACM SIGCOMM, August 2010.