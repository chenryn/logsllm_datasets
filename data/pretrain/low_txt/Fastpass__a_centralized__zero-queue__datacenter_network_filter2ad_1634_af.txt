### Acknowledgments

We would like to extend our gratitude to Doug Weimer for his generous assistance with the Facebook infrastructure. Additionally, we thank Garrett Wollman and Jon Proulx at MIT CSAIL for their invaluable help in setting up environments for our early experiments. We also appreciate the support of David Oran from Cisco. Special thanks go to John Ousterhout, Rodrigo Fonseca, Nick McKeown, George Varghese, Chuck Thacker, Steve Hand, Andreas Nowatzyk, Tom Rodeheffer, and the SIGCOMM reviewers for their insightful feedback. This work was partially supported by the National Science Foundation grant IIS-1065219. John Ousterhout's contributions were made possible by a Jacobs Presidential Fellowship and a Hertz Foundation Fellowship. We are also grateful to the industrial members of the MIT Center for Wireless Networks and Mobile Computing for their support and encouragement.

### References

[1] Packet processing on Intel architecture. http://www.intel.com/go/dpdk.
[2] Intel 64 and IA-32 Architectures Optimization Reference Manual. Number 248966-029. March 2014.
[3] M. Ajmone Marsan, E. Leonardi, M. Mellia, and F. Neri. On the stability of input-buffer cell switches with speed-up. In INFOCOM, 2000.
[4] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity Data Center Network Architecture. In SIGCOMM, 2008.
[5] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: Dynamic Flow Scheduling for Data Center Networks. In NSDI, 2010.
[6] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, M. Sridharan, C. Faster, and D. Maltz. DCTCP: Efficient Packet Transport for the Commoditized Data Center. In SIGCOMM, 2010.
[7] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat, and M. Yasuda. Less is More: Trading a Little Bandwidth for Ultra-Low Latency in the Data Center. In NSDI, 2012.
[8] M. Alizadeh, S. Yang, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker. Deconstructing Datacenter Packet Transport. In HotNets, 2012.
[9] T. E. Anderson, S. S. Owicki, J. B. Saxe, and C. P. Thacker. High-Speed Switch Scheduling for Local-Area Networks. ACM Trans. on Comp. Sys., 11(4):319–352, 1993.
[10] L. A. Barroso, J. Dean, and U. Holzle. Web Search for a Planet: The Google Cluster Architecture. IEEE Micro, 23(2):22–28, 2003.
[11] M. Chowdhury, M. Zaharia, J. Ma, M. Jordan, and I. Stoica. Managing Data Transfers in Computer Clusters with Orchestra. In SIGCOMM, 2011.
[12] R. Cole, K. Ost, and S. Schirra. Edge-Coloring Bipartite Multigraphs in O(E logD) Time. Combinatorica, 21(1):5–12, 2001.
[13] J. Dai and B. Prabhakar. The throughput of data switches with and without speedup. In INFOCOM, 2000.
[14] J. Duato, S. Yalamanchili, and L. Ni. Interconnection Networks. Morgan Kaufmann, 2003.
[15] A. Elwalid, C. Jin, S. Low, and I. Widjaja. MATE: MPLS Adaptive Traffic Engineering. In INFOCOM, 2001.
[16] N. Farrington and A. Andreyev. Facebook’s Data Center Network Architecture. In IEEE Optical Interconnects Conf., 2013.
[17] N. Farrington, G. Porter, Y. Fainman, G. Papen, and A. Vahdat. Hunting Mice with Microsecond Circuit Switches. In HotNets, 2012.
[18] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A Scalable and Flexible Data Center Network. In SIGCOMM, 2009.
[19] C.-Y. Hong, S. Kandula, R. Mahajan, M. Zhang, V. Gill, M. Nanduri, and R. Wattenhofer. Achieving High Utilization with Software-Driven WAN. In SIGCOMM, 2013.
[20] C. Y. Hong, M. Caesar, and P. Godfrey. Finishing Flows Quickly with Preemptive Scheduling. In SIGCOMM, 2012.
[21] F. Hwang. Control Algorithms for Rearrangeable Clos Networks. IEEE Trans. on Comm., 31(8):952–954, 1983.
[22] V. Jeyakumar, M. Alizadeh, D. Mazieres, B. Prabhakar, and C. Kim. EyeQ: Practical Network Performance Isolation for the Multi-Tenant Cloud. In HotCloud, 2012.
[23] A. Kapoor and R. Rizzi. Edge-coloring bipartite graphs. Journal of Algorithms, 34(2):390–396, 2000.
[24] N. McKeown. The iSLIP Scheduling Algorithm for Input-Queued Switches. IEEE/ACM Trans. on Net., 7(2):188–201, 1999.
[25] N. McKeown, A. Mekkittikul, V. Anantharam, and J. Walrand. Achieving 100% Throughput in an Input-Queued Switch. IEEE Trans. Comm., 47(8):1260–1267, 1999.
[26] D. Nagle, D. Serenyi, and A. Matthews. The Panasas ActiveScale Storage Cluster: Delivering Scalable High Bandwidth Storage. In Supercomputing, 2004.
[27] R. Niranjan Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat. PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric. In SIGCOMM, 2009.
[28] R. Nishtala, H. Fugal, S. Grimm, M. Kwiatkowski, H. Lee, H. C. Li, R. McElroy, M. Paleczny, D. Peek, P. Saab, et al. Scaling memcache at Facebook. In NSDI, 2013.
[29] P. Ohly, D. N. Lombard, and K. B. Stanton. Hardware Assisted Precision Time Protocol: Design and Case Study. In LCI Intl. Conf. on High-Perf. Clustered Comp., 2008.
[30] D. Shah. Maximal matching scheduling is good enough. In GLOBECOM, 2003.
[31] D. Shah, N. Walton, and Y. Zhong. Optimal Queue-Size Scaling in Switched Networks. In SIGMETRICS, 2012.
[32] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha. Sharing the Data Center Network. In NSDI, 2011.
[33] R. Takano, T. Kudoh, Y. Kodama, and F. Okazaki. High-Resolution Timer-Based Packet Pacing Mechanism on the Linux Operating System. IEICE Trans. on Comm., 2011.
[34] Y. Tamir and H.-C. Chi. Symmetric crossbar arbiters for VLSI communication switches. IEEE Trans. Par. Dist. Sys., 4(1):13–27, 1993.
[35] B. C. Vattikonda, G. Porter, A. Vahdat, and A. C. Snoeren. Practical TDMA for Datacenter Ethernet. In EuroSys, 2012.
[36] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowtron. Better Never Than Late: Meeting Deadlines in Datacenter Networks. In SIGCOMM, 2011.
[37] H. Wu, Z. Feng, C. Guo, and Y. Zhang. ICTCP: Incast Congestion Control for TCP in Data Center Networks. In CoNext, 2010.
[38] X. Wu and X. Yang. DARD: Distributed Adaptive Routing for Datacenter Networks. In ICDCS, 2012.

### Appendix: Theoretical Properties of the Timeslot Allocator

#### System Model

We consider a network with \( N \) endpoints, denoted by \( 1, \ldots, N \), where each endpoint can potentially have requests for any other endpoint. Time is divided into discrete timeslots. In each unit timeslot, each endpoint can transmit at most one MTU-sized packet to any other endpoint and receive at most one packet from any other endpoint. New requests arrive at endpoint \( i \) for endpoint \( j \) with probability \( p_{ij} \) in each timeslot. Each arriving request brings a random number of packets, distributed according to a distribution \( G_{ij} \) and independent of everything else.

Let \( E[G_{ij}] = g_{ij} \). Thus, on average, \( l_{ij} = p_{ij}g_{ij} \) packets arrive at endpoint \( i \) for endpoint \( j \) per timeslot. The matrix \( L = [l_{ij}] \) denotes the net average data traffic between endpoints arriving in each timeslot. We assume a non-oversubscribed (also called full bisection bandwidth) network: the network can support any traffic where each node is paired to at most one other node per timeslot.

In this setup, all traffic matrices \( L \) that can be served by any system architecture must be doubly sub-stochastic. That is,
\[
\sum_{k=1}^N l_{ik} < 1, \quad \text{for all } i,
\]
\[
\sum_{k=1}^N l_{kj} < 1, \quad \text{for all } j.
\]
By the celebrated result of Birkhoff and von Neumann, all doubly stochastic matrices can be decomposed as a weighted sum of permutation matrices (i.e., matchings) with the sum of the weights being at most 1. Therefore, non-oversubscribed networks can support all doubly stochastic traffic matrices. A traffic matrix \( L \) is called admissible if and only if \( r(L) < 1 \), where the system load \( r(L) \) is defined as
\[
r(L) = \max \left( \sum_{k=1}^N l_{ik}, \sum_{k=1}^N l_{kj} \right).
\]

Finally, let \( Q_{ij}(t) \) denote the total number of packets (potentially across different requests) waiting to be transferred from endpoint \( i \) to endpoint \( j \) at time \( t \). This setup is similar to that used in literature on input-queued switches [25], enabling us to view the network as a large input-queued switch with \( Q_{ij}(t) \) representing the Virtual Output Queue sizes.

#### Main Result

The arbiter's timeslot allocation algorithm described in §3 is equivalent to the following: each queue \( (i, j) \) has a "priority score" associated with it. At the beginning of each timeslot, the arbiter processes queues in non-decreasing order of these priority scores. While processing, the arbiter allocates a timeslot to queue \( (i, j) \) if it has not already allocated another packet starting from \( i \) or destined to \( j \) in this timeslot. Therefore, at the end of processing the timeslot, the allocations correspond to a maximal matching between endpoints in the bipartite graph between endpoints, where an edge is present between \( (i, j) \) if there are packets waiting at endpoint \( i \) destined for \( j \).

From the literature on input-queued switches, it is well-known that any maximal matching provides 50% throughput guarantees [13, 3]. Building upon these results as well as [30], we state the following property of our algorithm.

**Theorem 1.** For any \( r < 1 \), there exists \( L \) with \( r(L) = r \) such that for any allocator,
\[
\liminf_{t \to \infty} \frac{E\left[\sum_{i,j} Q_{ij}(t)\right]}{t} \geq \frac{Nr}{2(1 - r)}.
\]
Further, let \( V \geq 1 \) be such that \( E[G_{ij}^2] \leq V E[G_{ij}] \) for all \( i, j \) (bounded \( G_{ij} \)); if we allow the Fastpass arbiter to schedule (as well as transmit through the network) twice per unit timeslot, then the induced average queue-size
\[
\limsup_{t \to \infty} \frac{E\left[\sum_{i,j} Q_{ij}(t)\right]}{t} \leq \frac{Nr(r + V)}{2(1 - r)}.
\]

**Proof Sketch.** To establish the lower bound (3) for any scheduling algorithm, it is sufficient to consider a specific scenario of our setup. Concretely, let the traffic matrix be uniform, i.e., \( L = [l_{ij}] \) with \( l_{ij} = \frac{r}{N-1} \) for all \( i \neq j \) and 0 when \( i = j \); \( p_{ij} = 1 \) for all \( i \neq j \); and let \( G_{ij} \) be Poisson variables with parameter \( l_{ij} \). The network can be viewed as unit-sized packets arriving at each endpoint according to a Poisson arrival process of rate \( r \) and processed (transferred by the network) at unit rate. That is, the queue-size for each endpoint \( j \) is bounded below by that of an M/D/1 queue with load \( r \), which is known to be bounded below by \( \frac{r}{2(1 - r)} \). Therefore, the network-wide queue-size is bounded below by \( \frac{Nr}{2(1 - r)} \).

To establish an upper bound, we use the fact that the algorithm effectively achieves a maximal matching in the weighted bipartite graph between endpoints in each timeslot. Given this fact, and under the assumption that Fastpass can schedule as well as transfer data at twice the speed, this is effectively a speedup of 2 in the classical terminology of switch scheduling. Therefore, for the Lyapunov function (cf. [13]),
\[
L(t) = \sum_{i, j} Q_{ij}(t)[Q_{i \cdot}(t) + Q_{\cdot j}(t)],
\]
it can be shown using calculations similar to [30] that
\[
E[L(t + 1) - L(t) | Q(t)] \leq 4(r - 1) \sum_{i, j} Q_{ij}(t) + 2Nr^2 + 2V Nr.
\]
Telescoping this inequality for \( t \geq 0 \) and using the fact that the system reaches equilibrium due to ergodicity, we obtain the desired result. 

#### Implications

Equation (3) states that there is some (worst-case) input workload for which any allocator will have an expected aggregate queue length at least as large as \( \frac{Nr}{2(1 - r)} \). Equation (4) states that with a speedup of 2 in the network fabric, for every workload, the expected aggregate queue length will be no larger than \( \frac{Nr(r + V)}{2(1 - r)} \). Here, \( V \) is effectively a bound on burst size; if it is small, say 1, then it is within a factor of 2 of the lower bound! There is, however, a gap between theory and practice here, as in switch scheduling; many workloads observed in practice seem to require only small queues even with no speedup.