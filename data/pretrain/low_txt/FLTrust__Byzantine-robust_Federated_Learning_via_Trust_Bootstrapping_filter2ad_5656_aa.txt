# FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping

## Authors
Xiaoyu Cao<sup>∗1</sup>, Minghong Fang<sup>∗2</sup>, Jia Liu<sup>2</sup>, Neil Zhenqiang Gong<sup>1</sup>
- <sup>1</sup>Duke University, {xiaoyu.cao, neil.gong}@duke.edu
- <sup>2</sup>The Ohio State University, {fang.841, liu.1736}@osu.edu
- <sup>∗</sup>Equal contribution

## Abstract
Byzantine-robust federated learning (FL) aims to enable a service provider to learn an accurate global model even when a bounded number of clients are malicious. Existing methods typically perform statistical analysis on the local model updates and remove suspicious ones before aggregating them. However, malicious clients can still corrupt the global model by sending carefully crafted local model updates. The fundamental issue is the lack of a root of trust in these methods, as every client could potentially be malicious from the service provider's perspective.

In this work, we introduce FLTrust, a new federated learning method that addresses this gap by bootstrapping trust. Specifically, the service provider collects a small, clean training dataset (called the root dataset) and maintains a server model based on it. In each iteration, the service provider assigns a trust score to each local model update, where the score is lower if the direction of the update deviates more from the server model update. The service provider then normalizes the magnitudes of the local model updates to match that of the server model update, limiting the impact of malicious updates with large magnitudes. Finally, the service provider computes the weighted average of the normalized local model updates, using their trust scores as weights, to update the global model.

Our extensive evaluations on six datasets from different domains show that FLTrust is secure against both existing and strong adaptive attacks. For example, using a root dataset with fewer than 100 examples, FLTrust under adaptive attacks with 40%-60% of malicious clients can still train global models that are as accurate as those trained by FedAvg in non-adversarial settings.

## 1. Introduction
Federated learning (FL) [22, 28] is a distributed learning paradigm that enables multiple clients (e.g., smartphones, IoT devices, and edge devices) to collaboratively learn a global model without sharing their raw data. Each client holds a local training dataset, and the service provider (e.g., Google, Apple, or IBM) coordinates the learning process. Due to its potential for protecting private/proprietary client data, FL has been adopted by several high-profile companies. For instance, Google uses FL for next-word prediction in Android Gboard [1], WeBank for credit risk prediction [3], and over ten leading pharmaceutical companies for drug discovery in the MELLODDY project [2].

FL iteratively performs the following steps:
1. The server sends the current global model to the clients or a subset of them.
2. Each selected client trains a local model by fine-tuning the global model using its local training data and sends the local model updates back to the server.
3. The server aggregates the local model updates to update the global model. For example, FedAvg [28], a popular FL method in non-adversarial settings, computes the weighted average of the local model updates based on the sizes of the local training datasets.

However, FL is vulnerable to adversarial manipulations, such as data poisoning attacks [8, 32] and local model poisoning attacks [15, 7, 5, 43]. Malicious clients can corrupt the global model, causing incorrect predictions (untargeted attacks) or predicting specific target labels for chosen examples (targeted attacks). For instance, the global model in FedAvg can be manipulated by a single malicious client [9, 48].

Byzantine-robust FL methods [9, 12, 29, 46, 48] aim to address these issues by using robust aggregation rules that remove statistical outliers. However, recent studies [7, 15] have shown that these methods are still vulnerable to local model poisoning attacks due to the lack of a root of trust.

### Our Work
In this paper, we propose FLTrust, a new Byzantine-robust FL method. Instead of relying solely on local model updates, the server itself bootstraps trust. Specifically, the service provider collects a small, clean training dataset (the root dataset) and maintains a server model. In each iteration, the server updates the global model by considering both the server model update and the clients' local model updates.

### New Byzantine-robust Aggregation Rule
We design a new Byzantine-robust aggregation rule in FLTrust that incorporates the root of trust. A model update can be viewed as a vector characterized by its direction and magnitude. An attacker can manipulate both. Therefore, our aggregation rule considers both aspects. The server first assigns a trust score (TS) to each local model update, where the score is higher if the direction is more similar to that of the server model update. We use the cosine similarity between the local model update and the server model update to measure the similarity of their directions. To handle negative cosine similarities, we apply the ReLU operation to clip the scores. This ReLU-clipped cosine similarity is our trust score.

Next, FLTrust normalizes each local model update to have the same magnitude as the server model update. This normalization projects each local model update onto the same hyper-sphere as the server model update, limiting the impact of poisoned updates with large magnitudes. Finally, FLTrust computes the weighted average of the normalized local model updates, using their trust scores as weights, to update the global model.

### Empirical Evaluation
We evaluate FLTrust on six datasets from different domains, including five image classification datasets (MNIST-0.1, MNIST-0.5, Fashion-MNIST, CIFAR-10, and CH-MNIST) and a smartphone-based human activity recognition dataset (Human Activity Recognition). We compare FLTrust with existing Byzantine-robust FL methods, including Krum [9], Trimmed mean [48], and Median [48], and evaluate it against various poisoning attacks, such as label flipping, Krum attack, Trim attack, and Scaling attack [5].

Our results show that FLTrust is secure against these attacks, even with a root dataset of fewer than 100 examples. For example, a CNN global model learned using FLTrust has a testing error rate of 0.04 under all evaluated attacks on MNIST-0.1, while the Krum attack increases the testing error rate of the CNN global model learned by Krum from 0.10 to 0.90. Additionally, FLTrust under attacks achieves similar testing error rates to FedAvg under no attacks. We also study different variants of FLTrust and the impact of various system parameters.

### Robustness Against Adaptive Attacks
An attacker can adapt their attack to FLTrust. We evaluate FLTrust against adaptive attacks, specifically using the general framework of local model poisoning attacks proposed by Fang et al. [15]. Our empirical results show that FLTrust remains robust against such adaptive attacks. For instance, even with 60% of the clients being malicious, FLTrust can still learn a CNN global model with a testing error rate of 0.04 for MNIST-0.1, which is the same as the error rate achieved by FedAvg under no attacks.

### Contributions
- We propose FLTrust, the first Byzantine-robust federated learning method that bootstraps trust.
- We empirically evaluate FLTrust against existing and adaptive attacks, showing its robustness.
- We provide a comprehensive analysis of the performance of FLTrust under various conditions and parameters.

## 2. Background and Related Work

### A. Background on Federated Learning (FL)
Federated learning involves multiple clients, each with a local training dataset \( D_i \), \( i = 1, 2, \ldots, n \). The joint training data is denoted as \( D = \bigcup_{i=1}^n D_i \). The goal is to collaboratively learn a shared global model with the help of a service provider. The optimal global model \( w^* \) is a solution to the optimization problem \( w^* = \arg \min_w F(w) \), where \( F(w) = \mathbb{E}_{D \sim X}[f(D, w)] \) is the expectation of the empirical loss \( f(D, w) \) on the joint training dataset \( D \). Since the expectation is hard to evaluate, the global model is often learned by minimizing the empirical loss, i.e., \( \arg \min_w f(D, w) \).

Each client maintains a local model for its local training dataset, and the service provider's server maintains the global model by aggregating the local model updates. FL iteratively performs the following three steps (illustrated in Figure 1):

1. **Synchronizing the Global Model with Clients**: The server sends the current global model \( w \) to the clients or a subset of them.
2. **Training Local Models**: Each client trains a local model by fine-tuning the global model using its local training dataset. Formally, the \( i \)-th client solves the optimization problem \( \min_{w_i} f(D_i, w_i) \), where \( w_i \) is the client's local model. The client initializes its local model as the global model and uses stochastic gradient descent to update the local model for one or more iterations. Then, each client sends its local model update \( g_i = w_i - w \) to the server.
3. **Updating the Global Model via Aggregating Local Model Updates**: The server computes a global model update \( g \) by aggregating the local model updates according to some aggregation rule. The server then updates the global model using the global model update, i.e., \( w = w - \alpha \cdot g \), where \( \alpha \) is the global learning rate.

The aggregation rule plays a key role in FL. Different FL methods use different aggregation rules. Some popular aggregation rules include:

- **FedAvg** [28]: Proposed by Google, FedAvg computes the weighted average of the clients' local model updates, where each client is weighted by the size of its local training dataset. Formally, \( g = \sum_{i=1}^n \frac{|D_i|}{N} g_i \), where \( |D_i| \) is the size of the local training dataset on the \( i \)-th client and \( N \) is the total number of training examples. However, the global model in FedAvg can be arbitrarily manipulated by a single malicious client [9, 48].
- **Krum** [9]: Krum selects one of the \( n \) local model updates in each iteration as the global model update based on a square-distance score. Suppose at most \( f \) clients are malicious. The score for the \( i \)-th client is computed as:
  \[
  s_i = \sum_{g_j \in \Gamma_{i, n-f-2}} \|g_j - g_i\|_2^2,
  \]
  where \( \Gamma_{i, n-f-2} \) is the set of \( n-f-2 \) local model updates that have the smallest Euclidean distance to \( g_i \). The local model update of the client with the minimal score is chosen as the global model update.
- **Trimmed Mean (Trim-mean)** [48]: Trimmed mean is a coordinate-wise aggregation rule that removes the largest and smallest values for each model parameter and computes the mean of the remaining values. Given a trim parameter \( k < \frac{n}{2} \), the server removes the largest \( k \) and the smallest \( k \) values and computes the mean of the remaining \( n-2k \) values. Trim-mean can tolerate less than 50% of malicious clients.
- **Median** [48]: Median is another coordinate-wise aggregation rule that considers the median value of each parameter as the corresponding parameter value in the global model update.

### B. Poisoning Attacks to Federated Learning
Poisoning attacks aim to corrupt the training phase of machine learning. Data poisoning attacks modify the training data to corrupt the learnt model. These attacks have been demonstrated in various machine learning systems, including spam detection [32, 35], SVM [8], recommender systems [16, 17, 25, 45], neural networks [11, 18, 27, 30, 36, 37], graph-based methods [20, 41, 49], and distributed privacy-preserving data analytics [10, 14]. FL is also vulnerable to data poisoning attacks, where malicious clients can corrupt the global model by modifying, adding, or deleting examples in their local training datasets. For example, a label flipping attack changes the labels of the training examples on malicious clients while keeping their features unchanged.

Moreover, FL is further vulnerable to local model poisoning attacks [5, 7, 15, 43], where malicious clients poison the local models or their updates sent to the server. These attacks can be categorized into untargeted attacks [15] and targeted attacks [5, 7, 43]. Untargeted attacks aim to corrupt the global model so that it makes incorrect predictions for a large number of testing examples, while targeted attacks aim to predict specific target labels for chosen examples.

Recent studies [7, 15] have shown that local model poisoning attacks are more effective than data poisoning attacks against FL. Therefore, we focus on local model poisoning attacks in this work. Specifically, we discuss two state-of-the-art untargeted attacks (Krum attack and Trim attack) [15] and one targeted attack (Scaling attack) [5].

- **Krum Attack and Trim Attack** [15]: Fang et al. [15] proposed a general framework for local model poisoning attacks, which can optimize attacks for any given aggregation rule. Assuming the global model update without attack is \( g \), the attack is formulated to maximize the deviation of the global model from the desired direction.
- **Scaling Attack** [5]: Also known as a backdoor attack, the Scaling attack aims to embed a backdoor in the global model so that it predicts specific target labels for chosen examples.

## 3. Methodology
### 3.1. Trust Bootstrapping
In FLTrust, the service provider collects a small, clean training dataset (the root dataset) and maintains a server model. The root dataset is used to bootstrap trust in the learning process. The server model is updated in each iteration, and the server uses it to assign trust scores to the local model updates.

### 3.2. Trust Score Assignment
The trust score (TS) for a local model update is assigned based on the cosine similarity between the local model update and the server model update. The cosine similarity measures the similarity of their directions. To handle negative cosine similarities, we apply the ReLU operation to clip the scores. The ReLU-clipped cosine similarity is used as the trust score.

### 3.3. Normalization of Local Model Updates
Each local model update is normalized to have the same magnitude as the server model update. This normalization projects each local model update onto the same hyper-sphere as the server model update, limiting the impact of poisoned updates with large magnitudes.

### 3.4. Weighted Average of Normalized Local Model Updates
Finally, the server computes the weighted average of the normalized local model updates, using their trust scores as weights, to update the global model.

## 4. Experiments and Results
### 4.1. Datasets and Experimental Setup
We evaluate FLTrust on six datasets from different domains, including five image classification datasets (MNIST-0.1, MNIST-0.5, Fashion-MNIST, CIFAR-10, and CH-MNIST) and a smartphone-based human activity recognition dataset (Human Activity Recognition). We compare FLTrust with existing Byzantine-robust FL methods, including Krum [9], Trimmed mean [48], and Median [48], and evaluate it against various poisoning attacks, such as label flipping, Krum attack, Trim attack, and Scaling attack [5].

### 4.2. Results
Our results show that FLTrust is secure against these attacks, even with a root dataset of fewer than 100 examples. For example, a CNN global model learned using FLTrust has a testing error rate of 0.04 under all evaluated attacks on MNIST-0.1, while the Krum attack increases the testing error rate of the CNN global model learned by Krum from 0.10 to 0.90. Additionally, FLTrust under attacks achieves similar testing error rates to FedAvg under no attacks. We also study different variants of FLTrust and the impact of various system parameters.

### 4.3. Robustness Against Adaptive Attacks
We evaluate FLTrust against adaptive attacks, specifically using the general framework of local model poisoning attacks proposed by Fang et al. [15]. Our empirical results show that FLTrust remains robust against such adaptive attacks. For instance, even with 60% of the clients being malicious, FLTrust can still learn a CNN global model with a testing error rate of 0.04 for MNIST-0.1, which is the same as the error rate achieved by FedAvg under no attacks.

## 5. Conclusion
In this paper, we introduced FLTrust, a new Byzantine-robust federated learning method that bootstraps trust. FLTrust uses a small, clean root dataset to maintain a server model and assigns trust scores to local model updates based on their similarity to the server model update. The local model updates are normalized and aggregated using their trust scores to update the global model. Our extensive evaluations show that FLTrust is robust against both existing and adaptive attacks, making it a promising approach for secure federated learning.

## References
[1] G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep canonical correlation analysis. In *Proceedings of the 30th International Conference on Machine Learning (ICML)*, 2013.

[2] M. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth. Practical secure aggregation for privacy-preserving machine learning. In *Proceedings of the ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2017.

[3] Y. Chen, L. Su, and J. Xu. Secure outsourced matrix computation and application to neural networks. In *Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2015.

[4] T. Chen, S. Zhu, and Z. Li. Distributed machine learning via sufficient factor broadcasting. In *Proceedings of the 33rd International Conference on Machine Learning (ICML)*, 2016.

[5] P. Chen, H. Zhang, Y. Sharma, J. Yi, and C. Hsieh. Targeted backdoor attacks on deep learning systems using data poisoning. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2017.

[6] T. Chandra, E. Ie, K. Kansal, and K. K. Ramakrishnan. Towards efficient and accurate mobile visual search. In *Proceedings of the 2012 IEEE 11th International Conference on Data Mining (ICDM)*, 2012.

[7] Y. Fang, Y. Wu, J. Zhang, and X. Lin. Local model poisoning attacks to Byzantine-robust federated learning. In *Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2020.

[8] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In *Proceedings of the 29th International Conference on Machine Learning (ICML)*, 2012.

[9] Y. Blanchard, E. Mhamdi, R. Guerraoui, and J. Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In *Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)*, 2017.

[10] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. Overveldt, D. Petrou, D. Ramage, and J. Roselander. Towards federated learning at scale: System design. In *Proceedings of the 31st USENIX Symposium on Operating Systems Design and Implementation (OSDI)*, 2018.

[11] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In *Proceedings of the 25th USENIX Security Symposium (USENIX Security)*, 2016.

[12] Y. Chen, L. Su, and J. Xu. Secure outsourced matrix computation and application to neural networks. In *Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2015.

[13] T. Chen, S. Zhu, and Z. Li. Distributed machine learning via sufficient factor broadcasting. In *Proceedings of the 33rd International Conference on Machine Learning (ICML)*, 2016.

[14] P. Chen, H. Zhang, Y. Sharma, J. Yi, and C. Hsieh. Targeted backdoor attacks on deep learning systems using data poisoning. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2017.

[15] Y. Fang, Y. Wu, J. Zhang, and X. Lin. Local model poisoning attacks to Byzantine-robust federated learning. In *Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2020.

[16] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In *Proceedings of the 29th International Conference on Machine Learning (ICML)*, 2012.

[17] Y. Blanchard, E. Mhamdi, R. Guerraoui, and J. Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In *Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)*, 2017.

[18] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. Overveldt, D. Petrou, D. Ramage, and J. Roselander. Towards federated learning at scale: System design. In *Proceedings of the 31st USENIX Symposium on Operating Systems Design and Implementation (OSDI)*, 2018.

[19] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In *Proceedings of the 25th USENIX Security Symposium (USENIX Security)*, 2016.

[20] Y. Chen, L. Su, and J. Xu. Secure outsourced matrix computation and application to neural networks. In *Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2015.

[21] T. Chen, S. Zhu, and Z. Li. Distributed machine learning via sufficient factor broadcasting. In *Proceedings of the 33rd International Conference on Machine Learning (ICML)*, 2016.

[22] P. Chen, H. Zhang, Y. Sharma, J. Yi, and C. Hsieh. Targeted backdoor attacks on deep learning systems using data poisoning. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2017.

[23] Y. Fang, Y. Wu, J. Zhang, and X. Lin. Local model poisoning attacks to Byzantine-robust federated learning. In *Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2020.

[24] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In *Proceedings of the 29th International Conference on Machine Learning (ICML)*, 2012.

[25] Y. Blanchard, E. Mhamdi, R. Guerraoui, and J. Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In *Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)*, 2017.

[26] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. Overveldt, D. Petrou, D. Ramage, and J. Roselander. Towards federated learning at scale: System design. In *Proceedings of the 31st USENIX Symposium on Operating Systems Design and Implementation (OSDI)*, 2018.

[27] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In *Proceedings of the 25th USENIX Security Symposium (USENIX Security)*, 2016.

[28] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In *Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)*, 2017.

[29] A. Ghosh, J. Chung, and D. Yin. Byzantine-robust distributed learning: Towards optimal statistical rates. In *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 2018.

[30] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In *Proceedings of the 25th USENIX Security Symposium (USENIX Security)*, 2016.

[31] Y. Chen, L. Su, and J. Xu. Secure outsourced matrix computation and application to neural networks. In *Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2015.

[32] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In *Proceedings of the 29th International Conference on Machine Learning (ICML)*, 2012.

[33] Y. Blanchard, E. Mhamdi, R. Guerraoui, and J. Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In *Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)*, 2017.

[34] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. Overveldt, D. Petrou, D. Ramage, and J. Roselander. Towards federated learning at scale: System design. In *Proceedings of the 31st USENIX Symposium on Operating Systems Design and Implementation (OSDI)*, 2018.

[35] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In *Proceedings of the 25th USENIX Security Symposium (USENIX Security)*, 2016.

[36] Y. Chen, L. Su, and J. Xu. Secure outsourced matrix computation and application to neural networks. In *Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2015.

[37] T. Chen, S. Zhu, and Z. Li. Distributed machine learning via sufficient factor broadcasting. In *Proceedings of the 33rd International Conference on Machine Learning (ICML)*, 2016.

[38] P. Chen, H. Zhang, Y. Sharma, J. Yi, and C. Hsieh. Targeted backdoor attacks on deep learning systems using data poisoning. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2017.

[39] Y. Fang, Y. Wu, J. Zhang, and X. Lin. Local model poisoning attacks to Byzantine-robust federated learning. In *Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2020.

[40] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In *Proceedings of the 29th International Conference on Machine Learning (ICML)*, 2012.

[41] Y. Blanchard, E. Mhamdi, R. Guerraoui, and J. Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In *Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)*, 2017.

[42] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. Overveldt, D. Petrou, D. Ramage, and J. Roselander. Towards federated learning at scale: System design. In *Proceedings of the 31st USENIX Symposium on Operating Systems Design and Implementation (OSDI)*, 2018.

[43] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In *Proceedings of the 25th USENIX Security Symposium (USENIX Security)*, 2016.

[44] Y. Chen, L. Su, and J. Xu. Secure outsourced matrix computation and application to neural networks. In *Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2015.

[45] T. Chen, S. Zhu, and Z. Li. Distributed machine learning via sufficient factor broadcasting. In *Proceedings of the 33rd International Conference on Machine Learning (ICML)*, 2016.

[46] P. Chen, H. Zhang, Y. Sharma, J. Yi, and C. Hsieh. Targeted backdoor attacks on deep learning systems using data poisoning. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2017.

[47] Y. Fang, Y. Wu, J. Zhang, and X. Lin. Local model poisoning attacks to Byzantine-robust federated learning. In *Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 2020.

[48] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In *Proceedings of the 29th International Conference on Machine Learning (ICML)*, 2012.

[49] Y. Blanchard, E. Mhamdi, R. Guerraoui, and J. Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In *Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)*, 2017.

[50] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. Overveldt, D. Petrou, D. Ramage, and J. Roselander. Towards federated learning at scale: System design. In *Proceedings of the 31st USENIX Symposium on Operating Systems Design and Implementation (OSDI)*, 2018.

[51] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In *Proceedings of the 25th USENIX Security Symposium (USENIX Security)*, 2016.