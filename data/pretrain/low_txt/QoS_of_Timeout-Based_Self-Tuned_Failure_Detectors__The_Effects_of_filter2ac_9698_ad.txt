### Observations on Failure Detection Time

We observe that the failure detector takes a longer time to detect failures when using a prediction error-based (peb) margin, approximately 4 seconds. This delay is attributed to the significant variation in the safety margin. When the prediction error is large, the computed peb margin also increases, leading to a higher timeout duration (TD) and its upper bound. Conversely, average-based predictors, such as MEAN, do not generate large values in their predictions, resulting in smaller TDs. When combined with a small, conservative confidence interval-based (cib) margin, these predictors achieve better performance.

### Summary of Performance

To achieve optimal performance, a margin based on the prediction error should be used in conjunction with an accurate predictor. In contrast, a conservative or constant margin is more suitable for average-based predictors.

### Accuracy Issues

Next, we focus on the accuracy of the failure detectors (Table 5). Our results show that no combination of predictors and margins achieves the highest accuracy, as none simultaneously attain the smallest timeout margin (TM) and the largest timeout margin ratio (TMR). Instead, when TM is small, TMR is also small. For example, when using the peb margin, the ARIMA_FD failure detector has the lowest TM, while the MEAN_FD has the highest TMR. However, this trend reverses when the cib margin is used. This indicates that the choice of margin significantly affects the accuracy of the failure detector, and the effect varies depending on the communication delay predictor.

Examining the derived metrics λ and PA in Table 5, we find that accurate predictors perform better with the cib margin, similar to the behavior observed with a constant margin (Table 3). If a peb margin is chosen, the best accuracy is achieved using the conservative MEAN predictor. Therefore, the combination of predictor and margin is crucial for achieving high accuracy.

### Speed and Accuracy Analysis

By comparing the speed and accuracy (QoS) of the failure detectors (Tables 4 and 5), we observe that the MEAN-peb combination results in a more accurate failure detector. The average mistake rate for this combination is 306% higher than that of the ARIMA-peb combination and 263% higher than the WINMEAN-peb combination. However, it delays detection by 24% compared to the ARIMA-peb combination. On the other hand, the ARIMA-cib combination provides a better accuracy/performance balance. Its average mistake rate is 461% higher than the MEAN-cib combination and 207% higher than the WINMEAN-cib combination, while it delays detection by less than 1% and its upper bound by approximately 17% compared to the MEAN_FD. Thus, the choice of predictor-margin combination is essential for achieving good QoS, but it depends on the specific application requirements.

### Conclusions

In this paper, we evaluated the QoS of three implementations of a pull-style self-tuned failure detector: MEAN_FD, WINMEAN_FD, and ARIMA_FD. We explored different predictors (MEAN, WINMEAN, and ARIMA) and three distinct safety margins (constant, peb, and cib).

Our evaluation, using the QoS metrics proposed by Chen and Toueg [5] and a set of real traces, revealed that enhancing the accuracy of the round-trip communication delay predictor does not necessarily improve the QoS of a pull-style failure detector. An accurate predictor helps increase the mistake recurrence time but also increases the mistake duration.

We concluded that the combination of predictor and margin is crucial for achieving good QoS, and the choice depends on the application's requirements. Specifically, a peb margin should be used with an accurate predictor, while a conservative or constant margin is better suited for average-based predictors. The MEAN-peb combination results in a more accurate failure detector, whereas the ARIMA-cib combination provides a better accuracy/performance balance.

### Acknowledgements

We thank the anonymous referees for their valuable comments on the earlier version of this paper. We are also grateful to CAPES/Brazil and HP Brazil for their partial support of this work.

### References

[1] K. P. Birman. Building Secure and Reliable Network Applications. Greenwich: Manning Publications Co., 1996.
[2] M. Bertier, O. Marin, and P. Sens. Implementation and Performance Evaluation of an Adaptable Failure Detector. In: Proceedings of the International Conference on Dependable Systems and Networks (DSN), pages 354-363, June 2002.
[3] B. L. Bowerman and R. T. O'Connel. Forecasting and Time Series: an Applied Approach. Belmont: Duxbury Press, 1993.
[4] T. D. Chandra and S. Toueg. Unreliable Failure Detectors for Reliable Distributed Systems. Journal of the ACM, 43(2): 225-267, Mar. 1996.
[5] W. Chen, S. Toueg, M. K. Aguilera. On the Quality of Service of Failure Detectors. IEEE Transactions on Computers, 51(5): 561-580, May 2002.
[6] C. Delport-Gallet, H. Fauconnier, and R. Guerraoui. "A Realistic Look At Failure Detectors", In: Proceedings of the International Conference on Dependable Systems and Networks (DSN), pages 345-353, June 2002.
[7] P. A. Dinda and D. R. O'Hallaron. An Extensible Toolkit for Resource Prediction in Distributed Systems. Pittsburgh: Computer Science Department - Carnegie Mellon University, MU-CS-99-138, Jul. 1999.
[8] P. Felber. The CORBA Object Group Service - A Service Approach to Object Groups in CORBA. Lausanne: Départment d'Informatique, EPFL, 1998. (PhD Thesis)
[9] P. Felber, X. Défago, R. Guerraoui, and P. Oser. Failure Detectors as First Class Objects. In: Proceedings of the IEEE Intl. Symposium on Distributed Objects and Applications - DOA'99, pages 132-141, 1999.
[10] C. Fetzer, M. Raynal, and F. Tronel. An Adaptive Fault Detection Protocol. In Proc. of the IEEE Pacific Rim Symposium on Dependable Computing (PRDC-8), pages 146-153, December 2001.
[11] I. Gupta, T. D. Chandra, and G. Goldszmidt. On Scalable and Efficient Distributed Failure Detectors. In: Proceedings of the 20th Symposium on Principles of Distributed Computing (PODC), pages 170-179, Aug. 2001.
[12] M. G. Hayden. The Ensemble System. Ithaca: Department of Computer Science, Cornell University, Jan. 1998. (PhD Thesis)
[13] V. Jacobson. Congestion Avoidance and Control. In: Proceedings of the ACM Symposium on Communications, Architectures and Protocols (ACM SIGCOMM), pages 314-329, Aug. 1988.
[14] R. Macêdo. Failure Detection in Asynchronous Distributed Systems. In: Proceedings of the Test and Fault Tolerance Workshop (WTF), pages 76-81, May 2000.
[15] A. Montresor. System Support for Programming Object-Oriented Dependable Applications in Partitionable Systems. Bologna: Department of Computer Science, University of Bologna, 2000. (PhD Thesis)
[16] R. C. Nunes and I. Jansch-Pôrto. Modelling Communication Delays in Distributed Systems using Time Series. In: Proceedings of the IEEE Symposium on Reliable Distributed Systems (SRDS’2002), pages 268-273, October 2002.
[17] R. C. Nunes and I. Jansch-Pôrto. A Lightweight Interface to Predict Communication Delays using Time Series. In: Proceedings of the Latin-American Dependable Computing Symposium (LADC’2003), (LNCS 2847, Springer Verlag), pages 254-263, October 2003.
[18] M. Raynal and F. Tronel. Group Membership Failure Detection: a Simple Protocol and its Probabilistic Analysis. Distributed Systems Engineering Journal, 6(3): 95-102, September 1999.
[19] I. Sotoma and E. R. M. Madeira. Adaptation – to Adaptive Fault Monitoring and Algorithms and their Implementation on Corba. In Proc. of the Symposium on Distributed Objects and Application (DOA’2001), pages 219-228, September 2001.
[20] W. Vogels. World Wide Failures. In: Proceedings of the 7th ACM SIGOPS European Workshop, Sep. 1996.

---

**Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04)**
0-7695-2052-9/04 $ 20.00 © 2004 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021 at 12:29:59 UTC from IEEE Xplore. Restrictions apply.