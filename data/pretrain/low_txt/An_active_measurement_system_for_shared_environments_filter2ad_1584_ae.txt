### 0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.000
0.000
0.000
0.000
0.000

### Observations on System Load and Error Metrics
The interquartile range increased with the load, similar to our observations for send spacing timestamp errors, though the relative increase was much larger. These findings apply to both MAD (Measurement Accuracy Daemon) and the unprivileged process running in a standard slice. For instance, at the highest load scenario, the median error for the unprivileged process was approximately 16 microseconds, while the median error for MAD was around 10 microseconds. Similar results were observed when MAD ran as a privileged user-mode process and as a kernel module.

These results suggest that conducting packet pair experiments on highly loaded hosts may be inherently problematic. It is unclear whether the same issues affect probe methodologies using short streams at fixed rates, such as Pathload [21] or Yaz [41]. At the very least, these results warrant further experimentation with a wider range of load scenarios, exploring both CPU and network-intensive workloads. Additionally, it may be appropriate to examine a kernel-based approach that completely bypasses the system IP/UDP layers. Unfortunately, such an approach may require modifying the kernel.

### Summary of PlanetLab Tests
In summary, our tests using PlanetLab in a controlled environment reveal similar measurement problems as we observed using live PlanetLab systems. Our tests also show that using MAD significantly improves the situation and yields delay and loss measurements that accurately reflect the true state of the network. While our experiments show that MAD offers modest improvement for packet-pair experiments, additional study and improvements are needed.

### Non-virtualized Host Experiments
In our second laboratory setup, we used standard, unvirtualized workstations. Environments such as RON [10] form an important class of "raw system" testbeds for which MAD is designed. Therefore, it is crucial to evaluate MAD's performance in a similar setting.

We used two workstations in our setup, each running Linux kernel version 2.6.20. The machines were identical to those in our laboratory-based PlanetLab experiments, each equipped with a 2.0 GHz Pentium 4, 1 GB RAM, and Intel Pro/1000 network interfaces (with interrupt coalescence disabled). We again used Endace DAG 4.3 GE cards to gather ground truth measurements, similar to our previous experiments.

For these tests, we used Harpoon to create artificial CPU and network load on the hosts and testbed to compare the performance of an unprivileged user process for sending probes with MAD. Table 9 shows the four workload scenarios we used. As with the experiments using the controlled laboratory setup of PlanetLab, we configured the threads for generating constant bit-rate UDP traffic to consume a relatively large amount of processor time.

#### Table 9: Configurations and Characteristics of Laboratory Experiments with Non-virtualized Hosts
| Traffic Volume (each direction) | Average CPU Utilization |
|---------------------------------|-------------------------|
| 0 Mb/s                          | 0%                      |
| 2 Mb/s                          | 2%                      |
| 10 Mb/s                         | 60%                     |
| 50 Mb/s                         | 99%                     |

Using standard, unvirtualized hosts dramatically improved performance for the unprivileged user-mode application across all three measurement algorithms. In the case of round-trip delay, only the 99th percentile delay for the highest load scenario exceeded one millisecond, reaching two milliseconds. All other delay quantiles were below one millisecond. For packet loss, there was no loss measured in the lowest three load scenarios. However, in the highest load scenario, the loss frequency was 0.0053, and the mean duration of loss episodes was 1.12 seconds.

For the experiments using MAD, the 99th percentile delay was on the order of one hundred microseconds in all cases. There was no loss measured by the MAD-based probes in any case.

For the packet pair experiments, the results were similar to those in the PlanetLab-based laboratory experiments. Specifically, while errors in spacing of packet pairs upon sending and errors in timestamping the packet pair upon send were relatively low with a zero mean over a few tens of packet pairs, timestamp errors on receiving packet pairs grew larger with increased system load. The range in error values was similar to those in the controlled PlanetLab experiments. These results are true for both the unprivileged user-mode measurement application and for MAD. These results reinforce the hypothesis that running packet pair experiments on highly loaded hosts may be a situation to avoid entirely. Further experimentation and analysis are needed.

In summary, even with non-virtualized hosts, measurement inaccuracies can occur as system load becomes high. Our experiments show that MAD is also effective in these environments, eliminating spurious packet loss and yielding delay estimates that match ground truth measurements.

### Scalability of MAD
In our final laboratory experiments, we examined the scalability of MAD when subjected to a larger number of independent users and over a range of discrete time interval settings. We used a standard (unvirtualized) Linux host running kernel version 2.6.20, with the same machine configuration as in the other laboratory-based experiments. The duration of these experiments was two minutes.

We used up to 100 independent probe streams representing 100 users of MAD, each using a geometric probe process (Listing 2) with a single packet per probe of 100 bytes. The probability parameter for sending a probe at a given time slot was 0.2 for each probe stream. We also ran experiments using the BADABING code fragment (Listing 3) with three packets per probe, sent back-to-back. The results from those experiments were similar to the results for the geometrically distributed probe consisting of one packet.

We also examined MAD using a range of discrete time interval settings, from 10 milliseconds down to 100 microseconds. For each experiment, we measured system and user time available from the `getrusage` system call to derive a processor utilization figure for the MAD process. We also compared this utilization figure to that obtained using the standard `top` program. The results for each measurement technique were consistent.

#### Table 10: CPU Utilization Results of Running MAD with 100 Independent Probe Streams Over a Range of Discrete Time Intervals
| Interval (time slot) | CPU Utilization | Scheduler Errors (time slot misses) |
|----------------------|-----------------|------------------------------------|
| 10 milliseconds      | 0.1%            | 0                                  |
| 5 milliseconds       | 0.1%            | 0                                  |
| 1 millisecond        | 0.2%            | 0                                  |
| 500 microseconds     | 0.2%            | 0                                  |
| 100 microseconds     | 46.2%           | 8920                               |

From the table, we see that for time intervals as short as 500 microseconds, the overall utilization of MAD is minor, at about 0.2%. At intervals of 500 microseconds and larger, there were no scheduler errors reported. At an interval of 100 microseconds, however, utilization rises sharply and is accompanied by scheduler errors. With the default setting of 5 milliseconds (which is the same interval used in our earlier BADABING study) and even shorter intervals, MAD performs very well.

### Summary and Conclusions
Widely deployed, shared network testbeds are critical to the network research community. A particularly attractive class of experiments that could be considered in these environments are those that seek to measure end-to-end path properties such as delay and loss using active probe tools. Unfortunately, the resource contention overheads imposed in shared network testbeds can significantly bias measurement results from active probe tools—a compelling yet unfortunate example of the "tragedy of the commons" effect.

In this paper, we present results of a measurement study that quantifies the bias effects on active probe-based measurements in PlanetLab. Using hardware-based packet capture systems on our local PlanetLab nodes, we find that measurements of packet loss and delay from active probes can be skewed significantly.

These results motivate our development of MAD, a system for conducting highly accurate active measurements in shared environments. MAD is realized as a simple programming language that is made available to users via RPCs. The language enables a variety of active probe-based measurement streams to be scheduled in near real-time through the use of priority scheduling mechanisms available in recent Linux kernels. MAD's implementation as either a kernel module or user-mode daemon enables it to be deployed with minimal impact. Through a series of laboratory tests, we quantify the extent to which MAD can reduce bias in active probe-based measurements in both PlanetLab and non-virtualized environments. We show that MAD can improve measurement accuracy by orders of magnitude in PlanetLab, with lesser but still valuable effects in non-virtualized environments.

We plan to continue development of MAD in several ways. First, we intend to complete the implementation of the security mechanism that limits access of MAD to authorized users. Next, we plan to expand MADcode to support a broader set of active measurement methods, including those that are stream-based or adaptive. Finally, we will consider how basic MAD transmitter/receiver/reflector functionality might be ported to other OS environments so that highly accurate measurement capability might be more widely deployed.

### Acknowledgments
We thank our shepherd, Morley Mao, and the anonymous IMC reviewers for their input. We also thank Mike Blodgett for his assistance with the DAG systems. This work was supported in part by NSF grants CNS-0347252, CNS-0646256, CNS-0627102, and by Cisco Systems. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or Cisco Systems.

### References
[1] EverLab: Next Generation PlanetLab Network. http://www.everlab.org.
[2] KURT: Kansas University Real-time Linux. http://www.ittc.ku.edu/kurt/.
[3] MyPLC—A complete Planetlab Central (PLC) portable installation. http://www.planet-lab.org/doc/myplc.
[4] OneLab. http://www.fp6-ist-onelab.eu/.
[5] A new approach to kernel timers. http://lwn.net/Articles/152436/, September 2005.
[6] Linux kernel gains new real-time support. http://www.linuxdevices.com/news/-NS9566944929.html, October 2006.
[7] NSF CISE, GENI — Global Environment for Network Innovations. http://www.geni.net, 2007.
[27] A. Pásztor and D. Veitch. A precision infrastructure for active probing. In Proceedings of Passive and Active Measurement Workshop, Amsterdam, Netherlands, 2001.
[28] A. Pásztor and D. Veitch. PC-based Precision Timing without GPS. In Proceedings of ACM SIGMETRICS, Marina Del Rey, CA, June 2002.
[8] G. Almes, S. Kalidindi, and M. Zekauskas. A one-way delay metric for IPPM. IETF RFC 2679, September 1999.
[9] G. Almes, S. Kalidindi, and M. Zekauskas. A one way packet loss metric for IPPM. IETF RFC 2680, September 1999.
[30] V. Paxson, A. Adams, and M. Mathis. Experiences with NIMI. In Proceedings of Passive and Active Measurement Workshop, 2000.
[10] D. Andersen, H. Balakrishnan, F. Kaashoek, and R. Morris. Resilient overlay networks. In Proceedings of ACM Symposium on Operating Systems Principles, Banff, Alberta, Canada, 2001.
[11] M. Aron and P. Druschel. Soft Timers: Efficient Microsecond Software Timer Support for Network Processing. ACM Transactions on Computer Systems, August 2000.
[12] S. Banerjee, T. Griffin, and M. Pias. The interdomain connectivity of PlanetLab nodes. In Proceedings of Passive and Active Measurement Workshop, Antibes Juan-les-Pins, France, April 2004.
[13] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Heugebauer, I. Pratt, and A. Warfield. Xen and the Art of Virtualization. In Proceedings of ACM Symposium on Operating Systems Principles, October 2003.
[14] A. Bavier, M. Bowman, B. Chun, D. Culler, S. Karlin, S. Muir, L. Peterson, T. Roscoe, T. Spalink, and M. Wawrzoniak. Operating System Support for Planetary-Scale Network Services. In USENIX Symposium on Networked Systems Design and Implementation, March 2004.
[15] B. Bershad, S. Savage, P. Pardyak, E. Sirer, M. Fiuczynski, D. Becker, S. Eggers, and C. Chambers. Extensibility, safety, and performance in the SPIN operating system. In Proceedings of ACM Symposium on Operating Systems Principles, Copper Mountain Resort, CO, December 1995.
[16] J. Bolot. End-to-end packet delay and loss behavior in the internet. In Proceedings of ACM SIGCOMM, San Francisco, CA, September 1993.
[17] R. Carter and M. Crovella. Measuring bottleneck link speed in packet-switched networks. Performance Evaluation Review, 27-28:297–318, October 1996.
[18] National Research Council, editor. Looking Over the Fences at Networks: A Neighbor’s View of Networking Research. National Academy Press, 2001.
[19] D. Engler and M. Kaashoek. Exokernel: an operating system architecture for application-level resource management. In Proceedings of ACM Symposium on Operating Systems Principles, Copper Mountain Resort, CO, December 1995.
[20] V. Jacobson. Congestion avoidance and control. In Proceedings of ACM SIGCOMM, Stanford, CA, 1988.
[21] M. Jain and C. Dovrolis. End-to-end available bandwidth: Measurement methodology, dynamics, and relation to TCP throughput. In Proceedings of ACM SIGCOMM, Pittsburgh, PA, August 2002.
[22] S. Kalidindi and M. Zekauskas. Surveyor: An Infrastructure for Internet Performance Measurements. In Proceedings of INET ’99, 1999.
[23] R. Kapoor, L.-J. Chen, L. Lao, M. Gerla, and M. Y. Sanadidi. CapProbe: a simple and accurate capacity estimation technique. In Proceedings of ACM SIGCOMM, Portland, OR, August 2004.
[24] K. Lai and M. Baker. Measuring link bandwidths using a deterministic model of packet delay. In Proceedings of ACM SIGCOMM, Stockholm, Sweden, 2000.
[25] J. Liu and M. Crovella. Using loss pairs to discover network properties. In Proceedings of ACM Internet Measurement Workshop, San Francisco, CA, October 2001.
[26] K. Park and V. Pai. CoMon—A Monitoring Infrastructure for PlanetLab. http://comon.cs.princeton.edu/.
[31] H. Pucha, Y.C. Hu, and Z.M. Mao. On the Impact of Research Network based Testbeds on Wide-Area Experiments. In Proceedings of ACM Internet Measurement Conference, Rio de Janeiro, Brazil, October 2006.
[32] V. Ribeiro, R. Riedi, R. Baraniuk, J. Navratil, and L. Cottrell. pathChirp: Efficient Available Bandwidth Estimation for Network Paths. In Proceedings of Passive and Active Measurement Workshop, April 2003.
[33] M. Rosenblum and T. Garfinkel. Virtual Machine Monitors: Current Technology and Future Trends. IEEE Computer, May 2005.
[34] J. H. Saltzer, D. P. Reed, and D. D. Clark. End-to-end arguments in system design. ACM Transactions on Computer Systems, 2(4):277–288, November 1984.
[35] E. Sarmiento. Securing FreeBSD using Jail. Sys Admin, 10(5):31–37, May 2001.
[36] S. Soltesz, H. Pötzl, M. Fiuczynski, A. Bavier, and L. Peterson. Container-based Operating System Virtualization: A Scalable, High-performance Alternative to Hypervisors. In Proceedings of EuroSYS, 2007.
[37] J. Sommers and P. Barford. Self-configuring network traffic generation. In Proceedings of ACM Internet Measurement Conference, Taormina, Sicily, Italy, October 2004.
[38] J. Sommers, P. Barford, N. Duffield, and A. Ron. Improving Accuracy in End-to-end Packet Loss Measurement. In Proceedings of ACM SIGCOMM, Philadelphia, PA, August 2005.
[39] J. Sommers, P. Barford, N. Duffield, and A. Ron. A Framework for Multi-objective SLA Compliance Monitoring. In Proceedings of IEEE INFOCOM (minisymposium), Anchorage, AK, May 2007.
[40] J. Sommers, P. Barford, N. Duffield, and A. Ron. Accurate and Efficient SLA Compliance Monitoring. In To appear, Proceedings of ACM SIGCOMM, Kyoto, Japan, August 2007.
[41] J. Sommers, P. Barford, and W. Willinger. A proposed framework for calibration of available bandwidth estimation tools. In Proceedings of IEEE Symposium on Computer and Communication, Pula, Sardinia, Italy, June 2006.
[42] N. Spring, L. Peterson, A. Bavier, and V. Pai. Using PlanetLab for Network Research: Myths, Realities, and Best Practices. In Proceedings of the Second USENIX Workshop on Real, Large Distributed Systems (WORLDS ’05), San Francisco, CA, December 2005.
[43] N. Spring, D. Wetherall, and T. Anderson. Scriptroute: A Public Internet Measurement Facility. In Proceedings of USENIX Symposium on Internet Technologies and Systems (USITS), 2003.
[44] J. Strauss, D. Katabi, and F. Kaashoek. A measurement study of available bandwidth estimation tools. In Proceedings of ACM Internet Measurement Conference, Miami, FL, October 2003.
[45] Y. Zhang, R. West, and X. Qi. A virtual deadline scheduler for window-constrained service guarantees. In Proceedings of the 25th IEEE Real-time Systems Symposium (RTSS), December 2004.