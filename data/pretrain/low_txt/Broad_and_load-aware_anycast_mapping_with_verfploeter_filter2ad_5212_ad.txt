### Table 6: Quantifying Differences in B-Root Anycast with Different Measurement Methods and Times

Finally, we observe that the load in some regions, particularly in India, is higher than the number of IP blocks would suggest. This discrepancy can be attributed to a large number of users sharing relatively few IP blocks, often behind network address translation (NAT) devices.

### Figure 4: Geographic Distribution of DNS Traffic for B-Root and .nl

- **(a) Geographic distribution of load by site for B-Root, as inferred from Verfploeter (Dataset: LB-4-12).**
- **(b) Geographic distribution of load for .nl, as determined by traffic logs (Dataset: LN-4-12).**

**Quantifying Differences from VPs to Blocks to Load:**
While Figures 2 and 2b illustrate visual differences, Table 6 quantifies these differences and their impact on the assessment of catchment sizes in B-Root. Comparing Atlas, Verfploeter, and Verfploeter with load, we see significant variations in measurements (thousands of VPs, millions of blocks, or billions of queries per day). Load estimates (§3.2) determine different weighting factors, resulting in different fractions of traffic between the LAX and MIA sites, as shown in the “% LAX” column. In §5.5, we will compare these values to measured load to determine which method is most accurate. However, next, we will examine how these changes are even more pronounced for DNS services with less evenly distributed global load.

**Uneven Load:**
B-Root's load is global and largely follows the distribution of Internet users, so Figure 4a shows only moderate differences from Figure 2b. Other DNS systems, such as the .nl nameservers, are more regional. Figure 4b illustrates the load for four of the .nl nameservers, the country domain for the Netherlands. Since they cannot easily collect data from their two anycast nameservers, this plot may underrepresent global traffic, but it captures at least half of all global traffic to this domain.

Unlike B-Root, the majority of traffic to .nl originates from Europe and the Netherlands, with significant traffic also coming from the U.S. and other global sources. Given this client distribution, calibrating the measured catchment using load information is critical.

### 5.5 Using Verfploeter to Predict Load

Next, we evaluate the accuracy of Verfploeter’s load modeling in predicting future load. Our goal is to determine how unmappable blocks (§5.4) affect accuracy and how routing and load shift over time. We observe partial information and predict load for the unobserved remainder, then compare these predictions against complete information. A study of long-term predictions will require more experience with Verfploeter, but we address the basic accuracy question here.

We analyze the accuracy of load predictions with Verfploeter by examining the DNS network load at B-Root on 2017-05-15 (Dataset: LB-5-15) and the Verfploeter analysis performed on the same day (Dataset: SBV-5-15). Recall from Table 6 that although Verfploeter finds 87.8% of network blocks reach LAX, the load prediction is that 81.6% of traffic should go to LAX. This prediction does not account for blocks that send traffic to B-Root but do not respond to Verfploeter (12.9% from Table 5).

**Predicted vs. Measured Load:**
The last line of Table 6 shows the actual load of 81.4%, as measured at all B-Root sites on 2017-05-15. Our 81.6% prediction using same-day Verfploeter and load is very close to the measured result. This suggests that Verfploeter-unobservable blocks do not significantly affect our overall load estimate. Although they account for 17.6% of queries (Table 5, and the red slices in Figure 4a), the fraction of traffic that goes to each B-Root site appears to follow the ratio seen in measured blocks.

Our second observation is that load-weighted predictions are very close to observed load. Verfploeter without load adjustment is further off, with 87.8% of blocks going to LAX. We conclude that weighting by load is important. Surprisingly, Atlas estimates, at 82.4%, are actually closer than Verfploeter if Verfploeter is not load-weighted.

**Key Takeaway:**
With load-weighted Verfploeter, preliminary results suggest it is possible to make reasonable predictions about future anycast deployments by measuring the deployment on a test network and predicting future traffic levels using recent load data. We hope to expand these results beyond B-Root in ongoing work.

**Long-duration Predictions:**
Finally, we look at long-duration predictions. We performed a similar prediction analysis in advance of the B-Root deployment using Verfploeter data gathered on 2017-04-21 and network traffic from 2017-04-12. We observed a significant shift in blocks between these dates, with Verfploeter shifting from 82.4% to LAX in April to 87.8% in May. By weighting the SBV-4-21 Verfploeter dataset from the B-Root test prefix with the LB-4-12 measured load, we find that the predicted DNS request load arriving at LAX is 76.2%. This is significantly less than the 81.6% measured load in LB-5-15, highlighting the discrepancy between shifts in routing over one month.

This shift suggests that the accuracy of load estimates depends on the age of the data. Routing changes over time [9], indicating that long-duration predictions require careful consideration. We expect that predictions further into the future will be less accurate than short-term predictions. While we are collecting data to answer this question, such a study is part of future work.

### 6. Results: Understanding Anycast with Verfploeter

We use Verfploeter to explore three questions about anycast, each of which has been raised in prior work. Here, we use Verfploeter to revisit and refine these results.

**6.1 Use of AS Prepending in B-Root**
An important operational question for B-Root is understanding how to balance load between sites. Both sites can handle normal traffic, but DNS operators need to shift load during emergencies, such as DDoS attacks, which can be mitigated using multiple sites [33]. Operators may also want to control load during regular operation due to cost structures sensitive to traffic.

We used RIPE Atlas and Verfploeter to investigate the use of AS Prepending to adjust the catchment of a test prefix on B’s sites. AS Prepending is a traffic engineering approach where an operator increases the BGP path length at one site to make that route less desirable than other routes with shorter AS paths [37]. Figure 5 shows how the distribution changes as AS prepending is applied between the two sites, as measured with both methods. By default, with no prepending, 74% of Atlas VPs arrive at LAX, while Verfploeter shows that 78% of responsive /24 prefixes will arrive at LAX.

These results show that both measurement systems are useful for evaluating routing options. With only two sites, either method seems sufficient for rough analysis. We expect the greater precision of Verfploeter to be important with more sites and for deploying more subtle methods of route control, such as BGP communities [37].

**6.2 Discovering Divisions Within ASes**
Prior work often assumed that anycast catchments align with ASes, so one VP can represent the entire AS. While generally true for smaller ASes, this assumption is less likely to hold for large, multinational ASes, where different parts of the AS may be served by different anycast sites. Such ASes are likely to have geographically distributed peering locations and may prefer to direct some users to different anycast sites to reduce service latency.

Verfploeter allows us to test this assumption by looking for differences in anycast catchments within individual ASes. We first remove VPs showing instability (see §6.3) to prevent unstable routing from being classified as a division within the AS. Without removing these VPs, we observe approximately 2% more divisions. We count the number of sites seen within a single AS in a single measurement round.

In total, we see multiple sites from 7,188 ASes, or approximately 12.7% of all ASes announcing at least a single prefix at the time of the measurement. This is a lower bound; a larger and/or more diverse anycast service might determine a higher and more accurate percentage of ASes split into multiple individually routed parts.

Routing policies, like hot-potato routing, are a likely cause for these divisions. As routing on the Internet is largely determined by BGP, we show the number of prefixes announced via BGP by an AS versus the number of sites it sees in Figure 7. ASes that announce more prefixes tend to see a higher number of sites from their network.

Figure 8 shows the number of sites seen from announced prefixes, grouped by prefix length. VPs in prefixes longer than a /15 are mapped to more than a single site in most cases. Even though 80% of these routed prefixes are covered by one VP, these are all small prefixes. About 20% of these routed prefixes see more than one site and require multiple prefixes, but larger prefixes are often divided further—75% of prefixes larger than /10s see multiple sites and require multiple VPs. Although only 20% of prefixes, multiple VPs are required in prefixes that account for approximately 38% of the measured address space.

These results show that, to get a complete view of the catchment, more than a single VP per AS is often needed. While the quantitative results are specific to B-Root and Tangled, the qualitative result (ASes can be subdivided) applies more generally. Measurements from platforms with fewer VPs often assume that each VP can represent its AS but may lose precision in large ASes.

**6.3 Stability of Anycast for Clients**
A long-term concern with anycast is the stability of the association of an anycast client with its site [48]. Since TCP connections require shared state at both ends, if users switch anycast sites within the lifetime of a TCP connection, that connection will break and need to be restarted. The existence of multiple successful CDNs that use IP anycast (including Bing, Edgecast, and Cloudflare) suggests that anycast is almost always stable, but recent work has suggested that anycast may be persistently unstable for a tiny fraction of (user, service) combinations (less than 1%) [48]. From the viewpoint of a service operator, it is interesting to know if a single measurement can be representative for a longer time, or if the catchment is continuously in flux.

Verfploeter allows us to revisit this question from Tangled to many VPs. We measured the global catchment of our testbed every 15 minutes for a day (96 observations). Considering the short-lived nature of many TCP connections, this interval might be too long to detect rapid fluctuations, but it is enough to give an impression of the overall stability of catchments. We categorize the responses (or non-responses) into four groups: stable, flipped, to-NR, and from-NR. We do not count VPs that remain non-responsive after being counted as to-NR.

Figure 9 shows the results of one day of these measurements. The catchment is very stable across the measurement rounds, with a median of 3.54M (about 95% of the 3.71M that respond) VPs always replying and maintaining their prior catchment. The fraction of VPs that fluctuate between responsive and non-responsive states is small across all 96 measurements. A median of 89k (about 2.4%) VPs changed from responsive to non-responsive between measurements, and about the same number flipping back. Note that fluctuating and flipping VPs are not necessarily always the same ones.