### 3.8 Impact of DCA
All our experiments so far have been conducted with DCA (Direct Cache Access) enabled, which is the default setting on Intel Xeon processors. To understand the benefits of DCA, we reran the single flow scenario from §3.1 with DCA disabled. Figure 12(a) shows the throughput-per-core without DCA relative to the scenario with DCA enabled (Default), as each of the optimizations are incrementally enabled. As expected, with all optimizations enabled, we observed a 19% degradation in throughput-per-core when DCA was disabled. Specifically, the effectiveness of aRFS (Accelerated Receive Flow Steering) was reduced by approximately 50%, which is expected since disabling DCA reduces the data copy benefits of NIC DMAing the data directly into the L3 cache. The other benefits of aRFS (§3.1) still apply. Without DCA, the receiver-side remains the bottleneck, and we did not observe any significant shift in the CPU breakdowns at the sender and receiver (Figures 12(b) and 12(c)).

### 3.9 Impact of IOMMU
IOMMU (IO Memory Management Unit) is used in virtualized environments to efficiently virtualize fast IO devices. Even in non-virtualized environments, IOMMU is useful for memory protection. With IOMMU, devices specify virtual addresses in DMA requests, which the IOMMU translates into physical addresses while implementing memory protection checks. By default, the IOMMU is disabled in our setup. In this subsection, we study the impact of IOMMU on Linux network stack performance for the single flow scenario (§3.1).

The key takeaway from this subsection is that IOMMU, due to increased memory management overheads, results in significant degradation in network stack performance. As seen in Figure 12(a), enabling IOMMU reduces throughput-per-core by 26% (compared to Default). Figures 12(b) and 12(c) show the core reason for this degradation: memory alloc/dealloc becomes more prominent in CPU consumption at both the sender and receiver (now consuming 30% of CPU cycles at the receiver). This is because of two additional per-page operations required by IOMMU:
1. When the NIC driver allocates new pages for DMA, it has to insert these pages into the device’s pagetable (domain) on the IOMMU.
2. Once DMA is done, the driver has to unmap those pages.

These two additional per-page operations result in increased overheads.

### 3.10 Impact of Congestion Control Protocols
Our experiments so far have used TCP CUBIC, the default congestion control algorithm in Linux. We now study the impact of congestion control algorithms on network stack performance using two other popular algorithms implemented in Linux, BBR [8] and DCTCP [1], again for the single flow scenario (§3.1). Figure 13(a) shows that the choice of congestion control algorithm has minimal impact on throughput-per-core. This is because, as discussed earlier, the receiver-side is the core throughput bottleneck in high-speed networks; all these algorithms being "sender-driven" have minimal differences in the receiver-side logic. Indeed, the receiver-side CPU breakdowns are largely the same for all protocols (Figure 13(c)). BBR has relatively higher scheduling overheads on the sender-side (Figure 13(b)); this is because BBR uses pacing for rate control (with qdisc) [42], and repeated thread wakeups when packets are released by the pacer result in increased scheduling overhead.

### 4. Future Directions
We have already discussed several immediate avenues of future research in individual subsections—e.g., optimizations to today’s Linux network stack (e.g., independent scaling of each processing layer in the stack, rethinking TCP auto-tuning mechanisms for receive buffer sizing, window/rate mechanisms incorporating host bottlenecks, etc.), extensions to DCA (e.g., revisiting L3 cache management, support for NIC-remote NUMA nodes, etc.), and, in general, the idea of considering host bottlenecks when designing network stacks for high-speed networks. In this section, we outline a few more forward-looking avenues of future research.

#### Zero-copy Mechanisms
The Linux kernel has recently introduced new mechanisms to achieve zero-copy transmission and reception on top of the TCP/IP stack:
- **For zero-copy on the sender-side:** The kernel now has the MSG_ZEROCOPY feature [11] (since kernel 4.14), which pins application buffers upon a send system call, allowing the NIC to directly fetch this data through DMA reads.
- **For zero-copy on the receiver-side:** The kernel now supports a special mmap overload for TCP sockets [12] (since kernel 4.18). This implementation enables applications to obtain a virtual address that is mapped by the kernel to the physical address where the NIC DMAs the data.

Some specialized applications [13, 26] have demonstrated achieving ~100Gbps of throughput-per-core using the sender-side zero-copy mechanism. However, as we showed in §3, the receiver is likely to be the throughput bottleneck for many applications in today’s Linux network stack. Hence, it is more crucial to eliminate data copy overheads on the receiver-side. Unfortunately, the above receiver-side zero-copy mechanism requires changes in the memory management semantics, and thus requires non-trivial application-layer modifications. Linux eXpress Data Path (XDP) [23] offers zero-copy operations for applications that use AF_XDP socket [29] (introduced in kernel 4.18), but requires reimplementation of the entire network and transport protocols in the userspace. It would be interesting to explore zero-copy mechanisms that do not require application modifications and/or reimplementation of network protocols; if feasible, such mechanisms will allow today’s Linux network stack to achieve 100Gbps throughput-per-core with minimal or no modifications.

#### CPU-efficient Transport Protocol Design
The problem of transport design has traditionally focused on designing congestion and flow control algorithms to achieve a multi-objective optimization goal (e.g., a combination of objectives like low latency, high throughput, etc.). This state of affairs is because, for the Internet and for early incarnations of datacenter networks, performance bottlenecks were primarily in the core of the network. Our study suggests that this is no longer the case: adoption of high-bandwidth links shifts performance bottlenecks to the host. Thus, future protocol designs should explicitly orchestrate host resources (just like they orchestrate network resources today), e.g., by taking not just traditional metrics like latency and throughput into account, but also available cores, cache sizes, and DCA capabilities. Recent receiver-driven protocols [18, 35] have the potential to enable such fine-grained orchestration of both the sender and the receiver resources.

#### Rearchitecting the Host Stack
We discuss two directions in relatively clean-slate design for future network stacks:
1. **Dynamic Packet Processing Pipeline:** Today’s network stacks use a fairly static packet processing pipeline for each connection—the entire pipeline (buffers, protocol processing, host resource provisioning, etc.) is determined at the time of socket creation and remains unchanged during the socket lifetime, independent of other connections and their host resource requirements. This is one of the core reasons for the many bottlenecks identified in our study: when the core performing data copy becomes the bottleneck for long flows, there is no way to dynamically scale the number of cores performing data copy; even if short flows and long flows have different bottlenecks, the stack uses a completely application-agnostic processing pipeline; and there is no way to dynamically allocate host resources to account for changes in contention upon new flow arrivals. As performance bottlenecks shift to hosts, we should rearchitect the host network stack to achieve a design that is both more dynamic (allows transparent and independent scaling of host resources to individual connections) and more application-aware (exploits characteristics of applications colocated on a server to achieve improved host resource orchestration).
2. **Co-designing CPU Schedulers with Network Stack:** Specifically, CPU schedulers in operating systems have traditionally been designed independently of the network stack. This was beneficial for the independent evolution of the two layers. However, with increasingly many distributed applications and with performance bottlenecks shifting to hosts, we need to revisit such a separation. For instance, our study shows that network-aware CPU scheduling (e.g., scheduling applications that generate long flows on NIC-local NUMA node, scheduling long-flow and short-flow applications on separate CPU cores, etc.) has the potential to lead to efficient host stacks.

### 5. Conclusion
We have demonstrated that the recent adoption of high-bandwidth links in datacenter networks, coupled with relatively stagnant technology trends for other host resources (e.g., core speeds and count, cache sizes, etc.), marks a fundamental shift in host network stack bottlenecks. Using measurements and insights for Linux network stack performance for 100Gbps links, our study highlights several avenues for future research in designing CPU-efficient host network stacks. These are exciting times for networked systems research—with the emergence of Terabit Ethernet, the bottlenecks outlined in this study are going to become even more prominent, and it is only by bringing together operating systems, computer networking, and computer architecture communities that we will be able to design host network stacks that overcome these bottlenecks. We hope our work will enable a deeper understanding of today’s host network stacks and will guide the design of not just the future Linux kernel network stack, but also future network and host hardware.

### Acknowledgments
We thank our shepherd, Neil Spring, SIGCOMM reviewers, Shrijeet Mukherjee, Christos Kozyrakis, and Amin Vahdat for their insightful feedback. This work was supported by NSF grants CNS-1704742 and CNS-2047283, a Google faculty research scholar award, and a Sloan fellowship. This work does not raise any ethical concerns.

### References
[1] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010. Data Center TCP (DCTCP). In ACM SIGCOMM.
[2] Amazon. 2021. Amazon EC2 F1 Instances. https://aws.amazon.com/ec2/instance-types/f1/. (2021).
[3] Mina Tahmasbi Arashloo, Alexey Lavrov, Manya Ghobadi, Jennifer Rexford, David Walker, and David Wentzlaff. 2020. Enabling Programmable Transport Protocols in High-Speed NICs. In USENIX NSDI.
[4] Adam Belay, George Prekas, Ana Klimovic, Samuel Grossman, Christos Kozyrakis, and Edouard Bugnion. 2014. IX: A Protected Dataplane Operating System for High Throughput and Low Latency. In USENIX OSDI.
[5] Theophilus Benson, Aditya Akella, and David A Maltz. 2010. Network traffic characteristics of data centers in the wild. In IMC.
[6] Zhan Bokai, Yu Chengye, and Chen Zhonghe. 2005. TCP/IP Offload Engine (TOE) for an SOC System. https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/dc/_3_3-2005_taiwan_3rd_chengkungu-web.pdf. (2005).
[7] Qizhe Cai, Shubham Chaudhary, Midhul Vuppalapati, Jaehyun Hwang, and Rachit Agarwal. 2021. Understanding Host Network Stack Overheads. https://github.com/Terabit-Ethernet/terabit-network-stack-profiling. (2021).
[8] Neal Cardwell, Yuchung Cheng, C. Stephen Gunn, Soheil Hassas Yeganeh, and Van Jacobson. 2016. BBR: Congestion-Based Congestion Control. ACM Queue 14, September-October (2016), 20 – 53.
[9] Adrian M Caulfield, Eric S Chung, Andrew Putnam, Hari Angepat, Jeremy Fowers, Michael Haselman, Stephen Heil, Matt Humphrey, Puneet Kaur, Joo-Young Kim, et al. 2016. A cloud-scale acceleration architecture. In IEEE/ACM MICRO.
[10] Jonathan Corbet. 2009. JLS2009: Generic receive offload. https://lwn.net/Articles/358910/. (2009).
[11] Jonathan Corbet. 2017. Zero-copy networking. https://lwn.net/Articles/726917/. (2017).
[12] Jonathan Corbet. 2018. Zero-copy TCP receive. https://lwn.net/Articles/752188/. (2018).
[13] Patrick Dehkord. 2019. NVMe over TCP Storage with SPDK. https://ci.spdk.io/download/events/2019-summit/(Solareflare)+NVMe+over+TCP+Storage+with+SPDK.pdf. (2019).
[14] Jon Dugan, John Estabrook, Jim Ferbuson, Andrew Gallatin, Mark Gates, Kevin Gibbs, Stephen Hemminger, Nathan Jones, Gerrit Renker Feng Qin, Ajay Tirumala, and Alex Warshavsky. 2021. iPerf - The ultimate speed test tool for TCP, UDP and SCTP. https://iperf.fr/. (2021).
[15] Alireza Farshin, Amir Roozbeh, Gerald Q. Maguire Jr., and Dejan Kostić. 2020. Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks. In USENIX ATC.
[16] Daniel Firestone, Andrew Putnam, Sambhrama Mundkur, Derek Chiou, Alireza Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian Caulfield, Eric Chung, et al. 2018. Azure accelerated networking: SmartNICs in the public cloud. In USENIX NSDI.
[17] The Linux Foundation. 2016. Linux Foundation DocuWiki: napi. https://wiki.linuxfoundation.org/networking/napi. (2016).
[18] Peter X Gao, Akshay Narayan, Gautam Kumar, Rachit Agarwal, Sylvia Ratnasamy, and Scott Shenker. 2015. phost: Distributed near-optimal datacenter transport over commodity network fabric. In ACM CoNEXT.
[19] Sebastien Godard. 2021. Performance monitoring tools for Linux. https://github.com/sysstat/sysstat. (2021).
[20] Brendan Gregg. 2020. Linux perf Examples. http://www.brendangregg.com/perf.html. (2020).
[21] Sangjin Han, Scott Marshall, Byung-Gon Chun, and Sylvia Ratnasamy. 2012. MegaPipe: A New Programming Interface for Scalable Network I/O. In USENIX OSDI.
[22] HewlettPackard. 2021. Netperf. https://github.com/HewlettPackard/netperf. (2021).
[23] Toke Høiland-Jørgensen, Jesper Dangaard Brouer, Daniel Borkmann, John Fastabend, Tom Herbert, David Ahern, and David Miller. 2018. The eXpress Data Path: Fast Programmable Packet Processing in the Operating System Kernel. In ACM CoNEXT.
[24] Jaehyun Hwang, Qizhe Cai, Ao Tang, and Rachit Agarwal. 2020. TCP ≈ RDMA: CPU-efficient Remote Storage Access with i10. In USENIX NSDI.
[25] Intel. 2012. Intel® Data Direct I/O Technology. https://www.intel.com/content/dam/www/public/us/en/documents/technology-briefs/data-direct-i-o-technology-brief.pdf. (2012).
[26] Intel. 2020. SPDK NVMe-oF TCP Performance Report. https://ci.spdk.io/download/performance-reports/SPDK_tcp_perf_report_2010.pdf. (2020).
[27] EunYoung Jeong, Shinae Woo, Muhammad Jamshed, Haewon Jeong, Sunghwan Ihm, Dongsu Han, and KyoungSoo Park. 2014. mTCP: a Highly Scalable User-level TCP Stack for Multicore Systems. In USENIX NSDI.
[28] Srikanth Kandula, Sudipta Sengupta, Albert Greenberg, Parveen Patel, and Ronnie Chaiken. 2009. The nature of data center traffic: measurements & analysis. In IMC.
[29] Magnus Karlsson and Björn Töpel. 2018. The Path to DPDK Speeds for AF XDP. In Linux Plumbers Conference.
[30] Antoine Kaufmann, Tim Stamler, Simon Peter, Naveen Kr. Sharma, Arvind Krishnamurthy, and Thomas Anderson. 2019. TAS: TCP Acceleration as an OS Service. In ACM Eurosys.
[31] Yuliang Li, Rui Miao, Hongqiang Liu, Yan Zhuang, Fei Feng, Lingbo Tang, Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, and Minlan Yu. 2019. HPCC: High Precision Congestion Control. In ACM SIGCOMM.
[32] Xiaofeng Lin, Yu Chen, Xiaodong Li, Junjie Mao, Jiaquan He, Wei Xu, and Yuanchun Shi. 2016. Scalable Kernel TCP Design and Implementation for Short-Lived Connections. In ACM ASPLOS.
[33] Ilias Marinos, Robert NM Watson, and Mark Handley. 2014. Network stack specialization for performance. ACM SIGCOMM Computer Communication Review 44, 4 (2014), 175–186.
[34] Radhika Mittal, Alexander Shpiner, Aurojit Panda, Eitan Zahavi, Arvind Krishnamurthy, Sylvia Ratnasamy, and Scott Shenker. 2018. Revisiting Network Support for RDMA. In ACM SIGCOMM.
[35] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018. Homa: A Receiver-driven Low-latency Transport Protocol Using Network Priorities. In ACM SIGCOMM.
[36] George Prekas, Marios Kogias, and Edouard Bugnion. 2017. ZygOS: Achieving Low Tail Latency for Microsecond-scale Networked Tasks. In ACM SOSP.
[37] Quoc-Thai V Le, Jonathan Stern, and Stephen M Brenner. 2017. Fast memcpy with SPDK and Intel I/OAT DMA Engine. https://software.intel.com/content/www/us/en/develop/articles/fast-memcpy-using-spdk-and-ioat-dma-engine.html. (2017).
[38] Livio Soares and Michael Stumm. 2010. FlexSC: Flexible System Call Scheduling with Exception-Less System Calls. In USENIX OSDI.
[39] Amin Tootoonchian, Aurojit Panda, Chang Lan, Melvin Walls, Katerina Argyraki, Sylvia Ratnasamy, and Scott Shenker. 2018. ResQ: Enabling SLOs in Network Function Virtualization. In USENIX NSDI.
[40] Vijay Vasudevan, David G. Andersen, and Michael Kaminsky. 2011. The Case for VOS: The Vector Operating System. In USENIX HotOS.
[41] Kenichi Yasukata, Michio Honda, Douglas Santry, and Lars Eggert. 2016. StackMap: Low-Latency Networking with the OS Stack and Dedicated NICs. In USENIX ATC.
[42] Neal Cardwell Yuchung Cheng. [n. d.]. Making Linux TCP Fast. "https://netdevconf.info/1.2/papers/bbr-netdev-1.2.new.new.pdf". ([n. d.]).
[43] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn, Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and Ming Zhang. 2015. Congestion control for large-scale RDMA deployments. ACM SIGCOMM Computer Communication Review 45, 4 (2015), 523–536.