### National Service Port and Host Behavior Similarity

When considering national service ports, we expect a higher degree of similarity in the behavior of hosts. This is because destination ports are often associated with specific services, and most hosts will have popular ports open, such as port 80 (HTTP) and port 443 (HTTPS). In this context, crowd-blending fuzzes approximately 40% of all data points, while commoner privacy fuzzes around 20%.

For query q3 (Fig. 4(c)), which focuses on connection count, both crowd-blending and commoner privacy perform similarly, fuzzing 60-80% of all data points. This still provides a significant advantage over differential privacy.

Similarly, for query q4 (Fig. 4(d)), which examines traffic volume, crowd-blending and commoner privacy also perform comparably, fuzzing 65-95% of all data points. Commoner privacy typically fuzzes about 5% fewer points than crowd-blending. The STDEV and MAD algorithms perform equally well in this scenario.

### Quantifying Utility Loss and Privacy Risk

To evaluate the utility loss and privacy risk associated with each privacy mechanism, we use the following measures:

#### Utility Loss
We adopt a normalized error to quantify utility loss, defined as:
\[
\bar{E} = \frac{\sum_{i=1}^{N} |f_i - t_i|}{\sum_{i=1}^{N} |t_i|}
\]
where \( t_i \) are the true outputs of a query (without any privacy protections), and \( f_i \) are the outputs generated by the privacy mechanism. This measure represents the relative, cumulative difference between the true and fuzzed data points, with lower values indicating higher research utility. It reflects the fraction of change in the outputs due to fuzzing.

#### Privacy Risk
Privacy risk is measured by estimating the number of output data points that can be used to identify individuals (hosts in our case). We assume that any data about an individual can become a quasi-identifier if it is unique to that individual and can be detected from the mechanism's output. We consider a data point to be identifiable if we can reverse the fuzzing and if the data point contains contributions of individuals that are both unique and outliers.

The measure of privacy risk is defined as:
\[
\bar{I} = \frac{\sum_{i=1}^{N} \text{reverse\_and\_identifiable}(f_i)}{N}
\]
where the function `reverse_and_identifiable` returns 1 if we can reverse the fuzzing for the data point and it contains contributions of individuals that are both unique and outliers. Since both crowd-blending and commoner privacy remove outlier contributions (and crowd-blending also removes all unique contributions), their privacy risk measure is always zero. Differential privacy, however, adds noise to the data. If the noise is not sufficiently large, we may be able to reverse the fuzzing and potentially identify individuals.

Figure 5 illustrates the utility loss (\( \bar{E} \)) and privacy risk (\( \bar{I} \)) for queries q1–q4, protected by differential privacy, as we vary \( \Delta f / \epsilon \). As expected, high values lead to high privacy risk and low utility loss, while low values eliminate privacy risk but increase utility loss.

Figure 6 shows the utility loss for differential privacy, crowd-blending, and commoner privacy (STDEV and MAD) assuming the value of \( \Delta f / \epsilon \) results in zero privacy risk. We vary the parameter \( k \) for crowd-blending and commoner privacy. Both crowd-blending and commoner privacy maintain a utility loss under 1 for all queries, whereas differential privacy has a utility loss in the [100, 10,000] range. For query q1, commoner privacy with STDEV outlier detection loses only 47–82% of utility, while crowd-blending consistently loses 98–99%. Similarly, for query q2, commoner privacy loses less than 70–71% of utility, while crowd-blending consistently loses 98–99%. For queries q3 and q4, crowd-blending incurs around 99% utility cost, while commoner privacy significantly outperforms crowd-blending, achieving a smaller than 51% utility loss even with \( K = 50 \) in q3 and q4.

In summary, with comparable privacy protections, crowd-blending and commoner privacy provide many orders of magnitude higher utility than differential privacy. In many cases, commoner privacy outperforms crowd-blending, halving its relative utility cost and providing 9–49 times higher utility.

### Scalability of Query Introspection

We implemented query introspection in our Patrol system and evaluated its scalability as the number of related queries increases. We ran three select queries 100 times without optimizations proposed in Section 4.1: (qi1) histogram of all TCP packets, grouped by source port; (qi2) same as qi1 but excluding packets smaller than 100 bytes; (qi3) same as qi1 but excluding packets outside a certain, small time range. The overhead of query introspection grew linearly from 1% of the total query processing time for the second round to 35–60% of the total runtime for the 100th round. Note that our implementation has no optimizations and runs as a single thread. Further optimization and multi-threading could significantly reduce query introspection's overhead.

### Related Work

In this section, we discuss work closely related to commoner privacy.

De Montjoye et al. [5] discuss the re-identifiability of common user data, highlighting the conflicting requirements for utility and privacy.

Privacy protections for network trace data have been studied extensively [3, 4, 18, 19, 25, 30, 31, 33, 36], primarily through sanitization and release of sanitized data. However, we explore a different approach where data remains with its provider.

Secure queries on network traces were sketched by us in [26] and explored by others using differential privacy [24] or manual vetting of queries [28]. Mittal et al. [27] propose mediated trace analysis, where researchers submit annotated black-box programs, and data providers repeatedly apply them to network traces with cleverly modified IP addresses, comparing outputs to detect dependencies on IP address data. Our focus is on preventing host re-identification through any quasi-identifier, not limited to IP addresses.

Sweeney et al.’s k-anonymity [34] has been followed by various extensions, including (α, k)-anonymity [35], l-diversity [23], and t-closeness [22], applied to clustering [2], location privacy [13], etc. The main difference between k-anonymity and interactive k-anonymity is that the former is applied to original data, which is then released, while the latter is applied to individual contributions to query outputs. While k-anonymity is susceptible to tracker attacks [8, 34], we prove in Section 4.1 that commoner privacy is not.

Regarding differential privacy and its challenges, Haeberlen et al. [16] found that systems like Airavat can be manipulated to mark a whole query as "not differentially private" and abort, which can be used as proof of an individual's presence in a dataset. Researchers have struggled with setting appropriate values of \( \epsilon \) and the privacy budget to achieve strong privacy guarantees [16]. He et al. [17] propose Blowfish to automatically set these values to balance privacy and utility. Our work achieves better utility while providing sufficient privacy protection for network trace analysis.

### Conclusions

Data privacy is increasingly important as our daily interactions generate rich datasets that may be analyzed and shared by data providers. Differential privacy and, more recently, crowd-blending have been proposed as mechanisms to support complex queries over datasets while ensuring strong privacy protections. Differential privacy performs well in many scenarios but incurs high utility costs on long-tailed datasets. Crowd-blending reduces this utility cost but only when a sufficient number of individuals have the same records in the dataset, a rare situation in datasets with features having a large range of values.

We have proposed commoner privacy and a mechanism to achieve it—interactive k-anonymity. We have shown how commoner privacy can hold under query composition with careful recording and checking of queries. Commoner privacy improves the utility of query outputs on long-tailed and large-value-range datasets compared to differential and crowd-blending privacy, albeit at the cost of lower privacy guarantees. Specifically, commoner privacy cannot defend against an all-but-one adversary but defends against an interactive adversary who cannot learn anything specific about an individual nor whether an individual is present in the dataset. While commoner privacy may not be the best fit for some applications, we believe it is a competitive approach with a realistic adversary model, preserving much more research utility than state-of-the-art approaches at a modest privacy cost.

### Acknowledgments

This material is based upon work supported by the National Science Foundation, grant number 0914780. The authors are grateful to anonymous reviewers for their helpful comments.

### References

[1] MAWI Working Group Traffic Archive. http://tracer.csl.sony.co.jp/mawi/.

[2] G. Aggarwal, T. Feder, K. Kenthapadi, S. Khuller, R. Panigrahy, D. Thomas, and A. Zhu. Achieving anonymity via clustering. In Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 153–162. ACM, 2006.

[3] S. Coull, M. Collins, C. Wright, F. Monrose, and M. Reiter. On Web Browsing Privacy in Anonymized NetFlows. In Proceedings of the USENIX Security Symposium, August 2007.

[4] S. Coull, C. Wright, F. Monrose, M. Collins, and M. Reiter. Playing Devil’s Advocate: Inferring Sensitive Information from Anonymized Network Traces. In Proceedings of the Network and Distributed System Security Symposium, February 2007.

[5] Y.-A. de Montjoye, L. Radaelli, V. K. Singh, and A. S. Pentland. Unique in the shopping mall: On the re-identifiability of credit card metadata. High Impact Journal, 2014.

[6] X. Deng and J. Mirkovic. Patrol homepage. http://patrol.isi.edu/.

[7] D. E. Denning. A security model for the statistical database problem. In Proceedings of the Second International Workshop on Statistical Database Management, pages 368–390. Lawrence Berkeley Laboratory, 1983.

[8] D. E. Denning, P. J. Denning, and M. D. Schwartz. The tracker: A threat to statistical database security. ACM Transactions on Database Systems (TODS), 4(1):76–96, 1979.

[9] D. E. Denning and J. Schlörer. A fast procedure for finding a tracker in a statistical database. ACM Transactions on Database Systems (TODS), 5(1):88–102, 1980.

[10] C. Dwork. Differential Privacy. In Proceedings of the 33rd International Colloquium on Automata, Languages and Programming, 2006.

[11] C. Dwork. Differential privacy: A survey of results. In International Conference on Theory and Applications of Models of Computation, pages 1–19. Springer, 2008.

[12] S. R. Ganta, S. P. Kasiviswanathan, and A. Smith. Composition attacks and auxiliary information in data privacy. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 265–273. ACM, 2008.

[13] B. Gedik and L. Liu. A customizable k-anonymity model for protecting location privacy. 2004.

[14] J. Gehrke, M. Hay, E. Lui, and R. Pass. Crowd-blending privacy. In Advances in Cryptology, pages 479–496. Springer, 2012.

[15] I. Guttman and D. E. Smith. Investigation of rules for dealing with outliers in small samples from the normal distribution: I: Estimation of the mean. Technometrics, 11(3):527–550, 1969.

[16] A. Haeberlen, B. C. Pierce, and A. Narayan. Differential privacy under fire. In USENIX Security Symposium, 2011.

[17] X. He, A. Machanavajjhala, and B. Ding. Blowfish privacy: Tuning privacy-utility trade-offs using policies. In Proceedings of the 2014 ACM SIGMOD international conference on Management of data, pages 1447–1458. ACM, 2014.

[18] E. Kohler. Ipaggregate tool. http://www.cs.ucla.edu/~kohler/ipsumdump/aggcreateman.html.

[19] E. Kohler. Ipsumdump tool. http://www.cs.ucla.edu/~kohler/ipsumdump/.

[20] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Incognito: Efficient full-domain k-anonymity. In Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 49–60. ACM, 2005.

[21] C. Leys, C. Ley, O. Klein, P. Bernard, and L. Licata. Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median. Journal of Experimental Social Psychology, 49(4):764–766, 2013.

[22] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and l-diversity. In ICDE, volume 7, pages 106–115, 2007.

[23] A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam. l-diversity: Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1):3, 2007.

[24] F. McSherry and R. Mahajan. Differentially-private network trace analysis. ACM SIGCOMM Computer Communication Review, 41(4):123–134, 2011.

[25] G. Minshall. tcpdpriv tool. http://ita.ee.lbl.gov/html/contrib/tcpdpriv.html.

[26] J. Mirkovic. Privacy-safe network trace sharing via secure queries. In Proceedings of the 1st ACM workshop on Network data anonymization, pages 3–10. ACM, 2008.

[27] P. Mittal, V. Paxson, R. Sommer, and M. Winterrowd. Securing mediated trace access using black-box permutation analysis. In HotNets, 2009.

[28] J. C. Mogul and M. Arlitt. Sc2d: An alternative to trace anonymization. In Proceedings of the SIGCOMM 2006 Workshop on Mining Network Data, 2006.

[29] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 75–84. ACM, 2007.

[30] R. Pang, M. Allman, V. Paxson, and J. Lee. The devil and packet trace anonymization. ACM SIGCOMM Computer Communications Review, 36(1):29—38, 2006.

[31] R. Pang and V. Paxson. A High-level Programming Environment for Packet Trace Anonymization and Transformation. In Proceedings of ACM SIGCOMM, 2003.

[32] J. Schlörer. Disclosure from statistical databases: quantitative aspects of trackers. ACM Trans. Database Sys., 5(4):467–492, 1980.

[33] Q. Sun, D. R. Simon, Y. Wang, W. Russell, V. N. Padmanabhan, and L. Qiu. Statistical Identification of Encrypted Web Browsing Traffic. In Proceedings of the IEEE Symposium on Security and Privacy, 2002.

[34] L. Sweeney. k-anonymity: A model for protecting privacy. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(05):557–570, 2002.

[35] R. C.-W. Wong, J. Li, A. W.-C. Fu, and K. Wang. (α, k)-anonymity: an enhanced k-anonymity model for privacy preserving data publishing. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 754–759. ACM, 2006.

[36] J. Xu, J. Fan, M. H. Ammar, and S. B. Moon. Prefix-Preserving IP Address Anonymization: Measurement-Based Security Evaluation and a New Cryptography-Based Scheme. In Proceedings of the IEEE International Conference on Network Protocols, 2002.