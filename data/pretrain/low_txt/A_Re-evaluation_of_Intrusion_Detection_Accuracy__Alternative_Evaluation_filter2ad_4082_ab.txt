### 2.3 Experiments with KDD Variants

The KDD Cup 1999 dataset is one of the most well-known datasets for Network Intrusion Detection Systems (NIDS). However, several issues have been reported with this dataset [5][4], leading to the creation of different variations. In our experiments, we will use the NSL-KDD and gureKDD datasets.

- **NSL-KDD** is a refined version of the original KDD dataset [13].
- **gureKDD** is a newly generated dataset designed for the same purpose as KDD [6].

Several studies have utilized the NSL-KDD dataset, achieving high accuracy rates. For example:
- A study using Support Vector Machines (SVM) reported an accuracy of 99% for both binary and multiclass classification [7].
- Another study using K-Nearest Neighbors (K-NN) achieved 99.6% accuracy [8].
- A third study using Recurrent Neural Networks (RNN) reported an accuracy of 97.09% [14].

In our experiments, we tested various models on both NSL-KDD and gureKDD. The results, shown in Table 1, indicate that most models achieve an F1 score of approximately 99% for binary classification. We also extended these results to a multiclass problem, using 40 different attack classes, including normal traffic. The F1 scores for the multiclass problem are around 98%-99%.

As evident from Table 1, our results match or even exceed the current reported results when using the same evaluation strategy.

### 3. A Different Evaluation Strategy

While high accuracy can be achieved with the current evaluation methods, it raises questions about the practicality of these models. Specifically:
- Does the model truly understand the nature of attacks, or is it simply overfitting to the specific network traffic in the dataset?
- Can the model generalize to new, unseen data from different networks?

To address these concerns, we propose a new evaluation strategy. This involves using two datasets with the same domain but different traffic distributions. Both datasets should share the same features and purpose but be generated from different computer networks. This approach ensures that the model learns the abstract behavior of attacks rather than just the specific traffic patterns in a single network.

### 3.1 Methodology

We will use the NSL-KDD and gureKDD datasets. These datasets represent different computer networks, providing distinct traffic distributions while sharing the same set of features. The training will be performed on one dataset, and testing will be done on the other, and vice versa. We will use both traditional machine learning and deep learning models, focusing on binary classification. The F1 score will be the primary performance indicator.

### 3.2 Results and Discussion

During the training phase, all models achieved an accuracy of approximately 99%. However, the testing phase revealed significantly lower F1 scores. The results are summarized in Tables 2 and 3:

**Table 2: New Evaluation Strategy - NSL-KDD (Training), gureKDD (Testing)**

| Algorithm      | Accuracy   | F1 Score  |
|----------------|------------|-----------|
| Random Forest  | 97.65%     | 36.08%    |
| ANN            | 80.85%     | 9.59%     |
| LSTM           | 87.21%     | 17.5%     |

**Table 3: New Evaluation Strategy - gureKDD (Training), NSL-KDD (Testing)**

| Algorithm      | Accuracy   | F1 Score  |
|----------------|------------|-----------|
| Random Forest  | 52.71%     | 3.62%     |
| ANN            | 52.16%     | 5.19%     |
| LSTM           | 52.78%     | 7.40%     |

These results indicate that while the models perform well on the training dataset, they fail to generalize to the testing dataset, with F1 scores below 40% and often below 10%. This suggests that the models are not effectively learning the underlying patterns of intrusions but are instead overfitting to the specific characteristics of the training data.

### 3.3 Further Evaluation Scenarios

To further evaluate the models, we propose the following scenarios:
- **Same Attack Types**: Evaluate if the model can detect all types of attacks present in both datasets.
- **Shared Attack Categories**: Assess the model's ability to identify different categories of attacks.
- **Dataset Quality**: Determine the quality of the datasets by evaluating their ability to facilitate the transfer of learned intrusion detection to different datasets.
- **Zero-Day Attacks**: Test the model's capability to detect new, unseen attacks.
- **Multiple Dataset Learning**: Train the model on multiple datasets and test it on one or more different datasets.

### 4. Conclusions

For a long time, NIDS research has focused on achieving high accuracy within a single dataset, without considering the practical implications. Our experiments show that while it is relatively easy to achieve high accuracy, these results do not translate well to real-world scenarios. By using a new evaluation strategy, we found that models trained on one dataset and tested on another perform poorly, with F1 scores often below 10%. This paper calls for a reevaluation of the current methods used to measure the quality of NIDS models and datasets.

### References

[1] Agarap, A. F. M. (2018). A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data. In Proceedings of the 2018 10th International Conference on Machine Learning and Computing. ACM, 26–30.

[2] Kaspersky Lab. (2017). Number of the Year: 360,000 Malicious Files Detected Daily in 2017.

[3] McAfee Lab. (2017). Threat Report.

[4] Mahoney, M. V., & Chan, P. K. (2003). An analysis of the 1999 DARPA/Lincoln Laboratory evaluation data for network anomaly detection. In International Workshop on Recent Advances in Intrusion Detection. Springer, 220–237.

[5] McHugh, J. (2000). Testing intrusion detection systems: a critique of the 1998 and 1999 darpa intrusion detection system evaluations as performed by lincoln laboratory. ACM Transactions on Information and System Security (TISSEC) 3, 4 (2000), 262–294.

[6] Perona, I., Gurrutxaga, I., Arbelaitz, O., Martín, J. I., Muguerza, J., & Pérez, J. M. (2008). Service-independent payload analysis to improve intrusion detection in network traffic. In Proceedings of the 7th Australasian Data Mining Conference-Volume 87. Australian Computer Society, Inc., 171–178.

[7] Pervez, M. S., & Farid, D. M. (2014). Feature selection and intrusion classification in NSL-KDD cup 99 dataset employing SVMs. In Software, Knowledge, Information Management and Applications (SKIMA), 2014 8th International Conference on. IEEE, 1–6.

[8] Rao, B. B., & Swathi, K. (2017). Fast kNN Classifiers for Network Intrusion Detection System. Indian Journal of Science and Technology 10, 14.

[9] Singh, R., Kumar, H., & Singla, R. K. (2015). An intrusion detection system using network traffic profiling and online sequential extreme learning machine. Expert Systems with Applications 42, 22 (2015), 8609–8624.

[10] Song, J., Takakura, H., & Okabe, Y. (2006). Description of kyoto university benchmark data. Available at link: http://www.takakura.com/Kyoto_data/BenchmarkData-Description-v5.pdf [Accessed on 15 March 2016].

[11] Song, J., Takakura, H., Okabe, Y., Eto, M., Inoue, D., & Nakao, K. (2011). Statistical analysis of honeypot data and building of Kyoto 2006+ dataset for NIDS evaluation. In Proceedings of the First Workshop on Building Analysis Datasets and Gathering Experience Returns for Security. ACM, 29–36.

[12] Symantec. (2017). Internet Security Threat Report.

[13] Tavallaee, M., Bagheri, E., Lu, W., & Ghorbani, A. A. (2009). A detailed analysis of the KDD CUP 99 data set. In Computational Intelligence for Security and Defense Applications, 2009. CISDA 2009. IEEE Symposium on. IEEE, 1–6.

[14] Yin, C., Zhu, Y., Fei, J., & He, X. (2017). A deep learning approach for intrusion detection using recurrent neural networks. IEEE Access 5 (2017), 21954–21961.