### Fault Tolerance and Traffic Management

In the event that only one uplink remains operational within a given rack, and all other racks direct their fault traffic to this single uplink, the sum of the fault traffic and an equal share of local traffic will be allowed to exit the rack. This ensures that the overall algorithm remains starvation-free, with a bounded delivery time of \( Nm + 1 \) matching slots. The performance of RotorLB-FT is evaluated in Section 7.7.

### RotorNet Prototype Evaluation

To assess the feasibility of RotorNet and demonstrate the operation of Rotor switching, we constructed a small-scale prototype network. This prototype uses RotorLB to facilitate data communication between endpoints. The results from this prototype are used to validate our model of RotorNet, which is later employed to simulate the behavior of RotorNet at a larger scale in Section 7.

#### 6.1 Prototype Architecture

For the prototype, we required an optical Rotor switch to implement the matching patterns specified by the RotorNet architecture. However, no commercial optical switches support the partial connectivity needed to showcase the advantages of Rotor switching. Instead of building a Rotor switch from scratch, we modified a research device called a Selector switch [25], which is well-suited for extension to a Rotor switch. Specifically, the prototype Selector switch is a gang switch that allows an array of 61 single-mode fiber input ports to be switched, as a group, between four output arrays of 61 fibers, with a reconfiguration speed of 150 µs.

We transformed this Selector switch into a Rotor switch using an array of fiber optics to hard-wire the Rotor matchings. To support 8 endpoints, we divided the monolithic Selector switch into two Rotor switches, distributing a full set of 7 Rotor matchings across them, with each Rotor switch implementing either 3 or 4 matching patterns. We then programmed an FPGA to send control signals to the Rotor switches, enabling them to cycle through the Rotor matchings in an open loop. This setup is summarized in Figure 6. The final result is two Rotor switches that support up to 8 endpoints, reconfigure 100–1,000× faster than commercial OCSes, and are compatible with commercial optical transceivers without requiring optical amplification.

While the MEMS device used in the prototype is an off-the-shelf product and not optimized for speed, detailed optical analysis [25] shows that Selector switches can scale to at least 2,048 ports with a 20-µs reconfiguration speed and 2-dB insertion loss. These scaling results also apply to Rotor switches, as they are essentially Selector switches with Rotor matching patterns installed. In this section, we report data gathered with the 150-µs prototype switch, but assume a reconfiguration speed of 20 µs in the analysis in Section 7.

#### 6.2 Prototype Evaluation of RotorLB

Using the prototype Rotor switches, we built an eight-ToR RotorNet (\( NR = 8 \)), using commodity end hosts to emulate ToRs. Each end host was equipped with a dual-port 10-Gb/s Myricom NIC and two optical transceivers, establishing one 10-Gb/s optical link to each Rotor switch. We implemented RotorLB on the end hosts as a user-level process, using the Myricom Sniffer API to directly inject and retrieve packets from the NIC. The only requirement to run RotorLB in practice is that endpoints be aware of the Rotor switches' states. In a real implementation using ToR switches, each ToR could monitor the status of its optical links to determine when one matching ends (i.e., the link goes down) and the next matching begins (i.e., the link comes back up). The commercial NICs we used did not have a built-in low-latency way to detect link up/down events, so we used an out-of-band channel to notify end hosts of the switch reconfiguration events.

To evaluate the performance under various traffic conditions, we generated traffic patterns with different numbers of "heavy" connections, where each heavy connection attempts to send data at line rate. We define the traffic density as the fraction of heavy connections out of all possible connections (56 in our 8-endpoint prototype). For each traffic density, we repeated the experiments with 32 randomly-generated traffic matrices representing the inter-rack demand.

As a baseline, we first sent data through the network using only one-hop forwarding. Next, we repeated the experiments with RotorLB running on the endpoints. Figure 7 shows the relative network throughput under RotorLB normalized to that of one-hop forwarding. We observe that RotorLB significantly improves throughput for sparse traffic patterns. For a single active heavy connection, RotorLB improves throughput by the expected factor of \( NR - 1 \) (7 in this case), as traffic can now take advantage of all paths through the network. Further, our implementation of RotorLB adaptively converges to the throughput of one-hop forwarding for uniform traffic, as intended.

Figure 7 also overlays the modeled (Section 7) relative throughput of RotorLB to one-hop forwarding for the same set of traffic conditions used in the measurements. The close agreement between model and measurement demonstrates that our RotorLB implementation operates as designed and validates our model's ability to accurately predict RotorLB performance in practice. We use this model to explore the performance of RotorNet at scale.

### 7. Evaluation

In this section, we evaluate the behavior and performance of RotorNet at a larger scale than was feasible to prototype. We employ three distinct simulators at varying levels of fidelity and validate the results of our packet-level simulation against a Mininet-based emulation.

#### 7.1 Methodology

Depending on the simulator, we consider two types of traffic models: fluid flow and flow level. We describe each below.

**Fluid-flow model:** We start with a network model that treats traffic as a continuous fluid flow, allowing us to measure the bandwidth, delivery time, and buffering requirements for stable and dynamic traffic patterns using Matlab. This model maximizes the amount of data sent through RotorNet. For comparison, we model a Fat Tree as a single, ideal packet switch, which connects all racks with a single non-blocking crosspoint fabric and assigns per-port bandwidth using a linear program solver configured to maximize throughput. This idealized packet switch upper bounds the performance of a Fat Tree network, which in practice has lower performance due to transport protocol and hashing effects.

**Flow-level simulator:** We also use a custom-written flow-level simulator that relies on discrete flow events to track the arrival and completion times of flows in RotorNet. While we do not simulate packets in this method, the flow-level results approximate the performance of RotorNet with a lossless transport protocol running on top of it, such as RDMA over Converged Ethernet (RoCE).

**Packet-level simulator:** We model a baseline 3-level Fat Tree using the ns-3 packet-level simulator. This allows us to model the effects of TCP under flow hashing and port contention. Under moderate to heavy network load, these effects lead to a long tail in flow completion times in Fat Trees, as previously observed [33].

**Mininet emulator:** In addition to ns-3, we model the baseline 3-level Fat Tree using Mininet [18] with 100-Mb/s links. This allows us to corroborate our ns-3 findings using a real TCP stack, though we still require ns-3 to validate our work at multi-gigabit scale.

**7.1.1 Traffic models.** Our throughput analyses employ two distinct traffic models, described below. We defer discussion of the traffic characteristics used to evaluate flow completion time until Section 7.4.

**Synthetic stochastic:** We model inter-rack traffic as a random process, varying the total number of heavy connections (varying the traffic density as defined in Section 6.2). For the fluid-flow network models, we represent traffic as a random binary matrix with elements that are either 1, representing an elephant flow(s) between a given rack pair, or 0, representing no traffic exchanged between that rack pair. In a hybrid deployment, low-latency and mice flows would be "swept" to the packet switch, resulting in a 0 entry in the demand matrix. For each fixed traffic density, we individually evaluate 100 randomly-generated traffic matrices, showing the average and error bars across runs in the plots.

**Commercial traces:** In addition to randomly-generated binary demand matrices, we also model traffic patterns reported in production datacenters. Following the approach of Ghobadi et al. [15], we use published traffic-matrix heatmaps collected over 5-minute intervals to define the probability of inter-rack communication. We then use a Poisson generation process to determine flow arrival times and draw flow sizes from reported distributions. We use traces reported from a Microsoft datacenter by Ghobadi et al. [15] and from two different Facebook cluster types, Hadoop and front-end web-server [11, 27]. Given RotorNet’s cycle time of approximately 1 ms, we model flows arriving within 1-ms windows using the Poisson process. Finally, to emulate a hybrid RotorNet with 10% packet-switched bandwidth, we retain the largest flows that contributed 90% of the total bytes and input that to the RotorNet model.

#### 7.2 Throughput Analysis

We compare the throughput of RotorNet to that of an idealized packet switch using the fluid-flow model described in Section 7.1, considering basic one-hop forwarding, RotorLB, and a computationally intensive linear program that provides an upper bound on potential throughput. For reference, we include the throughput of a 3:1 over-subscribed Fat Tree, which is estimated in Section 4.2 to be cost-comparable with RotorNet.

Figure 8 shows the throughput of RotorLB normalized to that of the ideal, fully-provisioned electrical network. The matching fill factor, which is how perfectly the Rotor matchings are distributed into Rotor switches \(\left(\frac{NR - 1}{Nm \times Nsw}\right)\), upper bounds the throughput. While the fill factor is 0.9375 with only \( NR = 16 \) racks, Figure 8b shows that the matching fill factor approaches 1 by \( NR = 256 \) racks.

As expected, after accounting for the matching fill factor, one-hop forwarding results in 100% throughput for uniform traffic, but throughput is linearly reduced for skewed traffic. Before assessing RotorLB, we use a linear program (LP) solver to calculate the upper-bound throughput for RotorNet under ideal forwarding; that is, if we had perfect knowledge of the network-wide traffic demand, and indirect traffic was not restricted to a maximum of two hops. The result is plotted in Figure 8a for \( NR = 16 \) (it is not feasible to run the LP solver at larger scales).

Next, we calculate the throughput for the RotorLB protocol described in Section 5. These calculations assume constant traffic patterns; we explore the responsiveness of RotorLB to changing traffic patterns in Section 7.6. Figure 8a shows that RotorLB achieves close to the ideal (LP-solver) throughput over all traffic densities despite its fully distributed implementation. Figure 8b shows RotorLB’s throughput at scale for 256 racks. Figure 8c highlights that, at scale, RotorLB’s throughput is within a factor of two of an ideal fully-provisioned packet network independent of traffic conditions. Recall from Section 3 that this worst-case factor-of-two reduction in bandwidth is expected from RotorLB’s two-hop routing (i.e., up to half of RotorNet’s bandwidth is used by traffic on its second hop).

We find that, out of all traffic patterns studied, permutation traffic yields the worst-case relative throughput for RotorNet compared to an ideal packet network—a worst-case of 50% relative throughput. We observe this factor-of-two lower-bound holds for larger-scale networks with \( NR > 256 \).

Finally, following Section 7.1, we model the throughput of RotorLB in a 256-rack RotorNet under a number of production datacenter traffic patterns reported in the literature. The Microsoft traffic pattern is very sparse, with only a small number of rack pairs communicating in each time interval. The pattern varies between millisecond time intervals, leading to the slight variance in throughput indicated in Figure 8b. The Facebook Hadoop cluster traffic pattern exhibits denser connections, yielding a throughput nearly equivalent to random traffic. The Facebook front-end pattern displays heavy multicast and incast characteristics, setting its throughput apart from that of random traffic.

For the datacenter traffic patterns considered, RotorNet provides 70–95% the throughput of an ideal fully-provisioned electrical network, despite being significantly less expensive, as discussed in Section 4.2. Compared to a 3:1 over-subscribed Fat Tree of approximately equal cost, RotorNet delivers 1.6× the throughput under worst-case (permutation) traffic, 2.3× the throughput for reported datacenter traffic patterns, and up to 3× the throughput for uniform traffic.

#### 7.3 Bounded Delivery Time and Reordering

By design, RotorNet offers strictly bounded delivery time for one-hop traffic because it cyclically provisions bandwidth between all network endpoints. However, under RotorLB, two-hop traffic is delivered later than one-hop traffic. Because data from the same flow may be sent over both one-hop and two-hop paths (as discussed in Section 5), care must be taken to ensure data is delivered in order to the receiver. Here, we evaluate the delivery characteristics and requirements for ordered delivery of traffic under RotorLB.

Adopting terminology from MPTCP, we refer to the fractions of a single flow that are sent over different one-hop and two-hop paths as subflows. As discussed in Section 5, a given rack will only accept as much two-hop traffic to each destination as it can deliver in one matching slot. Therefore, we expect the worst-case delivery time of a two-hop subflow to be equal to \( Nm + 1 \) matching slots (i.e., one matching cycle + one matching slot). This worst case occurs when traffic is forwarded to an intermediate rack and that intermediate rack is currently connected to the destination. This can happen because RotorLB does not forward indirect traffic during the same matching slot in which it is received.

To test this behavior, we track the start time and tail delivery time of all subflows over a variety of traffic patterns in a RotorNet with 256 racks and \( Nm = 8 \) matchings per switch. The results are shown in Figure 9. Figure 9a shows that, indeed, no subflows exceed the designed delivery bound of \( Nm + 1 = 9 \) matching slots for any traffic density. Note that although only a subset of subflows are plotted, the scatter plot is representative of the total distribution.