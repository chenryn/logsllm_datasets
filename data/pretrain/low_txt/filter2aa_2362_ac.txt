# Reporting

## 8.2 Evaluating Fuzzers
### 8.2.1 Retrospective Testing
- **Page:** 224

### 8.2.2 Simulated Vulnerability Discovery
- **Page:** 225

### 8.2.3 Code Coverage
- **Page:** 225

### 8.2.4 Caveats
- **Page:** 226

## 8.3 Introducing the Fuzzers
### 8.3.1 GPF
- **Page:** 226

### 8.3.2 Taof
- **Page:** 227

### 8.3.3 ProxyFuzz
- **Page:** 227

### 8.3.4 Mu-4000
- **Page:** 228

### 8.3.5 Codenomicon
- **Page:** 228

### 8.3.6 beSTORM
- **Page:** 228

### 8.3.7 Application-Specific Fuzzers
- **Page:** 229

### 8.3.8 What’s Missing
- **Page:** 229

## 8.4 The Targets
- **Page:** 229

## 8.5 The Bugs
### 8.5.1 FTP Bug 0
- **Page:** 230

### 8.5.2 FTP Bugs 2, 16
- **Page:** 230

## 8.6 Results
- **Page:** 231

### 8.6.1 FTP
- **Page:** 232

### 8.6.2 SNMP
- **Page:** 233

### 8.6.3 DNS
- **Page:** 233

## 8.7 A Closer Look at the Results
### 8.7.1 FTP
- **Page:** 234

### 8.7.2 SNMP
- **Page:** 237

### 8.7.3 DNS
- **Page:** 240

## 8.8 General Conclusions
### 8.8.1 The More Fuzzers, the Better
- **Page:** 242

### 8.8.2 Generational-Based Approach Is Superior
- **Page:** 242

### 8.8.3 Initial Test Cases Matter
- **Page:** 242

### 8.8.4 Protocol Knowledge
- **Page:** 243

### 8.8.5 Real Bugs
- **Page:** 244

### 8.8.6 Does Code Coverage Predict Bug Finding?
- **Page:** 244

### 8.8.7 How Long to Run Fuzzers with Random Elements
- **Page:** 246

### 8.8.8 Random Fuzzers Find Easy Bugs First
- **Page:** 247

## 8.9 Summary
- **Page:** 247

# Chapter 9: Fuzzing Case Studies
- **Page:** 249

## 9.1 Enterprise Fuzzing
### 9.1.1 Firewall Fuzzing
- **Page:** 251

### 9.1.2 VPN Fuzzing
- **Page:** 253

## 9.2 Carrier and Service Provider Fuzzing
### 9.2.1 VoIP Fuzzing
- **Page:** 256

### 9.2.2 WiFi Fuzzing
- **Page:** 257

## 9.3 Application Developer Fuzzing
### 9.3.1 Command-Line Application Fuzzing
- **Page:** 259

### 9.3.2 File Fuzzing
- **Page:** 259

### 9.3.3 Web Application Fuzzing
- **Page:** 261

### 9.3.4 Browser Fuzzing
- **Page:** 262

## 9.4 Network Equipment Manufacturer Fuzzing
### 9.4.1 Network Switch Fuzzing
- **Page:** 263

### 9.4.2 Mobile Phone Fuzzing
- **Page:** 264

## 9.5 Industrial Automation Fuzzing
- **Page:** 265

## 9.6 Black-Box Fuzzing for Security Researchers
### 9.6.1 Select Target
- **Page:** 268

### 9.6.2 Enumerate Interfaces
- **Page:** 268

### 9.6.3 Choose Fuzzer/Fuzzer Type
- **Page:** 269

### 9.6.4 Choose a Monitoring Tool
- **Page:** 270

### 9.6.5 Carry Out the Fuzzing
- **Page:** 271

### 9.6.6 Post-Fuzzing Analysis
- **Page:** 272

## 9.7 Summary
- **Page:** 273

# About the Authors
- **Page:** 275

# Bibliography
- **Page:** 277

# Index
- **Page:** 279

---

# Foreword
It was a dark and stormy night. Really.

Sitting in my apartment in Madison in the fall of 1988, there was a wild midwest thunderstorm pouring rain and lighting up the late-night sky. That night, I was logged on to the Unix systems in my office via a dial-up phone line over a 1200 baud modem. With the heavy rain, there was noise on the line, and that noise was interfering with my ability to type sensible commands to the shell and programs that I was running. It was a race to type an input line before the noise overwhelmed the command.

This fighting with the noisy phone line was not surprising. What did surprise me was the fact that the noise seemed to be causing programs to crash. And even more surprising was the programs that were crashing—common Unix utilities that we all use every day.

The scientist in me said that we need to make a systematic investigation to try to understand the extent of the problem and its cause. 

That semester, I was teaching the graduate Advanced Operating Systems course at the University of Wisconsin. Each semester in this course, we hand out a list of suggested topics for the students to explore for their course project. I added this testing project to the list.

In the process of writing the description, I needed to give this kind of testing a name. I wanted a name that would evoke the feeling of random, unstructured data. After trying out several ideas, I settled on the term “fuzz.”

Three groups attempted the fuzz project that semester, and two failed to achieve any crash results. Lars Fredriksen and Bryan So formed the third group, and they were more talented programmers and conducted the most careful experiments; they succeeded well beyond my expectations. As reported in the first fuzz paper [cite], they could crash or hang between 25–33% of the utility programs on the seven Unix variants that they tested.

However, the fuzz testing project was more than a quick way to find program failures. Finding the cause of each failure and categorizing these failures gave the results deeper meaning and more lasting impact. The source code for the tools and scripts, the raw test results, and the suggested bug fixes were all made public. Trust and repeatability were crucial underlying principles for this work.

In the following years, we repeated these tests on more and varied Unix systems for a larger set of command-line utility programs and expanded our testing to GUI programs based on the then-new X-window system [cite fuzz 1995]. Windows followed several years later [cite fuzz 2000] and, most recently, MacOS [cite fuzz 2006]. In each case, over the span of the years, we found a lot of bugs and, in each case, we diagnosed those bugs and published all of our results.

In our more recent research, as we have expanded to more GUI-based application testing, we discovered that the classic 1983 testing tool, “The Monkey,” used on the earlier Macintosh computers [cite Hertzfeld book]. Clearly, a group ahead of their time.

In the process of writing our early fuzz papers, we came across strong resistance from the testing and software engineering community. The lack of a formal model and methodology and undisciplined approach to testing often offended experienced practitioners in the field. In fact, I still frequently come across hostile attitudes toward this type of “stone axes and bear skins” (my apologies to Mr. Spock) approach to testing.

My response was always simple: “We’re just trying to find bugs.” As I have said many times, fuzz testing is not meant to supplant more systematic testing. It is just one more tool, albeit an extremely easy one to use, in the tester’s toolkit.

As an aside, note that the fuzz testing has never been a funded research effort for me; it is a research avocation rather than a vocation. All the hard work has been done by a series of talented and motivated graduate students in our Computer Sciences Department. This is how we have fun.

Fuzz testing has grown into a major subfield of research and engineering, with new results taking it far beyond our simple and initial work. As reliability is the foundation of security, so has it become a crucial tool in the security evaluation of software. Thus, the topic of this book is both timely and extremely important. Every practitioner who aspires to write safe and secure software needs to add these techniques to their bag of tricks.

Barton Miller
Madison, Wisconsin
April 2008

---

# Operating System Utility Program Reliability—The Fuzz Generator

## Project Goal
The goal of this project is to evaluate the robustness of various Unix utility programs given an unpredictable input stream. This project has two parts. First, you will build a “fuzz” generator. This is a program that will output a random character stream. Second, you will take the fuzz generator and use it to attack as many Unix utilities as possible, with the goal of trying to break them. For the utilities that break, you will try to determine what type of input caused the break.

### The Program
The fuzz generator will generate an output stream of random characters. It will need several options to give you flexibility to test different programs. Below is the start for a list of options for features that fuzz will support. It is important when writing this program to use good C and Unix style, and good structure, as we hope to distribute this program to others.

- **-p** Only the printable ASCII characters
- **-a** All ASCII characters
- **-0** Include the null (0 byte) character
- **-l** Generate random length lines (\\n terminated strings)
- **-f name** Record characters in file “name”
- **-d nnn** Delay nnn seconds following each character
- **-r name** Replay characters in file “name” to output

### The Testing
The fuzz program should be used to test various Unix utilities. These utilities include programs like vi, mail, cc, make, sed, awk, sort, etc. The goal is to first see if the program will break and second to understand what type of input is responsible for the break.

---

# Preface

Still today, most software fails with negative testing, or fuzzing, as it is known by security people. I (Ari) have never seen a piece of software or a network device that passes all fuzz tests thrown at it. Still, things have hopefully improved a bit from 1996 when we started developing our first fuzzers, and at least from the 1970s when Dr. Boris Beizer and his team built their fuzzer-like test automation scripts. The key driver for the change is the adaptation of these tools and techniques, and the availability of the technical details on how this type of testing can be conducted. Fortunately, there has been enormous development in the fuzzer market, as can be seen from the wide range of available open-source and commercial tools for this test purpose.

The idea for this book came up in 2001, around the same time when we completed the PROTOS Classic project on our grammar-based fuzzers. Unfortunately, we were distracted by other projects. Back then, as a result of the PROTOS project, we spawned a number of related security “spin-offs.” One of them was the commercial company Codenomicon, which took over all technical development from PROTOS Classic and launched the first commercial fuzzers in early 2002 (those were for SIP, TLS, and GTP protocols if you are interested). Another was the PROTOS Genome project, which started looking at the next steps in fuzzing and automated protocol reverse-engineering, from a completely clean table (first publicly available tests were for various compression formats, released in March 2008). And the third was FRONTIER, which later spun out a company doing next-gen network analysis tools and was called Clarified Networks. At the same time, we kept our focus on fuzzer research and teaching on all areas of secure programming at the University of Oulu.

And all this was in a small town of about two hundred thousand people, so you could say that one out of a thousand people were experts in fuzzing in this far-north location. But, unfortunately, the book just did not fit into our plans at that time.

The idea for the book re-emerged in 2005 when I reviewed a paper Jared DeMott wrote for the Blackhat conference. For the first time since all the published and some unpublished works at PROTOS, I saw something new and unique in that paper. I immediately wrote to Jared to propose that he would co-author this fuzzer book project with me, and later also flew in to discuss with him to get to know him better. We had completely opposite experiences and thoughts on fuzzing, and therefore it felt like a good fit, and so finally this book was started. Fortunately, I had a dialog going on with Artech House for some time already, and we got to start the project almost immediately.

We wanted everything in the book to be product-independent and technology-independent. With our combined experiences, this seemed to be natural for the book. But something was still missing. As a last desperate action in our constant struggle to get this book completed by the end of 2007, we reached out to Charlie Miller.