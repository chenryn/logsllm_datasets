### Optimized Text

**Calvin, which does not require multiple partitions per node to enable parallelism, was used as a baseline in this test. Despite the efficient communication between sub-transactions of a Multi-Partition Transaction (MPT) via Unix Domain sockets, MPTs impose a significant overhead. This is due to the data exchanges between sibling sub-transactions, which necessitate a synchronization phase between worker threads of different partitions, leading to frequent processing stalls.**

**In the high contention workload (Figure 3b), Sparkle's peak throughput is lower than in the previous scenario. However, it still achieves up to approximately 6× speed-up compared to Calvin, which scales only up to 5 threads before being bottlenecked by the sequencing thread. This performance gain is notable, given that Sparkle incurs high contention rates due to its speculative nature and the high probability of transaction conflicts. The most dramatic performance drop is observed with S-SMR. When using more than one thread, the data (populated with a single warehouse) must be sharded across multiple partitions, forcing most transactions to access more than one partition. This is particularly burdensome for long read-only transactions like OrderStatus, which access hundreds of keys and require frequent synchronization between worker threads of different partitions.**

**C. Distributed Deployment**

**Next, we analyze Sparkle's performance when deployed over a medium-scale cluster of 8 machines. Figure 4 presents the results for the synthetic benchmark, considering four scenarios that differ by the percentage of MPTs they generate. In each of the four plots, we vary the percentage of dependent transactions on the X-axis and report the throughput and abort rate for all considered solutions under low (LC) and medium contention (MC) workloads. For S-SMR, since its performance is unaffected by the contention level (as it processes transactions sequentially at each partition), we only report results for the LC workload.**

**In the 50% MPTs scenario, Calvin achieves 30% higher throughput than Sparkle in the absence of dependent transactions. This can be attributed to the fact that Calvin's throughput is limited by the processing speed of MPTs, which take orders of magnitude longer than Single-Partition Transactions (SPTs). Additionally, Calvin's pessimistic/lock-based approach does not require MPTs to undergo a confirmation phase. Although Sparkle aims to minimize the impact of the MPTs' confirmation phase through scheduling techniques and Speculative Confirmation (SC) mechanisms, this still introduces additional communication overhead. Nonetheless, in the 50% MPT scenario, Sparkle outperforms Calvin as soon as the ratio of dependent transactions reaches 1%, achieving an average throughput gain of more than one order of magnitude. Similar gains are observed relative to S-SMR.**

**The results from the TPC-C benchmark (Figure 5) show that Sparkle outperforms both Calvin and S-SMR in all workloads, with peak gains of approximately 3× and 4×, respectively. S-SMR's poor performance is due to the generation of a small but non-negligible fraction of MPT transactions (ranging from approximately 1% for the 10% update workload to approximately 10% for the 90% update workload). Calvin's performance is hampered by the fact that three out of the five transaction profiles are dependent transactions, which impose a heavy load on the locking thread and frequently lead to restarts.**

**1) Benefits of SC and Scheduling: To quantify the performance benefits of using Speculative Confirmation (SC) and scheduling, either individually or in combination, we conducted an experiment. We aimed to measure how speculative transaction processing, particularly allowing MPTs to disseminate speculative data to their siblings, enhances MPT throughput. We compared the performance of four Sparkle variants:**

- **Sparkle:Cons: A conservative variant where MPTs can only send remote data to their siblings if they have observed a locally consistent snapshot, i.e., if their preceding transaction has fully committed. This avoids the need for any confirmation, but severely limits throughput by precluding parallelism between MPTs in the same partition.**
- **Sparkle:CC: Similar to Sparkle, MPTs disseminate data they read locally in a speculative fashion. However, this variant uses a conservative confirmation (CC) scheme, sending confirmation messages only upon final commit, not speculative commit. While CC is simpler, it inherently bounds MPT throughput by the rate of completion of the inter-partition confirmation phase, involving all-to-all synchronous communication.**
- **Sparkle:SC: Uses speculative confirmations but no scheduling.**
- **Sparkle:SC+Schedule: Uses both speculative confirmations and scheduling.**

**Using the low conflict micro-benchmark configuration, we generated varying ratios of MPTs. Figure 6 shows the normalized throughput of the three protocols allowing speculative reads across partitions compared to Sparkle:Cons. The plot reveals three key conclusions: First, all variants achieve significant (up to approximately 3×) throughput improvements over Sparkle:Cons, confirming the value of speculative processing. Second, without scheduling, SC provides no discernible benefit over a simpler CC approach, as most MPTs revert to using CC, resulting in nearly identical throughputs for Sparkle:CC and Sparkle:SC. Finally, the joint use of scheduling and SC yields up to a 2× throughput increase compared to Sparkle:CC.**

**VII. Conclusions**

**This paper introduced Sparkle, a novel distributed deterministic concurrency control for partially-replicated state machines. Sparkle achieves significant performance gains over state-of-the-art PRSM systems through the combined use of speculative transaction processing and scheduling techniques. Our extensive experimental study, encompassing both synthetic and realistic benchmarks, demonstrates that 1) Sparkle has negligible overhead compared to a protocol with no concurrency control in conflict-free workloads, and 2) Sparkle can achieve more than one order of magnitude throughput gains in workloads characterized by high conflict rates and frequent MPTs.**

**Acknowledgments: We are grateful to our shepherd Alexey Gotsman and the anonymous reviewers. This work is partially funded by the LightKone project (732505) in the EU H2020 Programme, the Erasmus Mundus Doctorate Programme (2012-0030), and FCT projects UID/CEC/50021/2019 and PTDC/EEISCR/1743/2014.**

**References:**

[1] A. L. P. N. Alonso. Database replication for enterprise applications. PhD thesis, Universidade do Minho, 2017.
[2] C. Basile, Z. Kalbarczyk, and R. Iyer. A preemptive deterministic scheduling algorithm for multithreaded replicas. In Proc. of the 33rd International Conference on Dependable Systems and Networks, pages 149–158, June 2003.
[3] T. Bergan, J. Devietti, N. Hunt, and L. Ceze. The deterministic execution hammer: How well does it actually pound nails. In Proc. of the 2nd Workshop on Determinism and Correctness in Parallel Programming, 2011.
[4] P. A. Bernstein, V. Hadzilacos, and N. Goodman. Concurrency control and recovery in database systems. 1987.
[5] C. E. Bezerra, F. Pedone, and R. V. Renesse. Scalable state-machine replication. In Proc. of the 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, pages 331–342, June 2014.
[6] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. Furman, S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild, W. Hsieh, S. Kanthak, E. Kogan, H. Li, A. Lloyd, S. Melnik, D. Mwaura, D. Nagle, S. Quinlan, R. Rao, L. Rolig, Y. Saito, M. Szymaniak, C. Taylor, R. Wang, and D. Woodford. Spanner: Google’s globally-distributed database. In Proc. of the 10th USENIX Symposium on Operating Systems Design and Implementation, pages 261–264, Hollywood, CA, 2012. USENIX Association.
[7] M. Couceiro, D. Didona, L. Rodrigues, and P. Romano. Self-tuning in Distributed Transactional Memory, pages 418–448. Springer International Publishing, Cham, 2015.
[8] M. Couceiro, P. Ruivo, P. Romano, and L. Rodrigues. Chasing the optimum in replicated in-memory transactional platforms via protocol adaptation. IEEE Transactions on Parallel and Distributed Systems, 26(11):2942–2955, Nov 2015.
[9] J. Devietti, B. Lucia, L. Ceze, and M. Oskin. DMP: Deterministic shared memory multiprocessing. In ACM SIGARCH Computer Architecture News, volume 37, pages 85–96. ACM, 2009.
[10] J. Devietti, J. Nelson, T. Bergan, L. Ceze, and D. Grossman. RCDC: A relaxed consistency deterministic computer. In ACM SIGPLAN Notices, volume 46, pages 67–78. ACM, 2011.
[11] J. Du, D. Sciascia, S. Elnikety, W. Zwaenepoel, and F. Pedone. Clock-RSM: Low-latency inter-datacenter state machine replication using loosely synchronized physical clocks. In Proc. of the 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, pages 343–354. IEEE, 2014.
[12] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in the presence of partial synchrony. Journal of the ACM, 35(2):288–323, 1988.
[13] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distributed consensus with one faulty process. Journal of the ACM, 32(2):374–382, 1985.
[14] R. Friedman and R. van Renesse. Packing messages as a tool for boosting the performance of total ordering protocols. In Proc. of the Sixth IEEE International Symposium on High Performance Distributed Computing, pages 233–242, Aug 1997.
[15] S. Frølund and R. Guerraoui. E-Transactions: End-to-end reliability for three-tier architectures. IEEE Transactions on Software Engineering, 28(4):378–395, Apr. 2002.
[16] J. Gray, P. Helland, P. O’Neil, and D. Shasha. The dangers of replication and a solution. In Proc. of the 22nd ACM International Conference on Management of Data, pages 173–182. ACM, 1996.
[17] Grid’5000. https://www.grid5000.fr/, 2018.
[18] Z. Guo, C. Hong, M. Yang, D. Zhou, L. Zhou, and L. Zhuang. Rex: Replication at the speed of multi-core. In Proc. of the Ninth European Conference on Computer Systems, page 11. ACM, 2014.
[19] S. Hirve, R. Palmieri, and B. Ravindran. Archie: A speculative replicated transactional system. In Proc. of the 15th International Middleware Conference, pages 265–276. ACM, 2014.
[20] T. Hoff. Latency is everywhere and it costs you sales - how to crush it. High Scalability, July 25, 2009.
[21] Intel. Threading Building Blocks. https://www.threadingbuildingblocks.org/, 2018.
[22] R. Jimenez-Peris, M. Patino-Martinez, B. Kemme, and G. Alonso. Improving the scalability of fault-tolerant database clusters. In Proc. of the 22nd International Conference on Distributed Computing Systems, pages 477–484, July 2002.
[23] R. Kallman, H. Kimura, J. Natkins, A. Pavlo, A. Rasin, S. Zdonik, E. P. C. Jones, S. Madden, M. Stonebraker, Y. Zhang, J. Hugg, and D. J. Abadi. H-Store: A high-performance, distributed main memory transaction processing system. Proc. of the VLDB Endowment, 1(2):1496–1499, Aug. 2008.
[24] M. Kapritsos, Y. Wang, V. Quema, A. Clement, L. Alvisi, and M. Dahlin. All about Eve: Execute-verify replication for multi-core servers. In Proc. of the 10th USENIX Symposium on Operating Systems Design and Implementation, pages 237–250, Hollywood, CA, 2012. USENIX.
[25] T. Kraska, G. Pang, M. J. Franklin, S. Madden, and A. Fekete. MDCC: Multi-data center consistency. In Proc. of the 8th ACM European Conference on Computer Systems, pages 113–126. ACM, 2013.
[26] L. Lamport. The part-time parliament. ACM Transactions on Computer Systems, 16(2):133–169, May 1998.
[27] J. Li, E. Michael, and D. R. Ports. Eris: Coordination-free consistent transactions using in-network concurrency control. In Proc. of the 26th Symposium on Operating Systems Principles, pages 104–120. ACM, 2017.
[28] Z. Li. Sparkle codebase. https://github.com/marsleezm/spec_calvin.
[29] Z. Li, P. V. Roy, and P. Romano. Sparkle: Scalable speculative replication for transactional datastores. Technical Report 4, INESC-ID, May 2018.
[30] Z. Li, P. Van Roy, and P. Romano. Enhancing throughput of partially replicated state machines via multi-partition operation scheduling. In Proc. of the IEEE 16th International Symposium on Network Computing and Applications, pages 1–10. IEEE, 2017.
[31] Z. Li, P. Van Roy, and P. Romano. Transparent speculation in geo-replicated transactional data stores. In Proc. of the 27th International Symposium on High-Performance Parallel and Distributed Computing, pages 255–266. ACM, 2018.
[32] P. J. Marandi, M. Primi, and F. Pedone. High performance state-machine replication. In Proc. of the 41st IEEE/IFIP International Conference on Dependable Systems and Networks, pages 454–465. IEEE, 2011.
[33] R. Palmieri, F. Quaglia, and P. Romano. Aggro: Boosting STM replication via aggressively optimistic transaction processing. In Proc. of the Ninth IEEE International Symposium on Network Computing and Applications, pages 20–27, July 2010.
[34] M. Patiño-Martinez, R. Jiménez-Peris, B. Kemme, and G. Alonso. Middle-R: Consistent database replication at the middleware level. ACM Transactions on Computer Systems, 23(4):375–423, 2005.
[35] F. Pedone and A. Schiper. Handling message semantics with generic broadcast protocols. Distributed Computing, 15(2):97–107, 2002.
[36] S. Peluso, J. Fernandes, P. Romano, F. Quaglia, and L. Rodrigues. Specula: Speculative replication of software transactional memory. In Proc. of the 31st IEEE International Symposium on Reliable Distributed Systems, pages 91–100, 2012.
[37] F. Quaglia and P. Romano. Ensuring e-transaction with asynchronous and uncoordinated application server replicas. IEEE Transactions on Parallel and Distributed Systems, 18(3):364–378, March 2007.
[38] P. Romano and M. Leonetti. Self-tuning batching in total order broadcast protocols via analytical modelling and reinforcement learning. In Proc. of International Conference on Computing, Networking and Communications, 2012.
[39] M. M. Saad, M. J. Kishi, S. Jing, S. Hans, and R. Palmieri. Processing transactions in a predefined order. In Proc. of the 24th Symposium on Principles and Practice of Parallel Programming, pages 120–132, New York, NY, USA, 2019. ACM.
[40] F. B. Schneider. Implementing fault-tolerant services using the state machine approach: A tutorial. ACM Computing Surveys, 22(4):299–319, 1990.
[41] A. Thomson. Calvin codebase. https://github.com/yaledb/calvin.
[42] A. Thomson, T. Diamond, S.-C. Weng, K. Ren, P. Shao, and D. J. Abadi. Calvin: Fast distributed transactions for partitioned database systems. In Proc. of the 39th ACM SIGMOD International Conference on Management of Data, pages 1–12. ACM, 2012.
[43] TPC-Consortium. TPC Benchmark-W Specification v. 1.8. http://www.tpc.org/tpc_documents_current_versions/pdf/tpc-c_v5.11.0.pdf.
[44] P. T. Wojciechowski, T. Kobus, and M. Kokociński. State-machine and deferred-update replication: Analysis and comparison. IEEE Transactions on Parallel and Distributed Systems, 28(3):891–904, March 2017.
[45] I. Zhang, N. K. Sharma, A. Szekeres, A. Krishnamurthy, and D. R. Ports. Building consistent transactions with inconsistent replication. In Proc. of the 25th Symposium on Operating Systems Principles, pages 263–278. ACM, 2015.