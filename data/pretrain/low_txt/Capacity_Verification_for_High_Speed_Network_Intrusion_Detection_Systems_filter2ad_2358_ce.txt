### Sequence of Events and Vulnerability Management

The release times for signatures in the Snort network intrusion detection system and rule updates for the Nessus vulnerability scanner were analyzed. It is anticipated that this sequence of events will continue, with intrusion detection signatures and vulnerability scanning rules being distributed simultaneously or shortly after the public disclosure of new vulnerabilities. This approach allows for the timely protection against exploits that leverage new vulnerabilities by scanning for and patching vulnerable hosts.

#### Minimizing the Window of Vulnerability

By adopting a strategy of immediate patching upon the announcement of new vulnerabilities, the "window of vulnerability" can be minimized. This period is when a host is susceptible to exploitation using a newly discovered vulnerability. Additionally, this strategy would eliminate the "window of visibility" where a network intrusion detection system (NIDS) using signatures detects successful remote-to-local attacks. 

One limitation of this analysis is that it examined a limited number of attacks, all of which were discovered by non-malicious security groups or individuals. Even if vulnerabilities are discovered and exploited by malicious actors, the analysis suggests that software developers would quickly develop and release patches, and intrusion detection signatures would be provided concurrently with these patches. The study focused on one NIDS and one vulnerability scanner, but similar results are likely for other systems, given that high priority is typically placed on developing patches.

#### Practicality for Small and Large Sites

For small sites, such as DMZ networks, the strategy of immediate patching is practical because the discovery rate for major vulnerabilities in Internet server software packages is relatively low, ranging from zero to seven per year. With three or four server software packages in a DMZ, serious security patches may be required only a few times a month. On such well-protected sites, NIDS should never detect successful remote-to-local attacks. However, they can serve as a backup, providing "defense in depth" by detecting failed remote-to-local attacks (probes), DoS attacks, and offering situation awareness through scan detection.

In contrast, NIDS are more useful on large sites with known vulnerabilities. It is often necessary to use hosts with known vulnerabilities due to the high cost or time required to update all software, operational or compatibility requirements, or the lack of a centralized authority to enforce a security policy across all vulnerable hosts. On such sites, NIDS can detect successful system compromises, but this requires constant monitoring and labor-intensive analysis of thousands of alerts per day caused by failed attacks and normal background traffic.

#### Alert Prioritization

An analysis of alerts from the Snort NIDS demonstrated that knowledge of the operating system, software packages, and vulnerabilities of monitored hosts can be used to prioritize these alerts. Out of approximately 830 daily remote-to-local alerts, 95% corresponded to vulnerabilities that did not exist on a site with about 10 web servers and could be categorized as low-priority alerts. This left only 5% or roughly 40 high-priority alerts per day that could have been caused by successful system compromises. Prioritization does not require detailed patch level information but rather the ability to prove that a host is not susceptible to a specific vulnerability by knowing its operating system or server extensions.

#### Summary and Future Work

In summary, vulnerability scanning and applying software patches should always be a component of site security. For small sites, these tactics can make NIDS serve a secondary role because vulnerabilities can be patched before NIDS have new signatures to detect attacks. On large sites, it is too expensive to patch all systems, and NIDS can serve a useful function by detecting successful compromises. Information on vulnerabilities and protected hosts is required to prioritize alerts and focus on those that might represent successful exploitation of known vulnerabilities.

Future work can explore the interrelationships between vulnerability scanning, software patching, gathering information on protected hosts, and intrusion detection. This paper was limited to known attacks, primarily remote-to-local attacks, and NIDS that use signatures to detect known attacks. Further analyses could include other types of attacks and NIDS that detect novel attacks without signatures. Such systems could provide early warnings and guide forensic analysis of new attacks, leading to the development of new patches and signatures.

### Acknowledgements

We would like to thank Mike Wilson, Hermann Segmuller, and Gene Solloway for their assistance in analyzing Snort intrusion detection alerts, and Marc Zissman and Peter Heldt for their advice and administrative expertise.

### References

1. Arbaugh, W.A., Fithen, W.L., & McHugh, J. (2000). Windows of Vulnerability: A Case Study Analysis. *IEEE Computer*, 33(12), 52-59. [Link](http://www.cs.umd.edu/~waa/pubs/Windows_of_Vulnerability.pdf)
2. CAIDA. (2001). Code-Red Worms: A Global Threat. Cooperative Association for Internet Data Analysis (CAIDA). [Link](http://www.caida.org/analysis/security/code-red/)
3. Chien, E. (2001). W32.Nimda.A@mm Worm. Symantec Corporation. [Link](http://www.symantec.com/avcenter/venc/data/w32.nimda.a@mm.html)
4. CVE. (2002). Common Vulnerabilities and Exposures. The MITRE Corporation. [Link](http://www.cve.mitre.org/)
5. Dayioglu, B. & Ozgit, A. (2001). Use of Passive Network Mapping to Enhance Signature Quality of Misuse Network Intrusion Detection Systems. *Proceedings of the Sixteenth International Symposium on Computer and Information Sciences*. [Link](http://www.dayioglu.net/publications/iscis2001.pdf)
6. Dittrich, D. (2001). Distributed Denial of Service (DDoS) Attacks/tools. University of Washington, Seattle. [Link](http://staff.washington.edu/dittrich/misc/ddos/)
7. Dougherty, C., Hernan, S., Havrilla, J., Carpenter, J., Manion, A., Finlay, I., & Shaffer, J. (2001). CERT Advisory CA-2001-11 sadmind/IIS Worm. CERT Coordination Center. [Link](http://www.cert.org/advisories/CA-2001-11.html)
8. Fearnow, M. & Stearns, W. (2001). Lion Worm. SANS Institute. [Link](http://www.incidents.org/react/lion.php)
9. Forristal, J. & Shipley, G. (2001). Vulnerability Assessment Scanners. *Network Computing*. [Link](http://www.networkcomputing.com/1201/1201f1b1.html)
10. Hassell, R., Permeh, R., & Maiffret, M. (2001). UPNP - Multiple Remote Windows XP/ME/98 Vulnerabilities. eEye Digital Security. [Link](http://www.eeye.com/html/Research/Advisories/AD20011220.html)
11. Internet Software Consortium. (2002). ISC Berkeley Internet Name Domain (BIND) Domain Name System (DNS). [Link](http://www.isc.org/products/BIND/)
12. Lestat, M. (2001). The Ramen Worm and its use of rpc.statd, wu-ftpd and LPRng Vulnerabilities in Red Hat Linux. SANS Institute. [Link](http://rr.sans.org/malicious/ramen.php)
13. Lippmann, R.P., Haines, J.W., Fried, D.J., Korba, J., & Das, K. (2000). The 1999 DARPA off-line intrusion detection evaluation. *Computer Networks*, 32, 579-595.
14. Mell, P. & Grance, T. (2002). The ICAT Metabase CVE Vulnerability Search Engine. National Institute of Standards and Technology. [Link](http://icat.nist.gov)
15. Mueller, P. & Shipley, G. (2001). To Catch a Thief. *Network Computing*. [Link](http://www.networkcomputing.com/1217/1217f1.html)
16. Netcraft Web Server Survey. (2001). Netcraft Ltd., Bath England. [Link](http://www.netcraft.com/survey/index-200110.html)
17. Nessus. (2002). The Nessus Security Scanner. [Link](http://www.nessus.org)
18. NSS Group. (2001). Intrusion Detection Systems Group Test (Edition 2). Oakwood House, Wennington, Cambridgeshire, England. [Link](http://www.nss.co.uk/ids/)
19. Power, R. (2001). 2001 CSI/FBI Computer Crime and Security Survey. Computer Security Institute. [Link](http://www.gocsi.com/forms/fbi/pdf.html)
20. Ptacek, T.H. & Newsham, T.N. (1998). Insertion, Evasion, and Denial of Service: Eluding Network Intrusion Detection. Secure Networks, Inc. [Link](http://secinf.net/info/ids/idspaper/idspaper.html)
21. Roesch, M. (1999). Snort - Lightweight Intrusion Detection for Networks. *USENIX 13th Systems Administration Conference - LISA '99*. Seattle, Washington. [Link](http://www.snort.org)
22. SANS. (2001). The Twenty Most Critical Internet Security Vulnerabilities (Updated). Bethesda, MD, System Administration, Networking, and Security (SANS) Institute. [Link](http://www.sans.org/top20.htm)
23. SANS. (2001). NIMDA Worm/Virus Report – Final. System Administration, Networking, and Security (SANS) Institute. [Link](http://www.incidents.org/react/nimda.pdf)
24. Spitzner, L. (2002). Know Your Enemy: Passive Fingerprinting. Honeynet Project. [Link](http://project.honeynet.org/papers/finger/)
25. Yocom, B., Brown, K., & DerVeer, D.V. (2001). Review: Intrusion-Detection Products Grow Up. *Network World Fusion*. [Link](http://www.nwfusion.com/reviews/2001/1008rev.html)

### Author Index

- Apap, Frank
- Balwalli, Niranjan
- Bidan, Christophe
- Cabrera, Joao B.D.
- Clough, Larry A.
- Coit, Jason
- Cui, Yun
- Danforth, Melissa
- Debar, Hervé
- Donoho, David L.
- Ducassé, Mireille
- Eskin, Eleazar
- Flesia, Ana Georgina
- Fong, Martin W.
- Goldman, Robert P.
- Gorodetski, Vladimir
- Hall, Mike
- Hershkop, Shlomo
- Honig, Andrew
- Johnston, Steven R.
- Just, James E.
- Killourhy, Kevin S.
- Kotenko, Igor
- Kruegel, Christopher
- Lee, Wenke
- Levitt, Karl N.
- Maglich, Ryan
- Maxion, Roy A.
- Mé, Ludovic
- Morin, Benjamin
- Ning, Peng
- Paxson, Vern
- Porras, Phillip A.
- Reeves, Douglas S.
- Reynolds, James C.
- Rowe, Jeff
- Saluja, Sunmeet
- Shankar, Umesh
- Staniford, Stuart
- Stetson, Douglas
- Stolfo, Sal
- Tan, Kymie M.C.
- Thomas, Ashley
- Toth, Thomas
- Valdes, Alfonso
- Webster, Seth
- Wiley, Kevin
- Yung, Kwong H.
- Zhang, Yi
- Zimmermann, Jacob