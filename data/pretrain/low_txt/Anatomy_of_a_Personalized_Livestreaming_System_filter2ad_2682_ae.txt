### Impact of Different Buffer Sizes on HLS Buffering Delay and Video Stalling

**Figure 17: HLS: The Impact of Different Buffer Sizes (for Pre-Download) on Buffering Delay and Video Stalling**

Increasing the buffer size enhances the smoothness of video playback but also results in slightly higher buffering delay. The benefit of pre-buffering is not significant for RTMP streaming, which is already smooth. In Figure 16(b), we observe that a small portion (10%) of broadcasts experience long buffering delays (>5s). This is due to the bursty arrival of video frames during the upload from the broadcaster.

HLS viewers, however, experience more stalling due to high variance in polling latency. To smooth out playback, the system needs to pre-buffer more video content (6-9s). Our controlled experiment traces show that Periscope configures a pre-buffer of 9s for HLS viewers. However, Figures 17(a) and 17(b) indicate that a less conservative value of 6s provides similar results on stalling while reducing buffering delay by 50% (3s). These findings suggest that Periscope's buffering configuration is conservative and can be optimized to significantly reduce buffering delay. Over time, as the volume of broadcasts increases, servers may need to increase chunk size and decrease polling frequency, leading to increased buffering.

Although our experiments do not consider last-mile delay, the results hold as long as the last-mile delay is stable. For viewers with a stable last-mile connection, such as good WiFi or LTE, a smaller buffer size could be applied to reduce buffering delay. In cases of poor connection, Periscope could fall back to the default 9s buffer to ensure smooth playback.

### Securing Livestream Broadcasts

During our investigation into livestreaming services, we discovered a common security vulnerability in both Periscope and Meerkat. Neither service authenticates video streams after the initial connection setup, making it easy for attackers to silently alter part or all of an ongoing broadcast. Specifically, an attacker can overwrite selected portions of an ongoing broadcast (video or audio or both) by tapping into the transmission at the source (in the broadcaster’s network) or at the last-mile network of one or more viewers. This section describes this vulnerability in the Periscope protocol, the results of our proof-of-concept experiments, and simple countermeasures to defend against this attack.

#### Validating the Vulnerability

To validate the vulnerability, we performed experiments to alter a Periscope stream at both the broadcaster’s local network and the viewer’s network. We took all possible precautions to ensure our tests did not affect any other users. First, we conducted the proof-of-concept experiments on broadcasts created by ourselves, set up so that they were only accessible to our own viewer. All involved parties (attacker and victim) were our own Periscope accounts. Second, our Periscope accounts have no followers, so our experiments did not push notifications to other users. Third, we made small changes that should have zero impact on any server-side operations. Finally, once we confirmed the vulnerability, we notified both Periscope and Meerkat about it and proposed a countermeasure (directly via phone to their respective CEOs). We also promised to delay any disclosures of the attack for months to give them sufficient time to implement and deploy a fix.

#### Attack Models

**Broadcast Tampering Attack:**
The broadcast tampering attack is possible because Periscope uses unencrypted and unauthenticated connections for video transmission. When a user starts a broadcast, she first obtains a unique broadcast token from a Periscope server via HTTPS. Next, she sends the broadcast token to Wowza and sets up an RTMP connection to upload video frames. This introduces two critical issues: (1) the broadcast token is sent to Wowza via RTMP in plaintext; (2) the RTMP video itself is unencrypted. As a result, an attacker can easily launch a man-in-the-middle attack to hijack and modify the video content.

**Tampering on Broadcaster Side:**
An attacker in the same edge network as the broadcaster can alter the stream before it reaches the upload server. This is a common scenario when users connect to public WiFi networks at work, coffee shops, or airports. The attacker simply connects to the same WiFi network and sniffs the victim’s traffic. No control over the WiFi access point is needed. The attacker performs ARP spoofing to redirect the victim’s traffic, parses the unencrypted RTMP packet, replaces the video frame with arbitrary content, and uploads modified video frames to Wowza servers, which are then broadcast to all viewers. The attack can commence anytime during the broadcast and is not noticeable by the victim because her phone will only display the original video captured by the camera.

**Tampering at the Viewer Network:**
An attacker can also selectively tamper with the broadcast to affect only a specific group of viewers by connecting to the viewers’ WiFi network. When the viewer downloads video content via WiFi, the attacker can modify the RTMP packets or HLS chunks using a similar approach. The broadcaster remains unaware of the attack.

**Experimental Validation:**
We performed proof-of-concept experiments to validate both attack models. For brevity, we describe the experiment to tamper at the broadcaster. We set up a victim broadcaster as a smartphone connected to a WiFi network, an attacker as a laptop connected to the same WiFi, and a viewer as another smartphone connected via cellular. The broadcaster begins a broadcast and points the camera to a running stopwatch (to demonstrate the “liveness” of the broadcast). When the attack commences, the attacker replaces the video content with a black screen.

Before and after the attack, the viewer sees a black screen (tampered), while the broadcaster sees the original video. The attacker runs ARP spoofing to perform the man-in-the-middle attack (using the Ettercap library) and replaces video frames in the RTMP packet with the desired frames. Our proof of concept uses simple black frames. We wrote our own RTMP parser to decode the original packet and make the replacements. Finally, we open the broadcast as a viewer from another phone to examine the attack impact. The viewer does not need to connect to this WiFi network. Figure 18 shows the screenshot results of the broadcaster and the viewer before and after the attack.

#### Defense

The most straightforward defense is to replace RTMP with RTMPS, which performs full TLS/SSL encryption (this is the approach chosen by Facebook Live). However, encrypting video streams in real-time is computationally costly, especially for smartphone apps with limited computation and energy resources. Thus, for scalability, Periscope uses RTMP/HLS for all public broadcasts and only uses RTMPS for private broadcasts.

Another simple countermeasure would protect (video) data integrity by embedding a simple periodic signature into the video stream. After a broadcaster obtains a broadcast token from the Periscope server (via HTTPS), she connects to Wowza using this broadcast token and securely exchanges a private-public key pair (TLS/SSL) with the server. When uploading video to Wowza (using RTMP), the broadcaster signs a secure one-way hash of each frame and embeds the signature into the metadata. The Wowza server verifies the signatures to validate that video frames have not been modified. To mitigate viewer-side attacks, Wowza can securely forward the broadcaster’s public key to each viewer, and they can verify the integrity of the video stream. Our solution is simple and lightweight, and we can further reduce overhead by signing only selective frames or signing hashes across multiple frames.

We reported this attack and countermeasure to the management teams at both Periscope and Meerkat in September 2015. To the best of our knowledge, Periscope is taking active steps to mitigate this threat.

### Discussion and Conclusion

Our work highlights the tension between scalability and delivery delay in today’s personalized livestreaming services. These systems are still in their early stages of development, and ongoing engineering efforts can significantly reduce per-broadcast overheads and improve scalability. Our results suggest that services like Periscope are already limiting user interactions to ensure minimal lag between the audience and broadcaster. Moving forward, these services will have to make a difficult decision between maintaining hard limits on user interactivity (limiting comments to the first 100 users connected to RTMP servers) or addressing issues of scale to support more inclusive and richer modes of audience interaction.

One potential alternative is to build a dramatically different delivery infrastructure for interactive livestreams. To avoid the costs of managing persistent connections to each viewer, we can leverage a hierarchy of geographically clustered forwarding servers. To access a broadcast, a viewer would forward a request through their local leaf server and up the hierarchy, setting up a reverse forwarding path in the process. Once built, the forwarding path can efficiently forward video frames without per-viewer state or periodic polling. The result is effectively a receiver-driven overlay multicast tree (similar to Scribe [12] and Akamai’s streaming CDN [34, 23]) layered on top of a collection of CDN or forwarding servers. We note that Akamai’s CDN is focused on scalability and uses a two-layer multicast tree that optimizes the transmission path from broadcaster to receiver [23]. Since its audience does not directly interact with the broadcaster, streams do not need to support real-time interactions.

Moving forward, we believe user-generated livestreams will continue to gain popularity as the next generation of user-generated content. Novel methods of interaction between broadcasters and their audience will be a differentiating factor between competing services, and issues of scalable, low-latency video delivery must be addressed.

Finally, following consultations with the Periscope team, we will make parts of our measurement datasets available to the research community at http://sandlab.cs.ucsb.edu/periscope/.

### Acknowledgments

The authors wish to thank the anonymous reviewers and our shepherd Fabian Bustamante for their helpful comments. This project was supported by NSF grants CNS-1527939 and IIS-1321083. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.

### References

[1] Accessing Fastly’s IP Ranges. https://docs.fastly.com/guides/securing-communications/accessing-fastlys-ip-ranges.
[2] Adobe RTMP Specification. http://www.adobe.com/devnet/rtmp.html.
[3] Apple HLS Specification. https://developer.apple.com/streaming/.
[4] Fastly. https://www.fastly.com/.
[5] Fastly Network Map. https://www.fastly.com/network.
[6] Huang, C., Wang, A., Li, J., Ross, K. W. Measuring and Evaluating Large-Scale CDNs. http://dl.acm.org/citation.cfm?id=1455517 (2008).
[7] Periscope - Live Streaming with Your GoPro. https://gopro.com/help/articles/Block/Periscope-Live-Streaming-with-your-GoPro.
[8] Wowza Stream Engine. https://www.wowza.com/products/streaming-engine.
[9] Adhikari, V., Guo, Y., Hao, F., Hilt, V., and Zhang, Z.-L. A Tale of Three CDNs: An Active Measurement Study of Hulu and Its CDNs. In INFOCOM Workshops (2012).
[10] Adhikari, V. K., Guo, Y., Hao, F., Varvello, M., Hilt, V., Steiner, M., and Zhang, Z.-L. Unreeling Netflix: Understanding and Improving Multi-CDN Movie Delivery. In Proc. of INFOCOM (2012).
[11] Bouzakaria, N., Concolato, C., and Le Feuvre, J. Overhead and Performance of Low Latency Live Streaming Using MPEG-DASH. In Proc. of IISA (2014).
[12] Castro, M., et al. Scribe: A Large-Scale and Decentralized Application-Level Multicast Infrastructure. IEEE JSAC 20, 8 (2002).
[13] Constine, J. Twitter Confirms Periscope Acquisition, and Here’s How the Livestreaming App Works. TechCrunch, March 2015.
[14] Cresci, E., and Halliday, J. How a Puddle in Newcastle Became a National Talking Point. The Guardian, January 2016.
[15] Dredge, S. Twitter’s Periscope Video App Has Signed Up 10m People in Four Months. The Guardian, August 2015.
[16] Hamilton, W. A., Garretson, O., and Kerne, A. Streaming on Twitch: Fostering Participatory Communities of Play Within Live Mixed Media. In Proc. of CHI (2014), ACM.
[17] Hei, X., Liang, C., Liang, J., Liu, Y., and Ross, K. W. A Measurement Study of a Large-Scale P2P IPTV System. IEEE Transactions on Multimedia 9, 8 (2007).
[18] Huang, T.-Y., Johari, R., McKeown, N., Trunnell, M., and Watson, M. A Buffer-Based Approach to Rate Adaptation: Evidence from a Large Video Streaming Service. In Proc. of SIGCOMM (2014).
[19] Jackson, R. How to Avoid Periscope’s Broadcast Too Full Message. Phandroid Blog, August 2015.
[20] Jill, J. 'Broadcast Is Too Full'? How to Share Your Periscope Comments. Scope Tips Blog, October 2015.
[21] Kaytoue, M., Silva, A., Cerf, L., Meira Jr, W., and Raïssi, C. Watch Me Playing, I Am a Professional: A First Study on Video Game Live Streaming. In MSND@WWW (2012).
[22] Khan, A. Broadcast Too Full & You Can’t Comment? Here Are 3 Ways to Get Your Message Out Anyway. Personal Blog, August 2015.
[23] Kontothanassis, L., Sitaraman, R., Wein, J., Hong, D., Kleinberg, R., Mancuso, B., Shaw, D., and Stodolsky, D. A Transport Layer for Live Streaming in a Content Delivery Network. Proc. of the IEEE 92, 9 (2004).
[24] Krishnan, R., Madhyastha, H. V., Srinivasan, S., Jain, S., Krishnamurthy, A., Anderson, T., and Gao, J. Moving Beyond End-to-End Path Information to Optimize CDN Performance. In Proc. of SIGCOMM (2009).
[25] Kupka, T., Griwodz, C., Halvorsen, P., Johansen, D., and Hovden, T. Analysis of a Real-World HTTP Segment Streaming Case. In Proc. of EuroITV (2013).
[26] Laine, S., and Hakala, I. H.264 QoS and Application Performance with Different Streaming Protocols. In MobiMedia (2015).
[27] Lederer, S., Müller, C., and Timmerer, C. Dynamic Adaptive Streaming Over HTTP Dataset. In Proc. of MMSys (2012).
[28] Li, Y., Zhang, Y., and Yuan, R. Measurement and Analysis of a Large Scale Commercial Mobile Internet TV System. In Proc. of IMC (2011).
[29] Lohmar, T., Einarsson, T., Fröjdh, P., Gabin, F., and Kampmann, M. Dynamic Adaptive HTTP Streaming of Live Content. In Proc. of WoWMoM (2011).
[30] Madrigal, A. C. The Interesting Problem with Periscope and Meerkat. Fusion, March 2015.
[31] Magharei, N., and Rejaie, R. PRIME: Peer-to-Peer Receiver-Driven Mesh-Based Streaming. IEEE/ACM TON 17, 4 (2009), 1052–1065.
[32] Mediati, N. Twitter Cuts Off Meerkat, Won’t Let It Import Who You Follow on Twitter. PCWorld, March 2015.
[33] Müller, C., Lederer, S., and Timmerer, C. An Evaluation of Dynamic Adaptive Streaming Over HTTP in Vehicular Environments. In Proc. of MoVid (2012).
[34] Nygren, E., Sitaraman, R. K., and Sun, J. The Akamai Network: A Platform for High-Performance Internet Applications. SIGOPS OSR 44, 3 (2010).
[35] Perez, S. Live Streaming App Periscope Touts 200 Million Broadcasts in Its First Year. TechCrunch, March 2016.
[36] Poblete, B., Garcia, R., Mendoza, M., and Jaimes, A. Do All Birds Tweet the Same?: Characterizing Twitter Around the World. In Proc. of CIKM (2011).
[37] Pramuk, J. Periscope CEO: How We’re Growing Live-Streaming. CNBC, December 2015.
[38] Pullen, J. P. You Asked: What Is the Meerkat App? Time, March 2015.
[39] Siekkinen, M., Masala, E., and Kämäräinen, T. Anatomy of a Mobile Live Streaming Service: The Case of Periscope. In Proc. of IMC (2016).
[40] Silverston, T., and Fourmaux, O. Measuring P2P IPTV Systems. In Proc. of NOSSDAV (2007).
[41] Small, T., Liang, B., and Li, B. Scaling Laws and Tradeoffs in Peer-to-Peer Live Multimedia Streaming. In Proc. of MM (2006).
[42] Sripanidkulchai, K., Ganjam, A., Maggs, B., and Zhang, H. The Feasibility of Supporting Large-Scale Live Streaming Applications with Dynamic Application End-Points. In Proc. of SIGCOMM (2004).
[43] Sripanidkulchai, K., Maggs, B., and Zhang, H. An Analysis of Live Streaming Workloads on the Internet. In Proc. of IMC (2004).
[44] Su, A.-J., Choffnes, D. R., Kuzmanovic, A., and Bustamante, F. E. Drafting Behind Akamai (Travelocity-Based Detouring). In Proc. of SIGCOMM (2006).
[45] Tang, J. C., Venolia, G., and Inkpen, K. M. Meerkat and Periscope: I Stream, You Stream, Apps Stream for Live Streams. In Proc. of CHI (2016).
[46] Wilson, C., Boe, B., Sala, A., Puttaswamy, K. P. N., and Zhao, B. Y. User Interactions in Social Networks and Their Implications. In Proc. of EuroSys (2009).
[47] Yin, X., Jindal, A., Sekar, V., and Sinopoli, B. A Control-Theoretic Approach for Dynamic Adaptive Video Streaming Over HTTP. In Proc. of SIGCOMM (2015).
[48] Zhang, C., and Liu, J. On Crowdsourced Interactive Live Streaming: A Twitch.TV-Based Measurement Study. In Proc. of NOSSDAV (2015).
[49] Zhang, X., Liu, J., Li, B., and Yum, T.-S. P. Coolstreaming/Donet: A Data-Driven Overlay Network for Peer-to-Peer Live Media Streaming. In Proc. of INFOCOM (2005).
[50] Zhao, X., Sala, A., Wilson, C., Wang, X., Gaito, S., Zheng, H., and Zhao, B. Y. Multi-Scale Dynamics in a Massive Online Social Network. In Proc. of IMC (2012), pp. 171–184.