### 5.3 Invariant Checks and Control Flow Divergence

- **Control Flow Divergence**: In some cases, the control flow diverges significantly in the faulty run compared to the correct run, leading to a different part of the application with fewer checks.
- **Fewer Invariant Checks**: In two other cases, both the correct and faulty runs have very few invariant checks, making it difficult for iSWAT to detect the faults.
- **No Mismatches in Monitored Values**: In two specific cases (in `mcf` and `gzip`), there are no mismatches in the monitored values within the 10M window. As a result, iSWAT cannot detect these faults because there are no checks after the 10M window.
- **Short-Term Divergence**: In most cases, the control flow diverges for a short period and then merges back, indicating that the divergence is not sustained.
- **Lower Order Bit Mismatches**: Regardless of whether the fault injection is in higher or lower order bits, almost all value mismatches in SDC cases occur in the lower order bits. This suggests that simple range-based or control-flow invariants may not be effective. In fact, most mismatches were in the lowest 3 bits. Therefore, other types of invariants or detectors are needed to effectively identify mismatches in lower order bits.

### 5.4 Latency Analysis

Table 7 presents the latency results for faults detected by SWAT and iSWAT, categorized into various bins from under 1k instructions to under 10M instructions. The numbers are presented as a percentage of the total number of faults detected by iSWAT (i.e., the number of detections in the <10M case).

- **Detection Latency**: The number of faults detected at a latency of under 1k instructions shows the largest increase of about 2% (the rest of the numbers are cumulative). This indicates that the latency of detection using invariants is significantly lower than that of other symptoms, thereby increasing the number of faults amenable to simple hardware recovery.
- **Latency Benefits**: Although the latency benefits offered by iSWAT are not substantial, using more sophisticated invariants may further improve the effectiveness of iSWAT in reducing latency.

| Latencies | SWAT | iSWAT |
|-----------|------|-------|
| <5k       | 41.1% | 43.1% |
| <100k     | 47.0% | 49.6% |
| <1k       | 50.7% | 53.4% |
| <10k      | 78.7% | 81.2% |
| <50k      | 81.0% | 83.3% |
| <500k     | 87.0% | 89.2% |
| <1M       | 90.3% | 92.7% |
| <5M       | 95.7% | 97.7% |
| <10M      | 98.7% | 100.0% |

**Table 7. Detection latencies for SWAT and iSWAT. The percentages are computed using the number of detections in iSWAT with <10M as the baseline. The invariants increase the faults amenable to hardware recovery by 2%.**

### 5.5 Overhead Evaluation

We evaluate the overhead of using invariants by running the binary (with invariants checking) on fault-free hardware, using two machines: a Sun UltraSPARC-III 1.2GHz machine with 1MB unified L2 cache and 2GB RAM, and an AMD Athlon(TM) dual-core MP 2100+ machine with 256KB L2 cache and 1.5GB RAM. The Sun machine is referred to as the Spare machine, and the AMD one as the x86 machine.

- **Overhead Comparison**: Figure 3 shows the overhead of using invariants checking in the programs as a percentage over the baseline program, which has no invariants checking. The geometric mean of the overheads is also shown for the two machines.
- **Sparc vs. x86 Overhead**: The Sparc machine exhibits a higher overhead when running the invariants code, with an average overhead of 14%, compared to 5% for the x86 machine. Specifically, the overhead for the application `mcf` is significantly higher in the Sparc machine (26%) than in the x86 machine (2%). The high overhead of the Sparc machine is likely due to its inability to hide the cache misses and branch mispredictions induced by the extra invariants. The x86 machine, on the other hand, is better at hiding these latencies, resulting in lower overheads.
- **Acceptable Overhead**: Despite these differences, the overheads produced by these invariants checks are within acceptable limits for the increased coverage they provide, motivating the use of the iSWAT system for increased resilience.

### Related Work

There is a growing body of work on using software-visible symptoms to detect hardware errors. Several papers propose the use of control path signatures to detect control-flow errors [1, 5, 21, 29, 30]. Wang and Patel suggest using branch mispredictions, cache misses, and exceptions as symptoms of faults [32]. Most of this work focuses on transient or intermittent faults and does not handle permanent faults (with a few exceptions discussed below). Permanent faults are important due to the expected increase in phenomena such as wear-out, insufficient burn-in, and design defects.

In our previous work [9], we used simple software symptoms to detect both permanent and transient faults, and we extend that system in this paper. Dynamically detected program invariants (likely invariants [4]), which are inherently unsound, have been studied for various applications, including program evolution [4], program understanding [7, 4], and detecting and diagnosing software bugs [4, 6, 12, 33, 13]. The only work we know of that uses likely invariants for online error detection comprises three recent papers on transient hardware fault detection [22, 24, 3]. Racunas et al. and Dimitrov et al. extract invariants using online hardware monitoring, whereas Pattabiraman et al. use ahead-of-time monitoring of program runs (similar to our work). However, they can only use their invariants for transient errors because they lack mechanisms to distinguish false positives from true hardware failures. In contrast, we can handle both transient and permanent faults.

Meixner et al. in the Argus project proposed using a program dataflow checker, combined with control flow signature checking, functional unit checkers, a memory checker, and parity on all data transfer and storage units to handle a wide range of faults [16, 17]. Their dataflow graph and control flow signatures conceptually are invariants encoded by the compiler in the binary and checked by the hardware. Unfortunately, this technique does not work with interrupts, I/O, etc., because these affect the control flow. Some parts of the Argus solution may also incur significant performance overhead. Coverage data is reported only for a synthetic microbenchmark, so the effectiveness of the technique for real programs is unclear [16]. Finally, the estimated area overhead is 17% of the core for Argus, and a fault in this part could lead to false positives. In contrast, we look at far cheaper detection techniques, combining software-extracted invariants with several other software symptoms that can be observed at near-zero cost.

### 7. Conclusion and Future Work

Existing methods for detecting hardware faults using software-level symptoms, such as SWAT [9], are promising due to their high coverage and low cost. Nevertheless, these systems need additional detectors to achieve reliability levels acceptable for most systems. In this work, we proposed and evaluated the first design (to our knowledge) that uses likely program invariants for detecting permanent faults. We used simple range-based invariants on single variable values, in conjunction with low-overhead symptom-based detection techniques already available in the SWAT System. Our results show that likely invariants can reduce the fraction of undetected errors from 4% to 2.8% when used in conjunction with other symptom-based detection techniques. Further, they reduce SDCs by 47% to 74.2%, which is important for any hardware fault tolerance solution. We also showed that by leveraging the diagnosis framework in SWAT, we could keep the overhead caused by false positives to acceptable levels.

These range-based invariants form a first step towards using invariants to detect hardware faults. We are now investigating more sophisticated invariant schemes to further improve the effectiveness of the iSWAT system. We also want to monitor other program values and design a strategy to select the most effective values for monitoring to reduce overhead. Additionally, we plan to evaluate the approach on more benchmarks and real applications.

### Acknowledgments

We would like to thank Robert Bocchino for many discussions and help in writing.

### References

[1] E. Borin, C. Wang, Y. Wu, and G. Araujo. Dynamic binary control-flow errors detection. SIGARCH Comput. Archit. News, 33(5), 2005.

[2] S. Borkar. Designing Reliable Systems from Unreliable Components: The Challenges of Transistor Variability and Degradation. IEEE Micro, 25(6), 2005.

[3] M. Dimitrov and H. Zhou. Unified architectural support for soft-error protection or software bug detection. In Proc. Int'l Conf on Parallel Architectures and Compilation Techniques (PACT), 2007.

[4] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin. Dynamically discovering likely program invariants to support program evolution. IEEE Trans. Software Eng., 2001.

[5] O. Goloubeva et al. Soft-Error Detection Using Control Flow Assertions. In Proc. of 18th IEEE Inti. Symp. on Defect and Fault Tolerance in VLSI Systems, 2003.

[6] S. Hangal and M. S. Lam. Tracking down software bugs using automatic anomaly detection. In Proceedings of the International Conference on Software Engineering, 2002.

[7] Y. Kataoka, M. D. Ernst, W. G. Griswold, and D. Notkin. Automated support for program refactoring using invariants. In IEEE Int'l Conf on Software Maintenance (ICSM), 2001.

[8] C. Lattner and V. Adve. LLVM: A Compilation Framework for Lifelong Program Analysis and Transformation. In Proc. Int'l Symp. on Code Generation and Optimization, 2004.

[9] M. Li, P. Ramachandran, S. Sahoo, S. Adve, V. Adve, and Y. Zhou. Understanding the Propagation of Hard Errors to Software and Implications for Resilient System Design. In Proc. Inti. Con! on Architectural Support for Programming Languages and Operating Systems (ASPL OS), 2008.

[10] M. Li, P. Ramachandran, S. K. Sahoo, S. V. Adve, V. S. Adve, and Y. Zhou. Trace Based Diagnosis of Permanent Hardware Faults. In International Conference on Dependable Systems and Networks, 2008.

[11] B. Liblit, A. Aiken, A. X. Zheng, and M. I. Jordan. Bug isolation via remote program sampling. In Proc. of Conf on Programming Language Design and Implementation, 2003.

[12] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, and M. I. Jordan. Scalable statistical bug isolation. In Proc. of Conf. on Programming Language Design and Implementation, 2005.

[13] S. Lu, J. Tucek, F. Qin, and Y. Zhou. Avio: detecting atomicity violations via access interleaving invariants. In Proc. Int'I Con! on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2006.

[14] M. Martin et al. Multifacet's General Execution-Driven Multiprocessor Simulator (GEMS) Toolset. SIGARCH Computer Architecture News, 33(4), 2005.

[15] C. J. Mauer, M. D. Hill, and D. A. Wood. Full-System Timing-First Simulation. SIGMETRICS Performance Evaluation Rev., 30(1), 2002.

[16] A. Meixner, M. E. Bauer, and D. J. Sorin. Argus: Low-cost, comprehensive error detection in simple cores. In Proc. ACM/IEEE Int'l Symposium on Microarchitecture, 2007.

[17] A. Meixner and D. J. Sorin. Error detection using dynamic dataflow verification. In Proc. Int'l Conf. on Parallel Architectures and Compilation Techniques (PA CT), 2007.

[18] I. Nakano et al. ReViveI/O: Efficient Handling of I/O in Highly-Available Rollback-Recovery Servers. In Int'l Symp. on High Performance Computer Architecture (HPCA), 2006.

[19] N. Nakka et al. An Architectural Framework for Detecting Process Hangs/Crashes. In European Dependable Computing Conference (EDCC), 2005.

[20] J. W. Nimmer and M. D. Ernst. Automatic generation of program specifications. In Proc. ACM SIGSOFT Int'l Symp. on Software Testing and Analysis, 2002.

[21] N. Oh, P. P. Shirvani, and E. J. McCluskey. Control-flow checking by software signatures. IEEE Trans. on Reliability, 51, March 2002.

[22] K. Pattabiraman, G. P. Saggesse, D. Chen, Z. Kalbarczyk, and R. Iyer. Dynamic derivation of application-specific error detectors and their hardware implementation. In Proc. of European Dependable Computing Conference (EDCC), 2006.

[23] M. Prvulovic et al. ReVive: Cost-Effective Architectural Support for Rollback Recovery in Shared-Memory Multiprocessors. In Int'l Symp. on Computer Architecture (ISCA), 2002.

[24] P. Racunas et al. Perturbation-based Fault Screening. In International Symposium on High Performance Computer Architecture (HPCA), 2007.

[25] V. Reddy et al. Assertion-Based Microarchitecture Design for Improved Fault Tolerance. In International Conference on Computer Design, 2006.

[26] G. A. Reis et al. Software-Controlled Fault Tolerance. ACM Transactions on Architectural Code Optimization, 2(4), 2005.

[27] E. Rotenberg. AR-SMT: A Microarchitectural Approach to Fault Tolerance in Microprocessors. In International Symposium on Fault-Tolerant Computing (FTCS), 1999.

[28] D. Sorin et al. SafetyNet: Improving the Availability of Shared Memory Multiprocessors with Global Checkpoint/Recovery. In Int 'I Symp. on Computer Architecture (ISCA), 2002.

[29] R. Vemu and J. A. Abraham. CEDA: Control-flow Error Detection through Assertions. In Inti. On-Line Test Symposium, 2006.

[30] R. Venkatasubramanian et al. Low-Cost On-Line Fault Detection Using Control Flow Assertions. In International On-Line Test Symposium, 2003.

[31] Virtutech. Simics Full System Simulator. Website, 2006. http://www.simics.net.

[32] N. Wang and S. Patel. ReStore: Symptom-Based Soft Error Detection in Microprocessors. IEEE Transactions on Dependable and Secure Computing, 3(3), July-Sept 2006.

[33] P. Zhou, W. Liu, F. Long, S. Lu, F. Qin, Y. Zhou, S. Midkiff, and J. Torrellas. Accmon: Automatically detecting memory-related bugs via program counter-based invariants. In Proc. ACM/IEEE Int'l Symposium on Microarchitecture, 2004.