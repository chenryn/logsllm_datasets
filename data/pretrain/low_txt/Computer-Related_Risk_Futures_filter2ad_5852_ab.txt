### Subsequent Attempt at a Cover-Up During Elections

In the elections of 2002, 2004, and 2006, insiders exploited a misleading user interface to alter votes before they were cast. This highlights the vulnerabilities in today's electronic voting systems, which are often held to much lower standards than those applied to gambling machines and lotteries. Internet voting, while highly desirable in principle, poses significant risks and requires more selective use, enhanced security controls, and rigorous oversight to be considered trustworthy. Comprehensive approaches to election integrity are essential, spanning the entire process from beginning to end, including the entire supply chain. For example, [7] outlines requirements for election integrity within the Common Criteria framework.

### Would-Be Remedies

Various approaches to mitigating these risks have been proposed. A quasi-analytic approach involves risk assessment to identify and quantify the most serious risks, often based on incomplete or false assumptions. However, such analyses are frequently followed by so-called risk management, which may ignore the results, declare them sufficiently unlikely, seek insurance coverage, or attempt to reduce or prevent the causes of potential risks. A more forward-thinking approach involves developing systems that meet well-specified and realistic requirements, with soundly layered and composable system architectures. This includes principled software engineering practices, inherently safer programming languages, and intelligent system administration (see fm.csl.sri.com/LAW09/#program for recent papers on the 2009 workshop on this topic, including [14], and John Rushby's insights on lessons from other disciplines).

A document developed for Doug Maughan at the Department of Homeland Security, "A Roadmap for Cybersecurity Research" [6], identifies 11 areas of hard problems in information security, such as scalable trustworthy systems, enterprise-level metrics, lifecycle of system evaluation, insider threats and malware, global-scale identity management, survivability of time-critical systems, situational awareness and attack attribution, provenance, privacy-aware security, and usable security. Each area requires significant effort in research, development, evaluation, and technology transfer. The interrelated nature of these areas underscores the need for holistic approaches, as discussed in [11]. While decoupling these areas may be desirable for various reasons, their interdependencies must be realistically accommodated throughout the R&D process, including the establishment of requirements, architectural design, implementation, and adaptability under subsequent evolutionary changes. In particular, inherent complexities must be addressed architecturally, either reduced or constructively managed, without compromising trustworthiness.

### Background on Constructive Approaches

Background on various constructive approaches for coping with complex requirements and architectures can be found in [10, 13, 14] and [1, 2, 4, 16, 18, 19, 20].

### Conclusions

Myopia regarding trustworthiness is extremely dangerous. The commonalities among different applications extend beyond those considered here, transcending single-discipline solutions. Therefore, massive cultural shifts are needed to consider information systems and their applications holistically and proactively. We must be able to develop and evaluate systems in their entirety, especially through compositions of evaluated subsystems with predictable aggregate behavior, ensuring usability and the soundness of operational configurations. This cultural shift is particularly important for applications with critical trustworthiness requirements. The pleas for such approaches in seminal papers (including those revisited in the ACSAC Classic Papers track) may seem old-fashioned but remain timely.

An inadequate understanding of the depth of these problems is also dangerous, as it typically leads to simplistic and untrustworthy solutions. Examples include the belief that cryptographic certificates, longer passwords, firewalls, and testing can ensure security. Conversely, an overemphasis on the pervasiveness of these problems can lead to a sense of hopelessness, resulting in short-sighted and ineffective solutions. In both cases, the danger of eschewing useful research and innovative developments, and placing trust in untrustworthy systems and people, often prevails.

Life-critical systems and other systems with stringent trustworthiness requirements should be held to higher standards than conventional software. Today’s reality, where architectures are poorly structured, development practices yield numerous vulnerabilities, and criteria and proprietary evaluations are inherently incomplete, needs to be overcome.

Market forces appear inadequate in driving high-assurance components for critical systems. Open systems, open interfaces, and nonproprietary source code need more supporting incentives. Regulation is a slippery slope, and attempting to invoke liability for the development or use of untrustworthy systems would require significant and difficult changes, including how to assign blame. Insurance and tax incentives are possible but may have exploitable loopholes. Better awareness of the risks of untrustworthiness is clearly warranted. Above all, there are no simple solutions.

I am frequently asked why the ACM Risks Forum does not include more success stories. There are several reasons: failures significantly outnumber successes, success stories are rarely submitted, and I continue to search for them. Supposed successes are sometimes over-hyped, even masking development problems. Henry Petroski noted long ago that we tend to learn little from successes but have a better chance to learn from our failures. The RISKS experience suggests that we do not learn enough from either. Fortunately, a better understanding of the past seems to be emerging in some areas, along with perhaps fewer aircraft crashes, automobile recalls, and nuclear power disasters. The ACSAC classic papers track provides some supporting perspectives, including those this year by Li Gong and Matt Bishop.

However, many problems remain in areas such as health care, power distribution, malware, and elections, with lurking trustworthiness issues in automated highways, autonomous systems, and cloud computing. The lessons of past vulnerabilities, system development failures, and human limitations must be considered more pervasively, and a deeper understanding of emerging threats and potential risks must be gained. I am optimistic about the contributions of the research and development communities, but less optimistic about achieving the necessary culture shifts due to the major changes required in governments, industry, education, economic policies, standards, procurement processes, and entrenched interests.

### Acknowledgments

This paper received support from ACCURATE: A Center for Correct, Usable, Reliable, Auditable, and Transparent Elections, under SRI’s National Science Foundation Grant Number 0524111. The author wishes to thank Douglas Maughan, who previously sponsored a program on high-assurance trustworthy systems (e.g., [10]) when he was a Program Manager at DARPA, and who continues to be responsible for cybersecurity in the Science and Technology Directorate of the Department of Homeland Security, pursuing approaches aimed at preventing risks such as those mentioned here (e.g., [6]).

### References

[1] C. Boettcher, R. DeLong, J. Rushby, and W. Sifre. The MILS component integration approach to secure information sharing. In 27th AIAA/IEEE Digital Avionics Systems Conference, St. Paul MN, October 2008. IEEE.

[2] D.D. Clark and D.R. Wilson. A comparison of commercial and military computer security policies. In Proceedings of the 1987 Symposium on Security and Privacy, pages 184–194, Oakland, California, April 1987. IEEE Computer Society.

[3] D.D. Clark et al. Computers at Risk: Safe Computing in the Information Age. National Research Council, National Academies Press, 2101 Constitution Ave., Washington, D.C., 5 December 1990. Final report of the System Security Study Committee.

[4] E.W. Dijkstra. The structure of the THE multiprogramming system. Communications of the ACM, 11(5), May 1968.

[5] S.E. Goodman and H.S. Lin, editors. Toward a Safer and More Secure Cyberspace. National Research Council, National Academies Press, 2101 Constitution Ave., Washington, D.C., 2007. Final report of the National Research Council Committee on Improving Cybersecurity Research in the United States.

[6] D. Maughan et al. A roadmap for cybersecurity research. Technical report, Department of Homeland Security, October 2009.

[7] R. Mercuri. Electronic Vote Tabulation Checks and Balances. PhD thesis, Department of Computer Science, University of Pennsylvania, 2001. http://www.notablesoftware.com/evote.html

[8] P.G. Neumann. The role of motherhood in the pop art of system programming. In Proceedings of the ACM Second Symposium on Operating Systems Principles, Princeton, New Jersey, pages 13–18. ACM, October 1969. http://www.multicians.org/pgn-motherhood.html

[9] P.G. Neumann. Computer-Related Risks. ACM Press, New York, and Addison-Wesley, Reading, Massachusetts, 1995.

[10] P.G. Neumann. Principled assuredly trustworthy composable architectures. Technical report, Computer Science Laboratory, SRI International, Menlo Park, California, December 2004. http://www.csl.sri.com/neumann/chats4.html, .pdf, and .ps.

[11] P.G. Neumann. Holistic systems. ACM Software Engineering Notes, 31(6):4–5, November 2006.

[12] P.G. Neumann. Risks of untrustworthiness. In Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC 2006), Classic Papers section, Miami, Florida, December 2006. IEEE Computer Society.

[13] P.G. Neumann. Reflections on system trustworthiness. In Marvin Zelkowitz, editor, Advances in Computers, volume 70, pages 269–310. Elsevier Inc., 2007.

[14] P.G. Neumann. Hierarchies, lowerarchies, anarchies, and plutarchies: Historical perspectives of composable high-assurance architectures. In Third Layered Assurance Workshop, San Antonio CA, August 2009. AFRL. Slides: http://www.csl.sri.com/neumann/law09+x4.pdf

[15] P.G. Neumann. Illustrative risks to the public in the use of computer systems and related technology, index to RISKS cases. Technical report, Computer Science Laboratory, SRI International, Menlo Park, California, 2009. Updated now and then: http://www.csl.sri.com/neumann/illustrative.html; also in .ps and .pdf form for printing in a denser format.

[16] D.L. Parnas. On the criteria to be used in decomposing systems into modules. Communications of the ACM, 15(12), December 1972.

[17] P. Porras. Reﬂections on conﬁcker. Communications of the ACM, 52(10), October 2009. Inside Risks column. http://www.csl.sri.com/neumann/insiderisks.html#219

[18] J.M. Rushby. The design and verification of secure systems. In Proceedings of the Eighth ACM Symposium on Operating System Principles, pages 12–21, Asilomar, California, December 1981. (ACM Operating Systems Review, 15(5)).

[19] J.H. Saltzer and F. Kaashoek. Principles of Computer System Design. Morgan Kaufmann, 2009. Chapters 1-6 only. Chapters 7-11 are online. http://ocw.mit.edu/Saltzer-Kaashoek

[20] J.H. Saltzer and M.D. Schroeder. The protection of information in computer systems. Proceedings of the IEEE, 63(9):1278–1308, September 1975.

[21] M. Schaefer et al. Multilevel Data Management Security. Air Force Studies Board, National Research Council, National Academies Press, 1983. Final report of the 1982 Multilevel Data Management Security Committee.

[22] F.B. Schneider and M. Blumenthal, editors. Trust in Cyberspace. National Research Council, National Academies Press, 2101 Constitution Ave., Washington, D.C., 1998. Final report of the National Research Council Committee on Information Trustworthiness.