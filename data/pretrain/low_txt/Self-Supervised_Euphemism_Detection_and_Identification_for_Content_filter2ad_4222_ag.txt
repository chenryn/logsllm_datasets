以下是优化后的参考文献列表，使其更加清晰、连贯和专业：

[1] T. Rauber and K. Berns, “Kernel multilayer perceptron,” in *Proceedings of SIBGRAPI Conference on Graphics, Patterns and Images*, 2011, pp. 337–343.

[2] H. Yang, X. Ma, K. Du, Z. Li, H. Duan, X. Su, G. Liu, Z. Geng, and J. Wu, “How to learn Klingon without a dictionary: Detection and measurement of black keywords used by the underground economy,” in *IEEE Symposium on Security and Privacy (SP)*, 2017, pp. 751–769.

[3] J. Taylor, M. Peignon, and Y.-S. Chen, “Surfacing contextual hate speech words within social media,” *arXiv preprint arXiv:1711.10093*, 2017.

[4] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word vectors with subword information,” *Transactions of the Association for Computational Linguistics (TACL)*, vol. 5, pp. 135–146, 2017.

[5] O. Levy and Y. Goldberg, “Dependency-based word embeddings,” in *Proceedings of the Association for Computational Linguistics (ACL)*, 2014, pp. 302–308.

[6] N. Christin, “Traveling the Silk Road: A measurement analysis of a large anonymous online marketplace,” in *Proceedings of the International Conference on World Wide Web (WWW)*, 2013, pp. 213–224.

[7] J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, and J. Han, “SetExpan: Corpus-based set expansion via context feature selection and rank ensemble,” in *Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)*, Springer, 2017, pp. 288–304.

[8] W. Zhu, H. Gong, J. Shen, C. Zhang, J. Shang, S. Bhat, and J. Han, “FUSE: Multi-faceted set expansion by coherent clustering of skip-grams,” in *Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)*, 2020.

[9] Y. Zhang, J. Shen, J. Shang, and J. Han, “Empower entity set expansion via language model probing,” in *Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)*, 2020.

[10] J. Huang, Y. Xie, Y. Meng, J. Shen, Y. Zhang, and J. Han, “Guiding corpus-based set expansion by auxiliary sets generation and co-expansion,” in *Proceedings of The Web Conference*, 2020, pp. 2188–2198.

[11] J. Shen, W. Qiu, J. Shang, M. Vanni, X. Ren, and J. Han, “SynSetExpan: An iterative framework for joint entity set expansion and synonym discovery,” in *Proceedings of the Empirical Methods in Natural Language Processing (EMNLP)*, 2020, pp. 8292–8307.

[12] X. Rong, Z. Chen, Q. Mei, and E. Adar, “EgoSet: Exploiting word ego-networks and user-generated ontology for multifaceted set expansion,” in *Proceedings of the Web Search and Data Mining (WSDM)*, 2016, pp. 645–654.

[13] W. L. Hamilton, K. Clark, J. Leskovec, and D. Jurafsky, “Inducing domain-specific sentiment lexicons from unlabeled corpora,” in *Proceedings of the Empirical Methods in Natural Language Processing (EMNLP)*, vol. 2016, NIH Public Access, 2016, p. 595.

[14] C. Yang, J. Zhang, and J. Han, “Co-embedding network nodes and hierarchical labels with taxonomy based generative adversarial networks,” in *Proceedings of the IEEE International Conference on Data Mining (ICDM)*, 2020.

[15] J. Huang, Y. Xie, Y. Meng, Y. Zhang, and J. Han, “COREL: Seed-guided topical taxonomy construction by concept learning and relation transferring,” in *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD)*, 2020, pp. 1928–1936.

[16] Y. Mao, T. Zhao, A. Kan, C. Zhang, X. L. Dong, C. Faloutsos, and J. Han, “OCTET: Online catalog taxonomy enrichment with self-supervision,” in *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD)*, 2020, pp. 2247–2257.

[17] J. Shang, X. Zhang, L. Liu, S. Li, and J. Han, “NetTaxo: Automated topic taxonomy construction from text-rich network,” in *Proceedings of The Web Conference*, 2020, pp. 1908–1919.

[18] J. Shen, Z. Shen, C. Xiong, C. Wang, K. Wang, and J. Han, “TaxoExpan: Self-supervised taxonomy expansion with position-enhanced graph neural network,” in *Proceedings of The Web Conference*, 2020, pp. 486–497.

[19] C. Zhang, F. Tao, X. Chen, J. Shen, M. Jiang, B. Sadler, M. Vanni, and J. Han, “TaxoGen: Constructing topical concept taxonomy by adaptive term embedding and clustering,” in *Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD)*, 2018.

[20] P. Bonacich, “Factoring and weighting approaches to status scores and clique identification,” *Journal of Mathematical Sociology*, vol. 2, no. 1, pp. 113–120, 1972.

[21] ——, “Technique for analyzing overlapping memberships,” *Sociological Methodology*, vol. 4, pp. 176–185, 1972.

[22] L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation ranking: Bringing order to the web,” *Stanford InfoLab, Tech. Rep.*, 1999.

[23] S. Ishiwatari, H. Hayashi, N. Yoshinaga, G. Neubig, S. Sato, M. Toyoda, and M. Kitsuregawa, “Learning to describe unknown phrases with local and global contexts,” in *Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)*, 2019, pp. 3467–3476.

[24] K. Ni and W. Y. Wang, “Learning to explain non-standard English words and phrases,” in *Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP)*, 2017, pp. 413–417.

[25] K. Taghipour and H. T. Ng, “Semi-supervised word sense disambiguation using word embeddings in general and specific domains,” in *Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)*, 2015, pp. 314–323.

[26] A. Raganato, C. D. Bovi, and R. Navigli, “Neural sequence learning models for word sense disambiguation,” in *Proceedings of the Empirical Methods in Natural Language Processing (EMNLP)*, 2017, pp. 1156–1167.

[27] A. Raganato, J. Camacho-Collados, and R. Navigli, “Word sense disambiguation: A unified evaluation framework and empirical comparison,” in *Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)*, vol. 1, 2017, pp. 99–110.

[28] I. Iacobacci, M. T. Pilehvar, and R. Navigli, “Embeddings for word sense disambiguation: An evaluation study,” in *Proceedings of the Association for Computational Linguistics (ACL)*, vol. 1, 2016, pp. 897–907.

[29] L. Weng, “Self-supervised learning,” *lilianweng.github.io/lil-log*, 2019. [Online]. Available: <https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html>

[30] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT: A lite BERT for self-supervised learning of language representations,” in *Proceedings of the International Conference on Learning Representations (ICLR)*, 2019.

[31] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A robustly optimized BERT pretraining approach,” *arXiv preprint arXiv:1907.11692*, 2019.

[32] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in *Proceedings of Advances in Neural Information Processing Systems (NeurIPS)*, 2020.

[33] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, “Big self-supervised models are strong semi-supervised learners,” in *Proceedings of Advances in Neural Information Processing Systems (NeurIPS)*, 2020.

[34] L. Liu, J. Shang, X. Ren, F. F. Xu, H. Gui, J. Peng, and J. Han, “Empower sequence labeling with task-aware neural language model,” in *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, AAAI Press, 2018, pp. 5253–5260.

[35] Z. Feng, C. Xu, and D. Tao, “Self-supervised representation learning by rotation feature decoupling,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2019, pp. 10 364–10 374.

[36] A. Kolesnikov, X. Zhai, and L. Beyer, “Revisiting self-supervised visual representation learning,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2019, pp. 1920–1929.

[37] M. Sabokrou, M. Khalooei, and E. Adeli, “Self-supervised representation learning via neighborhood-relational encoding,” in *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, 2019, pp. 8010–8019.

[38] W. Zhu, C. Zhang, S. Yao, X. Gao, and J. Han, “A spherical hidden Markov model for semantics-rich human mobility modeling,” in *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, 2018.

[39] O. Mees, M. Tatarchenko, T. Brox, and W. Burgard, “Self-supervised 3D shape and viewpoint estimation from single images for robotics,” in *Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, IEEE, 2019, pp. 6083–6089.

[40] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and S. Levine, “Combining self-supervised learning and imitation for vision-based rope manipulation,” in *Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)*, IEEE, 2017, pp. 2146–2153.

[41] L. Berscheid, P. Meißner, and T. Kröger, “Self-supervised learning for precise pick-and-place without object model,” *IEEE Robotics and Automation Letters*, vol. 5, no. 3, pp. 4828–4835, 2020.

[42] X. Zhai, A. Oliver, A. Kolesnikov, and L. Beyer, “S4L: Self-supervised semi-supervised learning,” in *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, 2019, pp. 1476–1485.

[43] Y. Sun, E. Tzeng, T. Darrell, and A. A. Efros, “Unsupervised domain adaptation through self-supervision,” *arXiv preprint arXiv:1909.11825*, 2019.

[44] J. Xu, L. Xiao, and A. M. López, “Self-supervised domain adaptation for computer vision tasks,” *IEEE Access*, vol. 7, pp. 156 694–156 706, 2019.

[45] H. Yin, P. Molchanov, J. M. Alvarez, Z. Li, A. Mallya, D. Hoiem, N. K. Jha, and J. Kautz, “Dreaming to distill: Data-free knowledge transfer via DeepInversion,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020, pp. 8715–8724.

[46] G. Kahn, A. Villaflor, B. Ding, P. Abbeel, and S. Levine, “Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation,” in *Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)*, IEEE, 2018, pp. 1–8.

[47] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser, “Learning synergies between pushing and grasping with self-supervised deep reinforcement learning,” in *Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, IEEE, 2018, pp. 4238–4245.

[48] V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine, “Skew-Fit: State-covering self-supervised reinforcement learning,” *arXiv preprint arXiv:1903.03698*, 2019.

[49] Y. Huo, Y. Lu, Y. Niu, Z. Lu, and J.-R. Wen, “Coarse-to-fine grained classification,” in *Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)*, 2019, pp. 1033–1036.

[50] W. Liu, C. Zhang, J. Zhang, and Z. Wu, “Global for coarse and part for fine: A hierarchical action recognition framework,” in *Proceedings of the 25th IEEE International Conference on Image Processing (ICIP)*, IEEE, 2018, pp. 2630–2634.

[51] Z. Li, Y. Wei, Y. Zhang, X. Zhang, and X. Li, “Exploiting coarse-to-fine task transfer for aspect-level sentiment classification,” in *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, vol. 33, 2019, pp. 4253–4260.

[52] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*, vol. 9, no. 8, pp. 1735–1780, 1997.

[53] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in *Proceedings of the International Conference on Learning Representations (ICLR)*, 2015.

[54] D. W. Hosmer Jr, S. Lemeshow, and R. X. Sturdivant, *Applied Logistic Regression*. John Wiley & Sons, 2013, vol. 398.

[55] F. Hoffa, “1.7 billion Reddit comments loaded on BigQuery,” 2016. [Online]. Available: <https://www.reddit.com/r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/>. As of 2021, the dataset is available at <https://console.cloud.google.com/bigquery?project=fh-bigquery>.

[56] C. Cimpanu, “Reddit bans community dedicated to dark web markets,” Mar. 2018. [Online]. Available: <https://www.bleepingcomputer.com/news/security/reddit-bans-community-dedicated-to-dark-web-markets/>.

[57] K. Soska and N. Christin, “Measuring the longitudinal evolution of the online anonymous marketplace ecosystem,” in *Proceedings of the 24th USENIX Security Symposium*, Washington, DC, Aug. 2015, pp. 33–48.

[58] Drug Enforcement Administration, “Slang terms and code words: A reference for law enforcement personnel,” DEA Intelligence Report DEA-HOU-DIR-022-18, 2018. [Online]. Available: <https://www.dea.gov/sites/default/files/2018-07/DIR-022-18.pdf>.

[59] S. Zannettou, B. Bradlyn, E. De Cristofaro, H. Kwak, M. Sirivianos, G. Stringini, and J. Blackburn, “What is Gab: A bastion of free speech or an alt-right echo chamber,” in *Companion Proceedings of The Web Conference*, 2018, pp. 1007–1014.

[60] C. D. Manning, H. Schütze, and P. Raghavan, *Introduction to Information Retrieval*. Cambridge University Press, 2008.

[61] K. Järvelin and J. Kekäläinen, “IR evaluation methods for retrieving highly relevant documents,” in *ACM SIGIR Forum*, vol. 51, no. 2, ACM New York, NY, USA, 2017, pp. 243–250.

[62] J. Pennington, R. Socher, and C. D. Manning, “GloVe: Global vectors for word representation,” in *Proceedings of the Empirical Methods in Natural Language Processing (EMNLP)*, 2014, pp. 1532–1543.

[63] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal representations by error propagation,” *California Univ San Diego La Jolla Inst for Cognitive Science, Tech. Rep.*, 1985.

[64] Y. Kim, “Convolutional neural networks for sentence classification,” in *Proceedings of the Empirical Methods in Natural Language Processing (EMNLP)*, 2014, pp. 1746–1751.

[65] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural networks for text classification,” in *Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI)*, 2015.

[66] Z. Lin, M. Feng, C. N. de Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio, “A structured self-attentive sentence embedding,” *arXiv preprint arXiv:1703.03130*, 2017.

### 附录

我们在表 IX 中展示了我们的方法在委婉语检测中的结果，并在表 X 中分析了药物数据集上的假阳性检测结果。我们将假阳性检测结果分为以下四类：

1. 它们是正确的委婉语，但在真实标签列表中缺失（表 X 中的案例 1-5）。
2. 它们本身不是委婉语，但它们包含在委婉语短语中。例如，表 X 中的案例 6，“oil” 本身不是药物委婉语，但“cbd oil” 是一个委婉语。
3. 尽管它们不是委婉语，但它们与药物或药物使用密切相关（表 X 中的案例 7-10）。案例 7 和 8 揭示了一些人们服用药物的方式（如与酒精或香烟一起使用）。
4. 错误检测。

案例研究表明，我们甚至可以找到一些不在真实标签列表中的正确委婉语，这表明委婉语的快速演变性质以及自动委婉语检测任务的必要性。

#### 表 IX
由我们的方法检测到的委婉语结果（彩色显示更佳）。紫色加粗的词是正确检测到的委婉语，并且在真实标签列表（即 DEA 列表）中。紫色下划线的词表示它们本身不正确，但包含在真实的委婉语短语中，例如“dog food”、“Chinese Tobacco”（分别是“heroin”和“opium”的委婉语）。那些未出现在真实标签列表中的词用黑色标记。

| 数据集 | 关键词 |
| --- | --- |
| 药物 | cannabis, weed, coke, alcohol, crack, speed, acid, pot, mushrooms, md, pills, hash, h, powder, tobacco, crystal, something, cigarettes, pressed, l, k, x, met, recovering, lean, spice, bud, narcotics, product, oil, grade, e, shatter, blow, anything, prescription, pill, research, heroine, shit, gold, use, psychedelic, hydro, white, medical, water, stuff, card, wax, substances, benz, products, fatal, fucking, addiction, sh, orange, new, coffee, sample, bars, others, sex, rc, smoking, lucy, blue, daily, money, pain, education, substance, coca, care, magnesium, tar, guns, everything, quality, treatment, peruvian, 2, legal, pure, mx, ir, synthetic, herb, amp, green, 4, medicine, chemicals, red, sleeping, possession, extract, depression, lithium |
| 性别 | breast, genitals, sex, pornography, nipple, dick, head, brain, cock, face, hair, body, balls, ass, man, heart, heads, hands, white, family, hand, mouth, woman, children, life, child, name, baby, finger, wife, gun, neck, mind, nose, skin, shit, teeth, blood, money, sex, fingers, blow, bodies, leg, one, private, legs, black, back, race, knife, soul, yes, brains, people, lives, son, daughter, throat, foot, red, feet, breasts, house, personality, tongue, country, bang, women, core, mother, job, point, suck, cannon, bullet, finger, burner, phone, gat, ball, one, hand, weapons, shot, guns, trigger, needle, car, heater, fuck, bucket, fence, handgun, bat, chance, trap, pipe, sword, bag, door, hammer, camera, bar, blade, piece, cigarette, barrel, hit, point, dog, flashlight, horse, house, revolver, fan, bomb, balloon, pill, brick, stick, lighter, key, fire, joint, pack, grenade, line, flag, blunt, box |
| 武器 | carbine, gatling, gun, rifle, pistol, ... |

#### 表 X
药物数据集上假阳性检测结果的案例研究。这些是来自 Reddit 的实际例子。

| ID | 候选委婉语 | 相关句子 |
| --- | --- | --- |
| 1 | cannabis | 可以轻松地在一个晚上吸掉15克，虽然我会有点晕，但MD的效果不再那么好了。<br>晶体确实有MD的味道，但被大麻的味道稍微盖过了。<br>我混合了7克红色MD与3茶匙柠檬汁和一点水并冷冻了一晚。<br>昨晚我第一次服用了MD，发现音乐听起来更好听了。<br>他的主要产品包括安非他明、摇头丸、蘑菇和冰毒。<br>供应商评论：TripWithScience vial液体蘑菇~9mg的裸盖菇素。<br>LSD、MDMA、蘑菇，尤其是氯胺酮和可卡因。<br>我在考虑是订购100片酸片还是50片和半盎司蘑菇。 |
| 2 | md | 我已经完全戒除了除LSD以外的所有毒品超过18个月，我是一个正在康复的瘾君子。<br>我主要是看他们的压制药片，你可以从他们那里买50片，价格不到250美元。<br>国内压制MDMA供应商。<br>5 x 3mg xanax GG249，价格1499美元，包括运费，收到的产品是5 x GG249 3mg压制复制品Xanax条，全部完好无损。 |
| 3 | cigarettes | 我不是一个经常吸烟的人，但最近几次服用蘑菇时，一根香烟会让旅行体验更好。<br>我听说有人将香烟浸入PCP中来吸食，MXE是否也可以这样做？<br>香烟感觉很棒，我感到很多爱，这是我吃过的最好的XTC药片之一。 |
| 4 | alcohol | 有没有人成功制作过酒精阿普唑仑溶液？<br>这是一种非常有趣的化学物质和体验，与酒精或大麻完全不同，这两种是我迄今为止使用的唯二毒品。<br>我把速溶咖啡放在酒精里一夜。<br>我可以在暗网上买到酒精吗？<br>酒精和Xanax没有效果。 |
| 5 | oil | 这种大麻CBD油让我和我的朋友们在大约半小时内变得非常高。<br>我侄子在他所在的药房买了一些CBD油，并给了我他认为的一大剂量。<br>简而言之，我在一家头饰店尝试了CBD油，变得非常兴奋并昏倒了。<br>我有一些非常好的70 kava蜂蜜油提取物。 |
| 6 | met | 所以我有500毫克的4-ACODMT和250毫克的met在我的购物车里。<br>25i-NBOMe + HO-Met = 很多幻觉，相当干净。<br>我更喜欢4-HO-MET而不是4-ACO-DMT，因为ACO让我在鼻吸时感到焦虑。<br>价格合理，支付了大约APS26，而我知道的唯一卖L的人每100ug收费APS10，真是恶心。<br>晚上抽烟真的很棒，我卷了一根2克的L，然后就睡着了。<br>点燃了一根蓝莓库什L，抽得非常高，看了三次电视重审了这篇评论。 |

通过这些案例研究，我们可以看到有些委婉语虽然不在真实标签列表中，但确实是正确的委婉语，这表明委婉语的快速演变性质以及自动委婉语检测任务的必要性。