# SMARTS and Other Sampling Methods

The following percentages represent the relative impact of various sampling methods on the FIT (Failure in Time) estimation. These values indicate that the listed methods are unlikely to cause significant changes in FIT estimation.

- 3.84%
- 3.03%
- 7.77%
- 2.76%
- 61.44%
- 32.24%
- 68.17%
- 286.06%

The corresponding standard deviations for these methods are:

- 1.77
- 4.85
- 2.23
- 7.02
- 1.41
- 3.37
- 2.77
- 1.15

And the coefficients of variation (CV) are:

- 0.068
- 0.147
- 0.173
- 0.194
- 0.866
- 1.087
- 1.888
- 3.290

All results presented in this paper were obtained with a seed value of 1.

## CV Estimation

We estimated the CV using a range of sampling rates from 1% to 20%. The variations in the estimated CVs differ by an average of 3%. This suggests that profiling can be effectively conducted using sampling experiments, even with a sampling rate lower than 10%, without introducing significant bias in the CV estimation. The CV estimation error can be easily compensated by increasing the estimated CV by 3%, as a higher CV guarantees a higher sampling rate, which in turn reduces sampling bias.

## Effect of Cache Size Variations

Table III presents the average FIT errors and slowdowns for various sampling schemes with UL2 cache sizes of 256KB, 1MB, and 4MB. LIM-2X for a 4MB cache has very small FIT errors because the data structure has an 8MB memory footprint. However, due to the larger memory footprint, the slowdown increases to 64.98. FIT errors increase in both SimPoint and SMARTS-INT when the size of the L2 cache is increased. For most benchmarks, the distribution of VCs (Virtual Channels) changes significantly with different cache sizes, but the CPI (Cycles Per Instruction) variance and the basic block execution sequence do not vary much. By accurately capturing the changes in VC distribution, PHYS estimates the FIT well across all cache sizes.

## Summary of Results

Table IV summarizes and compares the results obtained from various sampling schemes explored in this paper. The FIT error column provides the FIT estimation error for SDCs (Single Data Corruption) in an unprotected cache due to sampling. In the last column, we show the rank order of all schemes based on the product of FIT error and slowdown factor, sorted in ascending order. Hybrid, set, and interval samplings with 10% sampling rates achieve a good balance between FIT error and slowdown, and all three schemes perform equally well according to this metric. LIM-0.2X and SMARTS have relatively high FIT errors. SimPoint with one 50M detailed instruction window has a lower FIT error than SMARTS but still significantly higher than any of our proposed sampling approaches. In summary, PHYS shows the best result, with an average FIT error under 4% and a slowdown around 2 in all cases. Additionally, PHYS is the only scheme that provides statistical confidence on the FIT estimate.

## Simulations of Full SPEC2K Suite

Table V presents the profiling and simulation results for the entire SPEC 2K suite with reference input sets. We consider not only AVF (Architectural Vulnerability Factor) but also PARMA and MACAU reliability simulations. For the AVF model, we estimate the SDC FIT rate due to SBUs (Single Bit Upsets) in an unprotected cache. For the PARMA model, we estimate the SDC FIT rate due to temporal MBUs (Multi-Bit Upsets) caused by SBUs in a SECDED (Single Error Correction, Double Error Detection) protected cache. For the MACAU model, we estimate the SDC FIT rate due to spatial MBUs with up to 3BUs in a TECQED (Triple Error Correction/Quadruple Error Detection) protected cache. Profiling results reveal that a sampling rate of less than 0.1% is sufficient on average for AVF. PARMA and MACAU require higher sampling rates overall due to the need to address the temporal overlapping of SEUs (Single Event Upsets). Using PHYS, we were able to complete the reliability simulations of the entire SPEC2K benchmark suite within a month. Without sampling, it was impossible to finish AVF simulations (even in parallel) of the entire SPEC2K benchmark suite after 40 days; PARMA or MACAU run much slower.

As an example, the benchmark "art" takes 2.02 days to finish for base sim-outorder and 2.16 days for AVF with PHYS. However, without sampling, "art" does not finish AVF simulations even after one month. Detailed results are not included due to space limitations, but the results in [21] highlight the importance of simulating entire benchmarks with reference inputs. Using reduced data sets is not a good approach to solve the reliability simulation slowdown problem. For seven benchmarks, the discrepancy is between 40% and 234%, and for six benchmarks, it is between 10% and 40%. A major advantage of PHYS is that the results are statistically guaranteed to be within 10% of the error margin. Inaccurate FIT rates obtained from SimPoint may lead to incorrect design decisions regarding how to protect the L2 cache. Specifically, ten benchmarks had more than a 40% FIT rate difference, and seven benchmarks had more than a 15% FIT rate difference between the results obtained with PHYS and those obtained with 100M SimPoints for SDC FIT rates for AVF. The FIT estimation results with PHYS are statistically guaranteed to be within 10% of the error margin, demonstrating the need for reliability-aware sampling. The turnaround time of SimPoint simulations was much faster, achieving a slowdown of less than 1 for many workloads. However, the advantage of a faster simulation turnaround time does not compensate for the large reliability estimation error, as shown in [21].

## Conclusion

Model-based reliability benchmarking in architecture studies begins with reliability-lifetime analysis. The large memory overhead for reliability-lifetime analysis, combined with random accesses to the data structures, significantly slows down reliability simulations compared to traditional cycle-accurate performance simulations. We emphasized that slowdowns cannot be avoided by optimizing the size of data structures that track reliability-lifetime, as the problem of random accesses to these structures persists.

We showed that conventional sampling methods for performance studies fail in reliability studies. To address this, we proposed a new reliability-aware sampling approach called PHYS, which has low FIT error rates while significantly reducing simulation turnaround time. The usefulness of PHYS was demonstrated by showing that the average slowdown of reliability simulations is negligible for the entire SPEC2K suite with reference inputs. Our results point to a novel approach to enable reliability benchmarking of target systems for full benchmark suites.

## Acknowledgments

This work was supported by NSF grants NSF-1219186, NSF-CAREER-0954211, NSF-0834798, and NSF-0834799.

## References

[1] G. van Belle and D. C. Martin. Sample Size as a Function of Coefficient of Variation and Ratio of Means. The American Statistician, Vol. 47, No. 3. 165-167, 1993.

[2] D. R. Bellhouse. The Central Limit Theorem Under Simple Random Sampling. The American Statistician Vol. 55, No. 4, 352-357, 2001.

[3] A. Biswas, P. Racunas, R. Cheveresan, J. Emer, S. Mukherjee, and R. Rangan. Calculating Architectural Vulnerability Factors for Address-Based Structures. In Proceedings of the International Symposium on Computer Architecture, 532-543, 2005.

[4] D. Burger and T. M. Austin. The SimpleScalar Tool Set Version 2.0. Technical Report 1342, Computer Sciences Department, University of Wisconsin—Madison, 1997.

[5] L. Eeckhout, R. H. Bell Jr., B. Stougie, K. De Bosschere, and L. K. John. Control Flow Modeling in Statistical Simulation for Accurate and Efficient Processor Design Studies. In Proceedings of the International Symposium on Computer Architecture, 350-360, 2004.

[6] DARPA/IPTO Study. ExaScale Computing Study: Technology Systems Challenges Achieving Exascale in http://users.ece.gatech.edu/mrichard/ExascaleComputingStudyReports/exascale_final_report_100208.pdf, 2009.

[7] X. Fu, T. Li and J. Fortes. Sim-SODA: A Unified Framework for Architectural Level Software Reliability Analysis. In Workshop on Modeling, Benchmarking and Simulation, 2006.

[8] Intel Xeon Processor E7 family datasheet: http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/xeon-e7-8800-4800-2800-families-vol-2-datasheet.pdf.

[9] Intel Vtune Amplifier XE 2011: http://software.intel.com/en-us/articles/intel-vtune-amplifier-xe/.

[10] S. M. Khan, D. A. Jiménez, B. Falsafi, and Doug Burger. Using dead blocks as a virtual victim cache. In Proceedings of the International Conference on Parallel Architectures and Compilation Technologies, 2010.

[11] K. Kelley. Sample size planning for the coefficient of variation from the accuracy in parameter estimation approach. Behavior Research Methods, (39), 755-766, 2007.

[12] A. J. KleinOsowski, and D. J. Lilja. MinneSPEC: A New SPEC Benchmark Workload for Simulation-Based Computer Architecture Research. IEEE Computer Architecture. Letters. (1)1, 2002.

[13] X. Li, S. Adve, P. Bose, and J.A. Rivers. SoftArch: An Architecture Level Tool for Modeling and Analyzing Soft Errors. In Proceedings of the International Conference on Dependable Systems and Networks, 2005.

[14] L. Liu, J. K. Peir. Cache sampling by sets, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, (1)2, 98-105, 1993.

[15] S. S. Mukherjee. Architecture Design for Soft Errors, 1st Edition, Morgan Kauffman.

[16] S. S. Mukherjee, C. Weaver, J. Emer, S.K. Reinhardt, and T. Austin. A systematic methodology to compute the architectural vulnerability factors for a high-performance microprocessor. In Proceedings of the International Symposium on Microarchitecture, 29-40, 2003.

[17] A. A. Nair, S. Eyerman, L. Eeckhout, and L. K. John. A First-Order Mechanistic Model for Architectural Vulnerability Factor. In Proceedings of the International Symposium on Computer Architecture, 273-284, 2012.

[18] Semiconductor Industries Association. International Technology Roadmap for Semiconductors. 2007.

[19] S. K. Thompson. Sampling, 2nd edition. A Wiley-Interscience Publication, p13.

[20] T. Sherwood, E. Perelman, G. Hamerly and B. Calder. Automatically Characterizing Large Scale Program Behavior, In Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, 45-57, 2002.

[21] J. Suh “Models for Soft Errors in Low-level Caches”, Ph.D. dissertation, University of Southern California, 2012.

[22] J. Suh, M. Manoochehri, M. Annavaram, M. Dubois. “Soft error benchmarking of L2 caches with PARMA,” In Proceedings of the ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems, 2011.

[23] J. Suh, M. Annavaram, M. Dubois: “MACAU: A Markov model for reliability evaluations of caches under Single-bit and Multi-bit Upsets,” In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, 1-12, 2012.

[24] K. R. Walcott, G. Humphreys, and S. Gurumurthi. Dynamic prediction of architectural vulnerability from microarchitectural state. In Proceedings of the International Symposium on Computer Architecture, 2007, 516-527, 2007.

[25] B. P. Welford. Note on a Method for Calculating Corrected Sums of Squares and Products. Technometrics, (4)3 419-420, 1962.

[26] R. E. Wunderlich, T. F. Wenisch, B. Falsafi, and J. C. Hoe. SMARTS: accelerating microarchitecture simulation via rigorous statistical sampling. In Proceedings of the International Symposium on Computer Architecture, 84-97, 2003.