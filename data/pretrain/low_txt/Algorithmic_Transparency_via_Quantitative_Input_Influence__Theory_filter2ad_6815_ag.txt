### (b) Transparency Report for Mr. Y’s Negative Classification

**Figure 5: Mr. Y.**

In this section, we compare Mr. Y with another individual who is similar but received a different classification outcome. The goal is to identify the specific inputs that the classifier used to produce these differing outcomes. When group parity is used as a fairness criterion, QII can help identify the features that lead to group disparities, thereby revealing which features the classifier uses as proxies for sensitive attributes.

The determination of whether using certain proxies for sensitive attributes is discriminatory often depends on task-specific normative judgments. For example, using standardized test scores (e.g., SAT scores) for admissions decisions is generally accepted, although SAT scores may be a proxy for several protected attributes. In fact, several universities have recently announced that they will no longer use SAT scores for admissions, citing this reason [33], [34]. Our objective is not to provide such normative judgments. Instead, we aim to provide detailed transparency into input usage (e.g., the extent to which SAT scores influence decisions), which is useful for making determinations of discrimination from a specific normative perspective.

An interesting question is whether providing a sensitive attribute as an input to a classifier is fundamentally discriminatory behavior, even if QII shows that the sensitive input has no significant impact on the outcome. Our view is that this is a policy question, and different legal frameworks might take different viewpoints on it. From a technical standpoint, the two situations are identical: the sensitive input is not really used, although it is supplied. However, the very fact that it was supplied might indicate an intent to discriminate, even if that intended goal was not achieved. Regardless of the policy decision, QII remains a useful diagnostic tool for identifying discrimination due to the presence of proxy variables, as described earlier.

### IX. Related Work

#### A. Quantitative Causal Measures

Causal models and probabilistic interventions have been used in various settings. While the form of interventions in some of these settings may be similar, our generalization to account for different quantities of interest enables us to reason about a wide range of transparency queries for data analytics systems, from individual classification outcomes to group disparities. Additionally, the notion of marginal contribution, which we use to compute responsibility, does not appear in this line of prior work.

Janzing et al. [35] use interventions to assess the causal importance of relations between variables in causal graphs. To assess the causal effect of a relation between two variables \(X \rightarrow Y\), a new causal model is constructed where the value of \(X\) is replaced with a prior over possible values of \(X\). The influence of the causal relation is defined as the KL-Divergence of the joint distribution of all variables in the two causal models with and without the value of \(X\) replaced. This approach of intervening with a random value from the prior is similar to our method of constructing \(X-S\).

Independently, there has been considerable work in the machine learning community to define importance metrics for variables, mainly for feature selection (see [36] for a comprehensive overview). One important metric is Permutation Importance [37], which measures the importance of a feature by randomly permuting its values and then computing the difference in classification accuracies before and after the permutation. Replacing a feature with a random permutation can be viewed as sampling the feature independently from the prior.

There is extensive literature on establishing causal relations, as opposed to quantifying them. Prominently, Pearl’s work [38] provides a mathematical foundation for causal reasoning and inference. Tian and Pearl [39] discuss measures of causal strength for individual binary inputs and outputs in a probabilistic setting. Another thread of work by Halpern and Pearl discusses actual causation [40], which is extended in [41] to derive a measure of responsibility as the degree of causality. Chockler and Halpern [41] define the responsibility of a variable \(X\) to an outcome as the amount of change required to make \(X\) the counterfactual cause. As discussed in Appendix A-B, the Deegan-Packel index is strongly related to causal responsibility.

#### B. Quantitative Information Flow

Our results can be seen as a causal alternative to quantitative information flow. Quantitative information flow is a broad class of metrics that quantify the information leaked by a process by comparing the information contained before and after observing the outcome of the process. It traces its information-theoretic roots to the work of Shannon [42] and Rényi [43]. Recent works have proposed measures for quantifying the security of information by measuring the amount of information leaked from inputs to outputs by certain variables; see [44] for an overview and [45] for an exposition on information theory.

Quantitative Information Flow is concerned with information leaks and therefore needs to account for correlations between inputs that may lead to leakage. The dual problem of transparency, on the other hand, requires us to destroy correlations while analyzing the outcomes of a system to identify the causal paths for information leakage.

#### C. Interpretable Machine Learning

An orthogonal approach to adding interpretability to machine learning is to constrain the choice of models to those that are interpretable by design. This can be achieved through regularization techniques such as Lasso [46], which attempt to pick a small subset of the most important features, or by using models that structurally match human reasoning, such as Bayesian Rule Lists [47], Supersparse Linear Integer Models [48], or Probabilistic Scaling [49]. Since the choice of models in this approach is restricted, a loss in predictive accuracy is a concern, and the central focus in this line of work is minimizing the loss in accuracy while maintaining interpretability. In contrast, our approach to interpretability is forensic. We add interpretability to machine learning models after they have been learned, without constraining the choice of models that can be used.

#### D. Experimentation on Web Services

There is an emerging body of work on systematic experimentation to enhance transparency into web services such as targeted advertising [50-54]. The setting in this line of work is different because they have restricted access to the analytics systems through publicly available interfaces. As a result, they only have partial control of inputs, partial observability of outputs, and little or no knowledge of input distributions. The intended use of these experiments is to enable external oversight into web services without any cooperation. Our framework is more appropriate for a transparency mechanism where an entity proactively publishes transparency reports for individuals and groups. Our framework is also suitable for use as an internal or external oversight tool with access to mechanisms that have control and knowledge of input distributions, forming a basis for testing.

#### E. Game-Theoretic Influence Measures

Recent years have seen game-theoretic influence measures used in various settings. Datta et al. [55] define a measure for quantifying feature influence in classification tasks. Their measure does not account for the prior on the data, nor does it use interventions that break correlations between sets of features. In the terminology of this paper, the quantity of interest used by [55] is the ability to change the outcome by changing the state of a feature. This work extends and generalizes the concepts presented in [55] by accounting for interventions on sets and by generalizing the notion of influence to include a wide range of system behaviors, such as group disparity, group outcomes, and individual outcomes.

Game-theoretic measures have been used by various research disciplines to measure influence. Such measures are relevant whenever one is interested in measuring the marginal contribution of variables and when sets of variables can cause some measurable effect. Lindelauf et al. [56] and Michalak et al. [57] use game-theoretic influence measures on graph-based games to identify key members of terrorist networks. Del Pozo et al. [58] and Michalak et al. [59] use similar ideas for identifying important members of large social networks, providing scalable algorithms for influence computation. Bork et al. [60] use the Shapley value to assign importance to protein interactions in large, complex biological interaction networks, and Keinan et al. [61] employ the Shapley value to measure causal effects in neurophysical models. The novelty in our use of game-theoretic power indices lies in the conception of a cooperative game via a valuation function \(\iota(S)\), defined by a randomized intervention on inputs \(S\). Such an intervention breaks correlations and allows us to compute marginal causal influences on a wide range of system behaviors.

### X. Conclusion & Future Work

In this paper, we present QII, a general family of metrics for quantifying the influence of inputs in systems that process personal information. QII provides insights into the behavior of opaque machine learning algorithms by allowing us to answer a wide class of transparency queries, ranging from influence on individual causal outcomes to influence on disparate impact. To achieve this, QII breaks correlations between inputs to allow causal reasoning and computes the marginal influence of inputs in situations where inputs cannot affect outcomes alone. We also demonstrate that QII can be efficiently approximated and can be made differentially private with negligible noise addition in many cases.

An immediate next step in this line of work is to explore adoption strategies in areas that use personal information to aid decision-making. Areas such as healthcare [3], predictive policing [1], education [4], and defense [5] have a particularly acute need for transparency in their decision-making. Specific applications will guide us in choosing an appropriate QII metric, including a choice for our game-theoretic power index.

We have not considered situations where inputs do not have well-understood semantics. Such situations arise often in settings such as image or speech recognition and automated video surveillance. With the proliferation of immense processing power, complex machine learning models such as deep neural networks have become ubiquitous in these domains. Defining transparency and developing analysis techniques in such settings is important future work.

### References

[1] W. L. Perry, B. McInnis, C. C. Price, S. C. Smith, and J. S. Hollywood, Predictive Policing: The Role of Crime Forecasting in Law Enforcement Operations. RAND Corporation, 2013.
[2] T. Alloway, “Big data: Credit where credits due,” http://www.ft.com/cms/s/0/7933792e-a2e6-11e4-9c06-00144feab7de.html.
[3] T. B. Murdoch and A. S. Detsky, “The inevitable application of big data to health care,” http://jama.jamanetwork.com/article.aspx?articleid=1674245.
[4] “Big data in education,” https://www.edx.org/course/big-data-education-teacherscollegex-bde1x.
[5] “Big data in government, security, and homeland defense 2015-2020,” http://www.prnewswire.com/news-releases/big-data-in-government-defense-and-homeland-security-2015---2020.html.
[6] J. Podesta, P. Pritzker, E. Moniz, J. Holdern, and J. Zients, “Big data: Seizing opportunities, preserving values,” Executive Office of the President - the White House, Tech. Rep., May 2014.
[7] “E.G. Griggs v. Duke Power Co., 401 U.S. 424, 91 S. Ct. 849, 28 L. Ed. 2d 158 (1977).”
[8] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to sensitivity in private data analysis,” in Proceedings of the Third Conference on Theory of Cryptography, ser. TCC’06. Berlin, Heidelberg: Springer-Verlag, 2006, pp. 265–284. [Online]. Available: http://dx.doi.org/10.1007/11681878 14
[9] S. Kasiviswanathan, H. Lee, K. Nissim, S. Raskhodnikova, and A. Smith, “What can we learn privately?” in Proceedings of the 49th IEEE Symposium on Foundations of Computer Science (FOCS 2008), Oct 2008, pp. 531–540.
[10] M. Lichman, “UCI machine learning repository,” 2013. [Online]. Available: http://archive.ics.uci.edu/ml
[11] “National longitudinal surveys,” http://www.bls.gov/nls/.
[12] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness through awareness,” in Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (ITCS 2012), 2012, pp. 214–226.
[13] M. Lichman, “UCI machine learning repository,” 2013. [Online]. Available: http://archive.ics.uci.edu/ml
[14] L. Shapley, “A value for n-person games,” in Contributions to the Theory of Games, vol. 2, ser. Annals of Mathematics Studies, no. 28. Princeton University Press, 1953, pp. 307–317.
[15] M. Maschler, E. Solan, and S. Zamir, Game Theory. Cambridge University Press, 2013.
[16] L. S. Shapley and M. Shubik, “A method for evaluating the distribution of power in a committee system,” The American Political Science Review, vol. 48, no. 3, pp. 787–792, 1954.
[17] J. Banzhaf, “Weighted voting doesn’t work: a mathematical analysis,” Rutgers Law Review, vol. 19, pp. 317–343, 1965.
[18] J. Deegan and E. Packel, “A new index of power for simple n-person games,” International Journal of Game Theory, vol. 7, pp. 113–123, 1978.
[19] H. Young, “Monotonic solutions of cooperative games,” International Journal of Game Theory, vol. 14, no. 2, pp. 65–72, 1985.
[20] S. Kullback and R. A. Leibler, “On information and sufficiency,” Annals of Mathematical Statistics, vol. 22, no. 1, pp. 79–86, 1951.
[21] G. Chalkiadakis, E. Elkind, and M. Wooldridge, Computational Aspects of Cooperative Game Theory. Morgan and Claypool, 2011.
[22] Y. Bachrach, E. Markakis, E. Resnick, A. Procaccia, J. Rosenschein, and A. Saberi, “Approximating power indices: theoretical and empirical analysis,” Autonomous Agents and Multi-Agent Systems, vol. 20, no. 2, pp. 105–122, 2010.
[23] S. Maleki, L. Tran-Thanh, G. Hines, T. Rahwan, and A. Rogers, “Bounding the estimation error of sampling-based Shapley value approximation with/without stratifying,” CoRR, vol. abs/1306.4265, 2013.
[24] W. Hoeffding, “Probability inequalities for sums of bounded random variables,” Journal of the American Statistical Association, vol. 58, no. 301, pp. 13–30, March 1963. [Online]. Available: http://www.jstor.org/stable/2282952?
[25] N. Li, W. H. Qardaji, and D. Su, “Provably private data anonymization: Or, k-anonymity meets differential privacy,” CoRR, vol. abs/1101.2604, 2011. [Online]. Available: http://arxiv.org/abs/1101.2604
[26] Z. Jelveh and M. Luca, “Towards diagnosing accuracy loss in