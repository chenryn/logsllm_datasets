### Instruction Coverage

In this experiment, we measured the number of unique instruction addresses (i.e., EIP values) covered by MACE and the baseline approach in the program binary and its libraries. These numbers indicate the effectiveness of each approach in uncovering new code regions in the analyzed program. For Vino, RealVNC, and Samba, we used dynamic symbolic execution as the baseline approach, following the setup outlined in Section 6.1. We ran MACE for 2.5 hours of state-space exploration per inferred state. To ensure a fair comparison, we ran the baseline for an equivalent amount of time, which was the sum of MACE's inference and state-space exploration times.

As shown in Table 3, our results demonstrate that MACE significantly outperforms dynamic symbolic execution in terms of instruction coverage. It is important to note that our tool currently operates on user-space programs only. Since Windows SMB is largely implemented within the Windows kernel, the baseline approach performed poorly. To avoid an unfair comparison, we instead compared MACE against Samba's `gentest` test suite, which is regularly used by Samba developers to test the SMB protocol. Using the test suite, we generated test sequences and measured the resulting coverage. As with other experiments, we allocated the same amount of time to both the test suite and MACE. The experimental results clearly show MACE's ability to augment test suites manually written by developers.

### Number of Detected Crashes

Using the same setup as in the previous experiment, we measured the number of crashing input sequences generated by each approach. We reported both the total number of crashes and the number of unique crash locations. From each category of unique crash locations, we manually processed the first four reported crashes. All the vulnerabilities found (Table 2) were identified by processing the very first crash in each category. Subsequent crashes we processed were merely variants of the first reported crash. MACE found 30 crashing input sequences, with 9 of them having unique crash locations (the EIP of the crashed instruction). In comparison, the baseline approach only found 20 crashing input sequences, all of which had the same crash location.

### Exploration Depth

To measure the effectiveness of each approach in reaching deep states, we used the same setup as in the coverage experiment. The inferred state machine can be visualized as a directed graph. By computing a spanning tree (e.g., [13]) of this graph, we can analyze the depth of state exploration. The root of the graph is at level zero, its children are at level one, and so on. We measured the percentage of states reached at every level. Figure 5 clearly shows that MACE is superior to the baseline approach in reaching deep states in the inferred protocol.

### Limitations

Completeness is a challenge for any dynamic analysis technique, and MACE is no exception. MACE cannot guarantee the discovery of all protocol states due to the following reasons:
1. Each state-space explorer instance runs for a bounded amount of time, and some inputs may not be discovered before the timeout.
2. Among multiple shortest transfer sequences to the same abstract state, MACE selects one, potentially missing further exploration of alternative paths.
3. Similarly, among multiple concrete input messages with the same abstract behavior, MACE picks one and considers the rest redundant (Definition 2).

Our approach to model inference and refinement is not entirely automatic: users need to provide an abstraction function that maps concrete output messages into an abstract alphabet. Developing a good output abstraction function can be challenging. If the abstraction is too fine-grained, model inference may become computationally expensive or may not converge. Conversely, if the abstraction is too coarse-grained, the inferred model may fail to distinguish between two interesting states. Nevertheless, our approach represents a significant improvement over our prior work [11], which required abstraction functions for both input and output messages.

When using MACE to learn a model of a proprietary protocol, some level of protocol reverse-engineering is necessary. First, a basic understanding of the protocol interface is needed to correctly replay input messages to the analyzed program. This may involve overwriting fields such as cookies or session IDs in input messages to make them indistinguishable from real inputs. Second, an appropriate output abstraction is required, which necessitates an understanding of the output message formats. Message format reverse-engineering is an active area of research [14, 15, 6] but is beyond the scope of this paper.

Encryption poses a significant challenge for protocol inference techniques. To address this, we configure the analyzed programs to disable encryption. However, for proprietary protocols, such a configuration may not be available, and techniques [5, 29] that automatically reverse-engineer message encryption are required.

### Acknowledgements

We would like to thank the anonymous reviewers for their insightful comments, which helped improve this manuscript. This material is based upon work partially supported by the NSF under Grants No. 0311808, 0832943, 0448452, 0842694, 0627511, 0842695, 0831501, and 0424422, by the AFRL under Grant No. P010071555, by the ONR under MURI Grant No. N000140911081, and by the MURI program under AFOSR Grants No. FA9550-08-1-0352 and FA9550-09-1-0539. The second author is also supported by the NSERC (Canada) PDF fellowship.

### Conclusions and Future Work

We have proposed MACE, a new approach to software state-space exploration. MACE iteratively infers and refines an abstract model of the protocol implemented by the program and uses this model to more effectively explore the program's state space. By applying MACE to four server applications, we have shown that MACE:
1. Improves coverage by up to 58.86%.
2. Discovers significantly more vulnerabilities (seven vs. one).
3. Performs significantly deeper searches than the baseline approach.

Further research is needed in several directions:
1. A deeper analysis of the correspondence between the inferred finite state models and the structure and state space of the analyzed application could reveal how models can be used even more effectively.
2. It remains an open question whether effective automatic abstractions of concrete input messages can be designed. The filtering function we propose in this paper is effective but might drop important messages.
3. Finite-state models may not be expressive enough for all types of applications. Subsequential transducers [28] might be a more expressive representation that allows us to model protocols more precisely without significantly increasing the inference cost.
4. MACE currently does not perform white-box analysis, except for dynamic symbolic execution to discover new concrete input messages. MACE could monitor the value of program variables, consider them as inputs and outputs, and automatically learn a high-level model of the program's state space. This extension would allow MACE to be applied to a broader class of programs.

### References

[1] ANGLUIN, D. Learning regular sets from queries and counterexamples. Information and Computation 75, 2 (1987), 87–106.

[2] BALL, T., MAJUMDAR, R., MILLSTEIN, T., AND RAJAMANI, S. Automatic Predicate Abstraction of C Programs. In PLDI’01: Proc. of the ACM SIGPLAN 2001 Conf. on Programming Language Design and Implementation (2001), vol. 36 of ACM SIGPLAN Notices, ACM Press, pp. 203–213.

[3] BARNETT, M., DELINE, R., FÄHNDRICH, M., JACOBS, B., LEINO, K. R., SCHULTE, W., AND VENTER, H. Verified software: Theories, tools, experiments. Springer-Verlag, 2008, ch. The Spec# Programming System: Challenges and Directions, pp. 144–152.

[4] BENZEL, T., BRADEN, R., KIM, D., NEUMAN, C., JOSEPH, A., SKLOWER, K., OSTRENGA, R., AND SCHWAB, S. Design, deployment, and use of the DETER testbed. In Proc. of the DETER Community Workshop on Cyber Security Experimentation and Test (2007), USENIX Association.

[5] CABALLERO, J., POOSANKAM, P., KREIBICH, C., AND SONG, D. Dispatcher: Enabling active botnet infiltration using automatic protocol reverse-engineering. In CCS’09: Proc. of the 16th ACM conference on Computer and communications security (2009), ACM, pp. 621–634.

[6] CABALLERO, J., YIN, H., LIANG, Z., AND SONG, D. Polyglot: Automatic extraction of protocol message format using dynamic binary analysis. In CCS’07: Proc. of the 14th ACM Conf. on Computer and Communications Security (2007), ACM, pp. 317–329.

[7] CADAR, C., DUNBAR, D., AND ENGLER, D. KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs. In OSDI’08: Proc. of the 8th USENIX Symposium on Operating Systems Design and Implementation (2008), USENIX Association, pp. 209–224.

[8] CADAR, C., AND ENGLER, D. R. Execution generated test cases: How to make systems code crash itself. In SPIN’05: Proc. of the 12th Int. SPIN Workshop on Model Checking Software (2005), vol. 3639 of Lecture Notes in Computer Science, Springer, pp. 2–23.

[9] CADAR, C., GANESH, V., PAWLOWSKI, P. M., DILL, D. L., AND ENGLER, D. R. Exe: Automatically generating inputs of death. ACM Trans. Inf. Syst. Secur. 12 (2008), 1–38.

[10] CHO, C. Y., BABIĆ, D., SHIN, R., AND SONG, D. Inference and analysis of formal models of botnet command and control protocols. In CCS’10: Proc. of the 2010 ACM Conf. on Computer and Communications Security (2010), ACM, pp. 426–440.

[11] CHO, C. Y., CABALLERO, J., GRIER, C., PAXSON, V., AND SONG, D. Insights from the inside: A view of botnet management from infiltration. In LEET’10: Proc. of the 3rd USENIX Workshop on Large-Scale Exploits and Emergent Threats (2010), USENIX Association, pp. 1–1.

[12] COMPARETTI, P. M., WONDRACEK, G., KRUEGEL, C., AND KIRDA, E. Prospex: Protocol specification extraction. In S&P’09: Proc. of the 2009 30th IEEE Symposium on Security and Privacy (2009), IEEE Computer Society, pp. 110–125.

[13] CORMEN, T. H., LEISERSON, C. E., RIVEST, R. L., AND STEIN, C. Introduction to Algorithms, 2nd ed. The MIT Press, 2001.

[14] CUI, W., KANNAN, J., AND WANG, H. J. Discoverer: Automatic protocol reverse engineering from network traces. In Proc. of 16th USENIX Security Symposium (2007), USENIX Association, pp. 1–14.

[15] CUI, W., PEINADO, M., CHEN, K., WANG, H. J., AND IRÚN-BRIZ, L. Tupni: Automatic reverse engineering of input formats. In CCS’08: Proc. of the 15th ACM Conf. on Computer and Communications Security (2008), ACM, pp. 391–402.

[16] DE LA HIGUERA, C. Grammatical Inference: Learning Automata and Grammars. Cambridge University Press, 2010.

[17] GODEFROID, P., KLARLUND, N., AND SEN, K. DART: Directed automated random testing. In PLDI’05: Proc. of the ACM SIGPLAN Conf. on Programming Language Design and Implementation (2005), ACM, pp. 213–223.

[18] GODEFROID, P., LEVIN, M. Y., AND MOLNAR, D. A. Automated whitebox fuzz testing. In NDSS’08: Proc. of the Network and Distributed System Security Symposium (2008), The Internet Society.

[19] GULAVANI, B. S., HENZINGER, T. A., KANNAN, Y., NORI, A. V., AND RAJAMANI, S. K. SYNERGY: A new algorithm for property checking. In FSE’06: Proc. of the 14th ACM SIGSOFT Int. Symp. on Foundations of Software Engineering (2006), ACM, pp. 117–127.

[20] HENZINGER, T. A., JHALA, R., MAJUMDAR, R., AND SUTRE, G. Software Verification with Blast. In SPIN’03: Proc. of the 10th Int. Workshop on Model Checking of Software (2003), vol. 2648 of LNCS, Springer-Verlag, pp. 235–239.

[21] HO, P. H., SHIPLE, T., HARER, K., KUKULA, J., DAMIANO, R., BERTACCO, V., TAYLOR, J., AND LONG, J. Smart simulation using collaborative formal and simulation engines. In ICCAD’00: Proc. of the 2000 IEEE/ACM Int. Conf. on Computer-aided design (2000), IEEE Press, pp. 120–126.

[22] KING, J. C. Symbolic execution and program testing. Communications of the ACM 19, 7 (1976), 385–394.

[23] MEALY, G. H. A method for synthesizing sequential circuits. Bell System Technical Journal 34, 5 (1955), 1045–1079.

[24] PELED, D., VARDI, M. Y., AND YANNAKAKIS, M. Black box checking. In Proc. of the IFIP TC6 WG6.1 Joint Int. Conf. on Formal Description Techniques for Distributed Systems and Communication Protocols (FORTE XII) and Protocol Specification, Testing and Verification (PSTV XIX) (1999), Kluwer, B.V., pp. 225–240.

[25] SEN, K., MARINOV, D., AND AGHA, G. Cute: A concolic unit testing engine for C. SIGSOFT Softw. Eng. Notes 30 (2005), 263–272.

[26] SHAHBAZ, M., AND GROZ, R. Inferring Mealy machines. In FM’09: Proc. of the 2nd World Congress on Formal Methods (2009), Springer, pp. 207–222.

[27] VEANES, M., CAMPBELL, C., GRIESKAMP, W., SCHULTE, W., TILLMANN, N., AND NACHMANSON, L. Formal methods and testing. Springer-Verlag, 2008, ch. Model-based testing of object-oriented reactive systems with Spec Explorer, pp. 39–76.

[28] VILAR, J. M. Query learning of subsequential transducers. In Proc. of the 3rd Int. Colloquium on Grammatical Inference: Learning Syntax from Sentences (1996), Springer-Verlag, pp. 72–83.

[29] WANG, Z., JIANG, X., CUI, W., WANG, X., AND GRACE, M. ReFormat: Automatic reverse engineering of encrypted messages. In ESORICS’09: 14th European Symposium on Research in Computer Security (2009), vol. 5789 of Lecture Notes in Computer Science, Springer, pp. 200–215.

[30] ZHANG, L., MADIGAN, C. F., MOSKEWICZ, M. H., AND MALIK, S. Efficient conflict driven learning in a Boolean satisfiability solver. In ICCAD’01: Proc. of the Int. Conf. on Computer-Aided Design (2001), IEEE Press, pp. 279–285.