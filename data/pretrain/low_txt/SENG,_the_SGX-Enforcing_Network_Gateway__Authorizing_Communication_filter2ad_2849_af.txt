### 1.9.1 Implementation Details

Our implementation is based on OpenSSL 1.0.2g and the challenger code of `sgx-ra-tls`. The core functionality consists of approximately 1,300 lines of code, with an additional 1,500 lines for supporting SENG server sockets. The server uses a TUN device as an IP-level virtual network interface to the gateway. The SENG server configures the TUN device as the default gateway for connected SENG runtime clients and links each DTLS tunnel to the client’s enclave IP address.

### 9. Evaluation

In this section, we evaluate our prototype implementation in terms of efficacy and overhead. We use iPerf3 [26] to measure network throughput and then demonstrate how these results translate to real-world client (cURL, Telnet) and server (NGINX) applications. We also provide microbenchmarks to measure the setup phase of the SENG runtime. Additionally, we revisit NGINX performance and significantly improve it by porting NGINX to the SENG-SDK. Finally, we discuss the scalability of the SENG server under an increasing number of enclaves and corresponding tunnels.

#### 9.1 Experimental Setup

For our experiments, the SENG server runs on a workstation equipped with an Intel® Core™ i5-4690 CPU (4 cores), 32 GB of RAM, and Debian 9 with a 4.9 Linux kernel. The SGX-enabled client system has an Intel® Core™ i7-6700 CPU (8 cores), 64 GB of RAM, and runs SGX enclaves inside a Ubuntu 16.04.4 LTS Docker container with a 4.15 Linux kernel. The underlying host runs Ubuntu 18.04.2 LTS. Both systems are connected to the local network via 1 Gbps NICs (Intel I217-LM/I219-LM). We route the client's traffic via the SENG server to ensure that all traffic to and from the SGX client system passes through our virtual network gateway.

We use the native execution of the applications ("native") as the baseline for our evaluation and compare it with the performance of Graphene-SGX ("pure") and SENG ("SENG"). This allows us to attribute any overhead to either Graphene-SGX or the additional latency and overhead introduced by the SENG runtime and SENG server components.

### 9.1 Network Performance

We first report on the maximum downlink throughput of a single TCP connection using iPerf3. iPerf3 sends TCP packets to another iPerf3 instance and measures the resulting throughput. We generate the traffic on the gateway and receive it inside the enclave on the client system. We keep the default configuration of iPerf3, which calculates the average over 10 seconds, and step-wise increase the bandwidth of the workload.

Figure 5 shows the average receive throughput over five iterations. The throughputs of all three approaches scale linearly with increased iPerf3 bandwidths, and SENG shows no overhead for bandwidths up to approximately 800 Mbps. The native and pure Graphene-SGX setups both reach a maximum throughput of 925.93 Mbps, whereas SENG’s peak average throughput is 867.66 Mbps (approximately 6% lower). Our 10-second measurements include TCP’s slow start, and we observed higher temporal throughputs of approximately 933 Mbps for native and pure, as well as approximately 899 Mbps for SENG, reducing the peak loss to 3–4%. The slightly lower peak throughput of SENG is caused by the additional latency added by the SENG-internal TCP/IP stack and the DTLS tunnel. We included the results of SENG with enclave exits on every syscall (approximately 390 Mbps) to highlight that exitless designs are key-enablers for I/O-intense enclaves [2, 42].

We conclude that the reduced throughput peak (3–7%) is acceptable, especially as clients and/or remote parties are typically bound to lower bandwidths, which showed no overhead.

### 9.2 Client Applications

#### cURL

cURL is a popular tool/library for transferring data via several common protocols. In our setting, an external partner could use cURL to exchange files with internal servers. We chose cURL to check if SENG readily supports and scales to real-world client applications. To this end, we set up an Apache web server and measured how long cURL takes to download files via HTTP. Apache runs on the local gateway to capture the overhead with minimal impact from network jitter, analogous to iPerf3. We used the built-in measurements of cURL and took the 30% trimmed mean over 50 iterations for each file size as a robust estimator [2].

Figure 6 shows the observed download time overhead relative to native execution. Graphene-SGX is again on par with the baseline as it shares the untrusted kernel network stack. For a file size of 1 MB, SENG shows minimal overhead due to the short download time. As the file size increases, SENG faces overhead of 8.8–14.1%, which is higher than the one reported for iPerf3 but still reasonable. We observed TCP segmentation for every cURL payload, which was not present during iPerf3 and adds reassembly load and delay on lwIP as it cannot use HW offloading and has a lightweight design.

We conclude that SENG also shows reasonable performance for real-world client applications. Note that exitless syscalls in Graphene-SGX are still experimental, and future versions might stabilize and further reduce the network overhead.

#### Telnet

Telnet (RFC 854) is widely used for remote terminal access and serves as our representative for remote login tools. SENG’s built-in DTLS tunnel protects plaintext Telnet against local system-level and on-path attackers within the organization network. Furthermore, SENG can restrict remote access to trusted, TLS-based login clients and shield them from local user- or system-level attackers (e.g., hooks).

We used a Telnet server on a local workstation and measured over 10 iterations the average time it takes for a Telnet client to log in, execute a set of Bash commands for entering a directory, list the contained files, and finally, display the content of a 1 kB document. Telnet takes 269.38 ms during native execution and faces 0.17% overhead for Graphene-SGX and 0.09% for SENG, which is practically negligible.

### 9.3 Server Application (NGINX)

Next, we evaluate a server setting where we aim to shield an internal server from internal MITM and system-level attackers. We chose NGINX as a demonstrator, which is a widely used event-based HTTP server. NGINX runs on the client host inside SGX and uses a single, poll-based worker thread to serve a 612-byte demo page via HTTP. We used the wrk2 benchmark tool from an internal workstation to issue HTTP requests under step-wise increasing request frequency. For each workload, wrk2 spawned two threads with 100 connections and calculated the mean reply latency over ten seconds.

Figure 7 shows the average latencies over five iterations. Graphene-SGX and SENG can handle approximately 15,000 requests per second with a per-reply latency of 1.5–2.5 ms before performance degrades. Native execution clearly outperforms "pure" and SENG with approximately 40,000. This is no surprise and follows the observations of Tsai et al. [9], because Graphene-SGX currently only supports synchronous syscalls, which cannot effectively overlap computation and I/O. We inspected the CPU utilization of NGINX under different loads and revealed that in the "pure" and "SENG" settings, the NGINX thread saturates the CPU via continuous polling and Graphene’s I/O overhead.

In conclusion, SENG cannot yet compete with native NGINX, but is on par with Graphene-SGX while providing more security guarantees and features on top of it. Furthermore, the bottleneck can be attributed to Graphene-SGX rather than to SENG, and we therefore expect better performance under future asynchronous or batched I/O support.

### 9.4 Setup Microbenchmark

We now measure the initialization overhead that the SENG runtime adds to Graphene-SGX, excluding the prototype-specific socket API handlers. As the setup time of Graphene-SGX depends on the enclave configuration, we measured the time for three configurations: (a) default values of LibOS-internal tests, (b) with reduced stack, heap, and thread number, and (c) with minimal accepted size. For SENG, we measured the different setup phases of the runtime.

Table 4 breaks down the average setup times over ten iterations. The total startup overhead of the SENG runtime is 1,578.03 ms, i.e., it adds about 182% overhead on top of the Graphene-SGX initialization under default configuration. However, the vast majority of this overhead stems from two steps: (i) the init routine of the OpenSSL library (710.98 ms) and (ii) the IAS communication (639.05 ms). The high OpenSSL startup time is partially attributable to the default seeding of the random number generator. It could be reduced by switching to the RDRAND engine to approach a setup time of 867.05 ms, which is comparable to the default LibOS time (a). As discussed in Section 6.1, the remote attestation could be handled by an internal AS server with caching support instead. Thus, the total startup time could be further reduced to ideally 228 ms, i.e., about 26% of the default LibOS time (a).

We conclude that SENG adds a reasonable startup overhead which could be optimized to become comparable to that under reduced (b) or minimal (c) SENG runtime configurations.

### 9.5 Accelerating NGINX Using SENG-SDK

We next revisit the NGINX results of Section 9.3 and show that SENG performs significantly better when replacing Graphene-SGX with a faster primitive. SENG performed on par with "pure" Graphene-SGX for NGINX with approximately 15,000 requests per second, but was clearly outperformed by the native baseline of approximately 40,000 (cf. Figure 7). To show that SENG can overcome the bottleneck caused by Graphene-SGX, we dropped the LibOS and instead ported NGINX to our SENG-SDK. We ported only NGINX’s platform-specific code to preserve comparability with previous results and added about 1,100 lines of code for enclave setup and missing syscalls.

Figure 7 shows that SENG-SDK ("SENG-sdk") reaches approximately 36,000 requests per second with a per-reply latency of 1.5–2.0 ms. SENG-SDK significantly outperforms the Graphene-based SENG runtime by a factor of 2.4 and reaches up to 90% of native performance. Compared to Graphene-SGX, SENG-SDK provides more efficient OCALL interfaces tailored for the DTLS tunnel I/O and benefits from the more lightweight abstractions of Intel’s SGX SDK. However, note that SENG-SDK loses legacy support and drop-in deployment (AR1).

We conclude that SENG can significantly benefit from performance improvements of the underlying primitives, allowing it to handle complex applications like NGINX with small overhead. Our rudimentary port to SDK-SENG achieved 90% of native performance and could be further improved by adding NGINX-specific optimizations and an efficient file system shield. We are confident that the SENG runtime will likewise benefit from upcoming improvements of Graphene-SGX.

### 9.6 Server Scalability and Memory Overhead

We now discuss how the SENG server scales with respect to the number of clients and connections. The server has a small static memory footprint, with the TUN interface accounting for at most 750 kB under a full transmit queue. The dynamic memory overhead is largely determined by the send and receive buffers of the per-enclave DTLS tunnels. In common settings, these would consume 8 KiB to 256 KiB per enclave and direction, plus about 32 KiB for the SSL frame buffer, but can be tuned to lower values. When considering the upper range, this still means that we could handle about 2,000 clients per 1 GiB of memory, with a huge potential for swapping large parts of the typically unused buffers. For SOCKS servers, the memory overhead increases with the number of connections they have to perform on behalf of the clients. In contrast, the SENG server is oblivious to the tunneled client connections and therefore faces constant per-client overhead.

The limiting performance bottleneck of the SENG server is the computational overhead of de- and encryption of DTLS packets and the general network I/O. In our experiments, the server easily coped with any client bandwidth, and given its 1 Gbps network card, we cannot test higher loads. The CPU utilization (around 65% on a single core, including waiting time) at maximum bandwidths suggests that the non-optimized server implementation will scale to 6+ Gbps on our hardware. This performance could be further optimized by improving the server code (e.g., using vectored sending, replacing the tunnel device with DPDK kernel NICs, etc.).

### 10. Discussion

We conclude with a discussion on upcoming improvements and directions to overcome limitations of our prototype.

#### Overcoming Memory Limitations of Enclaves

TEEs like SGX face two common challenges in practice: (i) performance impacts of context switches and (ii) limited secure memory. In Sections 9.1 and 9.5, we have already presented that careful switchless designs and improvements in the underlying primitives can mitigate these issues.