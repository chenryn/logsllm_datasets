### 76.63
### 70.28 (↓ 6)

### 2) System and Architecture Defense Approaches

One possible way to mitigate the DeepSteal attack is to use software-based encryption schemes for streaming applications. For example, with software encryption, the ML framework can perform on-demand weight decryption during computation without storing the plaintext weight parameters in memory at any time. It is important to note that while the runtime overhead of such a software-based protection scheme needs to be evaluated, one block encryption (128-bit) using the AES-NI instruction set (an advanced instruction set designed specifically to accelerate AES encryption/decryption) still requires an 8-cycle latency [81]. This latency can accumulate substantially and result in potentially high run-time overhead, which may limit the deployment of such implementations in DNN applications.

Alternatively, DeepSteal can be potentially defeated by protecting the confidentiality and integrity of sensitive pages through trusted execution environments (TEE) [82]. In TEE, pages that belong to a protected enclave are encrypted using a processor-side memory encryption engine before leaving the processor chip. Therefore, even with rowhammer-based side-channel leakage, the attacker cannot retrieve any information about the actual data. However, DNNs typically come with large memory footprints that exceed TEE’s pre-allocated secure memory region. As a result, ML applications in contemporary TEEs are subject to considerable runtime overhead [83] due to the expensive operations of page swapping between secure and unsecure memory regions. Therefore, we believe designing TEE architectures tailored for ML applications is critical for the future.

### X. CONCLUSION

The training of deep neural networks (DNNs) requires substantial computational resources and sensitive domain-specific private user data. Any potential breach in model privacy through the leakage of sensitive model parameters could result in significant financial penalties for the service provider. Consequently, the intellectual property (IP) of a pre-trained DNN model is critical to protect against adversarial threats, such as model extraction. In this work, our proposed DeepSteal attack highlights the threat of an effective model extraction attack in practical settings. Specifically, our novel system-level weight bit extraction method, HammerLeak, enables fast and efficient weight stealing for large-scale DNN applications. It can recover a significant portion of the weight bits of a DNN model with millions of weight parameters. Additionally, our proposed Mean Clustering training algorithm can leverage this information to effectively launch a strong adversarial input attack on the victim model. The efficacy of the proposed attack algorithm is validated through extensive experimental evaluation. Such a model extraction threat should encourage future work in this direction to protect the IP of large-scale DNN models.

### ACKNOWLEDGEMENT

This work is supported in part by the National Science Foundation under Grant No. 2019548, No. 2019536, No. 1931871, and No. 2144751.

### AVAILABILITY

Tools and useful code for the DeepSteal system exploit are released at: <https://github.com/casrl/DeepSteal-exploit>. The detailed models and datasets for our substitute model training can be accessed at: <https://github.com/ASU-ESIC-FAN-Lab/DeepStealSP2022>.

**Authorized licensed use limited to: Tsinghua University. Downloaded on August 07, 2022, at 12:59:25 UTC from IEEE Xplore. Restrictions apply.**

---

### REFERENCES

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in *IEEE Conference on Computer Vision and Pattern Recognition*, 2016, pp. 770–778.

[2] He et al., “Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification,” in *IEEE International Conference on Computer Vision*, December 2015.

[3] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “Achieving human parity in conversational speech recognition,” *arXiv preprint arXiv:1610.05256*, 2016.

[4] V. Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha, and S. Yan, “Exploring connections between active learning and model extraction,” in *USENIX Security Symposium*, Aug. 2020, pp. 1309–1326.

[5] S. Hong, M. Davinroy, Y. Kaya, D. Dachman-Soled, and T. Dumitraș, “How to 0wn NAS in your spare time,” *arXiv preprint arXiv:2002.06776*, 2020.

[6] K. Murdock, D. Oswald, F. D. Garcia, J. Van Bulck, D. Gruss, and F. Piessens, “Plundervolt: Software-based fault injection attacks against Intel SGX,” in *IEEE Symposium on Security and Privacy*, 2020, pp. 1466–1482.

[7] A. Kwong, D. Genkin, D. Gruss, and Y. Yarom, “Rambleed: Reading bits in memory without accessing them,” in *IEEE Symposium on Security and Privacy*, 2020, pp. 695–711.

[8] D. Gruss, C. Maurice, and S. Mangard, “Rowhammer.js: A remote software-induced fault attack in JavaScript,” in *International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment*. Springer, 2016, pp. 300–321.

[9] Y. Kim, R. Daly, J. Kim, C. Fallin, J. H. Lee, D. Lee, C. Wilkerson, K. Lai, and O. Mutlu, “Flipping bits in memory without accessing them: An experimental study of DRAM disturbance errors,” in *ACM SIGARCH Computer Architecture News*, vol. 42, no. 3, 2014, pp. 361–372.

[10] Z. Kenjar, T. Frassetto, D. Gens, M. Franz, and A.-R. Sadeghi, “V0ltpwn: Attacking x86 processor integrity from software,” in *USENIX Security Symposium*, 2020, pp. 1445–1461.

[11] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot, “High accuracy and high fidelity extraction of neural networks,” in *USENIX Security Symposium*, 2020, pp. 1345–1362.

[12] V. Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha, and S. Yan, “Exploring connections between active learning and model extraction,” in *USENIX Security Symposium*, 2020, pp. 1309–1326.

[13] T. Orekondy, B. Schiele, and M. Fritz, “Knockoff nets: Stealing functionality of black-box models,” in *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019, pp. 4954–4963.

[14] A. Barbalau, A. Cosma, R. T. Ionescu, and M. Popescu, “Black-box ripper: Copying black-box models using generative evolutionary algorithms,” *arXiv preprint arXiv:2010.11158*, 2020.

[15] G. K. Nayak, K. R. Mopuri, V. Shaj, V. B. Radhakrishnan, and A. Chakraborty, “Zero-shot knowledge distillation in deep networks,” in *International Conference on Machine Learning*. PMLR, 2019, pp. 4743–4751.

[16] J. R. Correia-Silva, R. F. Berriel, C. Badue, A. F. de Souza, and T. Oliveira-Santos, “Copycat CNN: Stealing knowledge by persuading confession with random non-labeled data,” in *IEEE International Joint Conference on Neural Networks*, 2018, pp. 1–8.

[17] S. Pal, Y. Gupta, A. Shukla, A. Kanade, S. Shevade, and V. Ganapathy, “A framework for the extraction of deep neural networks by leveraging public data,” *arXiv preprint arXiv:1905.09165*, 2019.

[18] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical black-box attacks against machine learning,” in *ACM Asia Conference on Computer and Communications Security*, 2017, pp. 506–519.

[19] S. Milli, L. Schmidt, A. D. Dragan, and M. Hardt, “Model reconstruction from model explanations,” in *Conference on Fairness, Accountability, and Transparency*, 2019, pp. 1–9.

[20] D. Rolnick and K. Kording, “Reverse-engineering deep ReLU networks,” in *International Conference on Machine Learning*. PMLR, 2020, pp. 8178–8187.

[21] H. Naghibijouybari, A. Neupane, Z. Qian, and N. Abu-Ghazaleh, “Rendered insecure: GPU side channel attacks are practical,” in *ACM SIGSAC Conference on Computer and Communications Security*, 2018, pp. 2139–2153.

[22] F. Liu, Y. Yarom, Q. Ge, G. Heiser, and R. B. Lee, “Last-level cache side-channel attacks are practical,” in *IEEE Symposium on Security and Privacy*, 2015, pp. 605–622.

[23] F. Yao, G. Venkataramani, and M. Doroslovački, “Covert timing channels exploiting non-uniform memory access based architectures,” in *Great Lakes Symposium on VLSI*, 2017, pp. 155–160.

[24] M. H. I. Chowdhury, H. Liu, and F. Yao, “BranchSpec: Information leakage attacks exploiting speculative branch instruction executions,” in *IEEE 38th International Conference on Computer Design*, 2020, pp. 529–536.

[25] M. Yan, C. W. Fletcher, and J. Torrellas, “Cache telepathy: Leveraging shared resource attacks to learn DNN architectures,” in *USENIX Security Symposium*, 2020, pp. 2003–2020.

[26] H. Yu, H. Ma, K. Yang, Y. Zhao, and Y. Jin, “DeepEM: Deep neural networks model recovery through EM side-channel information leakage,” in *IEEE International Symposium on Hardware Oriented Security and Trust*, 2020, pp. 209–218.

[27] L. Batina, S. Bhasin, D. Jap, and S. Picek, “CSI-NN: Reverse engineering of neural network architectures through electromagnetic side channel,” in *USENIX Security Symposium*, Santa Clara, CA, Aug. 2019, pp. 515–532. [Online]. Available: <https://www.usenix.org/conference/usenixsecurity19/presentation/batina>

[28] X. Hu, L. Liang, S. Li, L. Deng, P. Zuo, Y. Ji, X. Xie, Y. Ding, C. Liu, T. Sherwood et al., “DeepSniffer: A DNN model extraction framework based on learning architectural hints,” in *International Conference on Architectural Support for Programming Languages and Operating Systems*, 2020, pp. 385–399.

[29] Y. Zhu, Y. Cheng, H. Zhou, and Y. Lu, “Hermes attack: Steal DNN models with lossless inference accuracy,” in *USENIX Security Symposium*, 2021.

[30] T. Nakai, D. Suzuki, and T. Fujino, “Timing black-box attacks: Crafting adversarial examples through timing leaks against DNNs on embedded devices,” *IACR Transactions on Cryptographic Hardware and Embedded Systems*, pp. 149–175, 2021.

[31] Y. Yarom and K. Falkner, “Flush+Reload: A high resolution, low noise, L3 cache side-channel attack,” in *USENIX Security Symposium*, 2014, pp. 719–732.

[32] D. Evtyushkin, R. Riley, N. C. Abu-Ghazaleh, ECE, and D. Ponomarev, “BranchScope: A new side-channel attack on directional branch predictor,” *ACM SIGPLAN Notices*, vol. 53, no. 2, pp. 693–707, 2018.

[33] M. Yan, R. Sprabery, B. Gopireddy, C. Fletcher, R. Campbell, and J. Torrellas, “Attack directories, not caches: Side channel attacks in a non-inclusive world,” in *IEEE Symposium on Security and Privacy*, 2019, pp. 888–904.

[34] M. H. I. Chowdhury and F. Yao, “Leaking secrets through modern branch predictors in the speculative world,” *IEEE Transactions on Computers*, 2021.

[35] Z. Zhang, Z. Zhan, D. Balasubramanian, B. Li, P. Volgyesi, and X. Koutsoukos, “Leveraging EM side-channel information to detect Rowhammer attacks,” in *IEEE Symposium on Security and Privacy*, 2020, pp. 729–746.

[36] A. S. Rakin, Z. He, and D. Fan, “Bit-flip attack: Crushing neural network with progressive bit search,” in *IEEE International Conference on Computer Vision*, October 2019.

[37] F. Yao, A. S. Rakin, and D. Fan, “DeepHammer: Depleting the intelligence of deep neural networks through targeted chain of bit flips,” in *USENIX Security Symposium*, 2020, pp. 1463–1480.

[38] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.

[39] S. Addepalli, G. K. Nayak, A. Chakraborty, and V. B. Radhakrishnan, “DeGAN: Data-enriching GAN for retrieving representative samples from a trained classifier,” in *AAAI Conference on Artificial Intelligence*, vol. 34, no. 04, 2020, pp. 3130–3137.

[40] N. Carlini, M. Jagielski, and I. Mironov, “Cryptanalytic extraction of neural network models,” in *Annual International Cryptology Conference*. Springer, 2020, pp. 189–218.

[41] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing machine learning models via prediction APIs,” in *USENIX Security Symposium*, 2016, pp. 601–618.

[42] Y. Zhang, R. Yasaei, H. Chen, Z. Li, and M. A. Al Faruque, “Stealing neural network structure through remote FPGA side-channel analysis,” in *ACM/SIGDA International Symposium on Field-Programmable Gate Arrays*, 2021, pp. 225–225.

[43] J. Wei, Y. Zhang, Z. Zhou, Z. Li, and M. A. Al Faruque, “Leaky DNN: Stealing deep-learning model secret with GPU context-switching side-channel,” in *IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)*, 2020, pp. 125–137.

[44] V. Duddu, D. Samanta, D. V. Rao, and V. E. Balas, “Stealing neural networks via timing side channels,” *arXiv preprint arXiv:1812.11720*, 2018.

[45] Y. Xiang, Z. Chen, Z. Chen, Z. Fang, H. Hao, J. Chen, Y. Liu, Z. Wu, Q. Xuan, and X. Yang, “Open DNN box by power side-channel attack,” *IEEE Transactions on Circuits and Systems II: Express Briefs*, vol. 67, no. 11, pp. 2717–2721, 2020.

[46] Z. Zhan, Z. Zhang, S. Liang, F. Yao, and X. Koutsoukos, “Graphics peeping unit: Exploiting EM side-channel information of GPUs to eavesdrop on your neighbors,” in *IEEE Symposium on Security and Privacy*, 2022.

[47] R. Callan, A. Zajić, and M. Prvulovic, “FASE: Finding amplitude-modulated side-channel emanations,” in *IEEE Annual International Symposium on Computer Architecture*, 2015, pp. 592–603.

[48] Z. Zhang, S. Liang, F. Yao, and X. Gao, “Red alert for power leakage: Exploiting Intel RAPL-induced side channels,” in *ACM Asia Conference on Computer and Communications Security*, 2021, pp. 162–175.

[49] F. Yao, H. Fang, M. Doroslovački, and G. Venkataramani, “COTSKnight: Practical defense against cache timing channel attacks using cache monitoring and partitioning technologies,” in *IEEE International Symposium on Hardware Oriented Security and Trust*, 2019, pp. 121–130.

[50] H. Fang, S. S. Dayapule, F. Yao, M. Doroslovački, and G. Venkataramani, “PRODACT: Prefetch-obfuscator to defend against cache timing channels,” *International Journal of Parallel Programming*, vol. 47, no. 4, pp. 571–594, 2019.

[51] O. Mutlu and J. S. Kim, “Rowhammer: A retrospective,” *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*, vol. 39, no. 8, pp. 1555–1571, 2019.

[52] D. Gruss, M. Lipp, M. Schwarz, D. Genkin, J. Juffinger, S. O’Connell, W. Schoechl, and Y. Yarom, “Another flip in the wall of Rowhammer defenses,” in *IEEE Symposium on Security and Privacy*, 2018, pp. 245–261.

[53] L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos, “Exploiting correcting codes: On the effectiveness of ECC memory against Rowhammer attacks,” in *IEEE Symposium on Security and Privacy*, 2019, pp. 55–71.

[54] M. Seaborn and T. Dullien, “Exploiting the DRAM Rowhammer bug to gain kernel privileges,” *Black Hat*, vol. 15, p. 71, 2015.

[55] Y. Jang, J. Lee, S. Lee, and T. Kim, “SGX-Bomb: Locking down the processor via Rowhammer attack,” in *Workshop on System Software for Trusted Execution*, 2017, pp. 1–6.

[56] K. Cai, M. H. I. Chowdhury, Z. Zhang, and F. Yao, “Seeds of SEED: NMT-Stroke: Diverting neural machine translation through hardware-based faults,” 2021.

[57] M. Ribeiro, K. Grolinger, and M. A. Capretz, “MLaaS: Machine Learning as a Service,” in *IEEE International Conference on Machine Learning and Applications*, 2015, pp. 896–902.

[58] F. Yao, M. Doroslovacki, and G. Venkataramani, “Are coherence protocol states vulnerable to information leakage?” in *IEEE International Symposium on High Performance Computer Architecture*, 2018, pp. 168–179.

[59] R. K. Konoth, M. Oliverio, A. Tatar, D. Andriesse, H. Bos, C. Giuffrida, and K. Razavi, “ZEBRAM: Comprehensive and compatible software protection against Rowhammer attacks,” in *OSDI*, 2018, pp. 697–710.

[60] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “RandAugment: Practical automated data augmentation with a reduced search space,” in *IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*, 2020, pp. 702–703.

[61] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “AutoAugment: Learning augmentation strategies from data,” in *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019, pp. 113–123.

[62] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “PyTorch: An imperative style, high-performance deep learning library,” *Advances in Neural Information Processing Systems*, vol. 32, pp. 8026–8037, 2019.

[63] A. Tatar, C. Giuffrida, H. Bos, and K. Razavi, “Defeating software mitigations against Rowhammer: A surgical precision hammer,” in *International Symposium on Research in Attacks, Intrusions, and Defenses*. Springer, 2018, pp. 47–66.

[64] K. Razavi, B. Gras, E. Bosman, B. Preneel, C. Giuffrida, and H. Bos, “Flip Feng Shui: Hammering a needle in the software stack,” in *USENIX Security Symposium*, Austin, TX, Aug. 2016, pp. 1–18. [Online]. Available: <https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/razavi>

[65] M. Gorman, *Understanding the Linux Virtual Memory Manager*. Prentice Hall Upper Saddle River, 2004.

[66] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in machine learning: From phenomena to black-box attacks using adversarial samples,” *arXiv preprint arXiv:1605.07277*, 2016.

[67] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning models resistant to adversarial attacks,” in *International Conference on Learning Representations*, 2018. [Online]. Available: <https://openreview.net/forum?id=rJzIBfZAb>

[68] D. Khudia, J. Huang, P. Basu, S. Deng, H. Liu, J. Park, and M. Smelyansky, “FBGEMM: Enabling high-performance low-precision deep learning inference,” *arXiv preprint arXiv:2101.05615*, 2021.

[69] M. Dukhan, Y. Wu, “QNNPACK: Open source library for optimized mobile deep learning.” [Online]. Available: <https://engineering.fb.com/2018/10/29/ml-applications/qnnpack/>

[70] Y. Langsam, M. Augenstein, and A. M. Tenenbaum, *Data Structures Using C and C++*. Prentice Hall New Jersey, 1996, vol. 2.

[71] W. Cui, X. Li, J. Huang, W. Wang, S. Wang, and J. Chen, “Substitute model generation for black-box adversarial attack based on knowledge distillation,” in *IEEE International Conference on Image Processing*, 2020, pp. 648–652.

[72] H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan, “Theoretically principled trade-off between robustness and accuracy,” in *International Conference on Machine Learning*, 2019.

[73] H. Hassan, Y. C. Tugrul, J. S. Kim, V. Van der Veen, K. Razavi, and O. Mutlu, “Uncovering in-DRAM Rowhammer protection mechanisms: A new methodology, custom Rowhammer patterns, and implications,” in *IEEE International Symposium on Microarchitecture*, 2021, pp. 1198–1213.

[74] L. Orosa, A. G. Yaglikci, H. Luo, A. Olgun, J. Park, H. Hassan, M. Patel, J. S. Kim, and O. Mutlu, “A deeper look into Rowhammer’s sensitivities: Experimental analysis of real DRAM chips and implications on future attacks and defenses,” in *IEEE International Symposium on Microarchitecture*, 2021, pp. 1182–1197.

[75] “Half-Double Next-Row-Over Assisted Rowhammer.” [Online]. Available: <https://github.com/google/hammer-kit/blob/main/20210525_half_double.pdf>

[76] P. Frigo, E. Vannacci, H. Hassan, V. Van Der Veen, O. Mutlu, C. Giuffrida, H. Bos, and K. Razavi, “TRRespass: Exploiting the many sides of Target Row Refresh,” in *IEEE Symposium on Security and Privacy*, 2020, pp. 747–762.

[77] W. Zhou, X. Hou, Y. Chen, M. Tang, X. Huang, X. Gan, and Y. Yang, “Transferable adversarial perturbations,” in *European Conference on Computer Vision*, 2018, pp. 452–467.

[78] C. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. L. Yuille, “Improving transferability of adversarial examples with input diversity,” in *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019, pp. 2730–2739.

[79] F. Liu, C. Zhang, and H. Zhang, “Towards transferable adversarial perturbations with minimum norm,” in *ICML Workshop on Adversarial Machine Learning*, 2021.

[80] M. Salzmann et al., “Learning transferable adversarial perturbations,” *Advances in Neural Information Processing Systems*, vol. 34, 2021.

[81] R. Benadjila, O. Billet, S. Gueron, and M. J. Robshaw, “The Intel AES-NI Instruction Set: A Case Study in Special-Purpose Hardware for Cryptography,” *Journal of Cryptographic Engineering*, vol. 1, no. 2, pp. 109–121, 2011.