### Client Requests Handling
Upon the arrival of client requests, we validate the local results. In the absence of faults, these results are forwarded to the respective clients.

### Preliminary Results
As part of our preliminary evaluation, we were interested in the overhead introduced by hardening a multithreaded key-value store using CROSSCHECK. Specifically, we measured the overhead of applying Generic Object Protection (GOP) to individual classes and when hardening all relevant classes.

For our evaluation, we used MEMCACHED++, which is based on version 1.4.10 of memcached. For ordering requests, we utilized the most recent version of the Spread toolkit (v. 4.3.0). To simulate clients, we used the memslap benchmark, which is part of libmemcached.

#### Description of Hardened Classes
- **Hashtable**: Key-value container.
- **Key-value container**: Class for managing key-value pairs.
- **Assoc**: Item management and statistics.
- **Item**: Items Management.
- **Slab**: Container for pre-allocated memory.
- **Slabs**: Management of Slab instances.

**Figure 4: GOP-hardened classes**

All evaluations were performed on a cluster of four machines, each equipped with a Core i7-3770 CPU (4 cores at 3.4 GHz, supporting 8 parallel threads), 16 GB RAM, and connected via a switched gigabit network. Three machines hosted replicas, while the fourth machine generated client requests.

We selected a write-intensive workload where each client performed 10,000 set requests with a key length of 100 B and a value length of 400 B. Each MEMCACHED++ instance used four worker threads to handle requests concurrently. We increased the workload by simulating more clients. The baseline of our measurement was a plain replicated version of MEMCACHED++ without generic object protection. Additionally, we individually hardened five classes (see Figure 4) with a CRC32 error detection code. In Figure 5(b), we also evaluated GOP with object state copy, enabling local recovery from detected state corruptions.

As shown in both figures, the overhead for individual classes varied significantly, ranging from 2% to 23%. This variance is due to two factors: the size of the protected data and the access pattern for the individual object. For example, the size of `Items` and `Item` objects ranges from 300 B to 712 B, and each request checksums these objects around 40 to 80 times. In contrast, `Assoc` and `Slab` objects are smaller and accessed less frequently, resulting in a lower overhead of 2% to 10%.

When all five classes are protected, the total overhead results in a 30% performance decrease for 256 clients. Adding a local object state copy further reduces performance by 9%, leading to a 39% performance reduction.

These results can be optimized by tailoring GOP to better fit the demands of CROSSCHECK. Currently, each time an object is accessed, at least one checksum is generated. However, during crosschecks, only the most recent checksums are compared. Instead of generating a checksum, we could log (e.g., using a dirty bit) if an object is accessed. At the end of executing a request, but before initiating the actual crosscheck, we generate checksums for all logged objects. This way, only one checksum is generated per object per request.

Unfortunately, this approach would reduce our error detection capabilities ahead of execution. To mitigate this, each class could be individually protected by GOP. For example, `Assoc` and `Slab` could be protected by CRC32 error detection code plus a local copy, while `Items` and `Item` could use the optimization described above. With these optimizations, we expect to decrease the overall performance overhead significantly.

**Figure 5: Overhead of generic object protection**
- (a) Without local object state copy
- (b) With local object state copy

### Related Work
Using checksums to compare and synchronize replicas in distributed systems has been proposed in various contexts. However, support is usually limited to persistent state or does not consider state corruptions. CROSSCHECK focuses on hardening and recovering the in-memory state of replicated services.

Correia et al. [11] proposed an approach for hardening distributed applications against arbitrary state corruptions through redundant execution at the granularity of requests within a single node. This approach contains state corruptions at the node level and masks them as crash faults. While effective, it requires a specific application structure and doubles memory and CPU demand.

Behrens et al. [12] refined this approach by saving checksums of an initial execution instead of a full state copy, reducing memory overhead. They also intercept and check data access via checksums at the level of memory pages. Behrens et al. [25] propose the use of encoded processing via AN-coding, offering fine-grained control-flow checks but with a computational overhead of a factor of five. Compared to these systems, CROSSCHECK achieves a similar fault-tolerance level with moderate additional resources due to its tight integration with State Machine Replication (SMR).

There is limited work that considers faults beyond crashes and allows multithreaded execution. Kapritsos et al. [26] batch requests to minimize concurrent access to state objects, reverting and re-executing sequentially if replicas do not reach a consistent state. This introduces significant overhead when many requests share state objects. Similarly, Kotla et al. [27] enable concurrent execution of requests that do not change shared state, leaving a middle ground where services can freely utilize threads while maintaining determinism.

### Conclusions
CROSSCHECK provides a method to tolerate arbitrary state corruptions for implementing highly available multithreaded services in data centers. Using generic object protection, CROSSCHECK can harden state objects flexibly and generically. Our initial evaluation based on a key-value store showed an overhead of 2% to 23% for different protected classes. Future steps include performance optimization, efficient recovery support, and a fault-injection campaign to assess error coverage.

### References
[1] M. Burrows, “The chubby lock service for loosely-coupled dist. sys-
tems,” in Proc. of the 7th Symp. on Operating Systems Design and
Implementation, 2006, pp. 335–350.
...
[27] R. Kotla and M. Dahlin, “High throughput byzantine fault tolerance,”
in Proc. of the 2004 Int. Conf. on Dependable Systems and Networks,
2004, pp. 575–.