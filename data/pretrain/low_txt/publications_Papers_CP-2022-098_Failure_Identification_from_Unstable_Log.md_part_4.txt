### Optimized Text

#### Performance on Recall
The input for the sequential-based baselines (pHMM, CV, and their combination) has a higher entropy, which complicates the discrimination against normal sequences, leading to an increase in false positives. Comparing these methods with the quantitative-based method (PCA) shows that while PCA performs well in precision, it has a reduced recall. The count vector representation and the limited modeling power of PCA (as a linear model) are potential causes for the incorrect detection of true failures. From an economic perspective, even a 0.1% improvement in F1 score can save hundreds of thousands of dollars [9].

#### RQ2: Effectiveness of CLog for Failure Type Identification
This research question evaluates the capability of the FTI module in CLog to reuse historical information from the operator to detect different types of failures. Specifically, we evaluate three representations of subprocess sequences for CLog:
1. Probability scores from the HMM (pHMM)
2. Count vectors (CV)
3. Combination of both (CV + pHMM)

These are compared against LogClass [4], which uses single logs as input. We randomly sample 60% of the labeled failure sequences/logs from the original dataset to train the multiclass classifier using the synthetic dataset generation procedure previously described. The remaining 40% is used for evaluation. To reduce bias, we repeated the experiments 30 times and report the average performance scores and their standard deviations.

**Table IV: CLog Failure Detection Evaluation on Synthetic Data**

| Injection Ratio | F1 Score | Precision | Recall |
|-----------------|----------|-----------|--------|
| 5%              | 0.94     | 0.97      | 0.91   |
| 10%             | 0.92     | 0.95      | 0.89   |
| 15%             | 0.90     | 0.95      | 0.86   |
| 20%             | 0.88     | 0.94      | 0.83   |

**Figure 6: Impact of Window Size on Detection Performance**

- **Window Size (s)**: 60, 120, 180, 240, 300
- **F1 Score, Precision, Recall**: Decrease as window size increases

CLog's detection method maintains high performance even under a high ratio of unstable sequences, demonstrating that the extracted subprocesses are sensitive to local changes within log sequences, making the performance robust.

#### RQ3: Influence of Sequence Entropy on Failure Detection
This question investigates the impact of unstable log event sequences on failure detection performance in a model-agnostic manner. We grouped input events into time intervals of increasing window sizes and evaluated several models of CLog, DeepLog, and PCA. The best average performance of CLog, averaged over the window sizes, was obtained for 30 subprocesses. Figure 6 shows the results, indicating that as the window size increases, the detection performance decreases. A negative correlation between increased entropy and failure detection performance is observed. Due to its greater modeling power, CLog and DeepLog show a relatively lower drop in performance (4-5%) compared to PCA (8-50%). CLog outperforms DeepLog due to smaller instability in the input.

#### RQ4: Robustness of CLog in Failure Detection
We conducted experiments on a synthetic dataset where the failure detector was trained on the original data. We randomly injected b-percentages of unstable sequences. The failure detector was trained using a deep learning neural network (LSTM) augmented with an attention mechanism. Other methods, such as SVM, decision trees, logistic regression, and nearest neighbors, were also considered. CLog showed robust performance, experiencing only a 6% performance drop under high ratios of injected unstable sequences.

#### Conclusion
This paper addresses the automation of log-based failure identification, a crucial maintenance task for enhancing reliability in cloud systems. CLog decouples the problem into two subproblems: failure detection and failure type identification. By representing log data as sequences of subprocesses instead of individual events, the entropy in the input, caused by unstable logs, is reduced. CLog introduces a novel subprocess extraction method that jointly trains context-aware deep learning and clustering methods to extract subprocesses. Experiments demonstrate that the extracted sequences improve performance in both failure detection (by 9-24% over baselines) and failure type identification (by 7% over the baseline). CLog also shows robust performance under high ratios of unstable sequences, experiencing only a 6% performance drop.

#### Related Work
**Failure Detection:**
- **Unsupervised Methods:** These do not require labels and include one-class methods like HMM, DeepLog, and PCA.
- **Supervised Methods:** Assume the availability of labels and include methods like LogRobust, SVM, decision trees, and logistic regression.

**Failure Type Identification:**
- Early works used keyword search and bag-of-words representations.
- CLog pairs count vectors from subprocesses with a multiclass classifier to use past information about similar failure types.

#### References
[1] P. Garraghan, R. Yang, Z. Wen, A. Romanovsky, J. Xu, R. Buyya, and R. Ranjan, "Emergent failures: Rethinking cloud reliability at scale," IEEE Cloud Computing, vol. 5, pp. 12–21, 2018.
[2] S. He, J. Zhu, P. He, and M. R. Lyu, "Experience report: System log analysis for anomaly detection," in Proc. of the 27th IEEE International Symposium on Software Reliability Engineering, 2016, pp. 207–218.
[3] S. He, P. He, Z. Chen, T. Yang, Y. Su, and M. R. Lyu, "A survey on automated log analysis for reliability engineering," ACM Comput. Surv., vol. 54, 2021.
[4] W. Meng, Y. Liu, S. Zhang, F. Zaiter, Y. Zhang, Y. Huang, Z. Yu, Y. Zhang, L. Song, M. Zhang, and D. Pei, "Logclass: Anomalous log identification and classification with partial labels," IEEE Trans. Netw., vol. 18, pp. 1870–1884, 2021.
[5] Q. Lin, H. Zhang, J.-G. Lou, Y. Zhang, and X. Chen, "Log clustering based problem identification for online service systems," in Proc. of 38th International Conference on Software Engineering Companion, 2016, p. 102–111.
[6] X. Zhang et al., "Robust log-based anomaly detection on unstable log data," in Proc. of the 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE), 2019, p. 807–817.
[7] S. He, Q. Lin, J.-G. Lou, H. Zhang, M. R. Lyu, and D. Zhang, "Identifying impactful service system problems via log analysis," in Proc. of the 26th Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2018, p. 60–70.
[8] Z. Chen, J. Liu, W. Gu, Y. Su, and M. R. Lyu, "Experience report: Deep learning-based system log analysis for anomaly detection," CoRR, vol. 2107.05908, 2021.
[9] Y. Zhu, W. Meng, Y. Liu, S. Zhang, T. Han, S. Tao, and D. Pei, "Unilog: Deploy one model and specialize it for all log analysis tasks," 2021.
[10] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, "Self-attentive classification-based anomaly detection in unstructured logs," CoRR, vol. abs/2008.09340, 2020.
[11] M. Du, F. Li, G. Zheng, and V. Srikumar, "Deeplog: Anomaly detection and diagnosis from system logs through deep learning," in Proc. of the ACM SIGSAC Conference on Computer and Communications Security (CCS), 2017, p. 1285–1298.
[12] D. Cotroneo, L. De Simone, P. Liguori, R. Natella, and N. Bidokhti, "How bad can a bug get? An empirical analysis of software failures in the openstack cloud computing platform," in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. New York, NY, USA: Association for Computing Machinery, 2019, pp. 200–211.
[13] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu, "Tools and benchmarks for automated log parsing," in Proc. of the 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 2019, p. 121–130.
[14] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, "Drain: An online log parsing approach with fixed depth tree," in 2017 IEEE International Conference on Web Services. NY, USA: Curran Associates, 2017, pp. 33–40.
[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of deep bidirectional transformers for language understanding," in Proc. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, 2019, pp. 4171–4186.
[16] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong, "Towards k-means-friendly spaces: Simultaneous deep learning and clustering," in Proc. of the 34th International Conference on Machine Learning, 2017.
[17] K. Yamanishi and Y. Maruyama, "Dynamic syslog mining for network failure monitoring," in Proc. of the 11nd SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005, p. 499–508.
[18] L. Breiman, "Random forests," Mach. Learn., vol. 45, pp. 5–32, 2001.
[19] J. R. Quinlan, "Induction of decision trees," Mach. Learn., vol. 1, pp. 81–106, 1986.
[20] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc., 2001.
[21] Y. Freund and R. E. Schapire, "A short introduction to boosting," in Proc. of the 16 International Joint Conference on Artificial Intelligence, 1999, pp. 1401–1406.
[22] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, "Detecting large-scale system problems by mining console logs," in Proc. of the 22nd Symposium on Operating Systems Principles, 2009, p. 117–132.
[23] H. Li, W. Shang, and A. E. Hassan, "Which log level should developers choose for a new logging statement?" Empir. Softw. Eng., vol. 22, p. 1684–1716, 2017.
[24] W. Meng et al., "Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs," in Proc. of the International Joint Conferences on Artificial Intelligence, 2019, pp. 4739–4745.
[25] J. Breier and J. Branisˇova´, "Anomaly detection from log files using data mining techniques," in Information Science and Applications. Berlin, Heidelberg: Springer Berlin Heidelberg, 2015, pp. 449–457.
[26] A. Oliner and J. Stearley, "What supercomputers say: A study of five system logs," in Proc. of the 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, 2007, pp. 575–584.
[27] W. Meng, Y. Liu, S. Zhang, D. Pei, H. Dong, L. Song, and X. Luo, "Device-agnostic log anomaly classification with partial labels," in Proc. of 26th International Symposium on Quality of Service, 2018, pp. 1–6.