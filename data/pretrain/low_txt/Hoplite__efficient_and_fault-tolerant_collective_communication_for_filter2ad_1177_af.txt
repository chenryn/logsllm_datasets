以下是优化后的参考文献列表，以确保其清晰、连贯和专业：

1. 未提供作者和标题信息。请补充完整引用信息。
2. Blumofe, R. D., Joerg, C. F., Kuszmaul, B. C., Leiserson, C. E., Randall, K. H., & Zhou, Y. (1996). Cilk: An efficient multithreaded runtime system. *Journal of Parallel and Distributed Computing*, 37(1), 55-69.
3. Castro, M., Druschel, P., Kermarrec, A.-M., & Rowstron, A. I. T. (2002). Scribe: a large-scale and decentralized application-level multicast infrastructure. *IEEE Journal on Selected Areas in Communications*, 20(8), 1489–1499. https://doi.org/10.1109/JSAC.2002.803069
4. Castro, M., Druschel, P., Kermarrec, A.-M., Nandi, A., Rowstron, A., & Singh, A. (2003). SplitStream: High-Bandwidth Multicast in Cooperative Environments. *SIGOPS Oper. Syst. Rev.*, 37(5), 298–313. https://doi.org/10.1145/1165389.945474
5. Chowdhury, M., & Stoica, I. (2015). Efficient Coflow Scheduling Without Prior Knowledge. In *Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication* (pp. 393–406). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/2785956.2787480
6. Chowdhury, M., Zaharia, M., Ma, J., Jordan, M. I., & Stoica, I. (2011). Managing Data Transfers in Computer Clusters with Orchestra. *SIGCOMM Comput. Commun. Rev.*, 41(4), 98–109. https://doi.org/10.1145/2043164.2018448
7. Chowdhury, M., Zhong, Y., & Stoica, I. (2014). Efficient Coflow Scheduling with Varys. In *Proceedings of the 2014 ACM Conference on SIGCOMM* (pp. 443–454). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/2619239.2626315
8. Crankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gonzalez, J. E., & Stoica, I. (2017). Clipper: A low-latency online prediction serving system. In *14th USENIX Symposium on Networked Systems Design and Implementation* (pp. 613–627).
9. Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., ... & Yang, K. (2012). Large scale distributed deep networks. In *Advances in Neural Information Processing Systems* (pp. 1223–1231).
10. Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified Data Processing on Large Clusters. *Commun. ACM*, 51(1), 107–113. https://doi.org/10.1145/1327452.1327492
11. Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., ... & et al. (2018). IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In *International Conference on Machine Learning* (pp. 1407–1416). PMLR.
12. Gloo. (2020). Collective communications library with various primitives for multi-machine training. Retrieved from https://github.com/facebookincubator/gloo.
13. Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., & He, K. (2017). Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677.
14. Graham, R. L., Woodall, T. S., & Squyres, J. M. (2005). Open MPI: A flexible high performance MPI. In *International Conference on Parallel Processing and Applied Mathematics* (pp. 228–239). Springer.
15. gRPC. (2020). gRPC. Retrieved from https://grpc.io/.
16. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (pp. 770–778).
17. Hydro. (2020). Hydro. Retrieved from https://github.com/hydro-project.
18. Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., & Keutzer, K. (2016). SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5 MB model size. arXiv preprint arXiv:1602.07360.
19. IPMulticast. (2020). IP Multicast Technology Overview. Retrieved from https://www.cisco.com/c/en/us/td/docs/ios/solutions_docs/ip_multicast/White_papers/mcst_ovr.html.
20. Keynote: Building a Fusion Engine with Ray. (2020). Retrieved from https://ray2020.sched.com/event/eGOL/keynote-building-a-fusion-engine-with-ray-dr-charles-he-chief-architect-of-storage-and-compute-ant-group.
21. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In *Advances in Neural Information Processing Systems* (pp. 1097–1105).
22. Lee, J., Turner, Y., Lee, M., Popa, L., Banerjee, S., Kang, J.-M., & Sharma, P. (2014). Application-Driven Bandwidth Guarantees in Datacenters. *SIGCOMM Comput. Commun. Rev.*, 44(4), 467–478. https://doi.org/10.1145/2740070.2626326
23. Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., Long, J., Shekita, E. J., & Su, B.-Y. (2014). Scaling distributed machine learning with the parameter server. In *11th USENIX Symposium on Operating Systems Design and Implementation* (pp. 583–598).
24. Li, M., Zhou, L., Yang, Z., Li, A., Xia, F., Andersen, D. G., & Smola, A. (2013). Parameter server for distributed machine learning. In *Big Learning NIPS Workshop*, Vol. 6, p. 2.
25. Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K., Gonzalez, J., Jordan, M., & Stoica, I. (2018). RLlib: Abstractions for distributed reinforcement learning. In *International Conference on Machine Learning* (pp. 3053–3062). PMLR.
26. Ma, N., Zhang, X., Zheng, H.-T., & Sun, J. (2018). ShuffleNet v2: Practical guidelines for efficient CNN architecture design. In *Proceedings of the European Conference on Computer Vision (ECCV)* (pp. 116–131).
27. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., & Kavukcuoglu, K. (2016). Asynchronous Methods for Deep Reinforcement Learning. In *Proceedings of the 33rd International Conference on Machine Learning* (pp. 1928–1937). JMLR.org.
28. Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I., & et al. (2018). Ray: A Distributed Framework for Emerging AI Applications. In *Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation* (pp. 561–577). USENIX Association, USA.
29. MPICH. (2020). MPICH. Retrieved from https://www.mpich.org/.
30. Murray, D. G., Schwarzkopf, M., Smowton, C., Smith, S., Madhavapeddy, A., & Hand, S. (2011). CIEL: a universal execution engine for distributed data-flow computing.
31. Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N. R., Ganger, G. R., Gibbons, P. B., & Zaharia, M. (2019). PipeDream: Generalized Pipeline Parallelism for DNN Training. In *Proceedings of the 27th ACM Symposium on Operating Systems Principles* (pp. 1–15). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3341301.3359646
32. NCCL. (2020). The NVIDIA Collective Communication Library (NCCL). Retrieved from https://developer.nvidia.com/nccl.
33. NumPy. (2020). NumPy. Retrieved from https://numpy.org/.
34. Olston, C., Li, F., Harmsen, J., Soyke, J., Gorovoy, K., Lao, L., Fiedel, N., Ramesh, S., & Rajashekhar, V. (2017). TensorFlow-Serving: Flexible, High-Performance ML Serving. In *Workshop on ML Systems at NIPS 2017*.
35. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In *Advances in Neural Information Processing Systems* (Vol. 32, pp. 8024–8035). Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf
36. Peng, Y., Zhu, Y., Chen, Y., Bao, Y., Yi, B., Lan, C., Wu, C., & Guo, C. (2019). A Generic Communication Scheduler for Distributed DNN Training Acceleration. In *Proceedings of the 27th ACM Symposium on Operating Systems Principles* (pp. 16–29). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3341301.3359642
37. Pu, Q., Ananthanarayanan, G., Bodik, P., Kandula, S., Akella, A., Bahl, P., & Stoica, I. (2015). Low Latency Geo-Distributed Data Analytics. In *Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication* (pp. 421–434). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/2785956.2787505
38. Pu, Q., Venkataraman, S., & Stoica, I. (2019). Shuffling, Fast and Slow: Scalable Analytics on Serverless Infrastructure. In *16th USENIX Symposium on Networked Systems Design and Implementation* (pp. 193–206). USENIX Association, Boston, MA. https://www.usenix.org/conference/nsdi19/presentation/pu
39. Ray Parameter Server. (2020). Parameter Server. Retrieved from https://ray.readthedocs.io/en/latest/auto_examples/plot_parameter_server.html.
40. Ray Serve. (2021). Ray Serve. Retrieved from https://docs.ray.io/en/master/serve/.
41. Redis. (2020). Redis. Retrieved from https://redis.io/.
42. Rocklin, M. (2015). Dask: Parallel computation with blocked algorithms and task scheduling. In *Proceedings of the 14th Python in Science Conference*. Citeseer.
43. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., & Chen, L.-C. (2018). MobileNetV2: Inverted residuals and linear bottlenecks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (pp. 4510–4520).
44. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
45. Sergeev, A., & Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in TensorFlow. arXiv:1802.05799 [cs.LG].
46. Sreekanti, V., Wu, C., Chhatrapati, S., Gonzalez, J. E., Hellerstein, J. M., & Faleiro, J. M. (2020). A fault-tolerance shim for serverless computing. In *Proceedings of the Fifteenth European Conference on Computer Systems* (pp. 1–15).
47. Tan, M., & Le, Q. (2019). EfficientNet: Rethinking model scaling for convolutional neural networks. In *International Conference on Machine Learning* (pp. 6105–6114). PMLR.
48. Simonyan, K., & Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. In *3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings* (Eds. Bengio, Y. & LeCun, Y.).
49. Wang, G., Venkataraman, S., Phanishayee, A., Devanur, N., Thelin, J., & Stoica, I. (2020). Blink: Fast and Generic Collectives for Distributed ML. In *Proceedings of Machine Learning and Systems* (Vol. 2, pp. 172–186). https://proceedings.mlsys.org/paper/2020/file/43ec517d68b6edd3015b3edc9a11367b-Paper.pdf
50. Wang, S., Liagouris, J., Nishihara, R., Moritz, P., Misra, U., Tumanov, A., & Stoica, I. (2019). Lineage Stash: Fault Tolerance off the Critical Path. In *Proceedings of the 27th ACM Symposium on Operating Systems Principles* (pp. 338–352). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3341301.3359653
51. Zaharia, M., Xin, R. S., Wendell, P., Das, T., Armbrust, M., Dave, A., Meng, X., Rosen, J., Venkataraman, S., Franklin, M. J., & et al. (2016). Apache Spark: A Unified Engine for Big Data Processing. *Commun. ACM*, 59(11), 56–65. https://doi.org/10.1145/2934664

**附录**

**A. 小对象的微基准测试**

我们展示了针对小对象（1KB、32KB）的多个集体通信原语的微基准测试结果（图14）。请注意，Hoplite将小于64KB的对象内容存储在对象目录服务中（§3.2），因此对于Hoplite没有集体通信。我们将其与Ray、Dask、OpenMPI和Gloo进行了比较。由于Horovod有三个后端：OpenMPI、Gloo和NCCL，而NCCL用于GPU，且Hoplite目前不支持GPU，所以我们没有与Horovod进行比较。

在所有这些替代方案中，Hoplite表现最佳或接近最佳。Gloo在广播和全减少操作中的性能最佳。Hoplite比Ray和Dask更高效，因为Hoplite直接在对象目录服务中存储对象数据。

**B. 关于减少树度数的选择研究**

在这里，我们在AWS EC2设置（§5）中研究了减少树度数\( d \)的选择。最佳的\( d \)取决于网络特性、要减少的对象大小以及参与者的数量。我们将三种选择的\( d \)进行比较：1（单链）、2（二叉树）和\( n \)（根连接其他所有人）。结果如图15所示。正如我们在§3.4中的分析所预期的那样，当对象较小时，\( d = n \)是最好的，因为主要瓶颈是网络延迟。当中等大小的对象（256KB、1MB）时，\( d = n \)在减少操作中变得不稳定。我们怀疑这是由于入站或gRPC特性造成的。当对象大小为4MB或8MB时，我们需要根据参与者的数量在\( d = 1 \)和\( d = 2 \)之间做出选择。这是因为树减少中的网络延迟和网络吞吐量都可能成为瓶颈。当对象大小为16MB或更大时，我们选择\( d = 1 \)以减轻减少操作中的吞吐量瓶颈。

**图14：Hoplite、OpenMPI、Ray、Dask和Gloo在标准集体通信原语（如广播、收集、减少、全减少）上的延迟比较，对象大小分别为1KB和32KB。为了更清晰地显示结果，我们将全减少的结果分为两组：(i) 包括Hoplite、Ray和Dask；(ii) 包括Hoplite、OpenMPI和Gloo中的两种不同的全减少算法。**

**图15：关于不同对象大小和参与者数量下减少树度数\( d \)的消融研究。**

![](figures/figure_14.png)
![](figures/figure_15.png)

希望这些改进能使您的文本更加清晰、连贯和专业。如果有更多需要调整的地方，请告诉我！