---
author: Fabian Reinartz
category: 软件开发
comments_data: []
count:
  commentnum: 0
  favtimes: 1
  likes: 0
  sharetimes: 0
  viewnum: 8413
date: '2019-06-11 18:08:00'
editorchoice: true
excerpt: 本文探讨了Prometheus时间序列数据库的设计思考，尽管撰写时间较早，但其设计理念和思路仍极具参考价值。
fromurl: https://fabxc.org/blog/2017-04-10-writing-a-tsdb/
id: 10964
islctt: true
largepic: /data/attachment/album/201906/11/180646l7cqbhazqs7nsqsn.jpg
permalink: /article-10964-1.html
pic: /data/attachment/album/201906/11/180646l7cqbhazqs7nsqsn.jpg.thumb.jpg
related: []
reviewer: wxy
selector: ''
summary: 本文探讨了Prometheus时间序列数据库的设计思考，尽管撰写时间较早，但其设计理念和思路仍极具参考价值。
tags:
- 监控
- 时间
- Prometheus
thumb: false
title: 从零开始构建一个时间序列数据库
titlepic: true
translator: LuuMing
updated: '2019-06-11 18:08:00'
---

编者按：Prometheus是CNCF旗下的开源监控告警解决方案，已成为Kubernetes生态系统中的核心监控系统。本文作者Fabian Reinartz是Prometheus的核心开发者，他在2017年撰写了这篇关于Prometheus中时间序列数据库设计的文章。虽然文章已有些时日，但其中的考虑和思路依然值得借鉴。请耐心阅读。

![](/data/attachment/album/201906/11/180646l7cqbhazqs7nsqsn.jpg)

我从事监控工作，尤其是在Prometheus项目上。Prometheus监控系统包含一个自定义的时间序列数据库，并且与Kubernetes紧密集成。

在很多方面，Kubernetes展现了Prometheus的所有设计用途。它使得持续部署、自动伸缩和其他高动态环境下的功能变得容易实现。查询语句、操作模型以及其他概念决策使Prometheus特别适合这种环境。然而，当监控的工作负载显著增加时，会给监控系统本身带来新的压力。考虑到这一点，我们致力于在高动态或瞬态服务环境下提升Prometheus的表现，而不是重新解决已经处理得很好的问题。

Prometheus的存储层历来表现出色，单一服务器能够以每秒数百万个时间序列的速度摄入多达一百万个样本，同时占用极少的磁盘空间。尽管当前的存储表现良好，但我提出了一种新的存储子系统设计，旨在修正现有解决方案的不足，并具备处理更大规模数据的能力。

> 备注：我没有数据库方面的背景。如果我的观点有误，请在Freenode的#prometheus频道上向我（fabxc）提出批评。

## 问题、挑战与领域

首先，快速概览一下我们的目标及其关键挑战。我们将回顾Prometheus目前的做法，为什么它表现优异，以及新设计旨在解决哪些问题。

### 时间序列数据

我们有一个收集一段时间内数据的系统：

```
identifier -> (t0, v0), (t1, v1), (t2, v2), (t3, v3), ....
```

每个数据点是一个时间戳和值的元组。在监控中，时间戳是一个整数，值可以是任意数字。64位浮点数对于计数器和测量值来说是一个好的表示方法，因此我们使用它。一系列严格单调递增的时间戳数据点构成一个序列，由标识符引用。我们的标识符是一个带有标签维度字典的度量名称。标签维度划分了单一指标的测量空间。每个指标名称加上一个唯一标签集就形成了一个独立的时间序列，它有一个关联的数据流。

这是一个典型的序列标识符集合，它是统计请求指标的一部分：

```
requests_total{path="/status", method="GET", instance=”10.0.0.1:80”}
requests_total{path="/status", method="POST", instance=”10.0.0.3:80”}
requests_total{path="/", method="GET", instance=”10.0.0.2:80”}
```

简化表示方法：度量名称可以作为另一个维度标签，在我们的例子中是 `__name__`。对于查询语句，可以对其进行特殊处理，但这与我们的存储方式无关，我们稍后会讨论。

```
{__name__="requests_total", path="/status", method="GET", instance=”10.0.0.1:80”}
{__name__="requests_total", path="/status", method="POST", instance=”10.0.0.3:80”}
{__name__="requests_total", path="/", method="GET", instance=”10.0.0.2:80”}
```

我们希望通过标签来查询时间序列数据。最简单的情况下，使用 `{__name__="requests_total"}` 选择所有属于 `requests_total` 指标的数据。对于所有选定的序列，我们在给定的时间窗口内获取数据点。

在更复杂的查询中，我们可能希望一次性选择满足多个标签的序列，并表示比相等条件更复杂的情况。例如，非语句（`method!="GET"`）或正则表达式匹配（`method=~"PUT|POST"`）。

这些在很大程度上定义了存储的数据及其获取方式。

### 纵向与横向

在一个简化的视图中，所有的数据点可以分布在二维平面上。水平维度代表时间，序列标识符域沿纵轴展开。

```
series
  ^   
  |   . . . . . . . . . . . . . . . . .   . . . . .   {__name__="request_total", method="GET"}
  |     . . . . . . . . . . . . . . . . . . . . . .   {__name__="request_total", method="POST"}
  |         . . . . . . .
  |       . . .     . . . . . . . . . . . . . . . .                  ... 
  |     . . . . . . . . . . . . . . . . .   . . . .   
  |     . . . . . . . . . .   . . . . . . . . . . .   {__name__="errors_total", method="POST"}
  |           . . .   . . . . . . . . .   . . . . .   {__name__="errors_total", method="GET"}
  |         . . . . . . . . .       . . . . .
  |       . . .     . . . . . . . . . . . . . . . .                  ... 
  |     . . . . . . . . . . . . . . . .   . . . . 
  v
```

Prometheus通过定期抓取一组时间序列的当前值来获取数据点。从中获取的实体称为目标。因此，写入模式完全垂直且高度并发，因为来自每个目标的样本是独立摄入的。

这里提供一些规模上的参考：单个Prometheus实例从数万个目标中收集数据点，每个数据点都暴露在数百到数千个不同的时间序列中。

在每秒采集数百万数据点这种规模下，批量写入是一个不能妥协的性能要求。在磁盘上分散地写入单个数据点会非常缓慢。因此，我们希望按顺序写入更大的数据块。

对于旋转式磁盘，磁头始终需要在物理上移动到不同的扇区，这是一个不足为奇的事实。而尽管SSD具有快速随机写入的特点，但它不能修改单个字节，只能写入一页或多页的4KiB数据。这意味着写入16字节的样本相当于写入满满一个4KiB的页。这种行为被称为[写入放大](https://en.wikipedia.org/wiki/Write_amplification)，会损耗SSD。因此，这不仅影响速度，还会在几天或几周内破坏硬件。

关于此问题更深层次的资料，[“Coding for SSDs”系列](http://codecapsule.com/2014/02/12/coding-for-ssds-part-1-introduction-and-table-of-contents/)博客是极好的资源。让我们回到主要的用处：顺序写入和批量写入分别对于旋转式磁盘和SSD来说都是理想的写入模式。大道至简。

查询模式与写入模式明显不同。我们可以查询单个序列的一个数据点，也可以对10000个序列查询一个数据点，还可以查询一个序列几周的数据点，甚至是10000个序列几周的数据点。因此，在我们的二维平面上，查询范围不是完全水平或垂直的，而是形成矩形组合。

[记录规则](https://prometheus.io/docs/practices/rules/)可以减轻已知查询的问题，但对于即席查询并不是通用的解决方法。

我们知道我们希望批量写入，但我们得到的仅仅是一系列垂直数据点的集合。当查询一段时间窗口内的数据点时，我们不仅难以确定单独的点在哪里，还必须从磁盘上的大量随机位置读取。即使在最快的SSD上，一条查询语句可能会涉及数百万个样本，这也会非常慢。读取也会从磁盘上获取更多的数据，而不仅仅是16字节的样本。SSD会加载一整页，HDD至少会读取整个扇区。无论哪种情况，我们都在浪费宝贵的读取吞吐量。

因此，在理想情况下，同一序列的样本将按顺序存储，这样我们就能通过尽可能少的读取来扫描它们。最重要的是，我们只需要知道序列的起始位置就能访问所有的数据点。

显然，将收集到的数据写入磁盘的理想模式与能够显著提高查询效率的布局之间存在明显的冲突。这是我们TSDB需要解决的一个基本问题。

#### 当前的解决方案

现在来看一下Prometheus当前如何存储数据以解决这一问题，我们称之为“V2”。

我们创建一个时间序列文件，包含所有样本并按顺序存储。因为每几秒附加一个样本数据到所有文件中非常昂贵，我们在内存中打包1KiB样本序列的数据块，一旦打包完成就附加这些数据块到单独的文件中。这种方法解决了大部分问题。写入目前是批量的，样本也是按顺序存储的。基于给定的同一序列的样本相对之前的数据仅发生非常小的改变这一特性，它还支持非常高效的压缩格式。Facebook在其Gorilla TSDB论文中描述了一个类似的数据块方法，并引入了一种压缩格式，能够将16字节的样本减少到平均1.37字节。V2存储使用了包括Gorilla变体在内的各种压缩格式。