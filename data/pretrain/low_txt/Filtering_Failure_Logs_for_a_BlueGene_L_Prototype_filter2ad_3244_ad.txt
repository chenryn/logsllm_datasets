### 4.3.2 Network Failures

When multiple hosts report the same hardware failure in a shared network, it indicates a common underlying issue such as a faulty link or switch. Any host attempting to access the shared resources can detect these problems. To accurately represent the number of network failures within the system, instead of counting each record as a separate failure, we need to consolidate them into a single failure event. This consolidation is achieved using the same clustering technique discussed for memory failures, which helps identify redundant records generated by multiple chips.

Partial results after applying this technique are shown in Table 4. The threshold used to form clusters is \( T_{\text{net}} = 15 \) minutes. This threshold is larger than the one used for the memory subsystem (i.e., 1 minute) because some network failures take longer to be resolved. Additionally, the same failures can be reported several times from the same midplane within a few minutes, possibly by different applications (which crash when encountering these failures). In contrast, in the memory subsystem, once an application crashes due to a program bug, the same failure is unlikely to repeat within a short time frame.

There are three typical types of network failures that are often reported simultaneously by a large number of chips:
1. **RTS Tree/Torus Link Training Failed**: Occurs when the chips assigned to an application cannot configure the torus by themselves. Possible causes include link failure, network interface failure, or incorrect configuration parameters.
2. **BIDI Train Failure**: Similar to the first type, this occurs when the torus configuration fails.
3. **External Input Interrupt: Uncorrectable Torus Error**: This happens when an uncorrectable error is encountered, causing the applications to crash.

After clustering, the location of a network failure is denoted by the midplane ID rather than the chip ID, as any chip within that midplane can encounter the failure. For clusters involving two midplanes (where the application is spread across two midplanes), we assume the failure occurs in both midplanes. After filtering, we identified 69 network failures over the 84-day period.

#### 4.3.3 Midplane Switch Failures

Midplane switches interconnect midplanes and are programmed by the control logic of the machine. These events typically report issues with certain ports that cannot be connected or cleared. When the control programs encounter such problems, it indicates a hardware issue, either with the switch itself or the link connecting the switches. If the switch has a problem, only the program running on the switch will report the failure; otherwise, several switches will be affected.

We use the same clustering algorithm (as described in Section 4.3.1) to form clusters and consolidate a failure cluster into a single failure. However, as shown in Figures 2(c) and 3(c), midplane switch failures are usually not clustered, with most cluster sizes being 1 or 2, and only a few reaching sizes of 5 or 6. Figure 2(c) shows the number of records per hour, but the threshold we use to determine the clusters is \( T_{\text{mps}} = 15 \) minutes, similar to the one used in the network subsystem. After coalescing these clustered events, we still have 152 midplane switch failures.

### 4.4 Applying the 3-Step Filtering Algorithm

After applying the 3-step filtering technique, we identified the following failures over the 1921-hour period:
- 61 network failures
- 76 memory failures (55 hardware, 21 software)
- 152 midplane switch failures
- 8 node card failures
- 14 service card failures

Figures 6(a)-(f) present the time series of these failures at an hourly granularity.

### 5 Concluding Remarks

Parallel system event/failure logging in production environments is widely applicable. It provides valuable information on hardware and software failures, aiding designers in making necessary revisions. Fine-grain event logging can accumulate large volumes of data over extended periods and across thousands of nodes. Logging mechanisms can also generate multiple records of the same events, necessitating cleanup for accurate analysis. In this study, we analyzed failures on an 8192-processor BlueGene/L prototype at IBM Rochester, currently ranked #8 in the Top-500 list. We presented a 3-step filtering algorithm:
1. Extracting and categorizing failure events from raw logs.
2. Performing temporal filtering to remove duplicate reports from the same location.
3. Coalescing failure reports across different locations.

Using this approach, we compressed error logs by removing 99.96% of the 828,387 entries recorded between August 26, 2004, and November 17, 2004. This filtering is performed weekly.

### References

[1] Blue Gene/L System Software Update. http://www-unix.mcs.anl.gov/2005/BGL-SSW13-LLNL-exper.pdf.
[2] TOP500 List 11/2004. http://www.top500.org.
[3] N. Adiga et al. An Overview of the BlueGene/L Supercomputer. In Proceedings of Supercomputing (SC2002), November 2002.
[4] D. Anderson, J. Dykes, and E. Riedel. More Than an Interface – SCSI vs. ATA. In Proceedings of the Conference on File and Storage Technology (FAST), March 2003.
[5] BlueGene/L Workshop. http://www.llnl.gov/asci/platforms/bluegene/.
[6] IBM Research. http://www.research.ibm.com/bluegene/.
[7] D. Brooks and M. Martonosi. Dynamic Thermal Management for High-Performance Microprocessors. In Proceedings of the Seventh International Symposium on High-Performance Computer Architecture (HPCA-7), January 2001.
[8] M. F. Buckley and D. P. Siewiorek. VAX/VMS Event Monitoring and Analysis. In FTCS-25, Computing Digest of Papers, pages 414–423, June 1995.
[9] M. F. Buckley and D. P. Siewiorek. Comparative Analysis of Event Tupling Schemes. In FTCS-26, Computing Digest of Papers, pages 294–303, June 1996.
[10] M. L. Fair, C. R. Conklin, S. B. Swaney, P. J. Meaney, W. J. Clarke, L. C. Alves, I. N. Modi, F. Freier, W. Fischer, and N. E. Weber. Reliability, Availability, and Serviceability (RAS) of the IBM eServer z990. IBM Journal of Research and Development, 48(3/4), 2004.
[11] R. L. Graham, S. E. Choi, D. J. Daniel, N. N. Desai, R. G. Minnich, C. E. Rasmussen, L. D. Risinger, and M. W. Sukalski. A Network-Failure-Tolerant Message-Passing System for Terascale Clusters. In Proceedings of the International Conference on Supercomputing, 2002.
[12] J. Hansen. Trend Analysis and Modeling of Uni/Multi-Processor Event Logs. PhD thesis, Dept. of Computer Science, Carnegie-Mellon University, 1988.
[13] G. Herbst. IBM’s Drive Temperature Indicator Processor (Drive-TIP) Helps Ensure High Drive Reliability. IBM White Paper, October 1997.
[14] R. Iyer, L. T. Young, and V. Sridhar. Recognition of Error Symptoms in Large Systems. In Proceedings of the Fall Joint Computer Conference, 1986.
[15] E. Krevat, J. G. Castanos, and J. E. Moreira. Job Scheduling for the BlueGene/L System. In Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing, 2003.
[16] T. Y. Lin and D. P. Siewiorek. Error Log Analysis: Statistical Modelling and Heuristic Trend Analysis. IEEE Trans. on Reliability, 39(4):419–432, October 1990.
[17] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt, and T. Austin. A Systematic Methodology to Compute the Architectural Vulnerability Factors for a High-Performance Microprocessor. In Proceedings of the International Symposium on Microarchitecture (MICRO), pages 29–40, 2003.
[18] A. Parashar, S. Gurumurthi, and A. Sivasubramaniam. A Complexity-Effective Approach to ALU Bandwidth Enhancement for Instruction-Level Temporal Redundancy. In Proceedings of the International Symposium on Computer Architecture (ISCA), 2004.
[19] R. Sahoo, A. Sivasubramaniam, M. Squillante, and Y. Zhang. Failure Data Analysis of a Large-Scale Heterogeneous Server Environment. In Proceedings of the 2004 International Conference on Dependable Systems and Networks, pages 389–398, 2004.
[20] P. Shivakumar, M. Kistler, S. Keckler, D. Burger, and L. Alvisi. Modeling the Effect of Technology Trends on Soft Error Rate of Combinational Logic. In Proceedings of the 2002 International Conference on Dependable Systems and Networks, pages 389–398, 2002.
[21] K. Skadron, M. R. Stan, W. Huang, S. Velusamy, K. Sankaranarayanan, and D. Tarjan. Temperature-Aware Microarchitecture. In Proceedings of the International Symposium on Computer Architecture (ISCA), pages 1–13, June 2003.
[22] J. Srinivasan, S. V. Adve, P. Bose, and J. A. Rivers. The Impact of Technology Scaling on Processor Lifetime Reliability. In Proceedings of the International Conference on Dependable Systems and Networks (DSN-2004), June 2004.
[23] D. Tang and R. K. Iyer. Impact of Correlated Failures on Dependability in a VAXcluster System. In Proceedings of the IFIP Working Conference on Dependable Computing for Critical Applications, 1991.
[24] D. Tang and R. K. Iyer. Analysis and Modeling of Correlated Failures in Multicomputer Systems. IEEE Transactions on Computers, 41(5):567–577, 1992.
[25] D. Tang, R. K. Iyer, and S. S. Subramani. Failure Analysis and Modelling of a VAXcluster System. In Proceedings of the International Symposium on Fault-tolerant Computing, pages 244–251, 1990.
[26] M. M. Tsao. Trend Analysis and Fault Prediction. PhD thesis, Dept. of Computer Science, Carnegie-Mellon University, 1983.
[27] K. Vaidyanathan, R. E. Harper, S. W. Hunter, and K. S. Trivedi. Analysis and Implementation of Software Rejuvenation in Cluster Systems. In Proceedings of the ACM SIGMETRICS 2001 Conference on Measurement and Modeling of Computer Systems, pages 62–71, June 2001.
[28] J. Xu, Z. Kallbarczyk, and R. K. Iyer. Networked Windows NT System Field Failure Data Analysis. Technical Report CRHC 9808 UIUC, 1999.
[29] J. F. Zeigler. Terrestrial Cosmic Rays. IBM Journal of Research and Development, 40(1):19–39, January 1996.