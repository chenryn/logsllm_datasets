# Performance Measurement and Analysis of H.323 Traffic

**Authors:**
- Prasad Calyam
- Mukundan Sridharan
- Weiping Mandrawa
- Paul Schopis

**Affiliations:**
1. OARnet, 1224 Kinnear Road, Columbus, Ohio 43212.
   - {pcalyam, wmandraw, pschopis}@oar.net
2. Department of Computer and Information Science, The Ohio State University, Columbus, OH 43210.

**Abstract:**
The widespread use of H.323 applications is evident from the billions of minutes of audio and video traffic on the Internet each month. This paper aims to establish performance bounds (Good, Acceptable, and Poor) for network metrics such as delay, jitter, and loss for H.323 applications, based on both objective and subjective quality assessments of various audio and video streams. To gather the necessary data, we utilized the H.323 Beacon tool we developed and conducted a series of videoconferencing tasks in both a local area network (LAN) and across multiple continents via diverse network paths on the Internet.

## 1. Introduction
H.323 [1] is a comprehensive standard for real-time multimedia communications, including audio and video conferencing, over packet-switched networks like the Internet. With the increasing use of H.323 systems in both industry and academia, there is a growing demand for better audio and video performance. Understanding how network conditions affect the perceived quality of H.323 applications is crucial. Several studies [2, 3, 4] and approaches [5, 6, 7] have been proposed to measure the performance quality of H.323 applications. These studies often use pre-recorded audio and video streams and focus on either network variations or audiovisual quality assessment methods.

In this paper, we focus on understanding how different levels of network health, characterized by delay, jitter, and loss, affect end-user perception of audiovisual quality. By systematically emulating various network scenarios and using a set of videoconferencing tasks, we determine performance bounds for these metrics. These bounds are then mapped to end-users' perceptions of overall audiovisual quality and categorized into grades such as Good, Acceptable, and Poor. Our results show that users are more sensitive to variations in jitter than in delay or loss. This study provides ISPs and videoconferencing operators with a better understanding of their end-users' experience of audiovisual quality under different network conditions.

To collect the necessary data, we used the H.323 Beacon tool [8] and a set of videoconferencing tasks. Over 500 one-on-one subjective quality assessments were collected from videoconferencing users, along with corresponding H.323 traffic traces. The testing included various network health scenarios in an isolated LAN environment and on the Internet, involving 26 videoconferencing endpoints across multiple continents, connected via different network paths.

The rest of the paper is organized as follows: Section 2 provides background information, Section 3 describes our testing methodology, Section 4 discusses the analysis of performance bounds for delay, jitter, and loss, and Section 5 concludes the paper.

## 2. Background

### 2.1 H.323 System Architecture
Several factors influence the performance of H.323 applications, which can be categorized into three groups: human factors, device factors, and network factors. Human factors include the perception of audio and video quality and human error due to negligence or lack of training. Device factors involve essential components such as H.323 terminals, Multipoint Control Units (MCUs), gatekeepers, firewalls, and Network Address Translators (NATs). Network factors include route changes, competing traffic, and congestion. In this paper, we focus on human factors related to end-user perception and network factors affecting network health. For details on device factors, see [9].

### 2.2 Audiovisual Quality Assessment Metrics
There are two primary methods for assessing audiovisual quality: subjective and objective. Subjective quality assessment involves playing audiovisual clips to participants and collecting their judgments. Objective quality assessment uses automated procedures such as signal-to-noise ratio (SNR) measurements and algorithms like Mean Square Error (MSE), Frequency weighted MSE, Segmented SNR, Perceptual Analysis Measurement System (PAMS) [10], Perceptual Evaluation of Speech Quality (PESQ) [11], and Emodel [5]. Subjective methods are influenced by individual perceptions, while objective methods may not fully reflect the end-user experience. Studies [12] show that combining both methods yields comparable results.

In our study, we use both subjective and objective quality assessment methods. For subjective assessments, we extended the slider methodology from [7] and integrated it into our H.323 Beacon tool. Participants rated the audiovisual quality on a scale of 1 to 5 using the Mean Opinion Score (MOS) technique. For objective assessments, we used the Telchemy VQMon tool [12], which implements the Emodel and uses traffic traces as input. The Emodel predicts subjective quality using a psycho-acoustic R-scale (0-100) that can be mapped to MOS rankings and user satisfaction (Fig. 1). Our data shows a reasonable correlation between subjective and objective quality scores. For more details on the Emodel, see [2, 5, 12].

### 2.3 Network Performance Metrics
The most influential variables in H.323 system deployments are network dynamics caused by route fluctuations, competing traffic, and congestion. These dynamics can be characterized by delay, jitter, and loss [13].

- **Delay** is the time a packet takes to travel from the sender to the receiver. It includes compression and transmission delays at the sender, propagation, processing, and queuing delays in the network, and buffering and decompression delays at the receiver. Recommended delay bounds for perceived performance are: Good (0ms-150ms), Acceptable (150ms-300ms), Poor (>300ms) [14].
  
- **Jitter** is the variation in packet arrival times, caused by congestion, varying packet sizes, out-of-order delivery, and other factors. Excessive jitter can cause packet discards and affect playback. Our studies suggest the following jitter bounds: Good (0ms-20ms), Acceptable (20ms-50ms), Poor (>50ms).

- **Loss** is the percentage of packets that do not reach their destination. Loss levels greater than 1% can severely affect audiovisual quality. Our studies suggest the following loss bounds: Good (0%-0.5%), Acceptable (0.5%-1.5%), Poor (>1.5%).

## 3. Test Methodology

### 3.1 Design of Experiments
Our approach to determining performance bounds for delay, jitter, and loss involves viewing network health as the combined effect of these parameters. All three parameters coexist and interact, affecting the Quality of Service (QoS) perceived by the end-user. For example, resolving a loss problem can unexpectedly increase jitter [16]. Therefore, network health must be maintained within Good performance bounds. We used a full factorial design to emulate 27 scenarios covering all possible permutations of delay, jitter, and loss levels.

We performed extensive LAN tests for all 27 scenarios and selected 9 representative scenarios for Internet measurements (Table 1 in Section 4). Each scenario involved a videoconferencing task, such as a casual conversation, intense discussion, or class lecture. The tasks were designed following guidelines from [6, 7] to ensure realistic scenarios. A subset of tasks used the H.323 Beacon tool's loopback feature for local playback of remote audio and video. For more details on the H.323 Beacon, see [8].

### 3.2 Test Setup
To obtain performance bounds, we used a two-phase approach. In the first phase, we emulated all 27 scenarios in a LAN environment. In the second phase, we tested 9 scenarios on the Internet. Both phases involved one-on-one testing with participants, collecting traffic traces, and conducting subjective and objective quality assessments. Figure 2 shows the overall test setup, and Figure 3 shows the participating sites and their last-mile network connections. We used the NISTnet [17] network emulator to create the various network health scenarios.

---

**References:**
1. ITU-T Recommendation H.323 (11/1996)
2. ...
3. ...
4. ...
5. ...
6. ...
7. ...
8. ...
9. ...
10. ...
11. ...
12. ...
13. ...
14. ...
15. ...
16. ...
17. ...

---

This revised version improves the clarity, coherence, and professionalism of the original text. It ensures that the content is well-organized and easy to follow, with clear headings and subheadings.