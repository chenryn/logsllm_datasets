[{"prompt": "User: 下面是一篇故障报告： # Summary of the AWS Lambda Service Event in Northern Virginia (US-EAST-1) Region  \nWe wanted to provide you with some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on June 13th, 2023.\n**Issue Summary** \nStarting at 11:49 AM PDT on June 13th, 2023, customers experienced increased error rates and latencies for Lambda function invocations within the Northern Virginia (US-EAST-1) Region. Some other AWS services -- including Amazon STS, AWS Management Console, Amazon EKS, Amazon\nConnect, and Amazon EventBridge -- also experienced increased error rates and latencies as a result of the degraded Lambda function\ninvocations. Lambda function invocations began to return to normal levels at 1:45 PM PDT, and all affected services had fully recovered by 3:37 PM PDT.\nTo explain this event, we need to share a little about the internals of AWS Lambda. AWS Lambda makes use of a cellular architecture, where each cell consists of multiple subsystems to serve function invocations for customer code. First, the Lambda Frontend is responsible for receiving and routing customer function invocations. Second, the Lambda Invocation\nManager is responsible for managing the underlying compute capacity -- in the form of Lambda Execution Environments -- depending on the scale of function invocation traffic on a per-function, per-account basis. \n At 10:01 AM PDT, the Lambda Frontend fleet started scaling in response to an increase in service traffic, within the normal daily traffic patterns, in the Northern Virginia (US-EAST-1) Region. At 11:49 AM PDT, the Lambda Frontend fleet, while adding additional compute capacity to handle the increase in service traffic, crossed a capacity threshold that had previously never been reached within a single cell. This triggered a latent software defect which caused Lambda Execution\nEnvironments to be successfully allocated for incoming requests, but never fully utilized by the Lambda Frontend. Since Lambda was not able to provision new Lambda Execution Environments for incoming requests, function invocations within the affected cell experienced increased error rates and latencies. Customers triggering Lambda functions through asynchronous or streaming event sources also saw an increase in their event backlog since the events were not being processed. Lambda function\ninvocations within other Lambda cells were not affected by this event.\nEngineering teams were immediately engaged and began investigating. By 12:26 PM PDT, engineers had identified the latent software defect and the impact on the provisioning of underlying compute capacity. As an\nimmediate mitigation, engineers were able to confirm that traffic levels had subsided, and initiated a scale down of the Lambda Frontend fleet to a level that no longer triggered the latent software defect. By 1:30 PM PDT, new Lambda function invocations began to see recovery. By 1:45 PM PDT, Lambda was fully recovered for synchronous function invocations. At this stage, the vast majority of affected AWS services began to fully recover. Between 1:45 PM and 3:37 PM PDT, Lambda completed processing the backlog of asynchronous events from various event sources, consistent with customer specified event source specific retry policies. By 3:37 PM PDT, the AWS Lambda service had resumed normal operations, and all services dependent on Lambda were operating normally. \nWe have taken several actions to prevent a recurrence of this event. We immediately disabled the scaling activities for the Lambda Frontend fleet activities that triggered the event, while we worked to address the latent bug that caused the issue; this bug has since been resolved and deployed to all Regions. This event also uncovered a gap in our Lambda cellular architecture for the scaling of the Lambda Frontend, which allowed a latent bug to cause impact as the affected cell scaled. Lambda has already completed several action items to address the immediate concern with cellular scaling and remains on track to complete a larger effort later this year to ensure that all cells are bounded to a well-tested size to avoid future unexpected scaling issues.\n**AWS Service Impact** \nSeveral AWS services experienced impact as a result of increased error rates and latencies for Lambda function invocations. Amazon Secure Token\nService (STS) experienced elevated error rates between 11:49 AM and 2:10 PM PDT with three distinct periods of impact. AWS Sign-in experienced elevated error rates for SAML federation in the US-EAST-1 Region between\n1:13 PM and 2:11 PM PDT. Customers using AWS Sign-in to federate from external identity providers (IDP) using SAML, experienced elevated error rates when throttles were put in place within Amazon STS. Existing IAM sessions were not impacted, but new sign-in federation via SAML was degraded. \nAmazon EventBridge supports routing events to Lambda and experienced elevated delivery latencies of up to 801 seconds between 11:49 AM and 1:45 PM PDT. Amazon EKS experience increased errors rates and latencies during the provisioning of new EKS clusters, however existing EKS clusters were not affected. At 1:45 PM PDT, these error rates returned to normal levels when the Lambda function invocation issue was resolved. AWS Management Console for the Northern Virginia (US-EAST-1) Region\nexperienced elevated error rates from 11:48 AM to 2:02 PM PDT. During this time, customers accessing the AWS Management Console saw either an 'AWS Management Console is currently unavailable', or a '504 Time-out' error page. AWS Management Consoles outside of the Northern\nVirginia (US-EAST-1) Region were not affected by this event.\nAmazon Connect uses AWS Lambda to process contacts and agent events. As a result of the Lambda event, Amazon Connect experienced degraded contact handling between 11:49 AM and 1:40 PM PDT. During this time, calls would have failed to connect, and chats and tasks would have failed to initiate. Agents would have experienced issues logging in and using Connect. AWS Support Center functionality was degraded between 11:49 AM and 2:38 PM PDT. During the first eleven minutes of impact, requests to create/view/update support cases may have failed. By 12:00 PM we had restored access to support cases. However, our call and chat functionality remained impaired, making these channels of communication unavailable. The ability to create/view/update cases via the web/email method was not impacted. AWS Support Center functionality was fully restored by 2:38 PM PDT after dependent services recovered alongside AWS Lambda.\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。 \nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（Logical）、桥接性（Bridging）、可操作性（Actionable）、简洁性（Simplicity）和流程完整性（Process Integrity）几个角度来分析AWS服务事件，并按照术语（Terminology）、背景（Background）、常见问题解答（FAQ）、流程（Flow）和附录（Appendix）进行总结整理。 \n### 术语（Terminology） \n- **AWS Lambda**: 一种无服务器计算服务，允许用户运行代码而无需预置或管理服务器。 - **Lambda Frontend**: 负责接收和路由客户函数调用的部分。 - **Lambda Invocation Manager**: 负责根据函数调用流量管理计算能力的组件。 - **Lambda Execution Environments**: 执行Lambda函数的环境。 - **EventBridge**: 支持将事件路由到Lambda等服务的AWS服务。 \n### 背景（Background） \n在2023年6月13日，美国东部时间上午11:49开始，位于北弗吉尼亚州（US-EAST-1）地区的AWS服务发生了中断。Lambda函数调用的错误率和延迟增加，影响了其他AWS服务，如Amazon STS、AWS管理控制台、Amazon EKS、Amazon Connect和Amazon EventBridge。Lambda函数调用在下午1:45恢复正常水平，所有受影响的服务在下午3:37完全恢复。 \n### 常见问题解答（FAQ） \n**Q: 事件的根本原因是什么？** A: Lambda Frontend在增加计算能力以应对服务流量增加时，触发了一个以前未达到的容量阈值，导致一个潜在的软件缺陷被触发，从而影响了Lambda Execution Environments的充分利用。 \n**Q: AWS如何应对这种情况？** A: 工程团队立即介入并开始调查。他们通过减少Lambda Frontend的规模来缓解问题，并解决了潜在的软件缺陷。 \n**Q: 客户如何受到影响？** A: 客户通过异步或流事件源触发的Lambda函数调用看到事件积压增加，因为事件没有得到处理。 \n### 流程（Flow） \n1. **服务中断发生**：由于Lambda Frontend的规模扩大，触发了软件缺陷。 2. **问题识别**：工程师在中午12:26识别了软件缺陷及其对计算能力配置的影响。 3. **缓解措施**：通过降低Lambda Frontend的规模来避免触发缺陷。 4. **恢复**：下午1:30开始，新的Lambda函数调用开始恢复。 5. **服务完全恢复**：下午1:45，Lambda同步函数调用恢复正常，大多数受影响的AWS服务开始完全恢复。 6. **积压处理**：Lambda在下午3:37之前完成了异步事件的积压处理。 \n### 附录（Appendix） \n- **受影响的服务**：包括Amazon STS、AWS管理控制台、Amazon EKS、Amazon Connect和Amazon EventBridge。 - **后续行动**：AWS已禁用触发事件的Lambda Frontend规模活动，并解决了潜在的软件缺陷。同时，Lambda已经完成了针对细胞规模的几个行动项目，并计划在今年晚些时候完成更大的工作，以确保所有细胞都被限制在经过良好测试的规模内，避免未来的意外规模问题。 \n通过上述分析和总结，我们可以清晰地理解AWS服务中断事件的逻辑、桥接性、可操作性、简洁性和流程完整性，并为客户提供了详细的背景信息、常见问题解答、事件处理流程和相关附录信息。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region\n\n**November, 25th 2020** \nWe wanted to provide you with some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on November 25th, 2020. \nAmazon Kinesis enables real-time processing of streaming data. In\naddition to its direct use by customers, Kinesis is used by several other AWS services. These services also saw impact during the event. The trigger, though not root cause, for the event was a relatively small addition of capacity that began to be added to the service at 2:44 AM PST, finishing at 3:47 AM PST. Kinesis has a large number of \"back-end\" cell-clusters that process streams. These are the workhorses in Kinesis, providing distribution, access, and scalability for stream processing. Streams are spread across the back-end through a sharding mechanism owned by a \"front-end\" fleet of servers. A back-end cluster owns many shards and provides a consistent scaling unit and fault-isolation. The front-end's job is small but important. It handles authentication, throttling, and request-routing to the correct stream-shards on the back-end clusters. \nThe capacity addition was being made to the front-end fleet. Each server in the front-end fleet maintains a cache of information, including membership details and shard ownership for the back-end clusters, called a shard-map. This information is obtained through calls to a microservice vending the membership information, retrieval of configuration information from DynamoDB, and continuous processing of messages from other Kinesis front-end servers. For the latter communication, each front-end server creates operating system threads for each of the other servers in the front-end fleet. Upon any addition\nof capacity, the servers that are already operating members of the fleet will learn of new servers joining and establish the appropriate threads. It takes up to an hour for any existing front-end fleet member to learn\nof new participants. \nAt 5:15 AM PST, the first alarms began firing for errors on putting and getting Kinesis records. Teams engaged and began reviewing logs. While the new capacity was a suspect, there were a number of errors that were unrelated to the new capacity and would likely persist even if the capacity were to be removed. Still, as a precaution, we began removing the new capacity while researching the other errors. The diagnosis work was slowed by the variety of errors observed. We were seeing errors in\nall aspects of the various calls being made by existing and new members of the front-end fleet, exacerbating our ability to separate side-effects from the root cause. At 7:51 AM PST, we had narrowed the root cause to a couple of candidates and determined that any of the most likely sources of the problem would require a full restart of the front-end fleet, which the Kinesis team knew would be a long and careful process. The resources within a front-end server that are used to populate the shard-map compete with the resources that are used to process incoming requests. So, bringing front-end servers back online too quickly would create contention between these two needs and result in very few resources being available to handle incoming requests, leading to increased errors and request latencies. As a result, these slow front-end servers could be deemed unhealthy and removed from the fleet, which in turn, would set back the recovery process. All of the candidate solutions involved changing every front-end server's configuration and restarting it. While the leading candidate (an issue that seemed to be creating memory pressure) looked promising, if we were wrong, we would double the recovery time as we would need to apply a second fix and restart again. To speed restart, in parallel with our investigation, we began adding a configuration to the front-end servers to obtain data directly from the authoritative metadata store rather than from front-end server neighbors during the bootstrap process. \nAt 9:39 AM PST, we were able to confirm a root cause, and it turned out this wasn't driven by memory pressure. Rather, the new capacity had caused all of the servers in the fleet to exceed the maximum number of threads allowed by an operating system configuration. As this limit was being exceeded, cache construction was failing to complete and front-end servers were ending up with useless shard-maps that left them unable to route requests to back-end clusters. We didn't want to increase the operating system limit without further testing, and as we had just completed the removal of the additional capacity that triggered the event, we determined that the thread count would no longer exceed the operating system limit and proceeded with the restart. We began bringing back the front-end servers with the first group of servers taking Kinesis traffic at 10:07 AM PST. The front-end fleet is composed of many thousands of servers, and for the reasons described earlier, we could only add servers at the rate of a few hundred per hour. We continued to slowly add traffic to the front-end fleet with the Kinesis error rate steadily dropping from noon onward. Kinesis fully returned to normal at 10:23 PM PST. \nFor Kinesis, we have a number of learnings that we will be implementing immediately. In the very short term, we will be moving to larger CPU and memory servers, reducing the total number of servers and, hence, threads required by each server to communicate across the fleet. This will provide significant headroom in thread count used as the total threads each server must maintain is directly proportional to the number of servers in the fleet. Having fewer servers means that each server maintains fewer threads. We are adding fine-grained alarming for thread consumption in the service. We will also finish testing an increase in\nthread count limits in our operating system configuration, which we believe will give us significantly more threads per server and give us significant additional safety margin there as well. In addition, we are making a number of changes to radically improve the cold-start time for the front-end fleet. We are moving the front-end server cache to a dedicated fleet. We will also move a few large AWS services, like CloudWatch, to a separate, partitioned front-end fleet. In the medium term, we will greatly accelerate the cellularization of the front-end fleet to match what we've done with the back-end. Cellularization is an\napproach we use to isolate the effects of failure within a service, and to keep the components of the service (in this case, the shard-map cache) operating within a previously tested and operated range. This had been under way for the front-end fleet in Kinesis, but unfortunately the work is significant and had not yet been completed. In addition to allowing us to operate the front-end in a consistent and well-tested range of total threads consumed, cellularization will provide better protection against any future unknown scaling limit. \nThere were a number of services that use Kinesis that were impacted as well. Amazon Cognito uses Kinesis Data Streams to collect and analyze API access patterns. While this information is extremely useful for operating the Cognito service, this information streaming is designed to be best effort. Data is buffered locally, allowing the service to cope with latency or short periods of unavailability of the Kinesis Data Stream service. Unfortunately, the prolonged issue with Kinesis Data Streams triggered a latent bug in this buffering code that caused the Cognito webservers to begin to block on the backlogged Kinesis Data Stream buffers. As a result, Cognito customers experienced elevated API failures and increased latencies for Cognito User Pools and Identity Pools, which prevented external users from authenticating or obtaining temporary AWS credentials. In the early stages of the event, the Cognito team worked to mitigate the impact of the Kinesis errors by adding additional capacity and thereby increasing their capacity to buffer calls to Kinesis. While this initially reduced impact, by 7:01 AM PST errors rates increased significantly. The team was working in parallel on a change to Cognito to reduce the dependency on Kinesis. At 10:15 AM PST, deployment of this change began and error rates began falling. By 12:15 PM PST, error rates were significantly reduced, and by 2:18 PM PST Cognito was operating normally. To prevent a recurrence of this issue, we have modified the Cognito webservers so that they can sustain Kinesis API errors without exhausting their buffers that resulted in these user errors. \nCloudWatch uses Kinesis Data Streams for the processing of metric and log data. Starting at 5:15 AM PST, CloudWatch experienced increased error rates and latencies for the PutMetricData and PutLogEvents APIs, and alarms transitioned to the INSUFFICIENT_DATA state. While some CloudWatch metrics continued to be processed throughout the event, the increased error rates and latencies prevented the vast majority of metrics from being successfully processed. At 5:47 PM PST, CloudWatch began to see early signs of recovery as Kinesis Data Stream's availability improved, and by 10:31 PM PST, CloudWatch metrics and alarms fully recovered. Delayed metrics and log data backfilling completed over the subsequent hours. While CloudWatch was experiencing these increased errors, both internal and external clients were unable to persist all metric data to the CloudWatch service. These errors will manifest as gaps in data in CloudWatch metrics. While CloudWatch currently relies on Kinesis for its complete metrics and logging capabilities, the CloudWatch team is making a change to persist 3-hours of metric data in the CloudWatch local metrics data store. This change will allow CloudWatch users, and services requiring CloudWatch metrics (including AutoScaling), to access these recent metrics directly from the CloudWatch local metrics data store. This change has been completed in the US-EAST-1 Region and will be deployed globally in the coming weeks. \nTwo services were also impacted as a result of the issues with CloudWatch metrics. First, reactive AutoScaling policies that rely on\nCloudWatch metrics experienced delays until CloudWatch metrics began to recover at 5:47 PM PST. And second, Lambda saw impact. Lambda function\ninvocations currently require publishing metric data to CloudWatch as part of invocation. Lambda metric agents are designed to buffer metric data locally for a period of time if CloudWatch is unavailable. Starting at 6:15 AM PST, this buffering of metric data grew to the point that it caused memory contention on the underlying service hosts used for Lambda function invocations, resulting in increased error rates. At 10:36 AM PST, engineers took action to mitigate the memory contention, which resolved the increased error rates for function invocations. \nCloudWatch Events and EventBridge experienced increased API errors and delays in event processing starting at 5:15 AM PST. As Kinesis availability improved, EventBridge began to deliver new events and slowly process the backlog of older events. Elastic Container Service (ECS) and Elastic Kubernetes Service (EKS) both make use of EventBridge to drive internal workflows used to manage customer clusters and tasks. This impacted provisioning of new clusters, delayed scaling of existing clusters, and impacted task de-provisioning. By 4:15 PM PST, the majority of these issues had been resolved. \nOutside of the service issues, we experienced some delays in\ncommunicating service status to customers during the early part of this event. We have two ways of communicating during operational events -- the Service Health Dashboard, which is our public dashboard to alert all customers of broad operational issues, and the Personal Health Dashboard, which we use to communicate directly with impacted customers. With an event such as this one, we typically post to the Service Health Dashboard. During the early part of this event, we were unable to update the Service Health Dashboard because the tool we use to post these updates itself uses Cognito, which was impacted by this event. We have a back-up means of updating the Service Health Dashboard that has minimal service dependencies. While this worked as expected, we encountered several delays during the earlier part of the event in posting to the Service Health Dashboard with this tool, as it is a more manual and less familiar tool for our support operators. To ensure customers were getting timely updates, the support team used the Personal Health Dashboard to notify impacted customers if they were impacted by the service issues. We also posted a global banner summary on the Service Health Dashboard to ensure customers had broad visibility into the event. During the remainder of event, we continued using a combination\nof the Service Health Dashboard, both with global banner summaries and service specific details, while also continuing to update impacted customers via Personal Health Dashboard. Going forward, we have changed our support training to ensure that our support engineers are regularly trained on the backup tool for posting to the Service Health Dashboard. \nFinally, we want to apologize for the impact this event caused for our customers. While we are proud of our long track record of availability with Amazon Kinesis, we know how critical this service, and the other AWS services that were impacted, are to our customers, their applications and end users, and their businesses. We will do everything we can to learn from this event and use it to improve our availability even further.\n\nAssistant:", "response": [["Scaling the front-end cache fleet for Kinesis caused all of the servers in the fleet to exceed the maximum number of threads allowed by an operating system configuration. Multiple critical downstream services affected, from Cognito to Lambda to CloudWatch"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the Amazon Kinesis Event in the Northern Virginia (US-EAST-1) Region\n\n**November, 25th 2020** \nWe wanted to provide you with some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on November 25th, 2020. \nAmazon Kinesis enables real-time processing of streaming data. In\naddition to its direct use by customers, Kinesis is used by several other AWS services. These services also saw impact during the event. The trigger, though not root cause, for the event was a relatively small addition of capacity that began to be added to the service at 2:44 AM PST, finishing at 3:47 AM PST. Kinesis has a large number of \"back-end\" cell-clusters that process streams. These are the workhorses in Kinesis, providing distribution, access, and scalability for stream processing. Streams are spread across the back-end through a sharding mechanism owned by a \"front-end\" fleet of servers. A back-end cluster owns many shards and provides a consistent scaling unit and fault-isolation. The front-end's job is small but important. It handles authentication, throttling, and request-routing to the correct stream-shards on the back-end clusters. \nThe capacity addition was being made to the front-end fleet. Each server in the front-end fleet maintains a cache of information, including membership details and shard ownership for the back-end clusters, called a shard-map. This information is obtained through calls to a microservice vending the membership information, retrieval of configuration information from DynamoDB, and continuous processing of messages from other Kinesis front-end servers. For the latter communication, each front-end server creates operating system threads for each of the other servers in the front-end fleet. Upon any addition\nof capacity, the servers that are already operating members of the fleet will learn of new servers joining and establish the appropriate threads. It takes up to an hour for any existing front-end fleet member to learn\nof new participants. \nAt 5:15 AM PST, the first alarms began firing for errors on putting and getting Kinesis records. Teams engaged and began reviewing logs. While the new capacity was a suspect, there were a number of errors that were unrelated to the new capacity and would likely persist even if the capacity were to be removed. Still, as a precaution, we began removing the new capacity while researching the other errors. The diagnosis work was slowed by the variety of errors observed. We were seeing errors in\nall aspects of the various calls being made by existing and new members of the front-end fleet, exacerbating our ability to separate side-effects from the root cause. At 7:51 AM PST, we had narrowed the root cause to a couple of candidates and determined that any of the most likely sources of the problem would require a full restart of the front-end fleet, which the Kinesis team knew would be a long and careful process. The resources within a front-end server that are used to populate the shard-map compete with the resources that are used to process incoming requests. So, bringing front-end servers back online too quickly would create contention between these two needs and result in very few resources being available to handle incoming requests, leading to increased errors and request latencies. As a result, these slow front-end servers could be deemed unhealthy and removed from the fleet, which in turn, would set back the recovery process. All of the candidate solutions involved changing every front-end server's configuration and restarting it. While the leading candidate (an issue that seemed to be creating memory pressure) looked promising, if we were wrong, we would double the recovery time as we would need to apply a second fix and restart again. To speed restart, in parallel with our investigation, we began adding a configuration to the front-end servers to obtain data directly from the authoritative metadata store rather than from front-end server neighbors during the bootstrap process. \nAt 9:39 AM PST, we were able to confirm a root cause, and it turned out this wasn't driven by memory pressure. Rather, the new capacity had caused all of the servers in the fleet to exceed the maximum number of threads allowed by an operating system configuration. As this limit was being exceeded, cache construction was failing to complete and front-end servers were ending up with useless shard-maps that left them unable to route requests to back-end clusters. We didn't want to increase the operating system limit without further testing, and as we had just completed the removal of the additional capacity that triggered the event, we determined that the thread count would no longer exceed the operating system limit and proceeded with the restart. We began bringing back the front-end servers with the first group of servers taking Kinesis traffic at 10:07 AM PST. The front-end fleet is composed of many thousands of servers, and for the reasons described earlier, we could only add servers at the rate of a few hundred per hour. We continued to slowly add traffic to the front-end fleet with the Kinesis error rate steadily dropping from noon onward. Kinesis fully returned to normal at 10:23 PM PST. \nFor Kinesis, we have a number of learnings that we will be implementing immediately. In the very short term, we will be moving to larger CPU and memory servers, reducing the total number of servers and, hence, threads required by each server to communicate across the fleet. This will provide significant headroom in thread count used as the total threads each server must maintain is directly proportional to the number of servers in the fleet. Having fewer servers means that each server maintains fewer threads. We are adding fine-grained alarming for thread consumption in the service. We will also finish testing an increase in\nthread count limits in our operating system configuration, which we believe will give us significantly more threads per server and give us significant additional safety margin there as well. In addition, we are making a number of changes to radically improve the cold-start time for the front-end fleet. We are moving the front-end server cache to a dedicated fleet. We will also move a few large AWS services, like CloudWatch, to a separate, partitioned front-end fleet. In the medium term, we will greatly accelerate the cellularization of the front-end fleet to match what we've done with the back-end. Cellularization is an\napproach we use to isolate the effects of failure within a service, and to keep the components of the service (in this case, the shard-map cache) operating within a previously tested and operated range. This had been under way for the front-end fleet in Kinesis, but unfortunately the work is significant and had not yet been completed. In addition to allowing us to operate the front-end in a consistent and well-tested range of total threads consumed, cellularization will provide better protection against any future unknown scaling limit. \nThere were a number of services that use Kinesis that were impacted as well. Amazon Cognito uses Kinesis Data Streams to collect and analyze API access patterns. While this information is extremely useful for operating the Cognito service, this information streaming is designed to be best effort. Data is buffered locally, allowing the service to cope with latency or short periods of unavailability of the Kinesis Data Stream service. Unfortunately, the prolonged issue with Kinesis Data Streams triggered a latent bug in this buffering code that caused the Cognito webservers to begin to block on the backlogged Kinesis Data Stream buffers. As a result, Cognito customers experienced elevated API failures and increased latencies for Cognito User Pools and Identity Pools, which prevented external users from authenticating or obtaining temporary AWS credentials. In the early stages of the event, the Cognito team worked to mitigate the impact of the Kinesis errors by adding additional capacity and thereby increasing their capacity to buffer calls to Kinesis. While this initially reduced impact, by 7:01 AM PST errors rates increased significantly. The team was working in parallel on a change to Cognito to reduce the dependency on Kinesis. At 10:15 AM PST, deployment of this change began and error rates began falling. By 12:15 PM PST, error rates were significantly reduced, and by 2:18 PM PST Cognito was operating normally. To prevent a recurrence of this issue, we have modified the Cognito webservers so that they can sustain Kinesis API errors without exhausting their buffers that resulted in these user errors. \nCloudWatch uses Kinesis Data Streams for the processing of metric and log data. Starting at 5:15 AM PST, CloudWatch experienced increased error rates and latencies for the PutMetricData and PutLogEvents APIs, and alarms transitioned to the INSUFFICIENT_DATA state. While some CloudWatch metrics continued to be processed throughout the event, the increased error rates and latencies prevented the vast majority of metrics from being successfully processed. At 5:47 PM PST, CloudWatch began to see early signs of recovery as Kinesis Data Stream's availability improved, and by 10:31 PM PST, CloudWatch metrics and alarms fully recovered. Delayed metrics and log data backfilling completed over the subsequent hours. While CloudWatch was experiencing these increased errors, both internal and external clients were unable to persist all metric data to the CloudWatch service. These errors will manifest as gaps in data in CloudWatch metrics. While CloudWatch currently relies on Kinesis for its complete metrics and logging capabilities, the CloudWatch team is making a change to persist 3-hours of metric data in the CloudWatch local metrics data store. This change will allow CloudWatch users, and services requiring CloudWatch metrics (including AutoScaling), to access these recent metrics directly from the CloudWatch local metrics data store. This change has been completed in the US-EAST-1 Region and will be deployed globally in the coming weeks. \nTwo services were also impacted as a result of the issues with CloudWatch metrics. First, reactive AutoScaling policies that rely on\nCloudWatch metrics experienced delays until CloudWatch metrics began to recover at 5:47 PM PST. And second, Lambda saw impact. Lambda function\ninvocations currently require publishing metric data to CloudWatch as part of invocation. Lambda metric agents are designed to buffer metric data locally for a period of time if CloudWatch is unavailable. Starting at 6:15 AM PST, this buffering of metric data grew to the point that it caused memory contention on the underlying service hosts used for Lambda function invocations, resulting in increased error rates. At 10:36 AM PST, engineers took action to mitigate the memory contention, which resolved the increased error rates for function invocations.  \nCloudWatch Events and EventBridge experienced increased API errors and delays in event processing starting at 5:15 AM PST. As Kinesis availability improved, EventBridge began to deliver new events and slowly process the backlog of older events. Elastic Container Service (ECS) and Elastic Kubernetes Service (EKS) both make use of EventBridge to drive internal workflows used to manage customer clusters and tasks. This impacted provisioning of new clusters, delayed scaling of existing clusters, and impacted task de-provisioning. By 4:15 PM PST, the majority of these issues had been resolved. \nOutside of the service issues, we experienced some delays in\ncommunicating service status to customers during the early part of this event. We have two ways of communicating during operational events -- the Service Health Dashboard, which is our public dashboard to alert all customers of broad operational issues, and the Personal Health Dashboard, which we use to communicate directly with impacted customers. With an event such as this one, we typically post to the Service Health Dashboard. During the early part of this event, we were unable to update the Service Health Dashboard because the tool we use to post these updates itself uses Cognito, which was impacted by this event. We have a back-up means of updating the Service Health Dashboard that has minimal service dependencies. While this worked as expected, we encountered several delays during the earlier part of the event in posting to the Service Health Dashboard with this tool, as it is a more manual and less familiar tool for our support operators. To ensure customers were getting timely updates, the support team used the Personal Health Dashboard to notify impacted customers if they were impacted by the service issues. We also posted a global banner summary on the Service Health Dashboard to ensure customers had broad visibility into the event. During the remainder of event, we continued using a combination\nof the Service Health Dashboard, both with global banner summaries and service specific details, while also continuing to update impacted customers via Personal Health Dashboard. Going forward, we have changed our support training to ensure that our support engineers are regularly trained on the backup tool for posting to the Service Health Dashboard. \nFinally, we want to apologize for the impact this event caused for our customers. While we are proud of our long track record of availability with Amazon Kinesis, we know how critical this service, and the other AWS services that were impacted, are to our customers, their applications and end users, and their businesses. We will do everything we can to learn from this event and use it to improve our availability even further.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（Logical）、桥接性（Bridging）、可操作性（Actionable）、简洁性（Simplicity）和流程完整性（Process Integrity）等几个角度来分析文档，并按照术语（Terminology）、背景（Background）、常见问题解答（FAQ）、流程（Flow）和附录（Appendix）进行分类整理。\n\n### 术语（Terminology）\n\n- **Amazon Kinesis**: 一项服务，能够实时处理流数据。\n- **Sharding**: 数据分片机制，用于在后端处理流数据。\n- **Front-end and Back-end**: 前端服务器负责认证、限流和请求路由，后端集群负责数据分布、访问和可扩展性。\n- **Cell-clusters**: 后端的单元，处理数据流。\n- **Thread limit**: 操作系统配置允许的最大线程数。\n- **Cellularization**: 一种隔离服务内部故障影响的方法。\n\n### 背景（Background）\n\n- 事件发生在2020年11月25日，位于美国弗吉尼亚北部（US-EAST-1）的AWS区域。\n- 事件的触发因素是一次小规模的前端服务器容量增加。\n- 多个使用Kinesis服务的AWS服务也受到了影响。\n\n### 常见问题解答（FAQ）\n\n**Q: 事件的根本原因是什么？**\nA: 新增加的容量导致所有服务器超过了操作系统配置允许的最大线程数，导致前端服务器无法完成缓存构建，从而无法路由请求。\n\n**Q: 受影响的服务有哪些？**\nA: 包括Amazon Cognito、CloudWatch、AutoScaling、Lambda、ECS、EKS等。\n\n**Q: AWS如何解决问题并恢复正常服务？**\nA: 通过重启前端服务器、更换更大的CPU和内存服务器、优化线程使用和提高冷启动时间等措施。\n\n### 流程（Flow）\n\n1. **问题发现**: 早上5:15 PST开始，Kinesis记录的放入和获取出现错误。\n2. **初步响应**: 团队介入并开始审查日志，同时作为预防措施开始移除新增加的容量。\n3. **问题定位**: 确定问题根源需要前端服务器重启。\n4. **解决方案实施**: 确定根本原因后，开始重启前端服务器，并逐渐恢复服务。\n5. **服务恢复**: Kinesis在晚上10:23 PM PST完全恢复正常。\n\n### 附录（Appendix）\n\n- **改进措施**: AWS将立即实施一系列改进措施，包括更换更大的服务器、增加细粒度的报警、提高线程限制和改进冷启动时间。\n- **沟通问题**: 事件早期，由于Cognito服务受到影响，AWS在更新Service Health Dashboard上出现了延迟。后续通过Personal Health Dashboard直接通知受影响的客户，并在Service Health Dashboard上发布全局横幅摘要。\n- **后续行动**: AWS将改进支持培训，确保支持工程师熟悉备用的Service Health Dashboard更新工具。\n\n通过以上分析和整理，我们可以看到AWS在面对服务中断时采取了一系列的诊断和恢复措施，并且从中吸取了教训，制定了改进计划以提高未来的服务可用性。同时，AWS也意识到了在沟通方面需要改进的地方，并采取了相应的措施。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： # Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region\n\n**December 10th, 2021** \nWe want to provide you with some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on December 7th, 2021. \n**Issue Summary** \nTo explain this event, we need to share a little about the internals of the AWS network. While the majority of AWS services and all customer applications run within the main AWS network, AWS makes use of an\ninternal network to host foundational services including monitoring, internal DNS, authorization services, and parts of the EC2 control plane. Because of the importance of these services in this internal network, we connect this network with multiple geographically isolated networking devices and scale the capacity of this network significantly to ensure high availability of this network connection. These networking devices provide additional routing and network address translation that allow AWS services to communicate between the internal network and the main AWS network. At 7:30 AM PST, an automated activity to scale capacity of one of the AWS services hosted in the main AWS network triggered an unexpected behavior from a large number of clients inside the internal network. This resulted in a large surge of connection\nactivity that overwhelmed the networking devices between the internal network and the main AWS network, resulting in delays for communication\nbetween these networks. These delays increased latency and errors for services communicating between these networks, resulting in even more connection attempts and retries. This led to persistent congestion and performance issues on the devices connecting the two networks. \nThis congestion immediately impacted the availability of real-time monitoring data for our internal operations teams, which impaired their ability to find the source of congestion and resolve it. Operators instead relied on logs to understand what was happening and initially identified elevated internal DNS errors. Because internal DNS is foundational for all services and this traffic was believed to be contributing to the congestion, the teams focused on moving the internal DNS traffic away from the congested network paths. At 9:28 AM PST, the team completed this work and DNS resolution errors fully recovered. This change improved the availability of several impacted services by reducing load on the impacted networking devices, but did not fully resolve the AWS service impact or eliminate the congestion. Importantly, monitoring data was still not visible to our operations team so they had to continue resolving the issue with reduced system visibility. Operators continued working on a set of remediation actions to reduce congestion on the internal network including identifying the top sources of traffic to isolate to dedicated network devices, disabling some heavy network traffic services, and bringing additional networking capacity online. This progressed slowly for several reasons. First, the impact on\ninternal monitoring limited our ability to understand the problem. Second, our internal deployment systems, which run in our internal network, were impacted, which further slowed our remediation efforts. Finally, because many AWS services on the main AWS network and AWS customer applications were still operating normally, we wanted to be extremely deliberate while making changes to avoid impacting functioning workloads. As the operations teams continued applying the remediation\nactions described above, congestion significantly improved by 1:34 PM PST, and all network devices fully recovered by 2:22 PM PST. \nWe have taken several actions to prevent a recurrence of this event. We immediately disabled the scaling activities that triggered this event and will not resume them until we have deployed all remediations. Our systems are scaled adequately so that we do not need to resume these activities in the near-term. Our networking clients have well tested request back-off behaviors that are designed to allow our systems to recover from these sorts of congestion events, but, a latent issue prevented these clients from adequately backing off during this event. This code path has been in production for many years but the automated scaling activity triggered a previously unobserved behavior. We are developing a fix for this issue and expect to deploy this change over the next two weeks. We have also deployed additional network configuration that protects potentially impacted networking devices even\nin the face of a similar congestion event. These remediations give us confidence that we will not see a recurrence of this issue. \n**AWS Service Impact** \nWhile AWS customer workloads were not directly impacted from the internal networking issues described above, the networking issues caused impact to a number of AWS Services which in turn impacted customers using these service capabilities. Because the main AWS network was not affected, some customer applications which did not rely on these capabilities only experienced minimal impact from this event. \nSeveral AWS services experienced impact to the control planes that are used for creating and managing AWS resources. These control planes use services hosted in the internal network. For example, while running EC2 instances were unaffected by this event, the EC2 APIs that customers use to launch new instances or to describe their current instances experienced increased error rates and latencies starting at 7:33 AM PST. By 1:15 PM PST, as congestion was improving, EC2 API error rates and latencies began to improve, except for launches of new EC2 instances, which recovered by 2:40 PM PST. Customers of AWS services like Amazon RDS, EMR, Workspaces would not have been able to create new resources because of the inability to launch new EC2 instances during the event. Similarly, existing Elastic Load Balancers remained healthy during the event, but the elevated API error rates and latencies for the ELB APIs resulted in increased provisioning times for new load balancers and delayed instance registration times for adding new instances to existing load balancers. Additionally, Route 53 APIs were impaired from 7:30 AM PST until 2:30 PM PST preventing customers from making changes to their DNS entries, but existing DNS entries and answers to DNS queries were not impacted during this event. Customers also experienced login failures to the AWS Console in the impacted region during the event. Console access was fully restored by 2:22 PM PST. Amazon Secure Token Service (STS) experienced elevated latencies when providing credentials for third party identity providers via OpenID Connect (OIDC). This resulted in login failures for other AWS services that utilize STS for authentication, such as Redshift. While latencies improved at 2:22 PM PST when the issue affecting network devices was addressed, full recovery for STS occurred at 4:28 PM PST. \nCustomers were also impacted by CloudWatch monitoring delays throughout this event and, as a result, found it difficult to understand impact to their applications. A small amount of CloudWatch monitoring data was not captured during this event and may be missing from some metrics for parts of the event. \nCustomers accessing Amazon S3 and DynamoDB were not impacted by this event. However, access to Amazon S3 buckets and DynamoDB tables via VPC Endpoints was impaired during this event. \nAWS Lambda APIs and invocation of Lambda functions operated normally throughout the event. However, API Gateway, which is often used to invoke Lambda functions as well as an API management service for customer applications, experienced increased error rates. API Gateway servers were impacted by their inability to communicate with the internal network during the early part of this event. As a result of these errors, many API Gateway servers eventually got into a state where they needed to be replaced in order to serve requests successfully. This normally happens through an automated recycling process, but this was not possible until the EC2 APIs began recovering. While API Gateways began seeing recovery at 1:35 PM PST, errors and latencies remained elevated as API Gateway capacity was recycled by the automated process working through the backlog of affected servers. The service largely recovered by 4:37 PM PST, but API Gateway customers may have continued to experience low levels of errors and throttling for several hours as API Gateways fully stabilized. The API Gateway team is working on a set of mitigations to ensure that API Gateway servers remain healthy even\nwhen the internal network is unavailable and making improvements to the recycling process to speed recovery efforts in the event of a similar issue in the future. EventBridge, which is also often used in\nconjunction with Lambda, experienced elevated errors during the initial phases of the event but saw some improvement at 9:28 AM PST when the internal DNS issue was resolved. However, during mitigation efforts to reduce the load on the affected network devices, operators disabled event delivery for EventBridge at 12:35 PM. Event delivery was re-enabled at 2:35 PM PST, however the service experienced elevated event delivery latency until 6:40 PM PST as it processed the backlog of events. \nThe AWS container services, including Fargate, ECS and EKS, experienced increased API error rates and latencies during the event. While existing container instances (tasks or pods) continued to operate normally during the event, if a container instance was terminated or experienced a failure, it could not be restarted because of the impact to the EC2 control plane APIs described above. At 1:35 PM PST, most of the container-related API error rates returned to normal, but Fargate experienced increased request load due to the backlog of container instances that needed to be started, which led to continued elevated error rates and Insufficient Capacity Errors as container capacity pools were being replenished. At 5:00 PM PST, Fargate API error rates began to return to normal levels. Some customers saw elevated Insufficient Capacity Errors for \"4 vCPU\" task sizes for several hours following recovery. \nAmazon Connect experienced elevated failure rates for handling phone calls, chat sessions, and task contacts during the event. Issues with API Gateways used by Connect for the execution of Lambda functions resulted in elevated failure rates for inbound phone calls, chat sessions or task contacts. At 4:41 PM PST, when the affected API Gateway fully recovered, Amazon Connect resumed normal operations. \n**Event Communication** \nWe understand that events like this are more impactful and frustrating when information about what's happening isn't readily available. The impairment to our monitoring systems delayed our understanding of this event, and the networking congestion impaired our Service Health Dashboard tooling from appropriately failing over to our standby region. By 8:22 AM PST, we were successfully updating the Service Health Dashboard. As the impact to services during this event all stemmed from a single root cause, we opted to provide updates via a global banner on\nthe Service Health Dashboard, which we have since learned makes it difficult for some customers to find information about this issue. Our Support Contact Center also relies on the internal AWS network, so the ability to create support cases was impacted from 7:33 AM until 2:25 PM PST. We have been working on several enhancements to our Support Services to ensure we can more reliably and quickly communicate with customers during operational issues. We expect to release a new version\nof our Service Health Dashboard early next year that will make it easier to understand service impact and a new support system architecture that actively runs across multiple AWS regions to ensure we do not have delays in communicating with customers. \n**In closing** \nFinally, we want to apologize for the impact this event caused for our customers. While we are proud of our track record of availability, we know how critical our services are to our customers, their applications and end users, and their businesses. We know this event impacted many customers in significant ways. We will do everything we can to learn\nfrom this event and use it to improve our availability even further.\n\nAssistant:", "response": [["At 7:30 AM PST, an automated activity to scale capacity of one of the AWS services hosted in the main AWS network triggered an unexpected behavior from a large number of clients inside the internal network. This resulted in a large surge of connection activity that overwhelmed the networking devices between the internal network and the main AWS network, resulting in delays for communication between these networks. These delays increased latency and errors for services communicating between these networks, resulting in even more connection attempts and retries. This led to persistent congestion and performance issues on the devices connecting the two networks"]]}]
[{"prompt": "User: 下面是一篇故障报告： # Summary of the AWS Service Event in the Northern Virginia (US-EAST-1) Region\n\n**December 10th, 2021** \nWe want to provide you with some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on December 7th, 2021. \n**Issue Summary** \nTo explain this event, we need to share a little about the internals of the AWS network. While the majority of AWS services and all customer applications run within the main AWS network, AWS makes use of an\ninternal network to host foundational services including monitoring, internal DNS, authorization services, and parts of the EC2 control plane. Because of the importance of these services in this internal network, we connect this network with multiple geographically isolated networking devices and scale the capacity of this network significantly to ensure high availability of this network connection. These networking devices provide additional routing and network address translation that allow AWS services to communicate between the internal network and the main AWS network. At 7:30 AM PST, an automated activity to scale capacity of one of the AWS services hosted in the main AWS network triggered an unexpected behavior from a large number of clients inside the internal network. This resulted in a large surge of connection\nactivity that overwhelmed the networking devices between the internal network and the main AWS network, resulting in delays for communication\nbetween these networks. These delays increased latency and errors for services communicating between these networks, resulting in even more connection attempts and retries. This led to persistent congestion and performance issues on the devices connecting the two networks. \nThis congestion immediately impacted the availability of real-time monitoring data for our internal operations teams, which impaired their ability to find the source of congestion and resolve it. Operators instead relied on logs to understand what was happening and initially identified elevated internal DNS errors. Because internal DNS is foundational for all services and this traffic was believed to be contributing to the congestion, the teams focused on moving the internal DNS traffic away from the congested network paths. At 9:28 AM PST, the team completed this work and DNS resolution errors fully recovered. This change improved the availability of several impacted services by reducing load on the impacted networking devices, but did not fully resolve the AWS service impact or eliminate the congestion. Importantly, monitoring data was still not visible to our operations team so they had to continue resolving the issue with reduced system visibility. Operators continued working on a set of remediation actions to reduce congestion on the internal network including identifying the top sources of traffic to isolate to dedicated network devices, disabling some heavy network traffic services, and bringing additional networking capacity online. This progressed slowly for several reasons. First, the impact on\ninternal monitoring limited our ability to understand the problem. Second, our internal deployment systems, which run in our internal network, were impacted, which further slowed our remediation efforts. Finally, because many AWS services on the main AWS network and AWS customer applications were still operating normally, we wanted to be extremely deliberate while making changes to avoid impacting functioning workloads. As the operations teams continued applying the remediation\nactions described above, congestion significantly improved by 1:34 PM PST, and all network devices fully recovered by 2:22 PM PST. \nWe have taken several actions to prevent a recurrence of this event. We immediately disabled the scaling activities that triggered this event and will not resume them until we have deployed all remediations. Our systems are scaled adequately so that we do not need to resume these activities in the near-term. Our networking clients have well tested request back-off behaviors that are designed to allow our systems to recover from these sorts of congestion events, but, a latent issue prevented these clients from adequately backing off during this event. This code path has been in production for many years but the automated scaling activity triggered a previously unobserved behavior. We are developing a fix for this issue and expect to deploy this change over the next two weeks. We have also deployed additional network configuration that protects potentially impacted networking devices even\nin the face of a similar congestion event. These remediations give us confidence that we will not see a recurrence of this issue. \n**AWS Service Impact** \nWhile AWS customer workloads were not directly impacted from the internal networking issues described above, the networking issues caused impact to a number of AWS Services which in turn impacted customers using these service capabilities. Because the main AWS network was not affected, some customer applications which did not rely on these capabilities only experienced minimal impact from this event. \nSeveral AWS services experienced impact to the control planes that are used for creating and managing AWS resources. These control planes use services hosted in the internal network. For example, while running EC2 instances were unaffected by this event, the EC2 APIs that customers use to launch new instances or to describe their current instances experienced increased error rates and latencies starting at 7:33 AM PST. By 1:15 PM PST, as congestion was improving, EC2 API error rates and latencies began to improve, except for launches of new EC2 instances, which recovered by 2:40 PM PST. Customers of AWS services like Amazon RDS, EMR, Workspaces would not have been able to create new resources because of the inability to launch new EC2 instances during the event. Similarly, existing Elastic Load Balancers remained healthy during the event, but the elevated API error rates and latencies for the ELB APIs resulted in increased provisioning times for new load balancers and delayed instance registration times for adding new instances to existing load balancers. Additionally, Route 53 APIs were impaired from 7:30 AM PST until 2:30 PM PST preventing customers from making changes to their DNS entries, but existing DNS entries and answers to DNS queries were not impacted during this event. Customers also experienced login failures to the AWS Console in the impacted region during the event. Console access was fully restored by 2:22 PM PST. Amazon Secure Token Service (STS) experienced elevated latencies when providing credentials for third party identity providers via OpenID Connect (OIDC). This resulted in login failures for other AWS services that utilize STS for authentication, such as Redshift. While latencies improved at 2:22 PM PST when the issue affecting network devices was addressed, full recovery for STS occurred at 4:28 PM PST. \nCustomers were also impacted by CloudWatch monitoring delays throughout this event and, as a result, found it difficult to understand impact to their applications. A small amount of CloudWatch monitoring data was not captured during this event and may be missing from some metrics for parts of the event. \nCustomers accessing Amazon S3 and DynamoDB were not impacted by this event. However, access to Amazon S3 buckets and DynamoDB tables via VPC Endpoints was impaired during this event. \nAWS Lambda APIs and invocation of Lambda functions operated normally throughout the event. However, API Gateway, which is often used to invoke Lambda functions as well as an API management service for customer applications, experienced increased error rates. API Gateway servers were impacted by their inability to communicate with the internal network during the early part of this event. As a result of these errors, many API Gateway servers eventually got into a state where they needed to be replaced in order to serve requests successfully. This normally happens through an automated recycling process, but this was not possible until the EC2 APIs began recovering. While API Gateways began seeing recovery at 1:35 PM PST, errors and latencies remained elevated as API Gateway capacity was recycled by the automated process working through the backlog of affected servers. The service largely recovered by 4:37 PM PST, but API Gateway customers may have continued to experience low levels of errors and throttling for several hours as API Gateways fully stabilized. The API Gateway team is working on a set of mitigations to ensure that API Gateway servers remain healthy even\nwhen the internal network is unavailable and making improvements to the recycling process to speed recovery efforts in the event of a similar issue in the future. EventBridge, which is also often used in\nconjunction with Lambda, experienced elevated errors during the initial phases of the event but saw some improvement at 9:28 AM PST when the internal DNS issue was resolved. However, during mitigation efforts to reduce the load on the affected network devices, operators disabled event delivery for EventBridge at 12:35 PM. Event delivery was re-enabled at 2:35 PM PST, however the service experienced elevated event delivery latency until 6:40 PM PST as it processed the backlog of events. \nThe AWS container services, including Fargate, ECS and EKS, experienced increased API error rates and latencies during the event. While existing container instances (tasks or pods) continued to operate normally during the event, if a container instance was terminated or experienced a failure, it could not be restarted because of the impact to the EC2 control plane APIs described above. At 1:35 PM PST, most of the container-related API error rates returned to normal, but Fargate experienced increased request load due to the backlog of container instances that needed to be started, which led to continued elevated error rates and Insufficient Capacity Errors as container capacity pools were being replenished. At 5:00 PM PST, Fargate API error rates began to return to normal levels. Some customers saw elevated Insufficient Capacity Errors for \"4 vCPU\" task sizes for several hours following recovery. \nAmazon Connect experienced elevated failure rates for handling phone calls, chat sessions, and task contacts during the event. Issues with API Gateways used by Connect for the execution of Lambda functions resulted in elevated failure rates for inbound phone calls, chat sessions or task contacts. At 4:41 PM PST, when the affected API Gateway fully recovered, Amazon Connect resumed normal operations. \n**Event Communication** \nWe understand that events like this are more impactful and frustrating when information about what's happening isn't readily available. The impairment to our monitoring systems delayed our understanding of this event, and the networking congestion impaired our Service Health Dashboard tooling from appropriately failing over to our standby region. By 8:22 AM PST, we were successfully updating the Service Health Dashboard. As the impact to services during this event all stemmed from a single root cause, we opted to provide updates via a global banner on\nthe Service Health Dashboard, which we have since learned makes it difficult for some customers to find information about this issue. Our Support Contact Center also relies on the internal AWS network, so the ability to create support cases was impacted from 7:33 AM until 2:25 PM PST. We have been working on several enhancements to our Support Services to ensure we can more reliably and quickly communicate with customers during operational issues. We expect to release a new version\nof our Service Health Dashboard early next year that will make it easier to understand service impact and a new support system architecture that actively runs across multiple AWS regions to ensure we do not have delays in communicating with customers. \n**In closing** \nFinally, we want to apologize for the impact this event caused for our customers. While we are proud of our track record of availability, we know how critical our services are to our customers, their applications and end users, and their businesses. We know this event impacted many customers in significant ways. We will do everything we can to learn\nfrom this event and use it to improve our availability even further.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（Logical）、桥接性（Bridging）、可操作性（Actionable）、简洁性（Simplicity）和流程完整性（Process Integrity）几个角度来分析AWS服务中断事件，并按照术语（Terminology）、背景（Background）、常见问题解答（FAQ）、流程（Flow）和附录（Appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **逻辑性（Logical）**：在文档中，AWS对于服务中断的原因进行了逻辑上的解释，包括内部网络的工作原理、自动化扩展活动触发的意外行为、以及这些行为如何导致网络拥堵。\n- **桥接性（Bridging）**：文档中提到了内部网络与主AWS网络之间的连接设备，这些设备在逻辑上桥接了两个网络，确保服务之间的通信。\n- **可操作性（Actionable）**：文档提供了AWS团队采取的具体行动来解决和缓解问题，例如禁用触发事件的扩展活动、改善网络客户端的请求回退行为、以及部署额外的网络配置。\n- **简洁性（Simplicity）**：尽管事件复杂，但文档尝试以简洁的方式解释问题，使客户能够理解发生了什么以及AWS如何应对。\n- **流程完整性（Process Integrity）**：文档详细描述了事件处理的流程，从问题发现到完全恢复的每一步，展现了AWS处理此类事件的完整性和系统性。\n\n### 背景（Background）\n\n- 事件发生在2021年12月7日，位于美国弗吉尼亚北部（US-EAST-1）的AWS区域。\n- 事件起因是内部网络的自动化扩展活动触发了大量客户端的意外行为，导致网络设备之间的连接活动激增，进而引发网络拥堵。\n- 拥堵影响了内部操作团队获取实时监控数据的能力，从而阻碍了问题的快速定位和解决。\n\n### 常见问题解答（FAQ）\n\n- **问：哪些AWS服务受到了影响？**\n  - 答：受影响的服务包括EC2 API、Amazon RDS、EMR、Workspaces、Elastic Load Balancers、Route 53 API、AWS Console登录、Amazon Secure Token Service (STS)、CloudWatch监控、API Gateway、EventBridge、Fargate、ECS、EKS和Amazon Connect。\n\n- **问：客户如何知道服务何时恢复？**\n  - 答：AWS通过Service Health Dashboard提供更新，并在问题解决后通过公告通知客户。\n\n- **问：AWS采取了哪些措施来防止此类事件再次发生？**\n  - 答：AWS禁用了触发事件的扩展活动，开发并部署了网络客户端的修复程序，并增加了网络配置以保护受影响的网络设备。\n\n### 流程（Flow）\n\n1. **事件触发**：自动化扩展活动导致内部网络客户端的意外行为。\n2. **网络拥堵**：连接活动激增导致网络设备之间的通信延迟。\n3. **影响服务**：内部DNS错误、EC2 API错误率和延迟增加、Elastic Load Balancers和Route 53 API受影响。\n4. **问题定位**：操作团队通过日志理解情况，并尝试隔离内部DNS流量。\n5. **缓解措施**：识别和隔离高流量服务，增加网络容量，改善网络客户端行为。\n6. **服务恢复**：随着拥堵的减轻，服务逐渐恢复，AWS Console和STS在稍后时间完全恢复。\n7. **后续改进**：AWS采取行动改进内部网络和监控系统，以防止未来类似事件的发生。\n\n### 附录（Appendix）\n\n- **服务影响详情**：详细列出了受影响服务的状态和恢复时间。\n- **客户沟通**：描述了AWS如何通过Service Health Dashboard和Support Contact Center与客户沟通。\n- **未来计划**：提到了即将推出的Service Health Dashboard新版本和支持系统架构的改进。\n\n通过以上分析和总结，我们可以看到AWS在处理这次服务中断事件时展现了高度的透明度和专业性，同时也表明了他们对于改进和提升服务质量的承诺。"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of AWS Direct Connect Event in the Tokyo (AP-NORTHEAST-1) Region  \nWe would like to provide additional information about the AWS Direct Connect service disruption that occurred in the Tokyo (AP-NORTHEAST-1) Region on September 2, 2021. Beginning 7:30 AM JST, Direct Connect customers began to experience intermittent connectivity issues and elevated packet loss for their traffic destined towards the Tokyo Region. This was caused by the failure of a subset of network devices on\none of the network layers along the network path from Direct Connect edge locations to the Datacenter network in the Tokyo Region, where customers' Virtual Private Clouds (VPCs) reside. Customers started seeing recovery by 12:30 PM JST and by 1:42 PM JST, connectivity issues were fully resolved. All other forms of network connectivity, including traffic between Availability Zones, internet connectivity to the Region, and AWS Virtual Private Network (VPN) connectivity (which some customers use as a back-up to Direct Connect) were not impacted. Direct Connect traffic to other AWS Regions was also not impacted. \nOn September 2, 2021 at 7:30 AM JST, internal alarms alerted AWS engineers to elevated packet loss for Direct Connect customers connecting to the Tokyo Region. The Direct Connect service provides private connectivity between a customer's data center and their AWS VPCs by forwarding traffic from the edge locations where AWS interconnects with customers, to the AWS Region through multiple network layers - each with many redundant network devices. These alarms identified that the impact was caused by the failure of several devices in a single layer of the Direct Connect network. While these devices were not correctly forwarding traffic, they were not being removed from the network through the normal automated processes that monitor and remove failed network devices. Our automation instead noticed a higher rate of failed devices than normal and alerted engineers to investigate and take remediation\naction. When engineers were alerted, they determined that there was enough redundancy at this layer and began removing the impacted devices from service so that traffic could be handled by other healthy devices. In parallel, the team investigated the cause of the failure. While the removal of additional devices provided temporary remediation, several other network devices subsequently began to experience the same failure, resulting in network congestion, connectivity issues, or elevated packet loss for Direct Connect customers. Engineers attempted several mitigations, such as resetting failed devices and slowly bringing them back into service, but the failures continued and the engineers were unable to maintain adequate healthy capacity to fully mitigate the customer impact. Engineers also looked for any recent deployments that may have triggered the failure. By 12:00 PM JST, engineers suspected that the failure may be related to a new protocol that was introduced to optimize the network's reaction time to infrequent network convergence events and fiber cuts. This new protocol was introduced many months prior and this change had been in production since then without any issues. However, engineers suspected that the failure was related to the interaction of this new protocol and a new traffic pattern on the network devices at this layer of the Direct Connect network. Engineers started disabling this new protocol in a single Availability Zone to monitor and establish sustained recovery, while in parallel preparing the change to be deployed across the Tokyo Region. Customers started reporting recovery to their applications by 12:30 PM JST and by 1:42 PM JST affected networking devices were restored to a stable operational state and the Direct Connect service returned to normal operations. \nWhile disabling the new protocol resolved the event, engineering teams have continued working to identify the underlying root cause. We have now confirmed that this event was caused by a latent issue within the network device operating system. This version of the operating system enables a new protocol which is used to improve the failover time of our network. The new operating system and protocol have been running successfully in production for multiple months. We use a controlled, automated, tested, and instrumented procedure for changing the operating system and introducing the new protocol to the AWS network. This procedure starts with a series of stress tests in a dedicated lab to validate the resiliency of the network device to both valid and invalid (i.e., malformed) packets. Any anomalies identified in lab testing are diagnosed, root causes identified, and remediated before the new code is released to production. Even with this comprehensive testing, it is not possible to test every traffic and packet permutation in a lab environment. Therefore, AWS uses a deployment procedure that releases network device operating system changes to production in a slow and controlled fashion. This procedure upgrades individual devices in\nspecific places where the upgraded devices can be exposed to production\ntraffic but where traffic can easily fail away from the upgraded devices to non-upgraded devices. During this gradual production deployment, the upgraded devices are extensively monitored for performance issues and functionality errors. This upgrade process has been used many times successfully and was followed with this most recent device operating system upgrade. The new protocol and the operating system were first deployed to production in January 2021. Over the last 8 months, this new protocol and the operating system have been gradually released to production in all AWS Regions and has been serving Direct Connect customer traffic without any indication of the latent issue. Over the last several days, engineers have been able to identify the defect in\nthe network operating system and determined that it requires a very specific set of packet attributes and contents to trigger the issue. While these conditions are very specific and unlikely, this event was triggered by customer traffic that was able to consistently generate packets that matched this signature. We have no reason to suspect malicious intent. We have disabled the new protocol that triggered this issue in the AWS Tokyo Region. We have also developed an enhanced way to detect and remediate this issue before customer impact, as we carefully apply this change to all other AWS Regions. We are confident that there will be no additional customer impact from this issue. \nWe understand how critical AWS services are for our customers and many businesses in Japan, and we sincerely apologize for the impact that this event may have caused. We have a long track record of operating our services with high levels of availability and will do everything possible to maintain our customers' trust and help them achieve the availability they need for their customers and businesses.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（Logical）、桥接性（Bridging）、可操作性（Actionable）、简洁性（Simplicity）和流程完整性（Process Integrity）几个角度来分析AWS Direct Connect服务中断事件，并按照术语（Terminology）、背景（Background）、常见问题解答（FAQ）、流程（Flow）和附录（Appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **AWS Direct Connect**: AWS提供的服务，通过专用网络连接将客户的数据中心与AWS的虚拟私有云（VPC）连接起来。\n- **Virtual Private Cloud (VPC)**: AWS客户的虚拟私有云，用于在AWS云中启动资源。\n- **Network Layer**: 网络层，指的是网络架构中的一个层次，负责数据传输和路由。\n- **Network Device**: 网络设备，用于转发和处理网络流量的硬件。\n- **Packet Loss**: 数据包丢失，指在网络传输过程中丢失的数据包。\n- **Redundancy**: 冗余性，网络设计中的一种特性，确保系统在部分组件失败时仍能正常运行。\n- **Automation**: 自动化，指系统或过程自动执行任务的能力。\n\n### 背景（Background）\n\n在2021年9月2日，东京地区（AP-NORTHEAST-1）的AWS Direct Connect服务遭遇了中断。从早上7:30开始，使用Direct Connect服务的客户开始经历连接问题和数据包丢失。问题源于Direct Connect边缘位置到东京地区数据中心网络路径上的一个网络层中的部分网络设备发生故障。\n\n### 常见问题解答（FAQ）\n\n**Q: 这次服务中断对哪些客户造成了影响？**\nA: 使用Direct Connect服务并将流量发送到东京地区的客户受到了影响。\n\n**Q: 其他网络连接方式是否受到影响？**\nA: 其他形式的网络连接，包括可用区之间的流量、互联网连接以及AWS VPN连接（一些客户将其作为Direct Connect的备份）均未受到影响。\n\n**Q: AWS是如何应对这次服务中断的？**\nA: AWS工程师通过内部警报得知问题，开始调查故障原因，并尝试通过移除受影响设备和服务重启来缓解问题。最终，通过禁用可能引起故障的新协议，服务在下午1:42完全恢复。\n\n### 流程（Flow）\n\n1. **服务中断发生**: 早上7:30，Direct Connect客户开始经历连接问题和数据包丢失。\n2. **内部警报触发**: AWS工程师被内部警报通知，并开始调查。\n3. **初步应对**: 工程师开始移除受影响的设备，并尝试通过重启服务来缓解问题。\n4. **故障分析**: 工程师怀疑新引入的协议可能与故障有关。\n5. **禁用新协议**: 在单个可用区禁用新协议，并准备在整个东京地区部署这一变更。\n6. **服务恢复**: 下午12:30开始，客户开始看到应用恢复，到下午1:42，网络设备恢复稳定，服务恢复正常。\n7. **根本原因分析**: 工程团队继续工作，确定事件是由网络设备操作系统内的潜在问题引起的。\n8. **长期解决方案**: 开发增强的检测和修复方法，以避免类似问题再次发生。\n\n### 附录（Appendix）\n\n- **新协议的引入**: 新协议旨在优化网络对不频繁的网络收敛事件和光纤断裂的反应时间。\n- **操作系统变更**: AWS使用控制、自动化、测试和监测的程序来更改网络设备操作系统和引入新协议。\n- **逐步部署**: 通过网络设备操作系统的逐步部署，AWS能够在不影响客户的情况下测试和修复潜在问题。\n- **客户通知**: AWS对日本客户和许多企业的重要性表示理解，并为此次事件可能造成的影响表示歉意。同时，AWS承诺将继续提供高可用性的服务。\n\n通过以上分析和总结，我们可以看到AWS在处理这次服务中断事件时表现出了高度的专业性、透明度和对客户的责任感。通过详细的内部流程和对外沟通，AWS不仅成功地解决了问题，还采取了措施防止未来类似事件的发生。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region\n\nWe would like to share more details with our customers about the events that occurred with Amazon Elastic Compute Cloud (\"EC2\"), Amazon Elastic Block Store (\"EBS\"), and Amazon Relational Database Service (\"RDS\") earlier this week, and what we are doing to prevent these sorts of issues from happening again. The service disruption primarily affected EC2 instances, RDS instances, and a subset of EBS volumes in a single Availability Zone in the EU West Region. \n**Primary Outage** \nThe service disruption began at 10:41 AM PDT on August 7th when our utility provider suffered a failure of a 110kV 10 megawatt transformer. This failure resulted in a total loss of electricity supply to all of their customers connected to this transformer, including a significant portion of the affected AWS Availability Zone. The initial fault diagnosis from our utility provider indicated that a lightning strike caused the transformer to fail. The utility provider now believes it was not a lightning strike, and is continuing to investigate root cause. \nNormally, when utility power fails, electrical load is seamlessly picked up by backup generators. Programmable Logic Controllers (PLCs) assure that the electrical phase is synchronized between generators before their power is brought online. In this case, one of the PLCs did not complete the connection of a portion of the generators to bring them online. We currently believe (supported by all observations of the state and behavior of this PLC) that a large ground fault detected by the PLC caused it to fail to complete its task. We are working with our supplier and performing further analysis of the device involved to confirm. With no utility power, and backup generators for a large portion of this Availability Zone disabled, there was insufficient power for all of the servers in the Availability Zone to continue operating. Uninterruptable Power Supplies (UPSs) that provide a short period of battery power quickly drained and we lost power to almost all of the EC2 instances and 58% of the EBS volumes in that Availability Zone. We also lost power to the EC2 networking gear that connects this Availability Zone to the Internet and connects this Availability Zone to the other Availability Zones in the Region. This caused connectivity issues to the affected Availability Zone and resulted in API errors when customers targeted API requests (RunInstance, CreateVolume, etc.) to the impacted Availability Zone. \nAt 11:05 AM PDT, we were seeing launch delays and API errors in all EU West Availability Zones. There were two primary factors that contributed to this. First, our EC2 management service (which handles API requests to RunInstance, CreateVolume, etc.), has servers in each Availability Zone. The management servers which receive requests continued to route requests to management servers in the affected Availability Zone. Because the management servers in the affected Availability Zone were inaccessible, requests routed to those servers failed. Second, the EC2 management servers receiving requests were continuing to accept RunInstances requests targeted at the impacted Availability Zone. Rather than failing these requests immediately, they were queued and our management servers attempted to process them. Fairly quickly, a large number of these requests began to queue up and we overloaded the management servers receiving requests, which were waiting for these queued requests to complete. The combination of these two factors caused long delays in launching instances and higher error rates for the EU West EC2 APIs. At 12:00 PM PDT, when we disabled EC2 launches in the affected Availability Zone and removed the failed management servers from service, EC2 API launch times for other Availability Zones recovered. \nAt 11:54 AM PDT, we had been able to bring some of the backup generators online by manually phase-synchronizing the power sources. This restored power to many of the EC2 instances and EBS volumes, but a majority of the networking gear was still without power, so these restored instances were still inaccessible. By 1:49 PM PDT, power had been restored to enough of our network devices that we were able to re-establish connectivity to the Availability Zone. Many of the instances and volumes in the Availability Zone became accessible at this time. \n**Recovering EBS in the Affected Availability Zone** \nTo understand why restoration of EBS took longer, it's helpful to understand a little about the EBS architecture. EBS volume data is replicated across a set of EBS nodes for durability and availability. These nodes serve read and write requests to EC2 instances. If one node loses connectivity to another node that it is replicating data to, it must find and replicate its data to a new node (this is called re-mirroring)-- and it will block writes until it has found that new node. From the perspective of an EC2 instance trying to do I/O on an EBS volume that is blocking writes, the volume will appear \"stuck.\" \nOn Sunday, when a large portion of the EBS servers lost power and shut down, EBS volumes in the affected Availability Zone entered one of three states: (1) online -- none of the nodes holding a volume's data lost power, (2) re-mirroring -- a subset of the nodes storing the volume were offline due to power loss and the remaining nodes were re-replicating their data, and (3) offline -- all nodes lost power. \nIn the first case, EBS volumes continued to function normally. \nIn the second case, the majority of nodes were able to leverage the significant amount of spare capacity in the affected Availability Zone, successfully re-mirror, and enable the volume to recover. But, because we had such an unusually large number of EBS volumes lose power, the spare capacity we had on hand to support re-mirroring wasn't enough. We ran out of spare capacity before all of the volumes were able to successfully re-mirror. As a result, a number of customers' volumes became \"stuck\" as they attempted to write to their volume, but their volume had not yet found a new node to receive a replica. In order to get the \"stuck\" volumes back online, we had to add more capacity. We brought in additional labor to get more onsite capacity online and trucked in servers from another Availability Zone in the Region. There were delays as this was nighttime in Dublin and the logistics of trucking required mobilizing transportation some distance from the datacenter. Once the additional capacity was available, we were able to recover the remaining volumes waiting for space to complete a successful re-mirror. \nIn the third case, when an EC2 instance and all nodes containing EBS volume replicas concurrently lose power, we cannot verify that all of the writes to all of the nodes are completely consistent. If we cannot confirm that all writes have been persisted to disk, then we cautiously assume that the volume is in an inconsistent state (even though in many cases the volume is actually consistent). Bringing a volume back in an\ninconsistent state without the customer being aware could cause undetectable, latent data corruption issues which could trigger a serious impact later. For the volumes we assumed were inconsistent, we produced a recovery snapshot to enable customers to create a new volume and check its consistency before trying to use it. The process of producing recovery snapshots was time-consuming because we had to first copy all of the data from each node to Amazon Simple Storage Service (Amazon S3), process that data to turn it into the snapshot storage format, and re-copy the data to make it accessible from a customer's account. Many of the volumes contained a lot of data (EBS volumes can\nhold as much as 1 TB per volume). By 6:04 AM PDT on August 9th, we had delivered approximately 38% of the recovery snapshots for these potentially inconsistent volumes to customers. By 2:37 AM PDT on August 10th, 85% of the recovery snapshots had been delivered. By 8:25 PM PDT on August 10th, we were 98% complete, with the remaining few snapshots requiring manual attention. \n**Impact on Amazon RDS** \nRDS Instances were also affected by the disruption. RDS database instances utilize EBS volumes for database and log storage. As a result, the power outage in the affected Availability Zone had significant impact on RDS as well. Single Availability Zone (\"Single-AZ\") database instances in the affected Availability Zone were almost all initially unavailable. They recovered when their corresponding EBS volumes were restored or their databases were restored to new volumes. All Amazon RDS Single-AZ database instances have automated backups turned on by default. The majority of customers whose databases did not recover when\nthe first tranche of EBS volumes came back online, or could not be recovered due to inconsistency of their volumes, used this backup functionality to initiate Point-in-Time-Restore operations, per our Service Health Dashboard instructions. Customers with automated backups turned off, could not initiate Point-in-Time-Restores. \nIn addition to impacting Single-AZ database instances, the severity of the event and nature of failure also caused a portion of Multiple Availability Zone (\"Multi-AZ\") database instances to be impacted. Rapid failover occurred for the vast majority of Multi-AZ databases, and all affected Multi-AZ databases in the EU-West Region failed over without data loss. However, a portion of Multi-AZ database instances experienced prolonged failover times. \nTo understand why some Multi-AZ database instances did not promptly failover, it is useful to understand how Multi-AZ databases work. RDS Multi-AZ database instances consist of a \"primary\" database instance and a synchronously replicated \"secondary\" database instance in another Availability Zone. When the system detects that a primary database instance might be failing, upon verification via a health check that the primary is no longer accepting traffic, the secondary is promoted to primary. This verification is important to avoid a \"split brain\" situation, one where both the primary and the secondary database instances are accepting writes and some writes exist on one database while some exist on another. Similarly, when the system detects that a secondary database instance is failing, upon performing the health check and verifying that the secondary hasn't assumed the role of primary, the primary will allow itself to continue as a Single-AZ database instance until a new secondary is established and connected to the primary, bringing the pair back into Multi-AZ status. \nDuring the event, there were failures of Multi-AZ primary database instances in the affected Availability Zone. However, for a portion of these Multi-AZ primary-secondary pairs, a DNS connectivity issue related to the power loss prevented the health check from finding the IP address it needed to contact and kept the secondary from immediately assuming the role of the primary. DNS connectivity was restored within 4 minutes, and the majority of Multi-AZ deployments then completed failover within\nan additional 10 minutes. However, the DNS connectivity issues triggered a software bug that caused failover times to the secondary database instance to extend significantly for a small subset of Multi-AZ deployments. \nThis DNS connectivity issue also triggered extended failover times for a small portion of Multi-AZ deployments with secondary replicas in the affected Availability Zone. For these deployments, DNS connectivity prevented the primary replicas from confirming their secondary replica's status. In the rare case where the status of the secondary cannot be determined, the primary does not make itself a Single AZ-mode database instance and instead immediately involves the RDS team. This cautious approach is taken to help prevent the \"split brain\" scenario described above. Instead, an RDS engineer makes the decision to either promote the secondary to primary (if the old primary is not functioning), or to move the primary to Single-AZ mode (if the secondary is not functioning). This approach minimizes the risk of data loss in edge cases, but extends the period of time the Multi-AZ instance is unavailable. \n**EBS Software Bug Impacting Snapshots** \nSeparately, and independent from issues emanating from the power disruption, we discovered an error in the EBS software that cleans up unused storage for snapshots after customers have deleted an EBS snapshot. An EBS snapshot contains a set of pointers to blocks of data, including the blocks shared between multiple snapshots. Each time a new snapshot is taken of an EBS volume, only the data that has been modified since the last snapshot is pushed to S3. When a snapshot is deleted, only the blocks not referenced by later snapshots should be deleted. A cleanup process runs periodically to identify all blocks that are no longer included in any snapshots. This snapshot cleanup identification\nprocess builds a list of the blocks included in the deleted customer snapshots, a list of blocks referenced by active EBS volumes, and a list of blocks referenced by other snapshots. Blocks that are referenced by active volumes or snapshots are removed from the list of blocks to cleanup. \nThe resulting cleanup list is saved, but not acted upon. At least one week passes from the time the snapshot cleanup identification process runs before any blocks it has flagged for deletion are allowed to be removed. Each day, it updates the lists of blocks to delete, blocks referenced by active volumes, and blocks referenced by other snapshots. It also compares its updated lists to the prior day's and if any block eligible for deletion the day before now shows up in the most recent list of blocks referenced by active EBS volumes or snapshots, the process flags those blocks for analysis. Typically, there are very few, if any, items that get flagged for analysis. But, this part of the process was introduced to protect against system or software errors that could result in blocks falsely flagged for deletion. Actual deletion is executed by an engineer who first, before running the actual deletion\nprocess, evaluates the blocks flagged for analysis and verifies that there are no blocks in the list scheduled to be deleted that have been\nflagged for analysis. The engineer must present their verification step to another engineer who approves the deletion. \nIn one of the days leading up to the Friday, August 5th deletion run, there was a hardware failure that the snapshot cleanup identification\nsoftware did not correctly detect and handle. The result was that the list of snapshot references used as input to the cleanup process was incomplete. Because the list of snapshot references was incomplete, the snapshot cleanup identification process incorrectly believed a number of blocks were no longer referenced and had flagged those blocks for deletion even though they were still referenced by customer snapshots. A subsequent run of the snapshot cleanup identification process detected the error and flagged blocks for further analysis that had been\nincorrectly scheduled for deletion. On August 5th, the engineer running the snapshot deletion process checked the blocks flagged for analysis before running the actual deletion process in the EU West Region. The human checks in this process failed to detect the error and the deletion\nprocess was executed. On Friday evening, an error accessing one of the affected snapshots triggered us to investigate. \nBy Sunday morning, August 7th, we had completed the work to fully understand root cause, prevent the problem from recurring, and build a tool that could create recovery snapshots for affected snapshots. We then started to do the work necessary to map these affected snapshots to customers and build the recovery snapshots, with the aim to communicate this information to customers by Sunday night. However, before we got very far in this endeavor, the power event began. We had to temporarily stop work on the snapshot issue to respond to the power event. Once we'd been able to restore the majority of the EBS volumes affected by the power event, we returned to working on the snapshot issue in parallel with restoring the remainder of the EBS volumes that were recovering from the power event. By 4:19 PM PDT on August 8th, we'd completed creating recovery snapshots for all affected snapshots, delivered them to customers' accounts, and communicated about the issue on the Service Health Dashboard. \n**Actions to Prevent Recurrence** \nThere are several actions we intend to take to protect against a similar occurrence. The following are some of the key ones. \nTo further prevent the loss of power, we will add redundancy and more isolation for our PLCs so they are insulated from other failures. Specifically, in addition to correcting the isolation of the primary PLC, a cold, environmentally isolated backup PLC is being worked with our vendors. We will deploy this as rapidly as possible. \nFor EC2, we are going to address the resource saturation that affected API calls at the beginning of the disruption. We will implement better load balancing to quickly take failed API management service hosts out of production. Over the last few months, we have been developing further isolation of EC2 control plane components (i.e. the APIs) to eliminate possible latency or failure in one Availability Zone from impacting our ability to process calls to other Availability Zones. While some of those mitigations significantly reduced the impact of this disruption\nand helped us recover the APIs quickly, we realize how important those APIs are to customers, especially during an event. It will take us several more months to complete some of the changes we're making, and we will test and roll out these changes carefully. At the time of the disruption, customers who had EC2 instances and EBS volumes independently operating in multiple EU West Region Availability Zones did not experience service interruption. We will continue to create additional capabilities that make it easier to develop and deploy applications in multiple Availability Zones. \nFor EBS, our primary action will be to drastically reduce the long recovery time required to recover stuck or inconsistent EBS volumes when\nthere is a substantial infrastructure disruption. While some volumes were recoverable immediately once we had power back, there was an\nextended period of time for many volumes to recover due to the need to create EBS snapshots within S3. As we described above, this long period of delay was caused by the time required to move a very large amount of data into S3 and then transfer that data to EBS recovery snapshots. To significantly reduce the time required to restore these volumes, we will create the capability to recover volumes directly on the EBS servers upon restoration of power, without having to move the data off of those servers. This will require providing a way for customers to know that a volume has been shut down and restored, but will avoid the need for restoration via snapshot. This will also substantially diminish any risk associated with lack of capacity, regardless of how many volumes fail. \nWe've made changes to our deletion process to prevent recurrence of the EBS software bug impacting snapshots. We are instrumenting an alarm that will alert us if there are any unusual situations discovered by the snapshot cleanup identification process, including blocks falsely flagged as being unreferenced. We're also adding another holding state for blocks flagged for deletion where they are logically unavailable but retrievable for an additional, longer period of time. This will provide additional time to discover and correct any problem without loss of data. \nWe learned a number of lessons from this event that we will use to continually improve the reliability of RDS Multi-AZ deployments. First, we will implement changes to our health checks to avoid customer impact in the event of a unique DNS connectivity issue like we experienced here. Second, we will promptly fix the software bug that extended failover times for a portion of Multi-AZ customers with primaries in the affected Availability Zone. Third, we will implement an improved handling of the edge case where either primary or secondary is down and the health check cannot complete. In such a case, the successfully running member of the Multi-AZ pair will initiate connection retries to confirm it is no longer in a \"split brain\" mode, such that involving an\nengineer might not be necessary. \n**Improving Communication** \nCommunication in situations like this is difficult. Customers are understandably anxious about the timing for recovery and what they should do in the interim. We always prioritize getting affected customers back to health as soon as possible, and that was our top priority in this event, too. But, we know how important it is to communicate on the Service Health Dashboard and AWS Support mechanisms. Based on prior customer feedback, we communicated more frequently during this event on our Service Health Dashboard than we had in other prior events, we had evangelists tweet links to key early dashboard updates, we staffed up our AWS Support team to handle much higher forum and Premium Support contacts, and we tried to give an approximate time-frame early on for when the people with extra-long delays could expect to start seeing recovery. Still, we know what matters most to customers in\ncircumstances like this is knowing the status of their resources, when\nthe impacted ones will be healthy, and what they should do now. While we provided best estimates for the long-lead recovery snapshots, we truly didn't know how long that process was going to take or we would have shared it. For those waiting for recovery snapshots, we tried to communicate what was possible. If customers were architected to operate across multiple Availability Zones, they could flip over to and/or deploy resources in other Availability Zones. If customers were architected such that spinning up new instances or volumes in the same Availability Zone worked, they could do that. But, for those single Availability Zone customers who needed a specific EBS volume to recover, and whose EBS volume was in the group waiting for recovery snapshots, there were really no short term actions possible. \nThere are several places we can improve on the communication front. First, we can accelerate the pace with which we staff up our Support team to be even more responsive in the early hours of an event. Second, we will do a better job of making it easier for customers (and AWS) to tell if their resources have been impacted. This will give customers (and AWS) important shared telemetry on what's happening to specific resources in the heat of the moment. We've been hard at work on\ndeveloping tools to allow you to see via the APIs if your instances/volumes are impaired, and hope to have this to customers in\nthe next few months. Third, as we were sending customers recovery snapshots, we could have been clearer and more instructive on how to run\nthe recovery tools, and provided better detail on the recovery actions customers could have taken. We sometimes assume a certain familiarity with these tools that we should not. \n**Service Credit for Affected Customers** \nFor customers with an attached EBS volume or a running RDS database instance in the affected Availability Zone in the EU West Region at the time of the disruption, regardless of whether their resources and application were impacted or not, we are going to provide a 10 day credit equal to 100% of their usage of EBS Volumes, EC2 Instances and RDS database instances that were running in the affected Availability Zone in the EU West region. Additionally, any customers impacted by the EBS software bug that accidentally deleted blocks in their snapshots will receive a 30 day credit for 100% of their EBS usage in the entire EU West Region (inclusive of snapshot storage and requests as well as volume storage and I/O). These customers will also have access to our Premium Support Engineers (via the AWS Support Center) if these customers need any additional technical assistance in recovering from this issue. \nThese customers will not have to do anything in order to receive the credits, as they will be automatically applied to customers' next AWS bill. The credits can also be viewed as they become available over the next few weeks by logging into the AWS Account Activity page. \n**Summary** \nLast, but certainly not least, we want to apologize. We know how critical our services are to our customers' businesses. We will do everything we can to learn from this event and use it to drive improvement across our services. As with any significant operational issue, we will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make changes to improve our services and processes.\n\nAssistant:", "response": [["An unknown event caused a transformer to fail. One of the PLCs that checks that generator power is in phase failed for an unknown reason, which prevented a set of backup generators from coming online. This affected EC2, EBS, and RDS in EU West"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region\n\nWe would like to share more details with our customers about the events that occurred with Amazon Elastic Compute Cloud (\"EC2\"), Amazon Elastic Block Store (\"EBS\"), and Amazon Relational Database Service (\"RDS\") earlier this week, and what we are doing to prevent these sorts of issues from happening again. The service disruption primarily affected EC2 instances, RDS instances, and a subset of EBS volumes in a single Availability Zone in the EU West Region. \n**Primary Outage** \nThe service disruption began at 10:41 AM PDT on August 7th when our utility provider suffered a failure of a 110kV 10 megawatt transformer. This failure resulted in a total loss of electricity supply to all of their customers connected to this transformer, including a significant portion of the affected AWS Availability Zone. The initial fault diagnosis from our utility provider indicated that a lightning strike caused the transformer to fail. The utility provider now believes it was not a lightning strike, and is continuing to investigate root cause. \nNormally, when utility power fails, electrical load is seamlessly picked up by backup generators. Programmable Logic Controllers (PLCs) assure that the electrical phase is synchronized between generators before their power is brought online. In this case, one of the PLCs did not complete the connection of a portion of the generators to bring them online. We currently believe (supported by all observations of the state and behavior of this PLC) that a large ground fault detected by the PLC caused it to fail to complete its task. We are working with our supplier and performing further analysis of the device involved to confirm. With no utility power, and backup generators for a large portion of this Availability Zone disabled, there was insufficient power for all of the servers in the Availability Zone to continue operating. Uninterruptable Power Supplies (UPSs) that provide a short period of battery power quickly drained and we lost power to almost all of the EC2 instances and 58% of the EBS volumes in that Availability Zone. We also lost power to the EC2 networking gear that connects this Availability Zone to the Internet and connects this Availability Zone to the other Availability Zones in the Region. This caused connectivity issues to the affected Availability Zone and resulted in API errors when customers targeted API requests (RunInstance, CreateVolume, etc.) to the impacted Availability Zone. \nAt 11:05 AM PDT, we were seeing launch delays and API errors in all EU West Availability Zones. There were two primary factors that contributed to this. First, our EC2 management service (which handles API requests to RunInstance, CreateVolume, etc.), has servers in each Availability Zone. The management servers which receive requests continued to route requests to management servers in the affected Availability Zone. Because the management servers in the affected Availability Zone were inaccessible, requests routed to those servers failed. Second, the EC2 management servers receiving requests were continuing to accept RunInstances requests targeted at the impacted Availability Zone. Rather than failing these requests immediately, they were queued and our management servers attempted to process them. Fairly quickly, a large number of these requests began to queue up and we overloaded the management servers receiving requests, which were waiting for these queued requests to complete. The combination of these two factors caused long delays in launching instances and higher error rates for the EU West EC2 APIs. At 12:00 PM PDT, when we disabled EC2 launches in the affected Availability Zone and removed the failed management servers from service, EC2 API launch times for other Availability Zones recovered. \nAt 11:54 AM PDT, we had been able to bring some of the backup generators online by manually phase-synchronizing the power sources. This restored power to many of the EC2 instances and EBS volumes, but a majority of the networking gear was still without power, so these restored instances were still inaccessible. By 1:49 PM PDT, power had been restored to enough of our network devices that we were able to re-establish connectivity to the Availability Zone. Many of the instances and volumes in the Availability Zone became accessible at this time. \n**Recovering EBS in the Affected Availability Zone** \nTo understand why restoration of EBS took longer, it's helpful to understand a little about the EBS architecture. EBS volume data is replicated across a set of EBS nodes for durability and availability. These nodes serve read and write requests to EC2 instances. If one node loses connectivity to another node that it is replicating data to, it must find and replicate its data to a new node (this is called re-mirroring)-- and it will block writes until it has found that new node. From the perspective of an EC2 instance trying to do I/O on an EBS volume that is blocking writes, the volume will appear \"stuck.\" \nOn Sunday, when a large portion of the EBS servers lost power and shut down, EBS volumes in the affected Availability Zone entered one of three states: (1) online -- none of the nodes holding a volume's data lost power, (2) re-mirroring -- a subset of the nodes storing the volume were offline due to power loss and the remaining nodes were re-replicating their data, and (3) offline -- all nodes lost power. \nIn the first case, EBS volumes continued to function normally. \nIn the second case, the majority of nodes were able to leverage the significant amount of spare capacity in the affected Availability Zone, successfully re-mirror, and enable the volume to recover. But, because we had such an unusually large number of EBS volumes lose power, the spare capacity we had on hand to support re-mirroring wasn't enough. We ran out of spare capacity before all of the volumes were able to successfully re-mirror. As a result, a number of customers' volumes became \"stuck\" as they attempted to write to their volume, but their volume had not yet found a new node to receive a replica. In order to get the \"stuck\" volumes back online, we had to add more capacity. We brought in additional labor to get more onsite capacity online and trucked in servers from another Availability Zone in the Region. There were delays as this was nighttime in Dublin and the logistics of trucking required mobilizing transportation some distance from the datacenter. Once the additional capacity was available, we were able to recover the remaining volumes waiting for space to complete a successful re-mirror. \nIn the third case, when an EC2 instance and all nodes containing EBS volume replicas concurrently lose power, we cannot verify that all of the writes to all of the nodes are completely consistent. If we cannot confirm that all writes have been persisted to disk, then we cautiously assume that the volume is in an inconsistent state (even though in many cases the volume is actually consistent). Bringing a volume back in an\ninconsistent state without the customer being aware could cause undetectable, latent data corruption issues which could trigger a serious impact later. For the volumes we assumed were inconsistent, we produced a recovery snapshot to enable customers to create a new volume and check its consistency before trying to use it. The process of producing recovery snapshots was time-consuming because we had to first copy all of the data from each node to Amazon Simple Storage Service (Amazon S3), process that data to turn it into the snapshot storage format, and re-copy the data to make it accessible from a customer's account. Many of the volumes contained a lot of data (EBS volumes can\nhold as much as 1 TB per volume). By 6:04 AM PDT on August 9th, we had delivered approximately 38% of the recovery snapshots for these potentially inconsistent volumes to customers. By 2:37 AM PDT on August 10th, 85% of the recovery snapshots had been delivered. By 8:25 PM PDT on August 10th, we were 98% complete, with the remaining few snapshots requiring manual attention. \n**Impact on Amazon RDS** \nRDS Instances were also affected by the disruption. RDS database instances utilize EBS volumes for database and log storage. As a result, the power outage in the affected Availability Zone had significant impact on RDS as well. Single Availability Zone (\"Single-AZ\") database instances in the affected Availability Zone were almost all initially unavailable. They recovered when their corresponding EBS volumes were restored or their databases were restored to new volumes. All Amazon RDS Single-AZ database instances have automated backups turned on by default. The majority of customers whose databases did not recover when\nthe first tranche of EBS volumes came back online, or could not be recovered due to inconsistency of their volumes, used this backup functionality to initiate Point-in-Time-Restore operations, per our Service Health Dashboard instructions. Customers with automated backups turned off, could not initiate Point-in-Time-Restores. \nIn addition to impacting Single-AZ database instances, the severity of the event and nature of failure also caused a portion of Multiple Availability Zone (\"Multi-AZ\") database instances to be impacted. Rapid failover occurred for the vast majority of Multi-AZ databases, and all affected Multi-AZ databases in the EU-West Region failed over without data loss. However, a portion of Multi-AZ database instances experienced prolonged failover times. \nTo understand why some Multi-AZ database instances did not promptly failover, it is useful to understand how Multi-AZ databases work. RDS Multi-AZ database instances consist of a \"primary\" database instance and a synchronously replicated \"secondary\" database instance in another Availability Zone. When the system detects that a primary database instance might be failing, upon verification via a health check that the primary is no longer accepting traffic, the secondary is promoted to primary. This verification is important to avoid a \"split brain\" situation, one where both the primary and the secondary database instances are accepting writes and some writes exist on one database while some exist on another. Similarly, when the system detects that a secondary database instance is failing, upon performing the health check and verifying that the secondary hasn't assumed the role of primary, the primary will allow itself to continue as a Single-AZ database instance until a new secondary is established and connected to the primary, bringing the pair back into Multi-AZ status. \nDuring the event, there were failures of Multi-AZ primary database instances in the affected Availability Zone. However, for a portion of these Multi-AZ primary-secondary pairs, a DNS connectivity issue related to the power loss prevented the health check from finding the IP address it needed to contact and kept the secondary from immediately assuming the role of the primary. DNS connectivity was restored within 4 minutes, and the majority of Multi-AZ deployments then completed failover within\nan additional 10 minutes. However, the DNS connectivity issues triggered a software bug that caused failover times to the secondary database instance to extend significantly for a small subset of Multi-AZ deployments. \nThis DNS connectivity issue also triggered extended failover times for a small portion of Multi-AZ deployments with secondary replicas in the affected Availability Zone. For these deployments, DNS connectivity prevented the primary replicas from confirming their secondary replica's status. In the rare case where the status of the secondary cannot be determined, the primary does not make itself a Single AZ-mode database instance and instead immediately involves the RDS team. This cautious approach is taken to help prevent the \"split brain\" scenario described above. Instead, an RDS engineer makes the decision to either promote the secondary to primary (if the old primary is not functioning), or to move the primary to Single-AZ mode (if the secondary is not functioning). This approach minimizes the risk of data loss in edge cases, but extends the period of time the Multi-AZ instance is unavailable. \n**EBS Software Bug Impacting Snapshots** \nSeparately, and independent from issues emanating from the power disruption, we discovered an error in the EBS software that cleans up unused storage for snapshots after customers have deleted an EBS snapshot. An EBS snapshot contains a set of pointers to blocks of data, including the blocks shared between multiple snapshots. Each time a new snapshot is taken of an EBS volume, only the data that has been modified since the last snapshot is pushed to S3. When a snapshot is deleted, only the blocks not referenced by later snapshots should be deleted. A cleanup process runs periodically to identify all blocks that are no longer included in any snapshots. This snapshot cleanup identification\nprocess builds a list of the blocks included in the deleted customer snapshots, a list of blocks referenced by active EBS volumes, and a list of blocks referenced by other snapshots. Blocks that are referenced by active volumes or snapshots are removed from the list of blocks to cleanup. \nThe resulting cleanup list is saved, but not acted upon. At least one week passes from the time the snapshot cleanup identification process runs before any blocks it has flagged for deletion are allowed to be removed. Each day, it updates the lists of blocks to delete, blocks referenced by active volumes, and blocks referenced by other snapshots. It also compares its updated lists to the prior day's and if any block eligible for deletion the day before now shows up in the most recent list of blocks referenced by active EBS volumes or snapshots, the process flags those blocks for analysis. Typically, there are very few, if any, items that get flagged for analysis. But, this part of the process was introduced to protect against system or software errors that could result in blocks falsely flagged for deletion. Actual deletion is executed by an engineer who first, before running the actual deletion\nprocess, evaluates the blocks flagged for analysis and verifies that there are no blocks in the list scheduled to be deleted that have been\nflagged for analysis. The engineer must present their verification step to another engineer who approves the deletion. \nIn one of the days leading up to the Friday, August 5th deletion run, there was a hardware failure that the snapshot cleanup identification\nsoftware did not correctly detect and handle. The result was that the list of snapshot references used as input to the cleanup process was incomplete. Because the list of snapshot references was incomplete, the snapshot cleanup identification process incorrectly believed a number of blocks were no longer referenced and had flagged those blocks for deletion even though they were still referenced by customer snapshots. A subsequent run of the snapshot cleanup identification process detected the error and flagged blocks for further analysis that had been\nincorrectly scheduled for deletion. On August 5th, the engineer running the snapshot deletion process checked the blocks flagged for analysis before running the actual deletion process in the EU West Region. The human checks in this process failed to detect the error and the deletion\nprocess was executed. On Friday evening, an error accessing one of the affected snapshots triggered us to investigate. \nBy Sunday morning, August 7th, we had completed the work to fully understand root cause, prevent the problem from recurring, and build a tool that could create recovery snapshots for affected snapshots. We then started to do the work necessary to map these affected snapshots to customers and build the recovery snapshots, with the aim to communicate this information to customers by Sunday night. However, before we got very far in this endeavor, the power event began. We had to temporarily stop work on the snapshot issue to respond to the power event. Once we'd been able to restore the majority of the EBS volumes affected by the power event, we returned to working on the snapshot issue in parallel with restoring the remainder of the EBS volumes that were recovering from the power event. By 4:19 PM PDT on August 8th, we'd completed creating recovery snapshots for all affected snapshots, delivered them to customers' accounts, and communicated about the issue on the Service Health Dashboard. \n**Actions to Prevent Recurrence** \nThere are several actions we intend to take to protect against a similar occurrence. The following are some of the key ones. \nTo further prevent the loss of power, we will add redundancy and more isolation for our PLCs so they are insulated from other failures. Specifically, in addition to correcting the isolation of the primary PLC, a cold, environmentally isolated backup PLC is being worked with our vendors. We will deploy this as rapidly as possible. \nFor EC2, we are going to address the resource saturation that affected API calls at the beginning of the disruption. We will implement better load balancing to quickly take failed API management service hosts out of production. Over the last few months, we have been developing further isolation of EC2 control plane components (i.e. the APIs) to eliminate possible latency or failure in one Availability Zone from impacting our ability to process calls to other Availability Zones. While some of those mitigations significantly reduced the impact of this disruption\nand helped us recover the APIs quickly, we realize how important those APIs are to customers, especially during an event. It will take us several more months to complete some of the changes we're making, and we will test and roll out these changes carefully. At the time of the disruption, customers who had EC2 instances and EBS volumes independently operating in multiple EU West Region Availability Zones did not experience service interruption. We will continue to create additional capabilities that make it easier to develop and deploy applications in multiple Availability Zones. \nFor EBS, our primary action will be to drastically reduce the long recovery time required to recover stuck or inconsistent EBS volumes when\nthere is a substantial infrastructure disruption. While some volumes were recoverable immediately once we had power back, there was an\nextended period of time for many volumes to recover due to the need to create EBS snapshots within S3. As we described above, this long period of delay was caused by the time required to move a very large amount of data into S3 and then transfer that data to EBS recovery snapshots. To significantly reduce the time required to restore these volumes, we will create the capability to recover volumes directly on the EBS servers upon restoration of power, without having to move the data off of those servers. This will require providing a way for customers to know that a volume has been shut down and restored, but will avoid the need for restoration via snapshot. This will also substantially diminish any risk associated with lack of capacity, regardless of how many volumes fail. \nWe've made changes to our deletion process to prevent recurrence of the EBS software bug impacting snapshots. We are instrumenting an alarm that will alert us if there are any unusual situations discovered by the snapshot cleanup identification process, including blocks falsely flagged as being unreferenced. We're also adding another holding state for blocks flagged for deletion where they are logically unavailable but retrievable for an additional, longer period of time. This will provide additional time to discover and correct any problem without loss of data. \nWe learned a number of lessons from this event that we will use to continually improve the reliability of RDS Multi-AZ deployments. First, we will implement changes to our health checks to avoid customer impact in the event of a unique DNS connectivity issue like we experienced here. Second, we will promptly fix the software bug that extended failover times for a portion of Multi-AZ customers with primaries in the affected Availability Zone. Third, we will implement an improved handling of the edge case where either primary or secondary is down and the health check cannot complete. In such a case, the successfully running member of the Multi-AZ pair will initiate connection retries to confirm it is no longer in a \"split brain\" mode, such that involving an\nengineer might not be necessary. \n**Improving Communication** \nCommunication in situations like this is difficult. Customers are understandably anxious about the timing for recovery and what they should do in the interim. We always prioritize getting affected customers back to health as soon as possible, and that was our top priority in this event, too. But, we know how important it is to communicate on the Service Health Dashboard and AWS Support mechanisms. Based on prior customer feedback, we communicated more frequently during this event on our Service Health Dashboard than we had in other prior events, we had evangelists tweet links to key early dashboard updates, we staffed up our AWS Support team to handle much higher forum and Premium Support contacts, and we tried to give an approximate time-frame early on for when the people with extra-long delays could expect to start seeing recovery. Still, we know what matters most to customers in\ncircumstances like this is knowing the status of their resources, when\nthe impacted ones will be healthy, and what they should do now. While we provided best estimates for the long-lead recovery snapshots, we truly didn't know how long that process was going to take or we would have shared it. For those waiting for recovery snapshots, we tried to communicate what was possible. If customers were architected to operate across multiple Availability Zones, they could flip over to and/or deploy resources in other Availability Zones. If customers were architected such that spinning up new instances or volumes in the same Availability Zone worked, they could do that. But, for those single Availability Zone customers who needed a specific EBS volume to recover, and whose EBS volume was in the group waiting for recovery snapshots, there were really no short term actions possible. \nThere are several places we can improve on the communication front. First, we can accelerate the pace with which we staff up our Support team to be even more responsive in the early hours of an event. Second, we will do a better job of making it easier for customers (and AWS) to tell if their resources have been impacted. This will give customers (and AWS) important shared telemetry on what's happening to specific resources in the heat of the moment. We've been hard at work on\ndeveloping tools to allow you to see via the APIs if your instances/volumes are impaired, and hope to have this to customers in\nthe next few months. Third, as we were sending customers recovery snapshots, we could have been clearer and more instructive on how to run\nthe recovery tools, and provided better detail on the recovery actions customers could have taken. We sometimes assume a certain familiarity with these tools that we should not. \n**Service Credit for Affected Customers** \nFor customers with an attached EBS volume or a running RDS database instance in the affected Availability Zone in the EU West Region at the time of the disruption, regardless of whether their resources and application were impacted or not, we are going to provide a 10 day credit equal to 100% of their usage of EBS Volumes, EC2 Instances and RDS database instances that were running in the affected Availability Zone in the EU West region. Additionally, any customers impacted by the EBS software bug that accidentally deleted blocks in their snapshots will receive a 30 day credit for 100% of their EBS usage in the entire EU West Region (inclusive of snapshot storage and requests as well as volume storage and I/O). These customers will also have access to our Premium Support Engineers (via the AWS Support Center) if these customers need any additional technical assistance in recovering from this issue. \nThese customers will not have to do anything in order to receive the credits, as they will be automatically applied to customers' next AWS bill. The credits can also be viewed as they become available over the next few weeks by logging into the AWS Account Activity page. \n**Summary** \nLast, but certainly not least, we want to apologize. We know how critical our services are to our customers' businesses. We will do everything we can to learn from this event and use it to drive improvement across our services. As with any significant operational issue, we will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make changes to improve our services and processes.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（logical）、桥接性（bridging）、可行性（actionable）、简洁性（simplicity）和流程完整性（process integrity）等几个角度来分析该文档，并按照术语（terminology）、背景（background）、常见问题解答（faq）、流程（flow）和附录（appendix）进行分类整理。\n\n### 术语（Terminology）\n\n- **Amazon EC2**: Amazon Elastic Compute Cloud，亚马逊提供的云计算服务，允许用户在云端租用虚拟计算机。\n- **Amazon EBS**: Amazon Elastic Block Store，亚马逊提供的块存储服务，用于为EC2实例提供持久性存储。\n- **Amazon RDS**: Amazon Relational Database Service，亚马逊提供的托管关系数据库服务。\n- **Availability Zone**: 可用区，AWS数据中心内的一个物理区域，用于隔离故障和提高可用性。\n- **PLCs (Programmable Logic Controllers)**: 可编程逻辑控制器，用于同步备用发电机的电力相位。\n- **UPSs (Uninterruptable Power Supplies)**: 不间断电源，提供短期电池电力支持。\n- **Multi-AZ**: 多可用区部署，RDS数据库实例的一个特性，允许在不同的可用区部署主数据库实例和从数据库实例。\n\n### 背景（Background）\n\n- 事件发生在欧盟西部地区（EU West Region）的AWS服务中，影响了EC2实例、RDS实例和一部分EBS卷。\n- 服务中断始于太平洋时间8月7日上午10:41，由于电力供应商的110kV 10兆瓦变压器故障导致电力供应完全中断。\n- 备用发电机未能正常启动，导致大量EC2实例和EBS卷失去电源，进而影响了网络设备和API请求。\n\n### 常见问题解答（FAQ）\n\n**Q: 服务中断的主要原因是什么？**\nA: 主要原因是电力供应商的变压器故障，以及备用发电机未能成功启动。\n\n**Q: AWS采取了哪些措施来恢复服务？**\nA: AWS手动同步了部分备用发电机，恢复了部分EC2实例和EBS卷的电力供应，并增加了额外的容量来完成EBS卷的恢复。\n\n**Q: 对于受影响的客户，AWS提供了哪些补偿？**\nA: 对于受影响的EBS卷、EC2实例和RDS数据库实例，AWS提供了相当于其在受影响可用区使用量的10天服务信用。对于因EBS软件错误而受到影响的客户，提供了30天的100% EBS使用量信用。\n\n### 流程（Flow）\n\n1. **电力中断**: 由于变压器故障，电力供应中断。\n2. **备用发电机失败**: PLCs未能成功启动备用发电机。\n3. **服务中断**: EC2实例、EBS卷和RDS实例受到影响。\n4. **手动恢复**: 通过手动同步部分备用发电机恢复电力。\n5. **增加容量**: 为了恢复EBS卷，AWS增加了额外的容量。\n6. **恢复网络**: 网络设备恢复供电，重新建立连接。\n7. **RDS恢复**: RDS实例在EBS卷恢复后或通过新卷恢复。\n8. **信用补偿**: AWS为受影响的客户提供服务信用。\n\n### 附录（Appendix）\n\n- **预防措施**: AWS计划增加PLCs的冗余和隔离，改进EC2的负载平衡，减少EBS恢复时间，并改进RDS Multi-AZ部署的可靠性。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1) Region  \nWe'd like to give you some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on\nthe morning of February 28th, 2017. The Amazon Simple Storage Service (S3) team was debugging an issue causing the S3 billing system to progress more slowly than expected. At 9:37AM PST, an authorized S3 team member using an established playbook executed a command which was intended to remove a small number of servers for one of the S3 subsystems that is used by the S3 billing process. Unfortunately, one of the inputs to the command was entered incorrectly and a larger set of servers was removed than intended. The servers that were inadvertently removed supported two other S3 subsystems.  One of these subsystems, the index subsystem, manages the metadata and location information of all S3 objects in the region. This subsystem is necessary to serve all GET, LIST, PUT, and DELETE requests. The second subsystem, the placement subsystem, manages allocation of new storage and requires the index subsystem to be functioning properly to correctly operate. The placement subsystem is used during PUT requests to allocate storage for new objects. Removing a significant portion of the capacity caused each of these systems to require a full restart. While these subsystems were being restarted, S3 was unable to service requests. Other AWS services in the US-EAST-1 Region that rely on S3 for storage, including the S3 console, Amazon Elastic Compute Cloud (EC2) new instance launches, Amazon Elastic Block Store (EBS) volumes (when data was needed from a S3 snapshot), and AWS Lambda were also impacted while the S3 APIs were unavailable.   \nS3 subsystems are designed to support the removal or failure of significant capacity with little or no customer impact. We build our systems with the assumption that things will occasionally fail, and we rely on the ability to remove and replace capacity as one of our core operational processes. While this is an operation that we have relied on\nto maintain our systems since the launch of S3, we have not completely restarted the index subsystem or the placement subsystem in our larger regions for many years. S3 has experienced massive growth over the last several years and the process of restarting these services and running the necessary safety checks to validate the integrity of the metadata took longer than expected. The index subsystem was the first of the two affected subsystems that needed to be restarted. By 12:26PM PST, the index subsystem had activated enough capacity to begin servicing S3 GET, LIST, and DELETE requests. By 1:18PM PST, the index subsystem was fully recovered and GET, LIST, and DELETE APIs were functioning normally.  The S3 PUT API also required the placement subsystem. The placement subsystem began recovery when the index subsystem was functional and finished recovery at 1:54PM PST. At this point, S3 was operating normally. Other AWS services that were impacted by this event began\nrecovering. Some of these services had accumulated a backlog of work during the S3 disruption and required additional time to fully recover. \nWe are making several changes as a result of this operational event. While removal of capacity is a key operational practice, in this instance, the tool used allowed too much capacity to be removed too quickly. We have modified this tool to remove capacity more slowly and added safeguards to prevent capacity from being removed when it will take any subsystem below its minimum required capacity level. This will prevent an incorrect input from triggering a similar event in the future. We are also auditing our other operational tools to ensure we have similar safety checks. We will also make changes to improve the recovery time of key S3 subsystems. We employ multiple techniques to allow our services to recover from any failure quickly. One of the most important involves breaking services into small partitions which we call cells. By factoring services into cells, engineering teams can assess and thoroughly test recovery processes of even the largest service or subsystem. As S3 has scaled, the team has done considerable work to refactor parts of the service into smaller cells to reduce blast radius and improve recovery. During this event, the recovery time of the index subsystem still took longer than we expected. The S3 team had planned further partitioning of the index subsystem later this year. We are reprioritizing that work to begin immediately. \nFrom the beginning of this event until 11:37AM PST, we were unable to update the individual services' status on the AWS Service Health Dashboard (SHD) because of a dependency the SHD administration console has on Amazon S3. Instead, we used the AWS Twitter feed (@AWSCloud) and SHD banner text to communicate status until we were able to update the individual services' status on the SHD.  We understand that the SHD provides important visibility to our customers during operational events and we have changed the SHD administration console to run across multiple AWS regions. \nFinally, we want to apologize for the impact this event caused for our customers. While we are proud of our long track record of availability with Amazon S3, we know how critical this service is to our customers, their applications and end users, and their businesses. We will do everything we can to learn from this event and use it to improve our availability even further.\n\nAssistant:", "response": [["Human error. On February 28th 2017 9:37AM PST, the Amazon S3 team was debugging a minor issue. Despite using an established playbook, one of the commands intending to remove a small number of servers was issued with a typo, inadvertently causing a larger set of servers to be removed. These servers supported critical S3 systems. As a result, dependent systems required a full restart to correctly operate, and the system underwent widespread outages for US-EAST-1 (Northern Virginia) until final resolution at 1:54PM PST. Since Amazon's own services such as EC2 and EBS rely on S3 as well, it caused a vast cascading failure which affected hundreds of companies"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the Amazon S3 Service Disruption in the Northern Virginia (US-EAST-1) Region  \nWe'd like to give you some additional information about the service disruption that occurred in the Northern Virginia (US-EAST-1) Region on\nthe morning of February 28th, 2017. The Amazon Simple Storage Service (S3) team was debugging an issue causing the S3 billing system to progress more slowly than expected. At 9:37AM PST, an authorized S3 team member using an established playbook executed a command which was intended to remove a small number of servers for one of the S3 subsystems that is used by the S3 billing process. Unfortunately, one of the inputs to the command was entered incorrectly and a larger set of servers was removed than intended. The servers that were inadvertently removed supported two other S3 subsystems.  One of these subsystems, the index subsystem, manages the metadata and location information of all S3 objects in the region. This subsystem is necessary to serve all GET, LIST, PUT, and DELETE requests. The second subsystem, the placement subsystem, manages allocation of new storage and requires the index subsystem to be functioning properly to correctly operate. The placement subsystem is used during PUT requests to allocate storage for new objects. Removing a significant portion of the capacity caused each of these systems to require a full restart. While these subsystems were being restarted, S3 was unable to service requests. Other AWS services in the US-EAST-1 Region that rely on S3 for storage, including the S3 console, Amazon Elastic Compute Cloud (EC2) new instance launches, Amazon Elastic Block Store (EBS) volumes (when data was needed from a S3 snapshot), and AWS Lambda were also impacted while the S3 APIs were unavailable.   \nS3 subsystems are designed to support the removal or failure of significant capacity with little or no customer impact. We build our systems with the assumption that things will occasionally fail, and we rely on the ability to remove and replace capacity as one of our core operational processes. While this is an operation that we have relied on\nto maintain our systems since the launch of S3, we have not completely restarted the index subsystem or the placement subsystem in our larger regions for many years. S3 has experienced massive growth over the last several years and the process of restarting these services and running the necessary safety checks to validate the integrity of the metadata took longer than expected. The index subsystem was the first of the two affected subsystems that needed to be restarted. By 12:26PM PST, the index subsystem had activated enough capacity to begin servicing S3 GET, LIST, and DELETE requests. By 1:18PM PST, the index subsystem was fully recovered and GET, LIST, and DELETE APIs were functioning normally.  The S3 PUT API also required the placement subsystem. The placement subsystem began recovery when the index subsystem was functional and finished recovery at 1:54PM PST. At this point, S3 was operating normally. Other AWS services that were impacted by this event began\nrecovering. Some of these services had accumulated a backlog of work during the S3 disruption and required additional time to fully recover. \nWe are making several changes as a result of this operational event. While removal of capacity is a key operational practice, in this instance, the tool used allowed too much capacity to be removed too quickly. We have modified this tool to remove capacity more slowly and added safeguards to prevent capacity from being removed when it will take any subsystem below its minimum required capacity level. This will prevent an incorrect input from triggering a similar event in the future. We are also auditing our other operational tools to ensure we have similar safety checks. We will also make changes to improve the recovery time of key S3 subsystems. We employ multiple techniques to allow our services to recover from any failure quickly. One of the most important involves breaking services into small partitions which we call cells. By factoring services into cells, engineering teams can assess and thoroughly test recovery processes of even the largest service or subsystem. As S3 has scaled, the team has done considerable work to refactor parts of the service into smaller cells to reduce blast radius and improve recovery. During this event, the recovery time of the index subsystem still took longer than we expected. The S3 team had planned further partitioning of the index subsystem later this year. We are reprioritizing that work to begin immediately. \nFrom the beginning of this event until 11:37AM PST, we were unable to update the individual services' status on the AWS Service Health Dashboard (SHD) because of a dependency the SHD administration console has on Amazon S3. Instead, we used the AWS Twitter feed (@AWSCloud) and SHD banner text to communicate status until we were able to update the individual services' status on the SHD.  We understand that the SHD provides important visibility to our customers during operational events and we have changed the SHD administration console to run across multiple AWS regions. \nFinally, we want to apologize for the impact this event caused for our customers. While we are proud of our long track record of availability with Amazon S3, we know how critical this service is to our customers, their applications and end users, and their businesses. We will do everything we can to learn from this event and use it to improve our availability even further.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（logical）、桥接性（bridging）、可操作性（actionable）、简洁性（simplicity）和流程完整性（process integrity）几个角度来分析亚马逊S3服务中断事件，并按照术语（terminology）、背景（background）、常见问题解答（faq）、流程（flow）和附录（appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **S3（Simple Storage Service）**：亚马逊提供的一种对象存储服务，用于存储和检索任意数量的数据。\n- **EC2（Elastic Compute Cloud）**：亚马逊提供的一种云计算服务，用于执行虚拟机。\n- **EBS（Elastic Block Store）**：亚马逊提供的块存储服务，用于EC2实例的数据持久化。\n- **AWS Lambda**：亚马逊提供的一种计算服务，用于运行代码而无需预置或管理服务器。\n- **SHD（Service Health Dashboard）**：AWS服务健康仪表板，用于监控AWS服务的状态。\n\n### 背景（Background）\n\n2017年2月28日上午，亚马逊S3服务在北弗吉尼亚（US-EAST-1）区域发生了服务中断。事件起因是S3团队在调试计费系统速度缓慢问题时，执行了一个旨在移除S3子系统的一小部分服务器的命令。由于命令输入错误，导致移除了比预期更多的服务器，影响了S3的索引子系统和放置子系统，进而影响了S3服务以及依赖S3存储的其他AWS服务。\n\n### 常见问题解答（FAQ）\n\n**Q: 服务中断的原因是什么？**\nA: 服务中断是由于S3团队在调试过程中错误地移除了更多的服务器，导致支持索引和放置子系统的服务器被意外移除。\n\n**Q: 受影响的服务有哪些？**\nA: 受影响的服务包括S3控制台、EC2新实例启动、EBS卷（当需要从S3快照中获取数据时）和AWS Lambda。\n\n**Q: 服务中断期间，AWS如何与客户沟通？**\nA: 由于SHD依赖于S3，AWS通过AWS Twitter账号 (@AWSCloud) 和SHD横幅文本来更新状态，直到能够在SHD上更新服务状态。\n\n### 流程（Flow）\n\n1. **问题发现**：S3计费系统处理速度缓慢。\n2. **调试过程**：S3团队执行命令以移除部分服务器。\n3. **错误发生**：命令输入错误导致更多服务器被移除。\n4. **影响扩散**：索引和放置子系统受影响，导致S3服务和其他AWS服务中断。\n5. **恢复措施**：重启索引和放置子系统。\n6. **服务恢复**：索引子系统首先恢复，随后放置子系统恢复，S3服务恢复正常。\n7. **后续改进**：AWS采取措施防止类似事件再次发生，并改善恢复时间。\n\n### 附录（Appendix）\n\n- **服务中断时间线**：\n  - 9:37 AM PST：错误命令执行。\n  - 12:26 PM PST：索引子系统开始恢复服务。\n  - 1:18 PM PST：索引子系统完全恢复。\n  - 1:54 PM PST：放置子系统恢复，S3恢复正常。\n- **改进措施**：\n  - 修改移除容量的工具，以减缓移除速度并增加安全检查。\n  - 审计其他操作工具，确保有类似的安全检查。\n  - 改善关键S3子系统的恢复时间。\n  - 重新优先考虑索引子系统的进一步分区工作。\n\n通过以上分析和总结，我们可以更全面地理解2017年2月发生在亚马逊S3服务中的中断事件，以及AWS采取的应对措施和后续的改进计划。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the AWS Service Event in the Sydney Region\n\nWe'd like to share more detail about the AWS service disruption that occurred this past weekend in the AWS Sydney Region.  The service disruption primarily affected EC2 instances and their associated Elastic Block Store (\"EBS\") volumes running in a single Availability Zone.  \n**Loss of Power** \nAt 10:25 PM PDT on June 4th, our utility provider suffered a loss of power at a regional substation as a result of severe weather in the area. This failure resulted in a total loss of utility power to multiple AWS facilities. In one of the facilities, our power redundancy didn't work as designed, and we lost power to a significant number of instances in that Availability Zone. \nNormally, when utility power fails, electrical load is maintained by multiple layers of power redundancy. Every instance is served by two independent power delivery line-ups, each providing access to utility power, uninterruptable power supplies (UPSs), and back-up power from generators. If either of these independent power line-ups provides power, the instance will maintain availability. During this weekend's event, the instances that lost power lost access to both their primary and secondary power as several of our power delivery line-ups failed to transfer load to their generators. These particular power line-ups utilize a technology known as a diesel rotary uninterruptable power supply (DRUPS), which integrates a diesel generator and a mechanical UPS. Under normal operation, the DRUPS uses utility power to spin a flywheel which stores energy. If utility power is interrupted, the DRUPS uses this stored energy to continue to provide power to the datacenter while the integrated generator is turned on to continue to provide power until utility power is restored. The specific signature of this weekend's utility power failure resulted in an unusually long voltage sag (rather than a complete outage). Because of the unexpected nature of this voltage sag, a set of breakers responsible for isolating the DRUPS from utility power failed to open quickly enough. Normally, these breakers would assure that the DRUPS reserve power is used to support the datacenter load during the transition to generator power. Instead, the DRUPS system's energy reserve quickly drained into the degraded power grid. The rapid, unexpected loss of power from DRUPS resulted in\nDRUPS shutting down, meaning the generators which had started up could not be engaged and connected to the datacenter racks. DRUPS shutting down this rapidly and in this fashion is unusual and required some inspection. Once our on-site technicians were able to determine it was safe to manually re-engage the power line-ups, power was restored at 11:46PM PDT. \n**Recovery** \nAs power was restored to the affected infrastructure, our automated systems began to bring customers' EC2 instances and EBS volumes back online. By 1:00 AM PDT, over 80% of the impacted customer instances and volumes were back online and operational. After power recovery, some instances in the Availability Zone experienced DNS resolution failures as the internal DNS hosts for that Availability Zone were brought back online and handled the recovery load. DNS error rates recovered by 2:49 AM PDT. \nA latent bug in our instance management software led to a slower than\nexpected recovery of the remaining instances. The team worked over the next several hours to manually recover these remaining instances. Instances were recovered continually during this time, and by 8AM PDT, nearly all instances had been recovered. \nThere were also a small number of EBS volumes (less than 0.01% of the volumes in the Availability Zone) that were unable to recover after power was restored.  EBS volumes are replicated to multiple storage servers in the same Availability Zone, which protects against most hardware failure scenarios and allows EBS to provide a 0.1%-0.2% annualized failure rate. This does mean volumes can be lost when\nmultiple servers fail at the same time. During the power event, a small number of storage servers suffered failed hard drives which led to a loss of the data stored on those servers. In cases where both of the replicas were hosted on failed servers, we were unable to automatically restore the volume. After the initial wave of automated recovery, the EBS team focused on manually recovering as many damaged storage servers as possible. This is a slow process, which is why some volumes took much longer to return to service. \nDuring the initial part of this event, customers experienced errors when\ntrying to launch new instances, or when trying to scale their auto-scaling groups.  To remediate this, our team had to manually fail away from degraded services in the affected zone. Starting at 11:42 PM PDT, the manual failover was complete and customers were able to launch instances in the unaffected Availability Zones.  When the APIs initially recovered, our systems were delayed in propagating some state changes and making them available via describe API calls. This meant that some customers could not see their newly launched resources, and some existing instances appeared as stuck in pending or shutting down when\ncustomers tried to make changes to their infrastructure in the affected Availability Zone. These state delays also increased latency of adding new instances to existing Elastic Load Balancing (ELB) load balancers.    \n**Remediation** \nWhile we have experienced excellent operational performance from the power configuration used in this facility, it is apparent that we need to enhance this particular design to prevent similar power sags from affecting our power delivery infrastructure. In order to prevent a recurrence of this correlated power delivery line-up failure, we are adding additional breakers to assure that we more quickly break connections to degraded utility power to allow our generators to activate before the UPS systems are depleted. \nAdditionally, we will be taking actions to improve our recovery systems.  The first is to fix the latent issue that led to our recovery systems not being able to automatically recover a subset of customer instances. That fix is already in testing, and will be deployed over the coming days. We will also be starting a program to regularly test our recovery processes on unoccupied, long-running hosts in our fleet. By continually testing our recovery workflows on long-running hosts, we can\nassure that no latent issues or configuration setting exists that would impact our ability to quickly remediate customer impact when instances need to be recovered. \nFor this event, customers that were running their applications across multiple Availability Zones in the Region were able to maintain\navailability throughout the event. For customers that need the highest availability for their applications, we continue to recommend running applications with this architecture. We know that it was problematic that for a period of time there were errors and delays for the APIs that launch instances. We are working on changes that will assure our APIs are even more resilient to failure and believe these changes will be rolled out to the Sydney Region in July.  \n**In Closing** \nWe apologize for any inconvenience this event caused. We know how critical our services are to our customers' businesses. We are never satisfied with operational performance that is anything less than\nperfect, and we will do everything we can to learn from this event and use it to drive improvement across our services. \n\nAssistant:", "response": [["At 10:25pm PDT on June 4, loss of power at an AWS Sydney facility resulting from severe weather in that area lead to disruption to a significant number of instances in an Availability Zone. Due to the signature of the power loss, power  isolation breakers did not engage, resulting in backup energy reserves draining into the degraded power grid"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the AWS Service Event in the Sydney Region\n\nWe'd like to share more detail about the AWS service disruption that occurred this past weekend in the AWS Sydney Region.  The service disruption primarily affected EC2 instances and their associated Elastic Block Store (\"EBS\") volumes running in a single Availability Zone.  \n**Loss of Power** \nAt 10:25 PM PDT on June 4th, our utility provider suffered a loss of power at a regional substation as a result of severe weather in the area. This failure resulted in a total loss of utility power to multiple AWS facilities. In one of the facilities, our power redundancy didn't work as designed, and we lost power to a significant number of instances in that Availability Zone. \nNormally, when utility power fails, electrical load is maintained by multiple layers of power redundancy. Every instance is served by two independent power delivery line-ups, each providing access to utility power, uninterruptable power supplies (UPSs), and back-up power from generators. If either of these independent power line-ups provides power, the instance will maintain availability. During this weekend's event, the instances that lost power lost access to both their primary and secondary power as several of our power delivery line-ups failed to transfer load to their generators. These particular power line-ups utilize a technology known as a diesel rotary uninterruptable power supply (DRUPS), which integrates a diesel generator and a mechanical UPS. Under normal operation, the DRUPS uses utility power to spin a flywheel which stores energy. If utility power is interrupted, the DRUPS uses this stored energy to continue to provide power to the datacenter while the integrated generator is turned on to continue to provide power until utility power is restored. The specific signature of this weekend's utility power failure resulted in an unusually long voltage sag (rather than a complete outage). Because of the unexpected nature of this voltage sag, a set of breakers responsible for isolating the DRUPS from utility power failed to open quickly enough. Normally, these breakers would assure that the DRUPS reserve power is used to support the datacenter load during the transition to generator power. Instead, the DRUPS system's energy reserve quickly drained into the degraded power grid. The rapid, unexpected loss of power from DRUPS resulted in\nDRUPS shutting down, meaning the generators which had started up could not be engaged and connected to the datacenter racks. DRUPS shutting down this rapidly and in this fashion is unusual and required some inspection. Once our on-site technicians were able to determine it was safe to manually re-engage the power line-ups, power was restored at 11:46PM PDT. \n**Recovery** \nAs power was restored to the affected infrastructure, our automated systems began to bring customers' EC2 instances and EBS volumes back online. By 1:00 AM PDT, over 80% of the impacted customer instances and volumes were back online and operational. After power recovery, some instances in the Availability Zone experienced DNS resolution failures as the internal DNS hosts for that Availability Zone were brought back online and handled the recovery load. DNS error rates recovered by 2:49 AM PDT. \nA latent bug in our instance management software led to a slower than\nexpected recovery of the remaining instances. The team worked over the next several hours to manually recover these remaining instances. Instances were recovered continually during this time, and by 8AM PDT, nearly all instances had been recovered. \nThere were also a small number of EBS volumes (less than 0.01% of the volumes in the Availability Zone) that were unable to recover after power was restored.  EBS volumes are replicated to multiple storage servers in the same Availability Zone, which protects against most hardware failure scenarios and allows EBS to provide a 0.1%-0.2% annualized failure rate. This does mean volumes can be lost when\nmultiple servers fail at the same time. During the power event, a small number of storage servers suffered failed hard drives which led to a loss of the data stored on those servers. In cases where both of the replicas were hosted on failed servers, we were unable to automatically restore the volume. After the initial wave of automated recovery, the EBS team focused on manually recovering as many damaged storage servers as possible. This is a slow process, which is why some volumes took much longer to return to service. \nDuring the initial part of this event, customers experienced errors when\ntrying to launch new instances, or when trying to scale their auto-scaling groups.  To remediate this, our team had to manually fail away from degraded services in the affected zone. Starting at 11:42 PM PDT, the manual failover was complete and customers were able to launch instances in the unaffected Availability Zones.  When the APIs initially recovered, our systems were delayed in propagating some state changes and making them available via describe API calls. This meant that some customers could not see their newly launched resources, and some existing instances appeared as stuck in pending or shutting down when\ncustomers tried to make changes to their infrastructure in the affected Availability Zone. These state delays also increased latency of adding new instances to existing Elastic Load Balancing (ELB) load balancers.    \n**Remediation** \nWhile we have experienced excellent operational performance from the power configuration used in this facility, it is apparent that we need to enhance this particular design to prevent similar power sags from affecting our power delivery infrastructure. In order to prevent a recurrence of this correlated power delivery line-up failure, we are adding additional breakers to assure that we more quickly break connections to degraded utility power to allow our generators to activate before the UPS systems are depleted. \nAdditionally, we will be taking actions to improve our recovery systems.  The first is to fix the latent issue that led to our recovery systems not being able to automatically recover a subset of customer instances. That fix is already in testing, and will be deployed over the coming days. We will also be starting a program to regularly test our recovery processes on unoccupied, long-running hosts in our fleet. By continually testing our recovery workflows on long-running hosts, we can\nassure that no latent issues or configuration setting exists that would impact our ability to quickly remediate customer impact when instances need to be recovered. \nFor this event, customers that were running their applications across multiple Availability Zones in the Region were able to maintain\navailability throughout the event. For customers that need the highest availability for their applications, we continue to recommend running applications with this architecture. We know that it was problematic that for a period of time there were errors and delays for the APIs that launch instances. We are working on changes that will assure our APIs are even more resilient to failure and believe these changes will be rolled out to the Sydney Region in July.  \n**In Closing** \nWe apologize for any inconvenience this event caused. We know how critical our services are to our customers' businesses. We are never satisfied with operational performance that is anything less than\nperfect, and we will do everything we can to learn from this event and use it to drive improvement across our services. \n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（Logical）、桥接性（Bridging）、可操作性（Actionable）、简洁性（Simplicity）和流程完整性（Process Integrity）几个角度来分析AWS悉尼区域服务中断事件的文档，并按照术语（Terminology）、背景（Background）、常见问题解答（FAQ）、流程（Flow）和附录（Appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **EC2实例**：Elastic Compute Cloud的简称，是AWS提供的一种可扩展的云计算服务。\n- **EBS卷**：Elastic Block Store的简称，是AWS提供的一种块存储服务。\n- **可用区（Availability Zone）**：AWS数据中心内的一个独立区域，具备冗余的电力、网络和设施。\n- **DRUPS（Diesel Rotary Uninterruptible Power Supply）**：一种结合了柴油发电机和机械UPS的不间断电源技术。\n- **DNS解析失败**：域名系统（Domain Name System）无法将域名转换为IP地址。\n- **自动伸缩组（Auto-Scaling Groups）**：AWS服务，用于自动调整EC2实例的数量以维持应用程序的所需容量。\n\n### 背景（Background）\n\n- 事件发生在AWS悉尼区域的一个可用区内，主要影响了EC2实例和相关的EBS卷。\n- 由于地区性恶劣天气，导致电力供应商的区域变电站失去电力，影响多个AWS设施。\n- 一个设施的电力冗余未能按设计工作，导致该可用区内大量实例失去电力。\n\n### 常见问题解答（FAQ）\n\n**Q: 电力中断是如何发生的？**\nA: 由于区域变电站失去电力，加上DRUPS系统未能及时切换到备用发电机，导致数据中心失去电力。\n\n**Q: AWS如何恢复服务？**\nA: 一旦电力恢复，自动系统开始重新启动客户的EC2实例和EBS卷。同时，团队手动恢复了剩余的实例。\n\n**Q: 有哪些问题在恢复过程中出现？**\nA: 恢复过程中出现了DNS解析失败和实例管理软件的潜在错误，导致恢复速度慢于预期。\n\n**Q: AWS如何防止此类事件再次发生？**\nA: AWS将增加额外的断路器以快速切断降级的电力，并改进恢复系统，包括修复潜在的软件问题和定期测试恢复流程。\n\n### 流程（Flow）\n\n1. **电力中断**：由于恶劣天气，电力供应商的区域变电站失去电力。\n2. **服务中断**：多个AWS设施失去电力，导致EC2实例和EBS卷受影响。\n3. **手动恢复**：技术人员手动重新连接电力线，并开始恢复服务。\n4. **自动恢复**：电力恢复后，自动系统开始重启受影响的实例和卷。\n5. **问题解决**：团队解决了DNS解析失败和自动伸缩组的问题。\n6. **改进措施**：AWS计划增强电力配置和测试恢复流程以防止未来类似事件。\n\n### 附录（Appendix）\n\n- **受影响的客户实例和卷**：超过80%的客户实例和卷在电力恢复后很快恢复在线。\n- **EBS卷丢失**：少于0.01%的卷在电力恢复后无法恢复。\n- **API延迟**：在恢复初期，客户在尝试启动新实例或调整自动伸缩组时遇到错误。\n- **客户建议**：建议客户跨多个可用区运行应用程序以保持高可用性。\n\n通过上述分析和总结，我们可以看到AWS团队对于服务中断事件的响应和恢复流程，以及他们为防止未来类似事件所采取的改进措施。这份文档提供了详细的事件分析和后续改进计划，有助于客户理解事件的影响和AWS的服务质量承诺。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region\n\nEarly Sunday morning, September 20, we had a DynamoDB service event in\nthe US-East Region that impacted DynamoDB customers in US-East, as well as some other services in the region. The following are some additional details on the root cause, subsequent impact to other AWS services that depend on DynamoDB, and corrective actions we're taking. \n**Some DynamoDB Context** \nAmong its many functions, DynamoDB stores and maintain tables for customers. A single DynamoDB table is separated into partitions, each containing a portion of the table's data. These partitions are spread onto many servers, both to provide consistent low-latency access and to replicate the data for durability. \nThe specific assignment of a group of partitions to a given server is called a \"membership.\" The membership of a set of table/partitions within a server is managed by DynamoDB's internal metadata service. The metadata service is internally replicated and runs across multiple datacenters. Storage servers hold the actual table data within a partition and need to periodically confirm that they have the correct membership. They do this by checking in with the metadata service and asking for their current membership assignment. In response, the metadata service retrieves the list of partitions and all related information from its own store, bundles this up into a message, and transmits back to the requesting storage server. A storage server will also get its membership assignment after a network disruption or on\nstartup. Once a storage server has completed processing its membership assignment, it verifies that it has the table/partition data locally stored, creates any new table/partitions assigned, and retrieves data from other storage servers to replicate existing partitions assigned. \n**The DynamoDB Event** \nOn Sunday, at 2:19am PDT, there was a brief network disruption that impacted a portion of DynamoDB's storage servers. Normally, this type of networking disruption is handled seamlessly and without change to the performance of DynamoDB, as affected storage servers query the metadata service for their membership, process any updates, and reconfirm their availability to accept requests. If the storage servers aren't able to retrieve this membership data back within a specific time period, they will retry the membership request and temporarily disqualify themselves from accepting requests. \nBut, on Sunday morning, a portion of the metadata service responses exceeded the retrieval and transmission time allowed by storage servers. As a result, some of the storage servers were unable to obtain their membership data, and removed themselves from taking requests. The reason\nthese metadata service requests were taking too long relates to a recent development in DynamoDB. Over the last few months, customers have rapidly adopted a new DynamoDB feature called Global Secondary Indexes (\"GSIs\"). GSIs allow customers to access their table data using alternate keys. Because GSIs are global, they have their own set of partitions on storage servers and therefore increase the overall size of a storage server's membership data. Customers can add multiple GSIs for a given table, so a table with large numbers of partitions could have its contribution of partition data to the membership lists quickly double or triple. With rapid adoption of GSIs by a number of customers with very large tables, the partitions-per-table ratio increased significantly. This, in turn, increased the size of some storage servers' membership lists significantly. With a larger size, the processing time inside the metadata service for some membership requests began to approach the retrieval time allowance by storage servers. We did not have detailed enough monitoring for this dimension (membership size), and didn't have enough capacity allocated to the metadata service to handle these much heavier requests. \nSo, when the network disruption occurred on Sunday morning, and a number of storage servers simultaneously requested their membership data, the metadata service was processing some membership lists that were now large enough that their processing time was near the time limit for retrieval. Multiple, simultaneous requests for these large memberships caused processing to slow further and eventually exceed the allotted time limit. This resulted in the disrupted storage servers failing to complete their membership renewal, becoming unavailable for requests, and retrying these requests. With the metadata service now under heavy load, it also no longer responded as quickly to storage servers uninvolved in the original network disruption, who were checking their membership data in the normal cadence of when they retrieve this information. Many of those storage servers also became unavailable for handling customer requests. Unavailable servers continued to retry requests for membership data, maintaining high load on the metadata service. Though many storage servers' renewal requests were succeeding, healthy storage servers that had successfully processed a membership request previously were having subsequent renewals fail and were transitioning back to an unavailable state. By 2:37am PDT, the error rate in customer requests to DynamoDB had risen far beyond any level experienced in the last 3 years, finally stabilizing at approximately 55%. \nInitially, we were unable to add capacity to the metadata service because it was under such high load, preventing us from successfully making the requisite administrative requests. After several failed attempts at adding capacity, at 5:06am PDT, we decided to pause requests to the metadata service. This action decreased retry activity, which relieved much of the load on the metadata service. With the metadata service now able to respond to administrative requests, we were able to add significant capacity. Once these adjustments were made, we were able to reactivate requests to the metadata service, put storage servers back into the customer request path, and allow normal load back on the metadata service. At 7:10am PDT, DynamoDB was restored to error rates low enough for most customers and AWS services dependent on DynamoDB to resume normal operations. \nThere's one other bit worth mentioning. After we resolved the key issue on Sunday, we were left with a low error rate, hovering between\n0.15%-0.25%. We knew there would be some cleanup to do after the event, and while this rate was higher than normal, it wasn't a rate that usually precipitates a dashboard post or creates issues for customers. As Monday progressed, we started to get more customers opening support cases about being impacted by tables being stuck in the updating or deleting stage or higher than normal error rates. We did not realize soon enough that this low overall error rate was giving some customers disproportionately high error rates. It was impacting a relatively small number of customers, but we should have posted the green-i to the dashboard sooner than we did on Monday. The issue turned out to be a metadata partition that was still not taking the amount of traffic that it should have been taking. The team worked carefully and diligently to restore that metadata partition to its full traffic volume, and closed this out on Monday. \nThere are several actions we'll take immediately to avoid a recurrence of Sunday's DynamoDB event. First, we have already significantly increased the capacity of the metadata service. Second, we are instrumenting stricter monitoring on performance dimensions, such as the membership size, to allow us to thoroughly understand their state and proactively plan for the right capacity. Third, we are reducing the rate at which storage nodes request membership data and lengthening the time allowed to process queries. Finally and longer term, we are segmenting the DynamoDB service so that it will have many instances of the metadata service each serving only portions of the storage server fleet. This will further contain the impact of software, performance/capacity, or infrastructure failures. \n</div> ::: \n::: title-wrapper ::: {.row .title} ::: {.twelve .columns} ### [Impact on Other Services]{#Impact_on_Other_Services} {#Impact_on_Other_Services} ::: ::: ::: \n::: aws-text-box <div> \nThere are several other AWS services that use DynamoDB that experienced problems during the event. Rather than list them all, which had similar explanations for their status, we'll list a few that customers most asked us about or where the actions are more independent from DynamoDB's Correction of Errors (\"COE\"). \n**Simple Queue Service (SQS)** \nIn the early stages of the DynamoDB event, the Amazon Simple Queue Service was delivering slightly elevated errors and latencies. Amazon\nSQS uses an internal table stored in DynamoDB to store information\ndescribing its queues. While the queue information is cached within SQS, and is not in the direct path for \"send-message\" and \"receive-message\" APIs, the caches are refreshed frequently to accommodate creation, deletion, and reassignment across infrastructure. When DynamoDB finished disabling traffic at 5:45am PDT (to enable the metadata service to recover), the Simple Queue Service was unable to read this data to refresh caches, resulting in significantly elevated error rates. Once DynamoDB began re-enabling customer traffic at 7:10am PDT, the Simple Queue Service recovered. No data in queues, or information describing queues was lost as a result of the event. \nIn addition to the actions being taken by the DynamoDB service, we will be adjusting our SQS metadata caching to ensure that send and receive operations continue even without prolonged access to the metadata table. \n**EC2 Auto Scaling** \nBetween 2:15am PDT and 7:10am PDT, the EC2 Auto Scaling Service delivered significantly increased API faults. From 7:10am PDT to 10:52am PDT, the Auto Scaling service was substantially delayed in bringing new instances into service, or terminating existing unhealthy instances. Existing instances continued to operate properly throughout the event. \nAuto Scaling stores information about its groups and launch configurations in an internal table in DynamoDB. When DynamoDB began to experience elevated error rates starting at 2:19am PDT, Auto Scaling could not update this internal table when APIs were called. Once DynamoDB began recovery at 7:10am PDT, the Auto Scaling APIs recovered. Recovery was incomplete at this time, as a significant backlog of scaling activities had built up throughout the event. The Auto Scaling service executes its launch and termination activities in a background scheduling service. Throughout the event, a very large amount of pending activities built up in this job scheduler and it took until 10:52am PDT to complete all of these tasks. \nIn addition to the actions taken by the DynamoDB team, to ensure we can\nrecover quickly when a large backlog of scaling activities accumulate, we will adjust the way we partition work on the fleet of Auto Scaling servers to allow for more parallelism in processing these jobs, integrate mechanisms to prune older scaling activities that have been\nsuperseded, and increase the capacity available to process scaling activities. \n**CloudWatch** \nStarting at 2:35am PDT, the Amazon CloudWatch Metrics Service began\nexperiencing delayed and missing EC2 Metrics along with slightly elevated errors. CloudWatch uses an internal table stored in DynamoDB to add information regarding Auto Scaling group membership to incoming EC2 metrics. From 2:35am PDT to 5:45am PDT, the elevated DynamoDB failure rates caused intermittent availability of EC2 metrics in CloudWatch. CloudWatch also observed an abnormally low rate of metrics publication\nfrom other services that were experiencing issues over this time period, further contributing to missing or delayed metrics. \nThen, from approximately 5:51am PDT to 7:10am PDT CloudWatch delivered significantly elevated error rates for PutMetricData calls affecting all AWS Service metrics and custom metrics. The impact was due to the significantly elevated error rates in DynamoDB for the group membership additions mentioned above. The CloudWatch Metrics Service was fully recovered at 7:29am PDT. \nWe understand how important metrics are, especially during an event. To further increase the resilience of CloudWatch, we will adjust our caching strategy for the DynamoDB group membership data and only require refresh for the smallest possible set of metrics. We also have been\ndeveloping faster metrics delivery through write-through caching. This cache will provide the ability to present metrics directly before persisting them and will, as a side benefit, provide additional protection during an event. \n**Console** \nThe AWS Console was impacted for some customers from 5:45am PDT to 7:10am PDT. Customers who were already logged into the Console would have continued to remain connected. Customers attempting to log into the Console during this period saw much higher latency in the login process. This was due to a very long timeout being set on an API call that relied on DynamoDB. The API call did not have to complete successfully to allow login to proceed but, with the long timeout, it blocked progress for tens of seconds while it waited to finish. It should have simply failed quickly and allowed progress on login to continue. \nThe timeout had already been changed in a version of the login code that has entered our test process. Unfortunately it wasn't yet rolled out when the event happened. We will make this change in the coming days. The reduced timeout will mitigate any impact of latency in the API call on the Console. \n**Final Words** \nWe apologize for the impact to affected customers. While we are proud of the last three years of availability on DynamoDB (it's effectively been\n100%), we know how critical this service is to customers, both because many use it for mission-critical operations and because AWS services also rely on it. For us, availability is the most important feature of DynamoDB, and we will do everything we can to learn from the event and to avoid a recurrence in the future.\n\nAssistant:", "response": [["A \"network disruption\" caused metadata services to experience load that caused response times to exceed timeout values, causing storage nodes to take themselves down. Nodes that took themselves down continued to retry, ensuring that load on metadata services couldn't decrease"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region\n\nEarly Sunday morning, September 20, we had a DynamoDB service event in\nthe US-East Region that impacted DynamoDB customers in US-East, as well as some other services in the region. The following are some additional details on the root cause, subsequent impact to other AWS services that depend on DynamoDB, and corrective actions we're taking. \n**Some DynamoDB Context** \nAmong its many functions, DynamoDB stores and maintain tables for customers. A single DynamoDB table is separated into partitions, each containing a portion of the table's data. These partitions are spread onto many servers, both to provide consistent low-latency access and to replicate the data for durability. \nThe specific assignment of a group of partitions to a given server is called a \"membership.\" The membership of a set of table/partitions within a server is managed by DynamoDB's internal metadata service. The metadata service is internally replicated and runs across multiple datacenters. Storage servers hold the actual table data within a partition and need to periodically confirm that they have the correct membership. They do this by checking in with the metadata service and asking for their current membership assignment. In response, the metadata service retrieves the list of partitions and all related information from its own store, bundles this up into a message, and transmits back to the requesting storage server. A storage server will also get its membership assignment after a network disruption or on\nstartup. Once a storage server has completed processing its membership assignment, it verifies that it has the table/partition data locally stored, creates any new table/partitions assigned, and retrieves data from other storage servers to replicate existing partitions assigned. \n**The DynamoDB Event** \nOn Sunday, at 2:19am PDT, there was a brief network disruption that impacted a portion of DynamoDB's storage servers. Normally, this type of networking disruption is handled seamlessly and without change to the performance of DynamoDB, as affected storage servers query the metadata service for their membership, process any updates, and reconfirm their availability to accept requests. If the storage servers aren't able to retrieve this membership data back within a specific time period, they will retry the membership request and temporarily disqualify themselves from accepting requests. \nBut, on Sunday morning, a portion of the metadata service responses exceeded the retrieval and transmission time allowed by storage servers. As a result, some of the storage servers were unable to obtain their membership data, and removed themselves from taking requests. The reason\nthese metadata service requests were taking too long relates to a recent development in DynamoDB. Over the last few months, customers have rapidly adopted a new DynamoDB feature called Global Secondary Indexes (\"GSIs\"). GSIs allow customers to access their table data using alternate keys. Because GSIs are global, they have their own set of partitions on storage servers and therefore increase the overall size of a storage server's membership data. Customers can add multiple GSIs for a given table, so a table with large numbers of partitions could have its contribution of partition data to the membership lists quickly double or triple. With rapid adoption of GSIs by a number of customers with very large tables, the partitions-per-table ratio increased significantly. This, in turn, increased the size of some storage servers' membership lists significantly. With a larger size, the processing time inside the metadata service for some membership requests began to approach the retrieval time allowance by storage servers. We did not have detailed enough monitoring for this dimension (membership size), and didn't have enough capacity allocated to the metadata service to handle these much heavier requests. \nSo, when the network disruption occurred on Sunday morning, and a number of storage servers simultaneously requested their membership data, the metadata service was processing some membership lists that were now large enough that their processing time was near the time limit for retrieval. Multiple, simultaneous requests for these large memberships caused processing to slow further and eventually exceed the allotted time limit. This resulted in the disrupted storage servers failing to complete their membership renewal, becoming unavailable for requests, and retrying these requests. With the metadata service now under heavy load, it also no longer responded as quickly to storage servers uninvolved in the original network disruption, who were checking their membership data in the normal cadence of when they retrieve this information. Many of those storage servers also became unavailable for handling customer requests. Unavailable servers continued to retry requests for membership data, maintaining high load on the metadata service. Though many storage servers' renewal requests were succeeding, healthy storage servers that had successfully processed a membership request previously were having subsequent renewals fail and were transitioning back to an unavailable state. By 2:37am PDT, the error rate in customer requests to DynamoDB had risen far beyond any level experienced in the last 3 years, finally stabilizing at approximately 55%. \nInitially, we were unable to add capacity to the metadata service because it was under such high load, preventing us from successfully making the requisite administrative requests. After several failed attempts at adding capacity, at 5:06am PDT, we decided to pause requests to the metadata service. This action decreased retry activity, which relieved much of the load on the metadata service. With the metadata service now able to respond to administrative requests, we were able to add significant capacity. Once these adjustments were made, we were able to reactivate requests to the metadata service, put storage servers back into the customer request path, and allow normal load back on the metadata service. At 7:10am PDT, DynamoDB was restored to error rates low enough for most customers and AWS services dependent on DynamoDB to resume normal operations. \nThere's one other bit worth mentioning. After we resolved the key issue on Sunday, we were left with a low error rate, hovering between\n0.15%-0.25%. We knew there would be some cleanup to do after the event, and while this rate was higher than normal, it wasn't a rate that usually precipitates a dashboard post or creates issues for customers. As Monday progressed, we started to get more customers opening support cases about being impacted by tables being stuck in the updating or deleting stage or higher than normal error rates. We did not realize soon enough that this low overall error rate was giving some customers disproportionately high error rates. It was impacting a relatively small number of customers, but we should have posted the green-i to the dashboard sooner than we did on Monday. The issue turned out to be a metadata partition that was still not taking the amount of traffic that it should have been taking. The team worked carefully and diligently to restore that metadata partition to its full traffic volume, and closed this out on Monday. \nThere are several actions we'll take immediately to avoid a recurrence of Sunday's DynamoDB event. First, we have already significantly increased the capacity of the metadata service. Second, we are instrumenting stricter monitoring on performance dimensions, such as the membership size, to allow us to thoroughly understand their state and proactively plan for the right capacity. Third, we are reducing the rate at which storage nodes request membership data and lengthening the time allowed to process queries. Finally and longer term, we are segmenting the DynamoDB service so that it will have many instances of the metadata service each serving only portions of the storage server fleet. This will further contain the impact of software, performance/capacity, or infrastructure failures. \n</div> ::: \n::: title-wrapper ::: {.row .title} ::: {.twelve .columns} ### [Impact on Other Services]{#Impact_on_Other_Services} {#Impact_on_Other_Services} ::: ::: ::: \n::: aws-text-box <div> \nThere are several other AWS services that use DynamoDB that experienced problems during the event. Rather than list them all, which had similar explanations for their status, we'll list a few that customers most asked us about or where the actions are more independent from DynamoDB's Correction of Errors (\"COE\"). \n**Simple Queue Service (SQS)** \nIn the early stages of the DynamoDB event, the Amazon Simple Queue Service was delivering slightly elevated errors and latencies. Amazon\nSQS uses an internal table stored in DynamoDB to store information\ndescribing its queues. While the queue information is cached within SQS, and is not in the direct path for \"send-message\" and \"receive-message\" APIs, the caches are refreshed frequently to accommodate creation, deletion, and reassignment across infrastructure. When DynamoDB finished disabling traffic at 5:45am PDT (to enable the metadata service to recover), the Simple Queue Service was unable to read this data to refresh caches, resulting in significantly elevated error rates. Once DynamoDB began re-enabling customer traffic at 7:10am PDT, the Simple Queue Service recovered. No data in queues, or information describing queues was lost as a result of the event. \nIn addition to the actions being taken by the DynamoDB service, we will be adjusting our SQS metadata caching to ensure that send and receive operations continue even without prolonged access to the metadata table. \n**EC2 Auto Scaling** \nBetween 2:15am PDT and 7:10am PDT, the EC2 Auto Scaling Service delivered significantly increased API faults. From 7:10am PDT to 10:52am PDT, the Auto Scaling service was substantially delayed in bringing new instances into service, or terminating existing unhealthy instances. Existing instances continued to operate properly throughout the event. \nAuto Scaling stores information about its groups and launch configurations in an internal table in DynamoDB. When DynamoDB began to experience elevated error rates starting at 2:19am PDT, Auto Scaling could not update this internal table when APIs were called. Once DynamoDB began recovery at 7:10am PDT, the Auto Scaling APIs recovered. Recovery was incomplete at this time, as a significant backlog of scaling activities had built up throughout the event. The Auto Scaling service executes its launch and termination activities in a background scheduling service. Throughout the event, a very large amount of pending activities built up in this job scheduler and it took until 10:52am PDT to complete all of these tasks. \nIn addition to the actions taken by the DynamoDB team, to ensure we can\nrecover quickly when a large backlog of scaling activities accumulate, we will adjust the way we partition work on the fleet of Auto Scaling servers to allow for more parallelism in processing these jobs, integrate mechanisms to prune older scaling activities that have been\nsuperseded, and increase the capacity available to process scaling activities. \n**CloudWatch** \nStarting at 2:35am PDT, the Amazon CloudWatch Metrics Service began\nexperiencing delayed and missing EC2 Metrics along with slightly elevated errors. CloudWatch uses an internal table stored in DynamoDB to add information regarding Auto Scaling group membership to incoming EC2 metrics. From 2:35am PDT to 5:45am PDT, the elevated DynamoDB failure rates caused intermittent availability of EC2 metrics in CloudWatch. CloudWatch also observed an abnormally low rate of metrics publication\nfrom other services that were experiencing issues over this time period, further contributing to missing or delayed metrics. \nThen, from approximately 5:51am PDT to 7:10am PDT CloudWatch delivered significantly elevated error rates for PutMetricData calls affecting all AWS Service metrics and custom metrics. The impact was due to the significantly elevated error rates in DynamoDB for the group membership additions mentioned above. The CloudWatch Metrics Service was fully recovered at 7:29am PDT. \nWe understand how important metrics are, especially during an event. To further increase the resilience of CloudWatch, we will adjust our caching strategy for the DynamoDB group membership data and only require refresh for the smallest possible set of metrics. We also have been\ndeveloping faster metrics delivery through write-through caching. This cache will provide the ability to present metrics directly before persisting them and will, as a side benefit, provide additional protection during an event. \n**Console** \nThe AWS Console was impacted for some customers from 5:45am PDT to 7:10am PDT. Customers who were already logged into the Console would have continued to remain connected. Customers attempting to log into the Console during this period saw much higher latency in the login process. This was due to a very long timeout being set on an API call that relied on DynamoDB. The API call did not have to complete successfully to allow login to proceed but, with the long timeout, it blocked progress for tens of seconds while it waited to finish. It should have simply failed quickly and allowed progress on login to continue. \nThe timeout had already been changed in a version of the login code that has entered our test process. Unfortunately it wasn't yet rolled out when the event happened. We will make this change in the coming days. The reduced timeout will mitigate any impact of latency in the API call on the Console. \n**Final Words** \nWe apologize for the impact to affected customers. While we are proud of the last three years of availability on DynamoDB (it's effectively been\n100%), we know how critical this service is to customers, both because many use it for mission-critical operations and because AWS services also rely on it. For us, availability is the most important feature of DynamoDB, and we will do everything we can to learn from the event and to avoid a recurrence in the future.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，并按照terminology、background、faq、flow、appendix的分类进行总结整理。\n\n### Terminology (术语解释)\n- **Logical (逻辑性)**: 指文档内容的条理性和合理性，包括对事件的逻辑解释和因果关系阐述。\n- **Bridging (桥接性)**: 指文档如何将技术细节与用户可能的疑问或需求连接起来，提供清晰的信息传递。\n- **Actionable (可操作性)**: 指文档提供的信息是否足够明确，以便用户或服务人员可以采取实际行动解决问题。\n- **Simplicity (简洁性)**: 指文档是否避免了不必要的复杂性，提供了简明扼要的信息。\n- **Process Integrity (流程完整性)**: 指文档描述的流程是否完整，是否包含了所有必要的步骤和信息。\n\n### Background (背景介绍)\n- **事件概述**: 2020年9月20日周日早晨，美国东部地区的DynamoDB服务发生了中断事件，影响了该地区的DynamoDB客户以及其他依赖DynamoDB的AWS服务。\n- **DynamoDB功能**: DynamoDB为客户存储和维护表格数据，通过分区来分散数据以提供低延迟访问和数据持久性。\n\n### FAQ (常见问题解答)\n- **Q: 事件的根本原因是什么？**\n  - A: 网络中断导致部分存储服务器无法及时获取元数据服务的成员资格数据，加之新的Global Secondary Indexes（GSIs）特性导致存储服务器的成员资格数据量增加，超出了存储服务器的检索和传输时间限制。\n- **Q: 采取了哪些纠正措施？**\n  - A: 暂停了对元数据服务的请求以减轻负载，增加了元数据服务的容量，并调整了监控和请求频率。\n- **Q: 受影响的其他AWS服务有哪些？**\n  - A: 包括Simple Queue Service (SQS)、EC2 Auto Scaling、CloudWatch和AWS Console。\n\n### Flow (流程描述)\n1. **事件发生**: 网络中断导致存储服务器无法获取成员资格数据。\n2. **影响扩散**: 由于元数据服务响应时间延长，更多存储服务器无法处理请求。\n3. **纠正措施**: 暂停元数据服务请求，增加服务容量，调整请求频率。\n4. **服务恢复**: 重新激活元数据服务，恢复正常运营。\n5. **后续改进**: 增加元数据服务容量，改进监控，调整工作流程，提高服务的韧性。\n\n### Appendix (附录)\n- **AWS支持的浏览器**: 从2022年7月31日起，不再支持Internet Explorer。推荐使用Chrome、Firefox、Edge和Safari。\n- **进一步行动**: AWS将继续监控和改进服务，以防止类似事件再次发生。\n\n通过上述分析和整理，我们可以清晰地理解DynamoDB服务中断事件的原因、影响、采取的措施以及未来的改进计划。文档提供了详细的背景信息、逻辑性强的解释、可操作的解决方案、简洁明了的流程描述，以及完整的附录信息，确保了信息的完整性和流程的完整性。"]]}]
[{"prompt": "User: 下面是一篇故障报告： **2019 年 8 月 28 日 (中国标准时间) 更新:** \n在我们一开始的总结中提到，这次事件影响了东京区中一个可用区（\"AZ\"）里的一小部分。被影响到的有Amazon\nEC2及EBS资源，一些使用到受损EC2资源作为底层硬件的其它服务(比如RDS,Redshift, ElastiCache 以及 Workspaces)亦受到了影响。在我们进一步和客户调查这个事件的时候，我们也看到了一些少数的例子，客户在用多个可用区的时候也受到了一定的影响（比如一些客户在同时使用Application\nLoad Balancer，以及AWS Web Application\nFirewall或者粘性会话时，遇到了比预期比例還高的内部服务器错误）。我们会直接和被这些少数问题影响的客户直接分享更多详细的信息。 \n## 关于 Amazon EC2 以及 Amazon EBS 在东京区域 (AP-NORTHEAST-1) 的服务事件的说明  \n针对在东京区域 (AP-NORTHEAST-1) 的服务中断事件，我们在这里提供更多信息。从 2019 年 8 月 23 日 11:36 AM CST (中国标准时间)开始，一小部分的 EC2 服务器在东京 (AP-NORTHEAST-1) 区域中单一可用区 (Availability Zone) 由于服务器过热造成停机。这导致在该可用区中受到影响的 EC2 实例与 EBS 卷效能降低。造成服务器过热的原因是控制系统故障，造成受影响的可用区的部分冷却系统失效。受到影响的冷却系统已经在 2:21 PM CST (中国标准时间)修复，服务器温度也恢复到正常状态。在温度恢复正常后，EC2 实例的电源供应也已恢复。在 5:30 PM CST (中国标准时间) ，大部分受影响的 EC2 实例与 EBS 卷都恢复正常工作，但仍有一小部分的实例与卷因为过热与断电暂时无法修复，因为底层硬件的故障，其中有些实例与卷需要更多的时间进行修复。 \n除了 EC2 实例与 EBS 卷受到影响外，在 12:21 PM CST (中国标准时间) EC2 RunInstances API 也受到了影响。在受影响的可用区中，尝试启动新的 EC2 实例和和尝试使用 RunInstances API 的 \"idempotency token\" 功能 (一个允许用户启动新的实例时重试而不会产生多余的实例的功能)时，也有发生错误。其他没有调用 \"idempotency token\"的 API 则可正常运作。这个事件也导致透过 \"idempotency token\" 使用 Auto Scaling 时，无法启动新实例。后台团队已经于 1:51 PM CST (中国标准时间) 修复了 \"idempotency token\" 与 Auto Scaling 相关的问题。并且于 3:05 PM CST(中国标准时间)在受影响的可用区中，修复了EC2 控制面板的子系统，开启新实例的功能已经可以正常工作。但在本事件中受到影响的卷所建立的新快照 (Snapshot) 依旧有一定的错误率。\n本次事件是由于数据中心负责控制和优化冷却的控制系统故障所造成，这个控制系统在多个主机都有部署以实现高可用性，本控制系统中包含了允许与风扇、冷却器和温度传感器等硬件组件相互传递信号的第三方的程序，该程序可以直接或透过 Programmable Logic Controllers (PLC) 来与实际的硬件组件沟通。在这事件发生前，数据中心的控制系统正在为了其中一台失效的控制主机进行备份处理，在备份处理中，控制系统要彼此互相交换信号 (例如：冷却装置与温度传感器交换信号)以保持最新的信息。由于该第三方程序中的一个错误，导致控制系统与组件过度的进行信息交换而造成控制系统无法回应。我们的数据中心被设计成一旦控制系统发生错误，冷却系统就会自动进入最冷的模式，直到控制系统恢复正常为止，这样的设计对于我们大部分的数据中心都是有效的，但有一小部分的数据中心，由于冷却系统无法正确进入安全降温模式，而造成系统关机。我们的数据中心加入了安全防护设计，在控制系统故障时，可以略过控制系统，直接进入净空模式将数据中心中的热空气迅速排出，但控制中心的团队在启动净空模式时发生了故障，所以数据中心的温度才会持续攀升，而服务器在到达温度上限后也开始自动关机了。由于数据中心的控制系统故障，维运团队无法得知数据中心冷却系统的即时信息，在进行故障排除时，团队必须要对所有组件进行逐一的人工检查，才能让控制系统进入最冷模式，在这故障排除的过程中，发现控制空调组件的 PLC 控制器无法回应，控制器需要进行重置，是 PLC 控制器的错误造成了预设的冷却模式与净空模式无法正确动作，在 PLC 控制器被重置之后，该可用区数据中心的冷却系统就可以正常工作了，而数据中心的过高的温度也开始慢慢降低。 \n我们仍在与第三方供应商合作以了解导致控制系统和受影响的 PLC 无响应的错误和后续交互。 在此期间，我们已禁用在我们的控制系统上触发此错误的故障转移模式，以确保我们不会再次出现此问题。 我们还培训了我们的本地运营团队，以便在发生这种情况时快速识别和修复这种情况，并且我们相信，如果再次发生类似情况，无论什么原因，我们可以在客户受影响之前重置系统。 最后，我们正在努力修改我们控制受影响的空气处理单元的方式，以确保\"清除模式\"能够完全绕过PLC控制器。 这是我们在最新的数据中心设计中开始使用的一种方法，即使 PLC 无响应，我们也会更加确信\"清除模式\"将起作用。 \n在这次事件中，EC2 实例以及 EBS 储存在同一区域的其它的可用区没有受到影响。同时在多个可用区上充分执行他们的应用程序的客户，在这次的事件中依然可以维持服务可用。对于需要绝对高可用的客户，我们持续建议您使用高可用性的架构设计。任何与应用程序相关的元件都应该采用这种容错设计。 \n针对这次事件造成的不便，我们感到十分抱歉。我们理解对于我们的用户的业务来说，稳定的服务有多么至关重要。我们也从未满足於我们所提供的响应速度以及服务品质。对于这次事件，我们将会尽一切所能从此次事件中学习并且继续不断改善我们的服务。\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从logical（逻辑性）、bridging（衔接性）、actionable（可操作性）、simplicity（简洁性）和process integrity（流程完整性）几个角度来分析文档，并按照terminology（术语）、background（背景）、faq（常见问题解答）、flow（流程）和appendix（附录）进行总结整理。\n\n### Terminology (术语)\n- **Availability Zone (AZ)**: 亚马逊云计算服务中的一个或多个数据中心，提供电力、网络和连接设施。\n- **EC2**: 亚马逊的弹性计算云服务，允许用户租用虚拟计算资源。\n- **EBS (Elastic Block Store)**: 亚马逊云计算服务中的一种块存储服务。\n- **Auto Scaling**: 自动调整计算资源以维持应用程序性能的服务。\n- **idempotency token**: 一种令牌，用于确保在重试操作时不会产生意外的结果。\n- **PLC (Programmable Logic Controller)**: 可编程逻辑控制器，用于自动化控制工业过程。\n\n### Background (背景)\n- 事件发生在2019年8月23日，影响了东京区域(AP-NORTHEAST-1)的单一可用区。\n- 由于控制系统故障，导致冷却系统失效，进而引发服务器过热和停机。\n- 影响了Amazon EC2和Amazon EBS资源，以及其他依赖EC2实例的服务。\n- 事件发生后，AWS团队迅速响应，通过手动重置和修复控制设备来恢复服务。\n\n### FAQ (常见问题解答)\n- **问: 这次事件的原因是什么？**\n  - 答: 控制系统故障导致冷却系统失效，从而引起服务器过热和停机。\n- **问: 受影响的服务有哪些？**\n  - 答: 受影响的服务包括Amazon EC2、Amazon EBS、RDS、Redshift、ElastiCache和Workspaces。\n- **问: AWS如何应对此类事件？**\n  - 答: AWS通过手动干预和修复控制设备来恢复服务，并与第三方供应商合作以防止未来事件的发生。\n\n### Flow (流程)\n1. **事件发生**: 2019年8月23日，东京区域的单一可用区发生冷却系统失效。\n2. **影响评估**: 确定受影响的服务和资源，包括EC2实例和EBS卷。\n3. **响应措施**: AWS团队手动重置控制设备，恢复冷却系统。\n4. **服务恢复**: 受影响的实例和卷逐步恢复，部分硬件因故障而退役。\n5. **后续改进**: AWS禁用了触发故障的控制系统的故障转移模式，并培训运营团队以快速应对未来可能的类似事件。\n\n### Appendix (附录)\n- **更新日志**: 2019年8月28日，AWS更新了事件的详细信息，包括对受影响客户的额外影响和AWS的响应措施。\n- **客户建议**: 对于需要高可用性的应用，AWS建议使用跨多个可用区的架构设计。\n- **支持信息**: AWS对此次事件造成的不便表示歉意，并承诺将持续改善服务质量。\n\n通过以上分析和总结，我们可以更清晰地理解AWS在面对服务中断事件时的响应流程、采取的措施以及对未来改进的承诺。这有助于客户更好地理解AWS服务的稳定性和可靠性，并为可能的未来事件做好准备。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region\n\n**April 29, 2011** \nNow that we have fully restored functionality to all affected services, we would like to share more details with our customers about the events that occurred with the Amazon Elastic Compute Cloud (\"EC2\") last week, our efforts to restore the services, and what we are doing to prevent this sort of issue from happening again. We are very aware that many of our customers were significantly impacted by this event, and as with any significant service issue, our intention is to share the details of what happened and how we will improve the service for our customers. \nThe issues affecting EC2 customers last week primarily involved a subset of the Amazon Elastic Block Store (\"EBS\") volumes in a single Availability Zone within the US East Region that became unable to service read and write operations. In this document, we will refer to these as \"stuck\" volumes. This caused instances trying to use these affected volumes to also get \"stuck\" when they attempted to read or write to them. In order to restore these volumes and stabilize the EBS cluster in that Availability Zone, we disabled all control APIs (e.g. Create Volume, Attach Volume, Detach Volume, and Create Snapshot) for EBS in the affected Availability Zone for much of the duration of the event. For two periods during the first day of the issue, the degraded EBS cluster affected the EBS APIs and caused high error rates and latencies for EBS calls to these APIs across the entire US East Region. As with any complicated operational issue, this one was caused by several root causes interacting with one another and therefore gives us many opportunities to protect the service against any similar event reoccurring. \n**Overview of EBS System** \nIt is helpful to understand the EBS architecture so that we can better explain the event. EBS is a distributed, replicated block data store that is optimized for consistency and low latency read and write access from EC2 instances. There are two main components of the EBS service: (i) a set of EBS clusters (each of which runs entirely inside of an\nAvailability Zone) that store user data and serve requests to EC2 instances; and (ii) a set of control plane services that are used to coordinate user requests and propagate them to the EBS clusters running in each of the Availability Zones in the Region. \nAn EBS cluster is comprised of a set of EBS nodes. These nodes store replicas of EBS volume data and serve read and write requests to EC2 instances. EBS volume data is replicated to multiple EBS nodes for durability and availability. Each EBS node employs a peer-to-peer based, fast failover strategy that aggressively provisions new replicas if one of the copies ever gets out of sync or becomes unavailable. The nodes in\nan EBS cluster are connected to each other via two networks. The primary network is a high bandwidth network used in normal operation for all necessary communication with other EBS nodes, with EC2 instances, and with the EBS control plane services. The secondary network, the replication network, is a lower capacity network used as a back-up network to allow EBS nodes to reliably communicate with other nodes in\nthe EBS cluster and provide overflow capacity for data replication. This network is not designed to handle all traffic from the primary network but rather provide highly-reliable connectivity between EBS nodes inside of an EBS cluster. \nWhen a node loses connectivity to a node to which it is replicating data to, it assumes the other node failed. To preserve durability, it must find a new node to which it can replicate its data (this is called re-mirroring). As part of the re-mirroring process, the EBS node searches its EBS cluster for another node with enough available server space, establishes connectivity with the server, and propagates the volume data. In a normally functioning cluster, finding a location for the new replica occurs in milliseconds. While data is being re-mirrored, all nodes that have copies of the data hold onto the data until they can\nconfirm that another node has taken ownership of their portion. This provides an additional level of protection against customer data loss. Also, when data on a customer's volume is being re-mirrored, access to that data is blocked until the system has identified a new primary (or writable) replica. This is required for consistency of EBS volume data under all potential failure modes. From the perspective of an EC2 instance trying to do I/O on a volume while this is happening, the volume will appear \"stuck\". \nIn addition to the EBS clusters, there is a set of control plane services that accepts user requests and propagates them to the appropriate EBS cluster. There is one set of EBS control plane services per EC2 Region, but the control plane itself is highly distributed across the Availability Zones to provide availability and fault tolerance. These control plane services also act as the authority to the EBS clusters when they elect primary replicas for each volume in the cluster (for consistency, there must only be a single primary replica for each volume at any time). While there are a few different services that comprise the control plane, we will refer to them collectively as the \"EBS control plane\" in this document. \n**Primary Outage** \nAt 12:47 AM PDT on April 21st, a network change was performed as part of our normal AWS scaling activities in a single Availability Zone in the US East Region. The configuration change was to upgrade the capacity of the primary network. During the change, one of the standard steps is to shift traffic off of one of the redundant routers in the primary EBS network to allow the upgrade to happen. The traffic shift was executed incorrectly and rather than routing the traffic to the other router on\nthe primary network, the traffic was routed onto the lower capacity redundant EBS network. For a portion of the EBS cluster in the affected Availability Zone, this meant that they did not have a functioning primary or secondary network because traffic was purposely shifted away from the primary network and the secondary network couldn't handle the traffic level it was receiving. As a result, many EBS nodes in the affected Availability Zone were completely isolated from other EBS nodes in its cluster. Unlike a normal network interruption, this change disconnected both the primary and secondary network simultaneously, leaving the affected nodes completely isolated from one another. \nWhen this network connectivity issue occurred, a large number of EBS nodes in a single EBS cluster lost connection to their replicas. When\nthe incorrect traffic shift was rolled back and network connectivity was restored, these nodes rapidly began searching the EBS cluster for available server space where they could re-mirror data. Once again, in a normally functioning cluster, this occurs in milliseconds. In this case, because the issue affected such a large number of volumes concurrently, the free capacity of the EBS cluster was quickly exhausted, leaving many of the nodes \"stuck\" in a loop, continuously searching the cluster for free space. This quickly led to a \"re-mirroring storm,\" where a large number of volumes were effectively \"stuck\" while the nodes searched the cluster for the storage space it needed for its new replica. At this point, about 13% of the volumes in the affected Availability Zone were in this \"stuck\" state. \nAfter the initial sequence of events described above, the degraded EBS cluster had an immediate impact on the EBS control plane. When the EBS cluster in the affected Availability Zone entered the re-mirroring storm and exhausted its available capacity, the cluster became unable to service \"create volume\" API requests. Because the EBS control plane (and the create volume API in particular) was configured with a long time-out period, these slow API calls began to back up and resulted in thread starvation in the EBS control plane. The EBS control plane has a regional pool of available threads it can use to service requests. When\nthese threads were completely filled up by the large number of queued requests, the EBS control plane had no ability to service API requests and began to fail API requests for other Availability Zones in that Region as well. At 2:40 AM PDT on April 21st, the team deployed a change that disabled all new Create Volume requests in the affected Availability Zone, and by 2:50 AM PDT, latencies and error rates for all other EBS related APIs recovered. \nTwo factors caused the situation in this EBS cluster to degrade further during the early part of the event. First, the nodes failing to find new nodes did not back off aggressively enough when they could not find space, but instead, continued to search repeatedly. There was also a race condition in the code on the EBS nodes that, with a very low probability, caused them to fail when they were concurrently closing a large number of requests for replication. In a normally operating EBS cluster, this issue would result in very few, if any, node crashes; however, during this re-mirroring storm, the volume of connection\nattempts was extremely high, so it began triggering this issue more frequently. Nodes began to fail as a result of the bug, resulting in\nmore volumes left needing to re-mirror. This created more \"stuck\" volumes and added more requests to the re-mirroring storm. \nBy 5:30 AM PDT, error rates and latencies again increased for EBS API calls across the Region. When data for a volume needs to be re-mirrored, a negotiation must take place between the EC2 instance, the EBS nodes with the volume data, and the EBS control plane (which acts as an\nauthority in this process) so that only one copy of the data is designated as the primary replica and recognized by the EC2 instance as the place where all accesses should be sent. This provides strong consistency of EBS volumes. As more EBS nodes continued to fail because of the race condition described above, the volume of such negotiations with the EBS control plane increased. Because data was not being successfully re-mirrored, the number of these calls increased as the system retried and new requests came in. The load caused a brown out of the EBS control plane and again affected EBS APIs across the Region. At 8:20 AM PDT, the team began disabling all communication between the degraded EBS cluster in the affected Availability Zone and the EBS control plane. While this prevented all EBS API access in the affected Availability Zone (we will discuss recovery of this in the next section), other latencies and error rates returned to normal for EBS APIs for the rest of the Region. \nA large majority of the volumes in the degraded EBS cluster were still functioning properly and the focus was to recover the cluster without affecting more volumes. At 11:30AM PDT, the team developed a way to prevent EBS servers in the degraded EBS cluster from futilely contacting other servers (who didn't have free space at this point anyway) without affecting the other essential communication between nodes in the cluster. After this change was made, the cluster stopped degrading further and additional volumes were no longer at risk of becoming \"stuck\". Before this change was deployed, the failed servers resulting from the race condition resulted in an additional 5% of the volumes in\nthe affected Availability Zone becoming \"stuck\". However, volumes were also slowly re-mirroring as some capacity was made available which allowed existing \"stuck\" volumes to become \"unstuck\". The net result was that when this change was deployed, the total \"stuck\" volumes in the affected Availability Zone was 13%. \nCustomers also experienced elevated error rates until Noon PDT on April 21st when attempting to launch new EBS-backed EC2 instances in\nAvailability Zones other than the affected zone. This occurred for approximately 11 hours, from the onset of the outage until Noon PM PDT on April 21st. Except for the periods of broader API issues describe above, customers were able to create EBS-backed EC2 instances but were experiencing significantly-elevated error rates and latencies. New EBS-backed EC2 launches were being affected by a specific API in the EBS control plane that is only needed for attaching new instances to volumes. Initially, our alarming was not fine-grained enough for this EBS control plane API and the launch errors were overshadowed by the general error from the degraded EBS cluster. At 11:30 AM PDT, a change to the EBS control plane fixed this issue and latencies and error rates for new EBS-backed EC2 instances declined rapidly and returned to near-normal at Noon PDT. \n**Recovering EBS in the Affected Availability Zone** \nBy 12:04 PM PDT on April 21st, the outage was contained to the one affected Availability Zone and the degraded EBS cluster was stabilized. APIs were working well for all other Availability Zones and additional volumes were no longer becoming \"stuck\". Our focus shifted to completing the recovery. Approximately 13% of the volumes in the Availability Zone remained \"stuck\" and the EBS APIs were disabled in that one affected Availability Zone. The key priority became bringing additional storage capacity online to allow the \"stuck\" volumes to find enough space to create new replicas. \nThe team faced two challenges which delayed getting capacity online. First, when a node fails, the EBS cluster does not reuse the failed node until every data replica is successfully re-mirrored. This is a conscious decision so that we can recover data if a cluster fails to behave as designed. Because we did not want to re-purpose this failed capacity until we were sure we could recover affected user volumes on\nthe failed nodes, the team had to install a large amount of additional new capacity to replace that capacity in the cluster. This required the time-consuming process of physically relocating excess server capacity from across the US East Region and installing that capacity into the degraded EBS cluster. Second, because of the changes made to reduce the node-to-node communication used by peers to find new capacity (which is what stabilized the cluster in the step described above), the team had difficulty incorporating the new capacity into the cluster. The team had to carefully make changes to their negotiation throttles to allow negotiation to occur with the newly-built servers without again\ninundating the old servers with requests that they could not service. This process took longer than we expected as the team had to navigate a number of issues as they worked around the disabled communication. At about 02:00AM PDT on April 22nd, the team successfully started adding significant amounts of new capacity and working through the replication\nbacklog. Volumes were restored consistently over the next nine hours and all but about 2.2% of the volumes in the affected Availability Zone were restored by 12:30PM PDT on April 22nd. While the restored volumes were fully replicated, not all of them immediately became \"unstuck\" from the perspective of the attached EC2 instances because some were blocked waiting for the EBS control plane to be contactable, so they could safely re-establish a connection with the EC2 instance and elect a new writable copy. \nOnce there was sufficient capacity added to the cluster, the team worked on re-establishing EBS control plane API access to the affected Availability Zone and restoring access to the remaining \"stuck\" volumes. There was a large backlog of state changes that had to be propagated both from the degraded EBS nodes to the EBS control plane and vice versa. This effort was done gradually to avoid impact to the restored volumes and the EBS control plane. Our initial attempts to bring API access online to the impacted Availability Zone centered on throttling the state propagation to avoid overwhelming the EBS control plane. We also began building out a separate instance of the EBS control plane, one we could keep partitioned to the affected Availability Zone to avoid impacting other Availability Zones in the Region, while we processed the backlog. We rapidly developed throttles that turned out to be too coarse-grained to permit the right requests to pass through and stabilize the system. Through the evening of April 22nd into the morning of April 23rd, we worked on developing finer-grain throttles. By Saturday morning, we had finished work on the dedicated EBS control plane and the finer-grain throttles. Initial tests of traffic against the EBS control plane demonstrated progress and shortly after 11:30 AM PDT on April 23rd we began steadily processing the backlog. By 3:35PM PDT, we finished enabling access to the EBS control plane to the degraded Availability Zone. This allowed most of the remaining volumes, which were waiting on the EBS control plane to help negotiate which replica would be writable, to once again be usable from their attached instances. At 6:15 PM PDT on April 23rd, API access to EBS resources was restored in the affected Availability Zone. \nWith the opening up of API access in the affected Availability Zone, APIs were now operating across all Availability Zones in the Region. The recovery of the remaining 2.2% of affected volumes required a more manual process to restore. The team had snapshotted these volumes to S3 backups early in the event as an extra precaution against data loss while the event was unfolding. At this point, the team finished developing and testing code to restore volumes from these snapshots and began processing batches through the night. At 12:30 PM PDT on April 24, we had finished the volumes that we could recover in this way and had recovered all but 1.04% of the affected volumes. At this point, the team began forensics on the remaining volumes which had suffered machine failure and for which we had not been able to take a snapshot. At 3:00 PM PDT, the team began restoring these. Ultimately, 0.07% of the volumes in the affected Availability Zone could not be restored for customers in\na consistent state. \nImpact on Amazon Relational Database Service (RDS) \nIn addition to the direct effect this EBS issue had on EC2 instances, it also impacted the Relational Database Service (\"RDS\"). RDS depends upon\nEBS for database and log storage, and as a result a portion of the RDS databases hosted in the primary affected Availability Zone became inaccessible. \nCustomers can choose to operate RDS instances either in a single Availability Zone (\"single-AZ\") or replicated across multiple Availability Zones (\"multi-AZ\"). Single-AZ database instances are exposed to disruptions in an Availability Zone. In this case, a single-AZ database instance would have been affected if one of the EBS volumes it was relying on got \"stuck\". In the primary affected Availability Zone, a peak of 45% of single-AZ instances were impacted with \"stuck\" I/O. This was a relatively-bigger portion of the RDS population than the corresponding EBS volume population because RDS database instances make use of multiple EBS volumes. This increases aggregate I/O capacity for database workloads under normal conditions, but means that a \"stuck\" I/O on any volume for a single-AZ database instance can make it inoperable until the volume is restored. The percentage of \"stuck\" single-AZ database instances in the affected Availability Zone decreased steadily during the event as the EBS recovery proceeded. The percentage of \"stuck\" single-AZ database instances in the affected Availability Zone decreased to 41.0% at the end of 24 hours, 23.5% at 36 hours and 14.6% at the end of 48 hours, and the rest recovered throughout the weekend. Though we recovered nearly all of the affected database instances, 0.4% of single-AZ database instances in the affected Availability Zone had an underlying EBS storage volume that was not recoverable. For these database instances, customers with automatic backups turned on (the default setting) had the option to initiate point-in-time database restore operations. \nRDS multi-AZ deployments provide redundancy by synchronously replicating data between two database replicas in different Availability Zones. In\nthe event of a failure on the primary replica, RDS is designed to automatically detect the disruption and fail over to the secondary replica. Of multi-AZ database instances in the US East Region, 2.5% did not automatically failover after experiencing \"stuck\" I/O. The primary cause was that the rapid succession of network interruption (which partitioned the primary from the secondary) and \"stuck\" I/O on the primary replica triggered a previously un-encountered bug. This bug left the primary replica in an isolated state where it was not safe for our monitoring agent to automatically fail over to the secondary replica without risking data loss, and manual intervention was required. We are actively working on a fix to resolve this issue. \n**Preventing the Event** \nThe trigger for this event was a network configuration change. We will audit our change process and increase the automation to prevent this mistake from happening in the future. However, we focus on building software and services to survive failures. Much of the work that will come out of this event will be to further protect the EBS service in the face of a similar failure in the future. \nWe will be making a number of changes to prevent a cluster from getting into a re-mirroring storm in the future. With additional excess capacity, the degraded EBS cluster would have more quickly absorbed the large number of re-mirroring requests and avoided the re-mirroring storm. We now understand the amount of capacity needed for large recovery events and will be modifying our capacity planning and alarming so that we carry the additional safety capacity that is needed for large scale failures. We have already increased our capacity buffer significantly, and expect to have the requisite new capacity in place in\na few weeks. We will also modify our retry logic in the EBS server nodes to prevent a cluster from getting into a re-mirroring storm. When a large interruption occurs, our retry logic will back off more aggressively and focus on re-establishing connectivity with previous replicas rather than futilely searching for new nodes with which to re-mirror. We have begun working through these changes and are confident we can address the root cause of the re-mirroring storm by modifying this logic. Finally, we have identified the source of the race condition\nthat led to EBS node failure. We have a fix and will be testing it and deploying it to our clusters in the next couple of weeks. These changes provide us with three separate protections against having a repeat of this event. \n**Impact to Multiple Availability Zones** \nEC2 provides two very important availability building blocks: Regions and Availability Zones. By design, Regions are completely separate deployments of our infrastructure. Regions are completely isolated from each other and provide the highest degree of independence. Many users utilize multiple EC2 Regions to achieve extremely-high levels of fault tolerance. However, if you want to move data between Regions, you need to do it via your applications as we don't replicate any data between\nRegions on our users' behalf. You also need to use a separate set of APIs to manage each Region. Regions provide users with a powerful availability building block, but it requires effort on the part of application builders to take advantage of this isolation. Within\nRegions, we provide Availability Zones to help users build fault-tolerant applications easily. Availability Zones are physically and logically separate infrastructure that are built to be highly independent while still providing users with high speed, low latency network connectivity, easy ways to replicate data, and a consistent set of management APIs. For example, when running inside a Region, users have the ability to take EBS snapshots which can be restored in any Availability Zone and can programmatically manipulate EC2 and EBS resources with the same APIs. We provide this loose coupling because it allows users to easily build highly-fault-tolerant applications. \nThis event had two distinct impacts. First, there was an impact to running applications in the affected Availability Zone because affected EBS volumes became \"stuck\". Because of the architecture of the EBS service, the impact to running instances was limited to the affected Availability Zone. As a result, many users who wrote their applications to take advantage of multiple Availability Zones did not have significant availability impact as a result of this event. Some customers reported that they had \"stuck\" EBS volumes in Availability Zones other than the impacted Availability Zone on Thursday. While our monitoring clearly shows the effect of the re-mirroring storm on the EBS control plane and on volumes within the affected Availability Zone, it does not reflect significant impact to existing EBS volumes within other Availability Zones in the Region. We do see that there were slightly more \"stuck\" volumes than we would have expected in the healthy Availability Zones, though still an extremely small number. To put this in perspective, the peak \"stuck\" volume percentage we saw in the Region\noutside of the affected Availability Zone was less than 0.07%. We investigated a number of these \"stuck\" volumes. The slightly-elevated number of \"stuck\" volumes in these non-impacted zones was caused by the delays in recovering from normal re-mirrors because of the increased latencies and error rates of the EBS control plane described above; there is always a background rate of volume re-mirroring going on. We also believe that the work described below to further insulate the EBS control plane will prevent even this slightly-elevated rate if something similar happened. \nWhile users' applications taking advantage of multiple Availability Zone (\"multi-AZ\") architectures were able to avoid impact from this event, there was definitely an impact on the EBS control plane that affected the ability to create and manipulate EBS volumes across the Region. One of the advantages of EC2 is the ability to rapidly replace failed resources. When the EBS control plane was degraded or unavailable, it made it difficult for customers with affected volumes to replace their volumes or EBS-booted EC2 instances in other healthy Availability Zones. Preventing this from reoccurring is a top priority. \nEven though we provide a degree of loose coupling for our customers, our design goal is to make Availability Zones indistinguishable from completely independent. Our EBS control plane is designed to allow users to access resources in multiple Availability Zones while still being tolerant to failures in individual zones. This event has taught us that we must make further investments to realize this design goal. There are three things we will do to prevent a single Availability Zone from impacting the EBS control plane across multiple Availability Zones. The first is that we will immediately improve our timeout logic to prevent thread exhaustion when a single Availability Zone cluster is taking too long to process requests. This would have prevented the API impact from 12:50 AM PDT to 2:40 AM PDT on April 21st. To address the cause of the second API impact, we will also add the ability for our EBS control plane to be more Availability Zone aware and shed load intelligently when it is over capacity. This is similar to other throttles that we already have in our systems. Additionally, we also see an opportunity to push more of our EBS control plane into per-EBS cluster services. By moving more functionality out of the EBS control plane and creating per-EBS cluster deployments of these services (which run in the same Availability Zone as the EBS cluster they are supporting), we can\nprovide even better Availability Zone isolation for the EBS control plane. \n**Making it Easier to Take Advantage of Multiple Availability Zones** \nWe also intend to make it easier for customers to take advantage of multiple Availability Zones. First, we will offer multiple Availability Zones for all of our services, including Amazon Virtual Private Cloud (\"VPC\"). Today, VPC customers only have access to a single Availably Zone. We will be adjusting our roadmap to give VPC customers access to multiple Availability Zones as soon as possible. This will allow VPC customers to build highly-available applications using multiple Availability Zones just as EC2 customers not using a VPC do today. \nA related finding from this event is we need to do a better job of making highly-reliable multi-AZ deployments easy to design and operate. Some customers' applications (or critical components of the application\nlike the database) are deployed in only a single Availability Zone, while others have instances spread across Availability Zones but still have critical, single points of failure in a single Availability Zone. In cases like these, operational issues can negatively impact application availability when a robust multi-Availability Zone deployment would allow the application to continue without impact. We will look to provide customers with better tools to create multi-AZ applications that can support the loss of an entire Availability Zone without impacting application availability. We know we need to help customers design their application logic using common design patterns. In this event, some customers were seriously impacted, and yet others had resources that were impacted but saw nearly no impact on their applications. \nIn order to work more closely with our customers and partners on best practices for architecting in [the cloud](/what-is-cloud-computing/), we will be hosting a series of free webinars starting Monday, May 2. The first topics we will cover will be Designing Fault-tolerant Applications, Architecting for the Cloud, and Web Hosting Best Practices. We anticipate adding more topics to the series over the next few weeks, and will continue to do these on a frequent ongoing basis. The webinars over the next two weeks will be hosted several times daily to support our customers around the world in multiple time zones. We will set aside a significant portion of the webinars for detailed Q&A. Follow-up discussions for customers or partners will also be arranged. These webinars, as well as a series of whitepapers on best practices for architecting for the AWS cloud, are available in a new Architecture Center on the AWS website. We'll also continue to deliver additional services like S3, SimpleDB and multi-AZ RDS that perform multi-AZ level balancing automatically so customers can benefit from multiple Availability Zones without doing any of the heavy-lifting in their applications. \n**Speeding Up Recovery** \nWe will also invest in increasing our visibility, control, and automation to recover volumes in an EBS cluster. We have a number of operational tools for managing an EBS cluster, but the fine-grained control and throttling the team used to recover the cluster will be built directly into the EBS nodes. We will also automate the recovery models that we used for the various types of volume recovery that we had to do. This would have saved us significant time in the recovery process. We will also look at what changes we can make to preserve volume functionality during periods of degraded cluster operation, including adding the ability to take a snapshot of a \"stuck\" volume. If customers had this ability, they would have been able to more easily recover their applications in other Availability Zones in the Region. \n**Improving Communication and Service Health Tools During Operational Issues** \nIn addition to the technical insights and improvements that will result from this event, we also identified improvements that need to be made in\nour customer communications. We would like our communications to be more frequent and contain more information. We understand that during an\noutage, customers want to know as many details as possible about what's going on, how long it will take to fix, and what we are doing so that it doesn't happen again. Most of the AWS team, including the entire senior leadership team, was directly involved in helping to coordinate, troubleshoot and resolve the event. Initially, our primary focus was on\nthinking through how to solve the operational problems for customers rather than on identifying root causes. We felt that that focusing our efforts on a solution and not the problem was the right thing to do for our customers, and that it helped us to return the services and our customers back to health more quickly. We updated customers when we had new information that we felt confident was accurate and refrained from speculating, knowing that once we had returned the services back to health that we would quickly transition to the data collection and analysis stage that would drive this post mortem. \nThat said, we think we can improve in this area. We switched to more regular updates part of the way through this event and plan to continue with similar frequency of updates in the future. In addition, we are already working on how we can staff our developer support team more expansively in an event such as this, and organize to provide early and meaningful information, while still avoiding speculation. \nWe also can do a better job of making it easier for customers to tell if their resources have been impacted, and we are developing tools to allow you to see via the APIs if your instances are impaired. \n**Service Credit for Affected Customers** \nFor customers with an attached EBS volume or a running RDS database instance in the affected Availability Zone in the US East Region at the time of the disruption, regardless of whether their resources and application were impacted or not, we are going to provide a 10 day credit equal to 100% of their usage of EBS Volumes, EC2 Instances and RDS database instances that were running in the affected Availability Zone. These customers will not have to do anything in order to receive this credit, as it will be automatically applied to their next AWS bill. Customers can see whether they qualify for the service credit by logging into their AWS Account Activity page. \n**In Conclusion** \nLast, but certainly not least, we want to apologize. We know how critical our services are to our customers' businesses and we will do everything we can to learn from this event and use it to drive improvement across our services. As with any significant operational issue, we will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make changes to improve our services and processes.\nAssistant:", "response": [["Human error during a routine networking upgrade led to a resource crunch, exacerbated by software bugs, that ultimately resulted in an outage across all US East Availability Zones as well as a loss of 0.07% of volumes"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region\n\n**April 29, 2011** \nNow that we have fully restored functionality to all affected services, we would like to share more details with our customers about the events that occurred with the Amazon Elastic Compute Cloud (\"EC2\") last week, our efforts to restore the services, and what we are doing to prevent this sort of issue from happening again. We are very aware that many of our customers were significantly impacted by this event, and as with any significant service issue, our intention is to share the details of what happened and how we will improve the service for our customers. \nThe issues affecting EC2 customers last week primarily involved a subset of the Amazon Elastic Block Store (\"EBS\") volumes in a single Availability Zone within the US East Region that became unable to service read and write operations. In this document, we will refer to these as \"stuck\" volumes. This caused instances trying to use these affected volumes to also get \"stuck\" when they attempted to read or write to them. In order to restore these volumes and stabilize the EBS cluster in that Availability Zone, we disabled all control APIs (e.g. Create Volume, Attach Volume, Detach Volume, and Create Snapshot) for EBS in the affected Availability Zone for much of the duration of the event. For two periods during the first day of the issue, the degraded EBS cluster affected the EBS APIs and caused high error rates and latencies for EBS calls to these APIs across the entire US East Region. As with any complicated operational issue, this one was caused by several root causes interacting with one another and therefore gives us many opportunities to protect the service against any similar event reoccurring. \n**Overview of EBS System** \nIt is helpful to understand the EBS architecture so that we can better explain the event. EBS is a distributed, replicated block data store that is optimized for consistency and low latency read and write access from EC2 instances. There are two main components of the EBS service: (i) a set of EBS clusters (each of which runs entirely inside of an\nAvailability Zone) that store user data and serve requests to EC2 instances; and (ii) a set of control plane services that are used to coordinate user requests and propagate them to the EBS clusters running in each of the Availability Zones in the Region. \nAn EBS cluster is comprised of a set of EBS nodes. These nodes store replicas of EBS volume data and serve read and write requests to EC2 instances. EBS volume data is replicated to multiple EBS nodes for durability and availability. Each EBS node employs a peer-to-peer based, fast failover strategy that aggressively provisions new replicas if one of the copies ever gets out of sync or becomes unavailable. The nodes in\nan EBS cluster are connected to each other via two networks. The primary network is a high bandwidth network used in normal operation for all necessary communication with other EBS nodes, with EC2 instances, and with the EBS control plane services. The secondary network, the replication network, is a lower capacity network used as a back-up network to allow EBS nodes to reliably communicate with other nodes in\nthe EBS cluster and provide overflow capacity for data replication. This network is not designed to handle all traffic from the primary network but rather provide highly-reliable connectivity between EBS nodes inside of an EBS cluster. \nWhen a node loses connectivity to a node to which it is replicating data to, it assumes the other node failed. To preserve durability, it must find a new node to which it can replicate its data (this is called re-mirroring). As part of the re-mirroring process, the EBS node searches its EBS cluster for another node with enough available server space, establishes connectivity with the server, and propagates the volume data. In a normally functioning cluster, finding a location for the new replica occurs in milliseconds. While data is being re-mirrored, all nodes that have copies of the data hold onto the data until they can\nconfirm that another node has taken ownership of their portion. This provides an additional level of protection against customer data loss. Also, when data on a customer's volume is being re-mirrored, access to that data is blocked until the system has identified a new primary (or writable) replica. This is required for consistency of EBS volume data under all potential failure modes. From the perspective of an EC2 instance trying to do I/O on a volume while this is happening, the volume will appear \"stuck\". \nIn addition to the EBS clusters, there is a set of control plane services that accepts user requests and propagates them to the appropriate EBS cluster. There is one set of EBS control plane services per EC2 Region, but the control plane itself is highly distributed across the Availability Zones to provide availability and fault tolerance. These control plane services also act as the authority to the EBS clusters when they elect primary replicas for each volume in the cluster (for consistency, there must only be a single primary replica for each volume at any time). While there are a few different services that comprise the control plane, we will refer to them collectively as the \"EBS control plane\" in this document. \n**Primary Outage** \nAt 12:47 AM PDT on April 21st, a network change was performed as part of our normal AWS scaling activities in a single Availability Zone in the US East Region. The configuration change was to upgrade the capacity of the primary network. During the change, one of the standard steps is to shift traffic off of one of the redundant routers in the primary EBS network to allow the upgrade to happen. The traffic shift was executed incorrectly and rather than routing the traffic to the other router on\nthe primary network, the traffic was routed onto the lower capacity redundant EBS network. For a portion of the EBS cluster in the affected Availability Zone, this meant that they did not have a functioning primary or secondary network because traffic was purposely shifted away from the primary network and the secondary network couldn't handle the traffic level it was receiving. As a result, many EBS nodes in the affected Availability Zone were completely isolated from other EBS nodes in its cluster. Unlike a normal network interruption, this change disconnected both the primary and secondary network simultaneously, leaving the affected nodes completely isolated from one another. \nWhen this network connectivity issue occurred, a large number of EBS nodes in a single EBS cluster lost connection to their replicas. When\nthe incorrect traffic shift was rolled back and network connectivity was restored, these nodes rapidly began searching the EBS cluster for available server space where they could re-mirror data. Once again, in a normally functioning cluster, this occurs in milliseconds. In this case, because the issue affected such a large number of volumes concurrently, the free capacity of the EBS cluster was quickly exhausted, leaving many of the nodes \"stuck\" in a loop, continuously searching the cluster for free space. This quickly led to a \"re-mirroring storm,\" where a large number of volumes were effectively \"stuck\" while the nodes searched the cluster for the storage space it needed for its new replica. At this point, about 13% of the volumes in the affected Availability Zone were in this \"stuck\" state. \nAfter the initial sequence of events described above, the degraded EBS cluster had an immediate impact on the EBS control plane. When the EBS cluster in the affected Availability Zone entered the re-mirroring storm and exhausted its available capacity, the cluster became unable to service \"create volume\" API requests. Because the EBS control plane (and the create volume API in particular) was configured with a long time-out period, these slow API calls began to back up and resulted in thread starvation in the EBS control plane. The EBS control plane has a regional pool of available threads it can use to service requests. When\nthese threads were completely filled up by the large number of queued requests, the EBS control plane had no ability to service API requests and began to fail API requests for other Availability Zones in that Region as well. At 2:40 AM PDT on April 21st, the team deployed a change that disabled all new Create Volume requests in the affected Availability Zone, and by 2:50 AM PDT, latencies and error rates for all other EBS related APIs recovered. \nTwo factors caused the situation in this EBS cluster to degrade further during the early part of the event. First, the nodes failing to find new nodes did not back off aggressively enough when they could not find space, but instead, continued to search repeatedly. There was also a race condition in the code on the EBS nodes that, with a very low probability, caused them to fail when they were concurrently closing a large number of requests for replication. In a normally operating EBS cluster, this issue would result in very few, if any, node crashes; however, during this re-mirroring storm, the volume of connection\nattempts was extremely high, so it began triggering this issue more frequently. Nodes began to fail as a result of the bug, resulting in\nmore volumes left needing to re-mirror. This created more \"stuck\" volumes and added more requests to the re-mirroring storm. \nBy 5:30 AM PDT, error rates and latencies again increased for EBS API calls across the Region. When data for a volume needs to be re-mirrored, a negotiation must take place between the EC2 instance, the EBS nodes with the volume data, and the EBS control plane (which acts as an\nauthority in this process) so that only one copy of the data is designated as the primary replica and recognized by the EC2 instance as the place where all accesses should be sent. This provides strong consistency of EBS volumes. As more EBS nodes continued to fail because of the race condition described above, the volume of such negotiations with the EBS control plane increased. Because data was not being successfully re-mirrored, the number of these calls increased as the system retried and new requests came in. The load caused a brown out of the EBS control plane and again affected EBS APIs across the Region. At 8:20 AM PDT, the team began disabling all communication between the degraded EBS cluster in the affected Availability Zone and the EBS control plane. While this prevented all EBS API access in the affected Availability Zone (we will discuss recovery of this in the next section), other latencies and error rates returned to normal for EBS APIs for the rest of the Region. \nA large majority of the volumes in the degraded EBS cluster were still functioning properly and the focus was to recover the cluster without affecting more volumes. At 11:30AM PDT, the team developed a way to prevent EBS servers in the degraded EBS cluster from futilely contacting other servers (who didn't have free space at this point anyway) without affecting the other essential communication between nodes in the cluster. After this change was made, the cluster stopped degrading further and additional volumes were no longer at risk of becoming \"stuck\". Before this change was deployed, the failed servers resulting from the race condition resulted in an additional 5% of the volumes in\nthe affected Availability Zone becoming \"stuck\". However, volumes were also slowly re-mirroring as some capacity was made available which allowed existing \"stuck\" volumes to become \"unstuck\". The net result was that when this change was deployed, the total \"stuck\" volumes in the affected Availability Zone was 13%. \nCustomers also experienced elevated error rates until Noon PDT on April 21st when attempting to launch new EBS-backed EC2 instances in\nAvailability Zones other than the affected zone. This occurred for approximately 11 hours, from the onset of the outage until Noon PM PDT on April 21st. Except for the periods of broader API issues describe above, customers were able to create EBS-backed EC2 instances but were experiencing significantly-elevated error rates and latencies. New EBS-backed EC2 launches were being affected by a specific API in the EBS control plane that is only needed for attaching new instances to volumes. Initially, our alarming was not fine-grained enough for this EBS control plane API and the launch errors were overshadowed by the general error from the degraded EBS cluster. At 11:30 AM PDT, a change to the EBS control plane fixed this issue and latencies and error rates for new EBS-backed EC2 instances declined rapidly and returned to near-normal at Noon PDT. \n**Recovering EBS in the Affected Availability Zone** \nBy 12:04 PM PDT on April 21st, the outage was contained to the one affected Availability Zone and the degraded EBS cluster was stabilized. APIs were working well for all other Availability Zones and additional volumes were no longer becoming \"stuck\". Our focus shifted to completing the recovery. Approximately 13% of the volumes in the Availability Zone remained \"stuck\" and the EBS APIs were disabled in that one affected Availability Zone. The key priority became bringing additional storage capacity online to allow the \"stuck\" volumes to find enough space to create new replicas. \nThe team faced two challenges which delayed getting capacity online. First, when a node fails, the EBS cluster does not reuse the failed node until every data replica is successfully re-mirrored. This is a conscious decision so that we can recover data if a cluster fails to behave as designed. Because we did not want to re-purpose this failed capacity until we were sure we could recover affected user volumes on\nthe failed nodes, the team had to install a large amount of additional new capacity to replace that capacity in the cluster. This required the time-consuming process of physically relocating excess server capacity from across the US East Region and installing that capacity into the degraded EBS cluster. Second, because of the changes made to reduce the node-to-node communication used by peers to find new capacity (which is what stabilized the cluster in the step described above), the team had difficulty incorporating the new capacity into the cluster. The team had to carefully make changes to their negotiation throttles to allow negotiation to occur with the newly-built servers without again\ninundating the old servers with requests that they could not service. This process took longer than we expected as the team had to navigate a number of issues as they worked around the disabled communication. At about 02:00AM PDT on April 22nd, the team successfully started adding significant amounts of new capacity and working through the replication\nbacklog. Volumes were restored consistently over the next nine hours and all but about 2.2% of the volumes in the affected Availability Zone were restored by 12:30PM PDT on April 22nd. While the restored volumes were fully replicated, not all of them immediately became \"unstuck\" from the perspective of the attached EC2 instances because some were blocked waiting for the EBS control plane to be contactable, so they could safely re-establish a connection with the EC2 instance and elect a new writable copy. \nOnce there was sufficient capacity added to the cluster, the team worked on re-establishing EBS control plane API access to the affected Availability Zone and restoring access to the remaining \"stuck\" volumes. There was a large backlog of state changes that had to be propagated both from the degraded EBS nodes to the EBS control plane and vice versa. This effort was done gradually to avoid impact to the restored volumes and the EBS control plane. Our initial attempts to bring API access online to the impacted Availability Zone centered on throttling the state propagation to avoid overwhelming the EBS control plane. We also began building out a separate instance of the EBS control plane, one we could keep partitioned to the affected Availability Zone to avoid impacting other Availability Zones in the Region, while we processed the backlog. We rapidly developed throttles that turned out to be too coarse-grained to permit the right requests to pass through and stabilize the system. Through the evening of April 22nd into the morning of April 23rd, we worked on developing finer-grain throttles. By Saturday morning, we had finished work on the dedicated EBS control plane and the finer-grain throttles. Initial tests of traffic against the EBS control plane demonstrated progress and shortly after 11:30 AM PDT on April 23rd we began steadily processing the backlog. By 3:35PM PDT, we finished enabling access to the EBS control plane to the degraded Availability Zone. This allowed most of the remaining volumes, which were waiting on the EBS control plane to help negotiate which replica would be writable, to once again be usable from their attached instances. At 6:15 PM PDT on April 23rd, API access to EBS resources was restored in the affected Availability Zone. \nWith the opening up of API access in the affected Availability Zone, APIs were now operating across all Availability Zones in the Region. The recovery of the remaining 2.2% of affected volumes required a more manual process to restore. The team had snapshotted these volumes to S3 backups early in the event as an extra precaution against data loss while the event was unfolding. At this point, the team finished developing and testing code to restore volumes from these snapshots and began processing batches through the night. At 12:30 PM PDT on April 24, we had finished the volumes that we could recover in this way and had recovered all but 1.04% of the affected volumes. At this point, the team began forensics on the remaining volumes which had suffered machine failure and for which we had not been able to take a snapshot. At 3:00 PM PDT, the team began restoring these. Ultimately, 0.07% of the volumes in the affected Availability Zone could not be restored for customers in\na consistent state. \nImpact on Amazon Relational Database Service (RDS) \nIn addition to the direct effect this EBS issue had on EC2 instances, it also impacted the Relational Database Service (\"RDS\"). RDS depends upon\nEBS for database and log storage, and as a result a portion of the RDS databases hosted in the primary affected Availability Zone became inaccessible. \nCustomers can choose to operate RDS instances either in a single Availability Zone (\"single-AZ\") or replicated across multiple Availability Zones (\"multi-AZ\"). Single-AZ database instances are exposed to disruptions in an Availability Zone. In this case, a single-AZ database instance would have been affected if one of the EBS volumes it was relying on got \"stuck\". In the primary affected Availability Zone, a peak of 45% of single-AZ instances were impacted with \"stuck\" I/O. This was a relatively-bigger portion of the RDS population than the corresponding EBS volume population because RDS database instances make use of multiple EBS volumes. This increases aggregate I/O capacity for database workloads under normal conditions, but means that a \"stuck\" I/O on any volume for a single-AZ database instance can make it inoperable until the volume is restored. The percentage of \"stuck\" single-AZ database instances in the affected Availability Zone decreased steadily during the event as the EBS recovery proceeded. The percentage of \"stuck\" single-AZ database instances in the affected Availability Zone decreased to 41.0% at the end of 24 hours, 23.5% at 36 hours and 14.6% at the end of 48 hours, and the rest recovered throughout the weekend. Though we recovered nearly all of the affected database instances, 0.4% of single-AZ database instances in the affected Availability Zone had an underlying EBS storage volume that was not recoverable. For these database instances, customers with automatic backups turned on (the default setting) had the option to initiate point-in-time database restore operations. \nRDS multi-AZ deployments provide redundancy by synchronously replicating data between two database replicas in different Availability Zones. In\nthe event of a failure on the primary replica, RDS is designed to automatically detect the disruption and fail over to the secondary replica. Of multi-AZ database instances in the US East Region, 2.5% did not automatically failover after experiencing \"stuck\" I/O. The primary cause was that the rapid succession of network interruption (which partitioned the primary from the secondary) and \"stuck\" I/O on the primary replica triggered a previously un-encountered bug. This bug left the primary replica in an isolated state where it was not safe for our monitoring agent to automatically fail over to the secondary replica without risking data loss, and manual intervention was required. We are actively working on a fix to resolve this issue. \n**Preventing the Event** \nThe trigger for this event was a network configuration change. We will audit our change process and increase the automation to prevent this mistake from happening in the future. However, we focus on building software and services to survive failures. Much of the work that will come out of this event will be to further protect the EBS service in the face of a similar failure in the future. \nWe will be making a number of changes to prevent a cluster from getting into a re-mirroring storm in the future. With additional excess capacity, the degraded EBS cluster would have more quickly absorbed the large number of re-mirroring requests and avoided the re-mirroring storm. We now understand the amount of capacity needed for large recovery events and will be modifying our capacity planning and alarming so that we carry the additional safety capacity that is needed for large scale failures. We have already increased our capacity buffer significantly, and expect to have the requisite new capacity in place in\na few weeks. We will also modify our retry logic in the EBS server nodes to prevent a cluster from getting into a re-mirroring storm. When a large interruption occurs, our retry logic will back off more aggressively and focus on re-establishing connectivity with previous replicas rather than futilely searching for new nodes with which to re-mirror. We have begun working through these changes and are confident we can address the root cause of the re-mirroring storm by modifying this logic. Finally, we have identified the source of the race condition\nthat led to EBS node failure. We have a fix and will be testing it and deploying it to our clusters in the next couple of weeks. These changes provide us with three separate protections against having a repeat of this event. \n**Impact to Multiple Availability Zones** \nEC2 provides two very important availability building blocks: Regions and Availability Zones. By design, Regions are completely separate deployments of our infrastructure. Regions are completely isolated from each other and provide the highest degree of independence. Many users utilize multiple EC2 Regions to achieve extremely-high levels of fault tolerance. However, if you want to move data between Regions, you need to do it via your applications as we don't replicate any data between\nRegions on our users' behalf. You also need to use a separate set of APIs to manage each Region. Regions provide users with a powerful availability building block, but it requires effort on the part of application builders to take advantage of this isolation. Within\nRegions, we provide Availability Zones to help users build fault-tolerant applications easily. Availability Zones are physically and logically separate infrastructure that are built to be highly independent while still providing users with high speed, low latency network connectivity, easy ways to replicate data, and a consistent set of management APIs. For example, when running inside a Region, users have the ability to take EBS snapshots which can be restored in any Availability Zone and can programmatically manipulate EC2 and EBS resources with the same APIs. We provide this loose coupling because it allows users to easily build highly-fault-tolerant applications. \nThis event had two distinct impacts. First, there was an impact to running applications in the affected Availability Zone because affected EBS volumes became \"stuck\". Because of the architecture of the EBS service, the impact to running instances was limited to the affected Availability Zone. As a result, many users who wrote their applications to take advantage of multiple Availability Zones did not have significant availability impact as a result of this event. Some customers reported that they had \"stuck\" EBS volumes in Availability Zones other than the impacted Availability Zone on Thursday. While our monitoring clearly shows the effect of the re-mirroring storm on the EBS control plane and on volumes within the affected Availability Zone, it does not reflect significant impact to existing EBS volumes within other Availability Zones in the Region. We do see that there were slightly more \"stuck\" volumes than we would have expected in the healthy Availability Zones, though still an extremely small number. To put this in perspective, the peak \"stuck\" volume percentage we saw in the Region\noutside of the affected Availability Zone was less than 0.07%. We investigated a number of these \"stuck\" volumes. The slightly-elevated number of \"stuck\" volumes in these non-impacted zones was caused by the delays in recovering from normal re-mirrors because of the increased latencies and error rates of the EBS control plane described above; there is always a background rate of volume re-mirroring going on. We also believe that the work described below to further insulate the EBS control plane will prevent even this slightly-elevated rate if something similar happened. \nWhile users' applications taking advantage of multiple Availability Zone (\"multi-AZ\") architectures were able to avoid impact from this event, there was definitely an impact on the EBS control plane that affected the ability to create and manipulate EBS volumes across the Region. One of the advantages of EC2 is the ability to rapidly replace failed resources. When the EBS control plane was degraded or unavailable, it made it difficult for customers with affected volumes to replace their volumes or EBS-booted EC2 instances in other healthy Availability Zones. Preventing this from reoccurring is a top priority. \nEven though we provide a degree of loose coupling for our customers, our design goal is to make Availability Zones indistinguishable from completely independent. Our EBS control plane is designed to allow users to access resources in multiple Availability Zones while still being tolerant to failures in individual zones. This event has taught us that we must make further investments to realize this design goal. There are three things we will do to prevent a single Availability Zone from impacting the EBS control plane across multiple Availability Zones. The first is that we will immediately improve our timeout logic to prevent thread exhaustion when a single Availability Zone cluster is taking too long to process requests. This would have prevented the API impact from 12:50 AM PDT to 2:40 AM PDT on April 21st. To address the cause of the second API impact, we will also add the ability for our EBS control plane to be more Availability Zone aware and shed load intelligently when it is over capacity. This is similar to other throttles that we already have in our systems. Additionally, we also see an opportunity to push more of our EBS control plane into per-EBS cluster services. By moving more functionality out of the EBS control plane and creating per-EBS cluster deployments of these services (which run in the same Availability Zone as the EBS cluster they are supporting), we can\nprovide even better Availability Zone isolation for the EBS control plane. \n**Making it Easier to Take Advantage of Multiple Availability Zones** \nWe also intend to make it easier for customers to take advantage of multiple Availability Zones. First, we will offer multiple Availability Zones for all of our services, including Amazon Virtual Private Cloud (\"VPC\"). Today, VPC customers only have access to a single Availably Zone. We will be adjusting our roadmap to give VPC customers access to multiple Availability Zones as soon as possible. This will allow VPC customers to build highly-available applications using multiple Availability Zones just as EC2 customers not using a VPC do today. \nA related finding from this event is we need to do a better job of making highly-reliable multi-AZ deployments easy to design and operate. Some customers' applications (or critical components of the application\nlike the database) are deployed in only a single Availability Zone, while others have instances spread across Availability Zones but still have critical, single points of failure in a single Availability Zone. In cases like these, operational issues can negatively impact application availability when a robust multi-Availability Zone deployment would allow the application to continue without impact. We will look to provide customers with better tools to create multi-AZ applications that can support the loss of an entire Availability Zone without impacting application availability. We know we need to help customers design their application logic using common design patterns. In this event, some customers were seriously impacted, and yet others had resources that were impacted but saw nearly no impact on their applications. \nIn order to work more closely with our customers and partners on best practices for architecting in [the cloud](/what-is-cloud-computing/), we will be hosting a series of free webinars starting Monday, May 2. The first topics we will cover will be Designing Fault-tolerant Applications, Architecting for the Cloud, and Web Hosting Best Practices. We anticipate adding more topics to the series over the next few weeks, and will continue to do these on a frequent ongoing basis. The webinars over the next two weeks will be hosted several times daily to support our customers around the world in multiple time zones. We will set aside a significant portion of the webinars for detailed Q&A. Follow-up discussions for customers or partners will also be arranged. These webinars, as well as a series of whitepapers on best practices for architecting for the AWS cloud, are available in a new Architecture Center on the AWS website. We'll also continue to deliver additional services like S3, SimpleDB and multi-AZ RDS that perform multi-AZ level balancing automatically so customers can benefit from multiple Availability Zones without doing any of the heavy-lifting in their applications. \n**Speeding Up Recovery** \nWe will also invest in increasing our visibility, control, and automation to recover volumes in an EBS cluster. We have a number of operational tools for managing an EBS cluster, but the fine-grained control and throttling the team used to recover the cluster will be built directly into the EBS nodes. We will also automate the recovery models that we used for the various types of volume recovery that we had to do. This would have saved us significant time in the recovery process. We will also look at what changes we can make to preserve volume functionality during periods of degraded cluster operation, including adding the ability to take a snapshot of a \"stuck\" volume. If customers had this ability, they would have been able to more easily recover their applications in other Availability Zones in the Region. \n**Improving Communication and Service Health Tools During Operational Issues** \nIn addition to the technical insights and improvements that will result from this event, we also identified improvements that need to be made in\nour customer communications. We would like our communications to be more frequent and contain more information. We understand that during an\noutage, customers want to know as many details as possible about what's going on, how long it will take to fix, and what we are doing so that it doesn't happen again. Most of the AWS team, including the entire senior leadership team, was directly involved in helping to coordinate, troubleshoot and resolve the event. Initially, our primary focus was on\nthinking through how to solve the operational problems for customers rather than on identifying root causes. We felt that that focusing our efforts on a solution and not the problem was the right thing to do for our customers, and that it helped us to return the services and our customers back to health more quickly. We updated customers when we had new information that we felt confident was accurate and refrained from speculating, knowing that once we had returned the services back to health that we would quickly transition to the data collection and analysis stage that would drive this post mortem. \nThat said, we think we can improve in this area. We switched to more regular updates part of the way through this event and plan to continue with similar frequency of updates in the future. In addition, we are already working on how we can staff our developer support team more expansively in an event such as this, and organize to provide early and meaningful information, while still avoiding speculation. \nWe also can do a better job of making it easier for customers to tell if their resources have been impacted, and we are developing tools to allow you to see via the APIs if your instances are impaired. \n**Service Credit for Affected Customers** \nFor customers with an attached EBS volume or a running RDS database instance in the affected Availability Zone in the US East Region at the time of the disruption, regardless of whether their resources and application were impacted or not, we are going to provide a 10 day credit equal to 100% of their usage of EBS Volumes, EC2 Instances and RDS database instances that were running in the affected Availability Zone. These customers will not have to do anything in order to receive this credit, as it will be automatically applied to their next AWS bill. Customers can see whether they qualify for the service credit by logging into their AWS Account Activity page. \n**In Conclusion** \nLast, but certainly not least, we want to apologize. We know how critical our services are to our customers' businesses and we will do everything we can to learn from this event and use it to drive improvement across our services. As with any significant operational issue, we will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make changes to improve our services and processes.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow 分类做总结整理。 \nAssistant:", "response": [["### Terminology\n\n- **Amazon EC2**: Amazon Elastic Compute Cloud，亚马逊提供的云计算服务平台，允许用户租用虚拟计算机来运行其应用程序。\n- **Amazon RDS**: Amazon Relational Database Service，亚马逊提供的托管关系数据库服务。\n- **EBS (Elastic Block Store)**: 亚马逊提供的一种网络存储服务，用于EC2实例的数据块存储。\n- **Availability Zone**: AWS数据中心中的物理位置，用于隔离故障和提高可用性。\n- **Re-mirroring Storm**: 在EBS集群中，大量数据卷同时尝试重新镜像（复制数据到新的存储节点）时引发的一系列连锁反应。\n\n### Background\n\n- 事件发生在2011年4月21日，影响了美国东部地区的EC2服务。\n- 问题主要涉及EBS卷中的一个子集，这些卷在一个可用区内无法进行读写操作。\n- 事件导致了EC2实例尝试使用这些受影响的卷时也变得无法响应。\n- AWS团队采取了一系列措施来恢复服务，并防止类似问题再次发生。\n\n### FAQ\n\n- **Q: 事件的根本原因是什么？**\n  - A: 事件的根本原因是网络配置变更错误，导致流量被错误地路由到了容量较小的网络，使得EBS节点之间失去连接。\n\n- **Q: AWS如何应对这种情况？**\n  - A: AWS通过禁用受影响可用区内的EBS控制API、增加额外的存储容量、改进节点间的通信逻辑等措施来应对。\n\n- **Q: 客户如何知道他们的资源是否受到影响？**\n  - A: AWS通过更新客户通知和提供API工具来帮助客户检查他们的实例是否受损。\n\n### Flow\n\n1. **事件发生** - 网络变更导致EBS卷无法服务读写操作。\n2. **问题识别** - AWS识别到EBS集群中的“stuck”卷，并开始尝试恢复。\n3. **服务降级** - 为恢复EBS集群，AWS禁用了受影响可用区内的EBS控制API。\n4. **容量增加** - AWS增加新的存储容量以帮助“stuck”卷恢复。\n5. **通信逻辑改进** - 改进EBS节点间的通信逻辑，防止未来出现类似的“re-mirroring storm”。\n6. **API恢复** - 逐步恢复受影响可用区内的EBS控制平面API访问。\n7. **手动恢复** - 对于无法自动恢复的卷，AWS团队进行了手动恢复。"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the December 17th event in the South America Region (SA-EAST-1)  \n**December 20, 2013** \nWe want to give you some additional insight into the event that impacted a single Availability Zone in the South America Region (SA-EAST-1). On\nDecember 17th at 10:05PM PST, the impacted Availability Zone lost utility power due to a fault that happened at the substation of the local utility provider. The impacted Availability Zone automatically switched over to run on generator power when utility power was lost. Availability Zones are built with multiple layers of redundancy, and are designed to continue to operate even when multiple components fail at the same time. In this particular case when we experienced a loss in\nutility power, the load switched over to our backup generators as designed. During that failover a breaker in front of one of the generators opened, rendering that generator unavailable. Shortly thereafter, a second generator independently failed due to a mechanical issue. The loss of utility power combined with the unavailability of two additional generators meant that there was more load in the facility than the remaining healthy generators could handle. With more load on\nthem than they could support, the remaining healthy generators also shut down. Our facilities team immediately began working to bring the failed generators back online. This facility uses an automated control system which allows it to aggregate power from multiple generators together. The team experienced several additional setbacks when trying to bring the power infrastructure back online, and eventually identified that the automated control system wasn't functioning properly. Once the team identified the issue, they bypassed the automated control system and began the slow process of manually bringing the generators online. Once there was sufficient generator capacity to fully support the facility, all of the impacted instances were recovered. While we have not completed forensics on the breaker opening and the generator mechanical failure, we find the double failure to be extremely unusual, and are deeply reviewing the operational records of the failed components. \nInstances in the second Availability Zone in the Region did not experience any power related issues, however instances in both Availability Zones did experience a total of 20 minutes of degraded network connectivity due to an error that was made in bringing our network back online once power was restored. As part of the recovery process, a network technician brought a network device up manually in\nthe power-impacted Availability Zone and introduced a bad configuration. That misconfiguration led to the device advertising an invalid network route when it came back online, which resulted in degraded Internet connectivity for both SA-EAST-1 Availability Zones. Once we understood the issue, we took the device out of service and full connectivity to the Region was restored. After power and networking were fully restored to the facility, all of our services were brought back online and full customer access was restored. \nWe apologize for any difficulty this event may have caused you. We appreciate how critical our services are to our customers, and will take steps to ensure this Availability Zone in Brazil is better able to withstand a similar power failure in the future.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（logical）、桥接性（bridging）、可操作性（actionable）、简洁性（simplicity）和流程完整性（process integrity）等几个角度来分析文档，并按照术语（terminology）、背景（background）、常见问题解答（faq）、流程（flow）和附录（appendix）进行分类整理。\n\n### 术语（Terminology）\n\n- **Availability Zone (AZ)**: 一个独立的物理位置，包含一个或多个数据中心，这些数据中心在同一区域（Region）内彼此隔离，但彼此之间通过低延迟的网络连接。\n- **Utility Power**: 指供电公司提供的电力。\n- **Generator Power**: 备用发电机提供的电力，用于在主电源失效时供电。\n- **Redundancy**: 系统设计中的多余或重复部分，用于确保系统在部分组件失败时仍能正常运行。\n- **Failover**: 系统从主要运行状态自动切换到备用状态的过程。\n- **Breaker**: 电路断路器，用于在电路出现问题时切断电流。\n- **Misconfiguration**: 错误的配置，可能导致系统或设备无法正常工作。\n\n### 背景（Background）\n\n- 事件发生在2013年12月17日，影响了南美洲区域（SA-EAST-1）中的一个可用区。\n- 由于当地电力供应商变电站的故障，该可用区在晚上10:05 PST失去了公用电力。\n- 该可用区设计有多层冗余，即使多个组件同时故障也能继续运行。\n\n### 常见问题解答（FAQ）\n\n**Q: 为什么会出现电力中断？**\nA: 由于当地电力供应商变电站的故障，导致供电中断。\n\n**Q: 备用发电机是如何响应的？**\nA: 可用区自动切换到备用发电机供电，但在此过程中，一个发电机前的断路器打开，另一个发电机因机械问题独立故障。\n\n**Q: 网络连接为何受到影响？**\nA: 在电力恢复后，由于网络技术人员手动启动了一个网络设备并引入了错误的配置，导致网络连接降级。\n\n### 流程（Flow）\n\n1. **电力中断**: 变电站故障导致可用区失去公用电力。\n2. **备用发电机启动**: 可用区按设计切换到备用发电机。\n3. **发电机故障**: 一个发电机的断路器打开，另一个发电机因机械问题故障。\n4. **发电机修复**: 设施团队开始手动修复发电机。\n5. **网络连接降级**: 网络设备错误配置导致网络连接问题。\n6. **服务恢复**: 修复发电机和网络设备后，所有服务和客户访问完全恢复。\n\n### 附录（Appendix）\n\n- **事件时间线**: 2013年12月17日晚上10:05 PST开始，至12月18日凌晨4:05巴西官方时间结束。\n- **后续措施**: AWS团队承诺将采取措施，确保该可用区能够更好地抵御类似的电力故障。\n- **浏览器支持信息**: 互联网探索者（Internet Explorer）的支持于2022年7月31日结束，推荐使用Chrome、Firefox、Edge和Safari等浏览器。\n\n通过上述分析和整理，我们可以看到AWS团队在处理这次电力中断事件中的响应措施和后续改进计划。这有助于理解事件发生的原因、处理过程以及AWS如何确保服务质量和可靠性。"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the Amazon SimpleDB Service Disruption  \nWe wanted to share what we've learned from our investigation of the June 13 SimpleDB disruption in the US East Region. The service was unavailable to all API calls (except a fraction of the eventually consistent read calls) from 9:16 PM to 11:16 PM (PDT). From 11:16 PM to 1:30 AM, we continued to have elevated error rates for CreateDomain and DeleteDomain API calls. \nSimpleDB is a distributed datastore that replicates customer data across multiple data centers. The service employs servers in various roles. Some servers are responsible for the storage of user data (\"storage nodes\"), with each customer Domain replicated across a group of storage nodes. Other nodes store metadata about each customer Domain (\"metadata nodes\"), such as which storage nodes it is located on. SimpleDB uses an\ninternal lock service to determine which set of nodes are responsible for a given Domain. This lock service itself is replicated across multiple data centers. Each node handshakes with the lock service periodically to verify it still has responsibility for the data or metadata it hosts. \nIn this event, multiple storage nodes became unavailable simultaneously in a single data center (after power was lost to the servers on which these nodes lived). While SimpleDB can handle multiple simultaneous node failures, and has successfully endured larger infrastructure failures in\nthe past without incident, the server failure pattern in this event resulted in a sudden and significant increase in load on the lock service as it rapidly de-registered the failed storage nodes from their respective replication groups. This simultaneous volume resulted in\nelevated handshake latencies between healthy SimpleDB nodes and the lock service, and the nodes were not able to complete their handshakes prior to exceeding a set \"handshake timeout\" value. After several handshake retries and subsequent timeouts, SimpleDB storage and metadata nodes removed themselves from the SimpleDB production cluster, and SimpleDB API requests returned error messages (http response code 500 for server-side error). The affected storage nodes were not able to rejoin\nthe SimpleDB cluster and serve API requests until receiving authorization to rejoin from metadata nodes. This process ensures that we do not allow a node with stale data to join the production cluster accidentally and start serving customer requests. However, in this case the metadata nodes were also down due to the same handshake timeout issue, and therefore could not authenticate the storage nodes. \nOnce the problem was identified, we had to manually increase the handshake timeout values and restart a subset of metadata nodes so that they could authorize the storage nodes. This allowed the affected storage nodes to rejoin the SimpleDB cluster and resume serving customer data requests. At this point (11:16 PM), all APIs but CreateDomain and DeleteDomain were functioning normally. To allow the rest of the metadata nodes to fully recover without risk, we throttled CreateDomain\nand DeleteDomain API calls (which are served from metadata nodes) until 1:30 AM. \nWe have identified two significant improvements that can be made to SimpleDB coming out of the event to prevent recurrence of similar issues. First, we will set a longer lock service handshake timeout. The original intent behind the low handshake timeout value we set was to enable rapid detection of replica failure. However, hindsight shows the value was too low. Second, the behavior of nodes removing themselves from the SimpleDB cluster immediately after experiencing multiple handshake timeouts increased the scope of the event and caused SimpleDB API errors. Instead, the nodes should have waited and retried handshake requests later with an increased handshake timeout value. We are addressing these two issues immediately and rolling out fixes to all SimpleDB Regions. We apologize for the impact this issue had on SimpleDB customers.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（logical）、桥接性（bridging）、可操作性（actionable）、简洁性（simplicity）和流程完整性（process integrity）几个角度来分析亚马逊SimpleDB服务中断事件，并按照术语（terminology）、背景（background）、常见问题解答（faq）、流程（flow）和附录（appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **SimpleDB**: 亚马逊提供的分布式数据存储服务，用于存储和查询大量数据。\n- **API calls**: 应用程序接口调用，允许程序请求服务并接收响应。\n- **Eventually consistent read calls**: 最终一致性读取调用，一种读取数据的方式，最终会反映所有最近的写入操作。\n- **Storage nodes**: 存储节点，负责存储用户数据的服务器。\n- **Metadata nodes**: 元数据节点，存储有关每个客户域的元数据信息。\n- **Lock service**: 锁服务，用于确定哪些节点负责给定域的内部服务。\n- **Handshake timeout**: 握手超时，节点与锁服务之间通信的预定时间限制。\n\n### 背景（Background）\n\n在2024年6月13日，美国东部地区的SimpleDB服务经历了一次中断。服务在晚上9:16至11:16（太平洋夏令时）期间对所有API调用（除了一小部分最终一致性读取调用）不可用。从晚上11:16至凌晨1:30，CreateDomain和DeleteDomain API调用继续出现错误率升高的情况。\n\n### 常见问题解答（FAQ）\n\n**Q: 为什么SimpleDB服务会出现中断？**\nA: 由于单个数据中心内多个存储节点同时失去电源而变得不可用，导致锁服务负载突然显著增加，进而导致健康节点与锁服务之间的握手延迟增加，并在多次握手重试和超时后，存储节点和元数据节点将自己从SimpleDB生产集群中移除。\n\n**Q: 服务中断期间用户可以做什么？**\nA: 用户在服务中断期间无法执行大多数API调用。AWS团队在识别问题后手动增加了握手超时值，并重启了一部分元数据节点以授权存储节点重新加入集群。\n\n**Q: AWS计划如何防止类似事件再次发生？**\nA: AWS计划通过设置更长的锁服务握手超时值，并改变节点在经历多次握手超时后立即从SimpleDB集群中移除的行为来防止类似事件的再次发生。\n\n### 流程（Flow）\n\n1. **事件发生**: 多个存储节点因电源问题同时不可用。\n2. **锁服务负载增加**: 锁服务尝试快速从复制组中注销失败的存储节点。\n3. **握手超时**: 健康节点与锁服务的握手延迟增加，导致超时。\n4. **节点移除**: 经过多次重试和超时后，节点将自己从生产集群中移除。\n5. **手动干预**: AWS团队手动增加握手超时值并重启元数据节点。\n6. **服务恢复**: 存储节点获得授权后重新加入集群，服务逐步恢复。\n7. **后续改进**: AWS识别并立即解决两个关键问题，以防止未来类似事件的发生。\n\n### 附录（Appendix）\n\n- **服务中断时间**: 2024年6月13日，晚上9:16至凌晨1:30（太平洋夏令时）。\n- **受影响的API调用**: 除了最终一致性读取调用外，所有API调用在中断期间不可用。\n- **改进措施**: 增加锁服务握手超时值，改变节点移除行为。\n- **支持的浏览器**: 由于Internet Explorer的支持在2022年7月31日结束，建议使用Chrome、Firefox、Edge和Safari浏览器访问AWS服务。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the AWS Service Event in the US East Region\n\n**July 2, 2012** \nWe'd like to share more about the service disruption which occurred last Friday night, June 29th, in one of our Availability Zones in the US East-1 Region. The event was triggered during a large scale electrical storm which swept through the Northern Virginia area. We regret the problems experienced by customers affected by the disruption and, in\naddition to giving more detail, also wanted to provide information on\nactions we'll be taking to mitigate these issues in the future. \nOur US East-1 Region consists of more than 10 datacenters structured into multiple Availability Zones. These Availability Zones are in\ndistinct physical locations and are engineered to isolate failure from each other. Last Friday, due to weather warnings of the approaching storm, all change activity in the US East-1 Region had been cancelled and extra personnel had been called into the datacenters for the evening. \nOn Friday night, as the storm progressed, several US East-1 datacenters in Availability Zones which would remain unaffected by events that evening saw utility power fluctuations. Backup systems in those datacenters responded as designed, resulting in no loss of power or customer impact. At 7:24pm PDT, a large voltage spike was experienced by the electrical switching equipment in two of the US East-1 datacenters supporting a single Availability Zone. All utility electrical switches in both datacenters initiated transfer to generator power. In one of the datacenters, the transfer completed without incident. In the other, the generators started successfully, but each generator independently failed to provide stable voltage as they were brought into service. As a result, the generators did not pick up the load and servers operated without interruption during this period on the Uninterruptable Power Supply (\"UPS\") units. Shortly thereafter, utility power was restored and our datacenter personnel transferred the datacenter back to utility power. The utility power in the Region failed a second time at 7:57pm PDT. Again, all rooms of this one facility failed to successfully transfer to generator power while all of our other datacenters in the Region continued to operate without customer impact. \nIn the single datacenter that did not successfully transfer to the generator backup, all servers continued to operate normally on\nUninterruptable Power Supply (\"UPS\") power. As onsite personnel worked to stabilize the primary and backup power generators, the UPS systems were depleting and servers began losing power at 8:04pm PDT. Ten minutes later, the backup generator power was stabilized, the UPSs were restarted, and power started to be restored by 8:14pm PDT. At 8:24pm PDT, the full facility had power to all racks. \nThe generators and electrical switching equipment in the datacenter that experienced the failure were all the same brand and all installed in\nlate 2010 and early 2011. Prior to installation in this facility, the generators were rigorously tested by the manufacturer. At datacenter commissioning time, they again passed all load tests (approximately 8 hours of testing) without issue. On May 12th of this year, we conducted a full load test where the entire datacenter switched to and ran\nsuccessfully on these same generators, and all systems operated correctly. The generators and electrical equipment in this datacenter are less than two years old, maintained by manufacturer representatives to manufacturer standards, and tested weekly. In addition, these generators operated flawlessly, once brought online Friday night, for just over 30 hours until utility power was restored to this datacenter. The equipment will be repaired, recertified by the manufacturer, and retested at full load onsite or it will be replaced entirely. In the interim, because the generators ran successfully for 30 hours after being manually brought online, we are confident they will perform properly if the load is transferred to them. Therefore, prior to completing the engineering work mentioned above, we will lengthen the amount of time the electrical switching equipment gives the generators to reach stable power before the switch board assesses whether the generators are ready to accept the full power load. Additionally, we will expand the power quality tolerances allowed when evaluating whether to switch the load to generator power. We will expand the size of the onsite 24x7 engineering staff to ensure that if there is a repeat event, the switch to generator will be completed manually (if necessary) before UPSs discharge and there is any customer impact. \nThough the resources in this datacenter, including Elastic Compute Cloud (EC2) instances, Elastic Block Store (EBS) storage volumes, Relational Database Service (RDS) instances, and Elastic Load Balancer (ELB) instances, represent a single-digit percentage of the total resources in\nthe US East-1 Region, there was significant impact to many customers. The impact manifested in two forms. The first was the unavailability of instances and volumes running in the affected datacenter. This kind of impact was limited to the affected Availability Zone. Other Availability Zones in the US East-1 Region continued functioning normally. The second form of impact was degradation of service \"control planes\" which allow customers to take action and create, remove, or change resources across the Region. While control planes aren't required for the ongoing use of resources, they are particularly useful in outages where customers are trying to react to the loss of resources in one Availability Zone by moving to another. \n**EC2 and EBS** \nApproximately 7% of the EC2 instances in the US-EAST-1 Region were in\nthe impacted Availability Zone and impacted by the power loss. These instances were offline until power was restored and systems restarted. EC2 instances operating in other Availability Zones within the US East-1 Region continued to function as they did prior to the event. Internet connectivity into the Region was unaffected. The vast majority of these instances came back online between 11:15pm PDT and just after midnight. Time for the completion of this recovery was extended by a bottleneck in\nour server booting process. Removing this bottleneck is one of the actions we'll take to improve recovery times in the face of power failure. EBS had a comparable percentage (relative to EC2) of its volumes in the Region impacted by this event. The majority of EBS servers had been brought up by 12:25am PDT on Saturday. However, for EBS data volumes that had in-flight writes at the time of the power loss, those volumes had the potential to be in an inconsistent state. Rather than return those volumes in a potentially inconsistent state, once the EBS servers are back up and available, EBS brings customer volumes back online in an impaired state where all I/O on the volume is paused. Customers can then verify the volume is consistent and resume using it. Though the time to recover these EBS volumes has been reduced dramatically over the last 6 months, the number of volumes requiring processing was large enough that it still took several hours to complete the backlog. By 2:45am PDT, 90% of outstanding volumes had been turned over to customers. We have identified several areas in the recovery process that we will further optimize to improve the speed of processing recovered volumes. \nThe control planes for EC2 and EBS were significantly impacted by the power failure, and calls to create new resources or change existing resources failed. From 8:04pm PDT to 9:10pm PDT, customers were not able to launch new EC2 instances, create EBS volumes, or attach volumes in\nany Availability Zone in the US-East-1 Region. At 9:10pm PDT, control plane functionality was restored for the Region. Customers trying to attach or detach impacted EBS volumes would have continued to experienced errors until their impacted EBS volumes were recovered. The duration of the recovery time for the EC2 and EBS control planes was the result of our inability to rapidly fail over to a new primary datastore. The EC2 and EBS APIs are implemented on multi-Availability Zone replicated datastores. These datastores are used to store metadata for resources such as instances, volumes, and snapshots. To protect against datastore corruption, currently when the primary copy loses power, the system automatically flips to a read-only mode in the other Availability Zones until power is restored to the affected Availability Zone or until we determine it is safe to promote another copy to primary. We are addressing the sources of blockage which forced manual assessment and required hand-managed failover for the control plane, and have work already underway to have this flip happen automatically. \n**Elastic Load Balancing** \nElastic Load Balancers (ELBs) allow web traffic directed at a single IP address to be spread across many EC2 instances. They are a tool for high availability as traffic to a single end-point can be handled by many redundant servers. ELBs live in individual Availability Zones and front EC2 instances in those same zones or in other Availability Zones. \nFor single-Availability Zone ELBs, the ELB service maintains one ELB in\nthe specified Availability Zone. If that ELB fails, the ELB control plane assigns its configuration and IP address to another ELB server in\nthat Availability Zone. This normally requires a very short period of time. If there is a large scale issue in the Availability Zone, there may be insufficient capacity to immediately provide a new ELB and replacement will wait for capacity to be made available. \nELBs can also be deployed in multiple Availability Zones. In this configuration, each Availability Zone's end-point will have a separate IP address. A single Domain Name will point to all of the end-points' IP addresses. When a client, such as a web browser, queries DNS with a Domain Name, it receives the IP address (\"A\") records of all of the ELBs in random order. While some clients only process a single IP address, many (such as newer versions of web-browsers) will retry the subsequent IP addresses if they fail to connect to the first. A large number of non-browser clients only operate with a single IP address. \nFor multi-Availability Zone ELBs, the ELB service maintains ELBs redundantly in the Availability Zones a customer requests them to be in\nso that failure of a single machine or datacenter won't take down the end-point. The ELB service avoids impact (even for clients which can\nonly process a single IP address) by detecting failure and eliminating the problematic ELB instance's IP address from the list returned by DNS. The ELB control plane processes all management events for ELBs including traffic shifts due to failure, size scaling for ELB due to traffic growth, and addition and removal of EC2 instances from association with a given ELB. \nDuring the disruption this past Friday night, the control plane (which encompasses calls to add a new ELB, scale an ELB, add EC2 instances to an ELB, and remove traffic from ELBs) began performing traffic shifts to account for the loss of load balancers in the affected Availability Zone. As the power and systems returned, a large number of ELBs came up in a state which triggered a bug we hadn't seen before. The bug caused the ELB control plane to attempt to scale these ELBs to larger ELB instance sizes. This resulted in a sudden flood of requests which began\nto backlog the control plane. At the same time, customers began\nlaunching new EC2 instances to replace capacity lost in the impacted Availability Zone, requesting the instances be added to existing load balancers in the other zones. These requests further increased the ELB control plane backlog. Because the ELB control plane currently manages requests for the US East-1 Region through a shared queue, it fell increasingly behind in processing these requests; and pretty soon, these requests started taking a very long time to complete. \nWhile direct impact was limited to those ELBs which had failed in the power-affected datacenter and hadn't yet had their traffic shifted, the ELB service's inability to quickly process new requests delayed recovery for many customers who were replacing lost EC2 capacity by launching new instances in other Availability Zones. For multi-Availability Zone ELBs, if a client attempted to connect to an ELB in a healthy Availability Zone, it succeeded. If a client attempted to connect to an ELB in the impacted Availability Zone and didn't retry using one of the alternate IP addresses returned, it would fail to connect until the backlogged traffic shift occurred and it issued a new DNS query. As mentioned, many modern web browsers perform multiple attempts when given multiple IP addresses; but many clients, especially game consoles and other consumer electronics, only use one IP address returned from the DNS query. \nAs a result of these impacts and our learning from them, we are breaking ELB processing into multiple queues to improve overall throughput and to allow more rapid processing of time-sensitive actions such as traffic shifts. We are also going to immediately develop a backup DNS re-weighting that can very quickly shift all ELB traffic away from an\nimpacted Availability Zone without contacting the control plane. \n**Relational Database Service (RDS)** \nRDS provides two modes of operation: Single Availability Zone (Single-AZ), where a single database instance operates in one Availability Zone; and Multi Availability Zone (Multi-AZ), where two database instances are synchronously operated in two different Availability Zones. For Multi-AZ RDS, one of the two database instances is the \"primary\" and the other is a \"standby.\" The primary handles all database requests and replicates to the standby. In the case where a primary fails, the standby is promoted to be the new primary. \nSingle-AZ RDS Instances, by default, have backups turned on. When a Single-AZ RDS instance fails, there are two kinds of recovery that are possible. If EBS volumes do not require recovery, the database instance can simply be restarted. If recovery is required, the backups are used to restore the database. In some cases, where backups have been turned off by customers, there can be no recovery and the instance is lost unless manual backups have been taken. \nMulti-AZ RDS Instances detect failure in the primary or standby and immediately take action. If the primary fails, the DNS CNAME record is updated to point to the standby. If the standby fails, a new instance is launched and instantiated from the primary as the new standby. Once failure is confirmed, failover can take place in less than a minute. \nWhen servers lost power in the impacted datacenter, many Single-AZ RDS instances in that Availability Zone became unavailable. There was no way to recover these instances until servers were powered up, booted, and brought online. By 10pm PDT, a large number of the affected Single-AZ RDS instances had been brought online. There were many remaining instances which required EBS to recover storage volumes. These followed the timeline described above for EBS impact. Once volumes were recovered, customers could apply backups and restore their Single-AZ RDS instances. In addition to the actions noted above with EBS, RDS will be working to improve the speed at which volumes available for recovery can\nbe processed. \nAt the point of power loss, most Multi-AZ instances almost instantly promoted their standby in a healthy AZ to \"primary\" as expected. However, a small number of Multi-AZ RDS instances did not complete failover, due to a software bug. The bug was introduced in April when we made changes to the way we handle storage failure. It is only manifested when a certain sequence of communication failure is experienced, situations we saw during this event as a variety of server shutdown\nsequences occurred. This triggered a failsafe which required manual intervention to complete the failover. In most cases, the manual work could be completed without EBS recovery taking place. The majority of remaining Multi-AZ failovers were completed by 11:00pm PDT. The remaining Multi-AZ instances were processed when EBS volume recovery completed for their storage volumes. \nTo address the issues we had with some Multi-AZ RDS Instances failovers, we have a mitigation for the bug in test and will be rolling it out in\nproduction in the coming weeks. \n**Final Thoughts** \nWe apologize for the inconvenience and trouble this caused for affected customers. We know how critical our services are to our customers' businesses. If you've followed the history of AWS, the customer focus we have, and the pace with which we iterate, we think you know that we will do everything we can to learn from this event and use it to drive improvement across our services. We will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make further changes to improve our services and processes\n\nAssistant:", "response": [["Bad weather caused power failures throughout AWS US East. A single backup generator failed to deliver stable power when power switched over to backup and the generator was loaded. This is despite having passed a load tests two months earlier, and passing weekly power-on tests"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the AWS Service Event in the US East Region\n\n**July 2, 2012** \nWe'd like to share more about the service disruption which occurred last Friday night, June 29th, in one of our Availability Zones in the US East-1 Region. The event was triggered during a large scale electrical storm which swept through the Northern Virginia area. We regret the problems experienced by customers affected by the disruption and, in\naddition to giving more detail, also wanted to provide information on\nactions we'll be taking to mitigate these issues in the future. \nOur US East-1 Region consists of more than 10 datacenters structured into multiple Availability Zones. These Availability Zones are in\ndistinct physical locations and are engineered to isolate failure from each other. Last Friday, due to weather warnings of the approaching storm, all change activity in the US East-1 Region had been cancelled and extra personnel had been called into the datacenters for the evening. \nOn Friday night, as the storm progressed, several US East-1 datacenters in Availability Zones which would remain unaffected by events that evening saw utility power fluctuations. Backup systems in those datacenters responded as designed, resulting in no loss of power or customer impact. At 7:24pm PDT, a large voltage spike was experienced by the electrical switching equipment in two of the US East-1 datacenters supporting a single Availability Zone. All utility electrical switches in both datacenters initiated transfer to generator power. In one of the datacenters, the transfer completed without incident. In the other, the generators started successfully, but each generator independently failed to provide stable voltage as they were brought into service. As a result, the generators did not pick up the load and servers operated without interruption during this period on the Uninterruptable Power Supply (\"UPS\") units. Shortly thereafter, utility power was restored and our datacenter personnel transferred the datacenter back to utility power. The utility power in the Region failed a second time at 7:57pm PDT. Again, all rooms of this one facility failed to successfully transfer to generator power while all of our other datacenters in the Region continued to operate without customer impact. \nIn the single datacenter that did not successfully transfer to the generator backup, all servers continued to operate normally on\nUninterruptable Power Supply (\"UPS\") power. As onsite personnel worked to stabilize the primary and backup power generators, the UPS systems were depleting and servers began losing power at 8:04pm PDT. Ten minutes later, the backup generator power was stabilized, the UPSs were restarted, and power started to be restored by 8:14pm PDT. At 8:24pm PDT, the full facility had power to all racks. \nThe generators and electrical switching equipment in the datacenter that experienced the failure were all the same brand and all installed in\nlate 2010 and early 2011. Prior to installation in this facility, the generators were rigorously tested by the manufacturer. At datacenter commissioning time, they again passed all load tests (approximately 8 hours of testing) without issue. On May 12th of this year, we conducted a full load test where the entire datacenter switched to and ran\nsuccessfully on these same generators, and all systems operated correctly. The generators and electrical equipment in this datacenter are less than two years old, maintained by manufacturer representatives to manufacturer standards, and tested weekly. In addition, these generators operated flawlessly, once brought online Friday night, for just over 30 hours until utility power was restored to this datacenter. The equipment will be repaired, recertified by the manufacturer, and retested at full load onsite or it will be replaced entirely. In the interim, because the generators ran successfully for 30 hours after being manually brought online, we are confident they will perform properly if the load is transferred to them. Therefore, prior to completing the engineering work mentioned above, we will lengthen the amount of time the electrical switching equipment gives the generators to reach stable power before the switch board assesses whether the generators are ready to accept the full power load. Additionally, we will expand the power quality tolerances allowed when evaluating whether to switch the load to generator power. We will expand the size of the onsite 24x7 engineering staff to ensure that if there is a repeat event, the switch to generator will be completed manually (if necessary) before UPSs discharge and there is any customer impact. \nThough the resources in this datacenter, including Elastic Compute Cloud (EC2) instances, Elastic Block Store (EBS) storage volumes, Relational Database Service (RDS) instances, and Elastic Load Balancer (ELB) instances, represent a single-digit percentage of the total resources in\nthe US East-1 Region, there was significant impact to many customers. The impact manifested in two forms. The first was the unavailability of instances and volumes running in the affected datacenter. This kind of impact was limited to the affected Availability Zone. Other Availability Zones in the US East-1 Region continued functioning normally. The second form of impact was degradation of service \"control planes\" which allow customers to take action and create, remove, or change resources across the Region. While control planes aren't required for the ongoing use of resources, they are particularly useful in outages where customers are trying to react to the loss of resources in one Availability Zone by moving to another. \n**EC2 and EBS** \nApproximately 7% of the EC2 instances in the US-EAST-1 Region were in\nthe impacted Availability Zone and impacted by the power loss. These instances were offline until power was restored and systems restarted. EC2 instances operating in other Availability Zones within the US East-1 Region continued to function as they did prior to the event. Internet connectivity into the Region was unaffected. The vast majority of these instances came back online between 11:15pm PDT and just after midnight. Time for the completion of this recovery was extended by a bottleneck in\nour server booting process. Removing this bottleneck is one of the actions we'll take to improve recovery times in the face of power failure. EBS had a comparable percentage (relative to EC2) of its volumes in the Region impacted by this event. The majority of EBS servers had been brought up by 12:25am PDT on Saturday. However, for EBS data volumes that had in-flight writes at the time of the power loss, those volumes had the potential to be in an inconsistent state. Rather than return those volumes in a potentially inconsistent state, once the EBS servers are back up and available, EBS brings customer volumes back online in an impaired state where all I/O on the volume is paused. Customers can then verify the volume is consistent and resume using it. Though the time to recover these EBS volumes has been reduced dramatically over the last 6 months, the number of volumes requiring processing was large enough that it still took several hours to complete the backlog. By 2:45am PDT, 90% of outstanding volumes had been turned over to customers. We have identified several areas in the recovery process that we will further optimize to improve the speed of processing recovered volumes. \nThe control planes for EC2 and EBS were significantly impacted by the power failure, and calls to create new resources or change existing resources failed. From 8:04pm PDT to 9:10pm PDT, customers were not able to launch new EC2 instances, create EBS volumes, or attach volumes in\nany Availability Zone in the US-East-1 Region. At 9:10pm PDT, control plane functionality was restored for the Region. Customers trying to attach or detach impacted EBS volumes would have continued to experienced errors until their impacted EBS volumes were recovered. The duration of the recovery time for the EC2 and EBS control planes was the result of our inability to rapidly fail over to a new primary datastore. The EC2 and EBS APIs are implemented on multi-Availability Zone replicated datastores. These datastores are used to store metadata for resources such as instances, volumes, and snapshots. To protect against datastore corruption, currently when the primary copy loses power, the system automatically flips to a read-only mode in the other Availability Zones until power is restored to the affected Availability Zone or until we determine it is safe to promote another copy to primary. We are addressing the sources of blockage which forced manual assessment and required hand-managed failover for the control plane, and have work already underway to have this flip happen automatically. \n**Elastic Load Balancing** \nElastic Load Balancers (ELBs) allow web traffic directed at a single IP address to be spread across many EC2 instances. They are a tool for high availability as traffic to a single end-point can be handled by many redundant servers. ELBs live in individual Availability Zones and front EC2 instances in those same zones or in other Availability Zones. \nFor single-Availability Zone ELBs, the ELB service maintains one ELB in\nthe specified Availability Zone. If that ELB fails, the ELB control plane assigns its configuration and IP address to another ELB server in\nthat Availability Zone. This normally requires a very short period of time. If there is a large scale issue in the Availability Zone, there may be insufficient capacity to immediately provide a new ELB and replacement will wait for capacity to be made available. \nELBs can also be deployed in multiple Availability Zones. In this configuration, each Availability Zone's end-point will have a separate IP address. A single Domain Name will point to all of the end-points' IP addresses. When a client, such as a web browser, queries DNS with a Domain Name, it receives the IP address (\"A\") records of all of the ELBs in random order. While some clients only process a single IP address, many (such as newer versions of web-browsers) will retry the subsequent IP addresses if they fail to connect to the first. A large number of non-browser clients only operate with a single IP address. \nFor multi-Availability Zone ELBs, the ELB service maintains ELBs redundantly in the Availability Zones a customer requests them to be in\nso that failure of a single machine or datacenter won't take down the end-point. The ELB service avoids impact (even for clients which can\nonly process a single IP address) by detecting failure and eliminating the problematic ELB instance's IP address from the list returned by DNS. The ELB control plane processes all management events for ELBs including traffic shifts due to failure, size scaling for ELB due to traffic growth, and addition and removal of EC2 instances from association with a given ELB. \nDuring the disruption this past Friday night, the control plane (which encompasses calls to add a new ELB, scale an ELB, add EC2 instances to an ELB, and remove traffic from ELBs) began performing traffic shifts to account for the loss of load balancers in the affected Availability Zone. As the power and systems returned, a large number of ELBs came up in a state which triggered a bug we hadn't seen before. The bug caused the ELB control plane to attempt to scale these ELBs to larger ELB instance sizes. This resulted in a sudden flood of requests which began\nto backlog the control plane. At the same time, customers began\nlaunching new EC2 instances to replace capacity lost in the impacted Availability Zone, requesting the instances be added to existing load balancers in the other zones. These requests further increased the ELB control plane backlog. Because the ELB control plane currently manages requests for the US East-1 Region through a shared queue, it fell increasingly behind in processing these requests; and pretty soon, these requests started taking a very long time to complete. \nWhile direct impact was limited to those ELBs which had failed in the power-affected datacenter and hadn't yet had their traffic shifted, the ELB service's inability to quickly process new requests delayed recovery for many customers who were replacing lost EC2 capacity by launching new instances in other Availability Zones. For multi-Availability Zone ELBs, if a client attempted to connect to an ELB in a healthy Availability Zone, it succeeded. If a client attempted to connect to an ELB in the impacted Availability Zone and didn't retry using one of the alternate IP addresses returned, it would fail to connect until the backlogged traffic shift occurred and it issued a new DNS query. As mentioned, many modern web browsers perform multiple attempts when given multiple IP addresses; but many clients, especially game consoles and other consumer electronics, only use one IP address returned from the DNS query. \nAs a result of these impacts and our learning from them, we are breaking ELB processing into multiple queues to improve overall throughput and to allow more rapid processing of time-sensitive actions such as traffic shifts. We are also going to immediately develop a backup DNS re-weighting that can very quickly shift all ELB traffic away from an\nimpacted Availability Zone without contacting the control plane. \n**Relational Database Service (RDS)** \nRDS provides two modes of operation: Single Availability Zone (Single-AZ), where a single database instance operates in one Availability Zone; and Multi Availability Zone (Multi-AZ), where two database instances are synchronously operated in two different Availability Zones. For Multi-AZ RDS, one of the two database instances is the \"primary\" and the other is a \"standby.\" The primary handles all database requests and replicates to the standby. In the case where a primary fails, the standby is promoted to be the new primary. \nSingle-AZ RDS Instances, by default, have backups turned on. When a Single-AZ RDS instance fails, there are two kinds of recovery that are possible. If EBS volumes do not require recovery, the database instance can simply be restarted. If recovery is required, the backups are used to restore the database. In some cases, where backups have been turned off by customers, there can be no recovery and the instance is lost unless manual backups have been taken. \nMulti-AZ RDS Instances detect failure in the primary or standby and immediately take action. If the primary fails, the DNS CNAME record is updated to point to the standby. If the standby fails, a new instance is launched and instantiated from the primary as the new standby. Once failure is confirmed, failover can take place in less than a minute. \nWhen servers lost power in the impacted datacenter, many Single-AZ RDS instances in that Availability Zone became unavailable. There was no way to recover these instances until servers were powered up, booted, and brought online. By 10pm PDT, a large number of the affected Single-AZ RDS instances had been brought online. There were many remaining instances which required EBS to recover storage volumes. These followed the timeline described above for EBS impact. Once volumes were recovered, customers could apply backups and restore their Single-AZ RDS instances. In addition to the actions noted above with EBS, RDS will be working to improve the speed at which volumes available for recovery can\nbe processed. \nAt the point of power loss, most Multi-AZ instances almost instantly promoted their standby in a healthy AZ to \"primary\" as expected. However, a small number of Multi-AZ RDS instances did not complete failover, due to a software bug. The bug was introduced in April when we made changes to the way we handle storage failure. It is only manifested when a certain sequence of communication failure is experienced, situations we saw during this event as a variety of server shutdown\nsequences occurred. This triggered a failsafe which required manual intervention to complete the failover. In most cases, the manual work could be completed without EBS recovery taking place. The majority of remaining Multi-AZ failovers were completed by 11:00pm PDT. The remaining Multi-AZ instances were processed when EBS volume recovery completed for their storage volumes. \nTo address the issues we had with some Multi-AZ RDS Instances failovers, we have a mitigation for the bug in test and will be rolling it out in\nproduction in the coming weeks. \n**Final Thoughts** \nWe apologize for the inconvenience and trouble this caused for affected customers. We know how critical our services are to our customers' businesses. If you've followed the history of AWS, the customer focus we have, and the pace with which we iterate, we think you know that we will do everything we can to learn from this event and use it to drive improvement across our services. We will spend many hours over the coming days and weeks improving our understanding of the details of the various parts of this event and determining how to make further changes to improve our services and processes\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。 \nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（Logical）、桥接性（Bridging）、可操作性（Actionable）、简洁性（Simplicity）和流程完整性（Process Integrity）几个角度来分析AWS服务事件，并按照术语（Terminology）、背景（Background）、常见问题解答（FAQ）、流程（Flow）和附录（Appendix）进行总结整理。\n### Terminology\n\n- **AWS Service Event**: 指亚马逊网络服务（Amazon Web Services）在2012年6月29日美国东部第一区域（US East-1）由于大规模雷暴天气导致的服务中断事件。\n- **Availability Zones**: AWS数据中心的不同物理位置，设计用来隔离故障，保证服务的高可用性。\n- **Elastic Compute Cloud (EC2)**: AWS提供的一种可扩展的计算服务，允许用户在云中运行虚拟服务器。\n- **Elastic Block Store (EBS)**: AWS的一种块存储服务，为EC2实例提供持久性存储。\n- **Relational Database Service (RDS)**: AWS提供的一种托管数据库服务，支持多种数据库引擎。\n- **Elastic Load Balancing (ELB)**: AWS提供的一种自动分配进入应用流量的服务，提高应用的可用性和容错能力。\n\n### Background\n\n- 事件发生在2012年6月29日，美国东部时间晚上，由于一场大规模雷暴天气影响了弗吉尼亚北部地区，导致AWS US East-1区域的部分数据中心出现电力波动。\n- 尽管AWS采取了预防措施，如取消所有变更活动并增加数据中心的人员，但仍然有部分数据中心未能成功切换到备用发电机供电，导致服务器在不间断电源（UPS）供电下运行，并最终失去电源。\n- 服务中断影响了EC2实例、EBS存储卷、RDS实例和ELB实例等资源，尽管这些资源在US East-1区域中所占比例不大，但对许多客户产生了重大影响。\n\n### FAQ\n\n- **Q: 服务中断的原因是什么？**\n- A: 主要是由于雷暴天气导致的电力波动，部分数据中心未能成功切换到备用发电机供电。\n\n- **Q: AWS采取了哪些措施来应对这次事件？**\n- A: AWS增加了数据中心的人员配置，对发生故障的发电机和电气切换设备进行了修复和重新认证，并延长了电气切换设备给予发电机达到稳定电源的时间。\n\n- **Q: 客户受到了哪些影响？**\n- A: 客户受到了实例和卷的不可用性影响，以及服务“控制平面”的退化影响，这影响了客户在区域内创建、删除或更改资源的能力。\n\n### Flow\n\n1. **事件发生前的准备**: 由于天气预报，AWS取消了所有变更活动并增加了数据中心的人员配置。\n2. **事件发展**: 雷暴导致电力波动，部分数据中心未能成功切换到备用发电机供电。\n3. **影响范围**: 服务中断影响了EC2、EBS、RDS和ELB等资源，控制平面功能受损。\n4. **恢复过程**: 通过手动和自动的方式，AWS逐步恢复了服务，并解决了由于电力恢复导致的ELB控制平面积压问题。\n5. **后续改进**: AWS采取了一系列措施，包括优化恢复流程、改进ELB处理、修复RDS服务等，以防止类似事件再次发生。\n\n### Appendix\n\n- **数据中心的备用电源系统**: 发电机和UPS系统在电力中断时提供了短暂的备用电源。\n- **控制平面的改进**: AWS计划通过多队列处理和快速DNS重新加权来提高ELB的控制平面性能和响应速度。\n- **RDS服务的修复**: 对于未能完成故障转移的Multi-AZ RDS实例，AWS已经测试了一个修复方案，并计划在未来几周内部署到生产环境中。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the October 22, 2012 AWS Service Event in the US-East Region\n\nWe'd like to share more about the service event that occurred on Monday, October 22nd in the US- East Region. We have now completed the analysis of the events that affected AWS customers, and we want to describe what happened, our understanding of how customers were affected, and what we are doing to prevent a similar issue from occurring in the future. \n**The Primary Event and the Impact to Amazon Elastic Block Store (EBS) and Amazon Elastic Compute Cloud (EC2)** \nAt 10:00AM PDT Monday, a small number of Amazon Elastic Block Store (EBS) volumes in one of our five Availability Zones in the US-East Region began seeing degraded performance, and in some cases, became \"stuck\" (i.e. unable to process further I/O requests). The root cause of the problem was a latent bug in an operational data collection agent that runs on the EBS storage servers. Each EBS storage server has an\nagent that contacts a set of data collection servers and reports information that is used for fleet maintenance. The data collected with this system is important, but the collection is not time- sensitive and the system is designed to be tolerant of late or missing data. Last week, one of the data collection servers in the affected Availability Zone had a hardware failure and was replaced. As part of replacing that server, a DNS record was updated to remove the failed server and add the replacement server. While not noticed at the time, the DNS update did not successfully propagate to all of the internal DNS servers, and as a result, a fraction of the storage servers did not get the updated server address and continued to attempt to contact the failed data collection\nserver. Because of the design of the data collection service (which is tolerant to missing data), this did not cause any immediate issues or set off any alarms. However, this inability to contact a data collection\nserver triggered a latent memory leak bug in the reporting agent on the storage servers. Rather than gracefully deal with the failed connection, the reporting agent continued trying to contact the collection server in\na way that slowly consumed system memory. While we monitor aggregate memory consumption on each EBS Server, our monitoring failed to alarm on\nthis memory leak. EBS Servers generally make very dynamic use of all of their available memory for managing customer data, making it difficult to set accurate alarms on memory usage and free memory. By Monday morning, the rate of memory loss became quite high and consumed enough memory on the affected storage servers that they were unable to keep up with normal request handling processes. \nThe memory pressure on many of the EBS servers had reached a point where EBS servers began losing the ability to process customer requests and the number of stuck volumes increased quickly. This caused the system to begin to failover from the degraded servers to healthy servers. However, because many of the servers became memory-exhausted at the same time, the system was unable to find enough healthy servers to failover to, and more volumes became stuck. By approximately 11:00AM PDT, a large number of volumes in this Availability Zone were stuck. To remedy this, at 11:10AM PDT, the team made adjustments to reduce the failover rate. These adjustments removed load from the service, and by 11:35AM PDT, the system began automatically recovering many volumes. By 1:40PM PDT, about 60% of the affected volumes had recovered. The team continued to work to understand the issue and restore performance for the remaining volumes. The large surge in failover and recovery activity in the cluster made it difficult for the team to identify the root cause of the event. At 3:10PM PDT, the team identified the underlying issue and was able to begin restoring performance for the remaining volumes by freeing the excess memory consumed by the misbehaving collection agent. At this point, the system was able to recover most of the remaining stuck volumes; and by 4:15PM PDT, nearly all affected volumes were restored and performing normally. \nWe have deployed monitoring that will alarm if we see this specific memory leak again in any of our production EBS servers, and next week, we will begin deploying a fix for the memory leak issue. We are also modifying our system memory monitoring on the EBS storage servers to monitor and alarm on each process's memory consumption, and we will be deploying resource limits to prevent low priority processes from consuming excess resources on these hosts. We are also updating our internal DNS configuration to further ensure that DNS changes are propagated reliably, and as importantly, make sure that our monitoring and alarming surface issues more quickly should these changes not succeed. These actions will address the problems that triggered the event. In addition, we are evaluating how to change the EBS failover logic that led to the rapid deterioration early in this event. We believe we can make adjustments to reduce the impact of any similar correlated failure or degradation of EBS servers within an Availability Zone. \n**Impact on the EC2 and EBS APIs** \nThe primary event only affected EBS volumes in a single Availability Zone, so those customers running with adequate capacity in other Availability Zones in the US East Region were able to tolerate the event with limited impact to their applications. However, many customers reported difficulty using the service APIs to manage their resources during this event. We have invested heavily in making our service APIs resilient to failure during events affecting a single Availability Zone. And, other than a few short periods, our monitoring showed what looked to be a healthy level of launch and create activity throughout the event. However, we've heard from customers that they struggled to use the APIs for several hours. We now understand that our API throttling during the event disproportionately impacted some customers and affected their ability to use the APIs. \nWe use throttling to protect our services from being overwhelmed by internal and external callers that intentionally or unintentionally put excess load on our services. A simple example of the kind of issue throttling protects against is a runaway application that naively retries a request as fast as possible when it fails to get a positive result. Our systems are scaled to handle these sorts of client errors, but during a large operational event, it is not uncommon for many users to inadvertently increase load on the system. So, while we always have a base level of throttling in place, the team enabled a more aggressive throttling policy during this event to try to assure that the system remained stable during the period where customers and the system were trying to recover. Unfortunately, the throttling policy that was put in\nplace was too aggressive. \nAt 12:06PM PDT, the team implemented this aggressive API throttling policy to help assure stability of the system during the recovery. The team monitored the aggregate throttling rate as well as the overall activity (launches, volume creation, etc.) and did not at the time believe that customers were being substantially impacted. We now understand that this throttling policy, for a subset of our customers, was throttling a higher percentage of API calls than we realized during the event. The service APIs were still handling the vast majority of customer requests to launch and terminate instances and make other changes to their EC2 and EBS resources, but many customers experienced high levels of throttling on calls to describe their resources (e.g. DescribeInstances, DescribeImages, etc.). This made it difficult for these customers and their management applications to successfully use the service APIs during this period. It also affected users' ability to successfully manage their EC2 and EBS resources from the AWS Management Console. This throttling policy was in effect until 2:33PM PDT, after which we reduced the level of throttling considerably. \nWe have changed our operational procedures to not use this more aggressive throttling policy during any future event. We believe that our other throttling policies will provide us with the necessary service protection while avoiding the impact that customers saw during this event. We are also modifying our operational dashboard to add per-customer throttling monitoring (rather than just aggregate throttling rates) so that we have better visibility into the number of customers seeing heavy throttling. This will allow us to quickly understand the impact throttling is having on individual customers, regardless of what the overall throttling rate is, and make appropriate adjustments more quickly. \nThrottling is a valuable tool for managing the health of our services, and we employ it regularly without significantly affecting customers' ability to use our services. While customers need to expect that they will encounter API throttling from time to time, we realize that the throttling policy we used for part of this event had a greater impact on\nmany customers than we understood or intended. While this did not meaningfully affect users running high-availability applications architected to run across multiple Availability Zones with adequate running capacity to failover during Availability Zone disruptions, it did lead to several hours of significant API degradation for many of our customers. This inhibited these customers' ability to use the APIs to recover from this event, and in some cases, get normal work done. Therefore, AWS will be issuing a credit to any customer whose API calls were throttled by this aggressive throttling policy (i.e. any customer whose API access was throttled between 12:06PM PDT and 2:33PM PDT) for 100% of their EC2, EBS and ELB usage for three hours of their Monday usage (to cover the period the aggressive throttling policy was in\nplace). Affected customers do not need to take any action; the credits will be automatically applied to their AWS account prior to their October 31 bill being calculated. \n**Impact on Amazon Relational Database Service (RDS)** \nThis event also had an impact on the Amazon Relational Database Service (\"RDS\"). RDS uses EBS for database and log storage, and as a result, a portion of the RDS databases hosted in the affected Availability Zone became inaccessible. Throughout the course of the event, customers were able to create new RDS instances and access existing RDS instances in\nthe unaffected Availability Zones in the region. \nAmazon RDS provides two modes of operation: Single Availability Zone (Single-AZ), where a single database instance operates in one Availability Zone; and Multi Availability Zone (Multi-AZ), where two database instances are synchronously operated in two different Availability Zones. For Multi-AZ RDS, one of the two database instances is the \"primary\" and the other is a \"standby.\" The primary handles all database requests and replicates to the standby. In the case where a primary fails, the standby is promoted to be the new primary and is available to handle database requests after integrity checks are completed. \nSingle-AZ database instances are exposed to disruptions in an\nAvailability Zone. In this case, a Single-AZ database instance would have been affected if one of the EBS volumes it was relying on got stuck. During this event, a significant number of the Single-AZ databases in the affected zone became stuck as the EBS volumes used by them were affected by the primary EBS event described above. In the case of these Single-AZ databases, recovery depended on waiting for the underlying EBS volumes to have their performance restored. By 1:30PM PDT, a significant number of the impaired Single-AZ RDS instances were restored as the volumes they depended on became unstuck. By 3:30PM PDT, the majority of the affected database instances were restored, and by 6:35PM PDT, almost all of the affected Single-AZ RDS instances were restored. \nDuring the course of the event, almost all of the Multi-AZ instances were promoted to their standby in a healthy Availability Zone, and were available to handle database requests after integrity checks were completed. However, a single digit percentage of Multi-AZ RDS instances in the affected Availability Zone did not failover automatically due to two different software bugs. The first group of RDS instances that did not failover as expected encountered an uncommon stuck I/O condition, which the automatic failover logic did not handle correctly. These instances required operator action and were fully restored by 11:30 AM PDT. We have developed a fix for this bug and are in the process of rolling it out. The second group of Multi-AZ instances did not failover automatically because the master database instances were disconnected from their standby for a brief time interval immediately before these master database instances' volumes became stuck. Normally these events are simultaneous. Between the period of time the masters were disconnected from their standbys and the point where volumes became stuck, the masters continued to process transactions without being able to replicate to their standbys. When these masters subsequently became stuck, the system blocked automatic failover to the out-of-date standbys. We have already been working on a fix for this issue which will allow the standby to be favored immediately when its master is in\nan impaired Availability Zone. Due to the subtle nature of the issues involved, we are still in the process of completing this fix and carefully testing it, but are on track to deploy it fully by December. Database instances affected by this condition were restored once the associated EBS volumes had performance restored. While we are disappointed with the impact to these Multi-AZ instances, we are confident that when we complete the deployment of these two bug fixes, the root cause of the Multi-AZ failures we observed during this event will be addressed. It is the top priority of the team to complete these fixes and get them deployed to the fleet. \nCustomers affected by the Multi-AZ RDS issues did not get the availability they or we expected. If an application is Multi-AZ and has enough resources running to continue operating if one Availability Zone is lost, then that application component should remain available (with minimal service disruption). Accordingly, AWS will issue service credits to customers whose RDS Multi-AZ instances took longer than 20 minutes to fail over to their secondary copies, equal to 10 days of charges for those affected Multi-AZ instances. Affected customers do not need to take any action; the credits will be automatically applied to their AWS account prior to their October 31 bill being calculated. \n**Impact on Amazon Elastic Load Balancing (ELB)** \nThis event also affected the Amazon Elastic Load Balancing (ELB) service. Each ELB load balancer uses one or more load balancer instances to route traffic to customers' EC2 instances. These ELB load balancer instances use EBS for storing configuration and monitoring information, and when the EBS volumes on these load balancer instances hung, some of the ELB load balancers became degraded and the ELB service began\nexecuting recovery workflows to either restore or replace the affected load balancer instances. Customers can use ELB with applications that the run in either single or multiple Availability Zones. \nFor customers using an ELB load balancer with an application running in\na single Available Zone, ELB provisions load balancer instances in the Availability Zone in which the application is running (effectively creating a Single-AZ load balancer). During this event, a number of Single-AZ load balancers in the affected Availability Zone became impaired when some or all of the load balancer instances used by the load balancer became inaccessible due to the primary EBS issue. These affected load balancers recovered as soon as the ELB system was able to provision additional EBS volumes in the affected Availability Zone, or in some cases, when the EBS volumes on which particular load balancers relied, were restored. By 1:10PM PDT, the majority of affected Single-AZ load balancers had recovered, and by 3:30PM PDT, most of the remaining load balancers had also been recovered. Recovery of the last remaining load balancers was then slowed by an issue encountered by the ELB recovery workflows. ELB uses Elastic IP addresses (EIPs) to reliably route traffic to load balancer instances. EIPs are consumed as new load balancers are created and as existing load balancers are scaled. The increased demand for EIPs from the ELB recovery workflows (and the overall increase of customer activity during this period) caused ELB to consume all of the EIPs that were available to it. This stalled the recovery workflows and delayed recovery of the final affected load balancers. The team continued to manually recover the remaining impaired load balancers and was able to remediate the EIP shortage at 9:50PM PDT. \nWe are working on a number of improvements to shorten the recovery time of ELB for all customers. We will ensure that we have additional EIP capacity available to the ELB system at all times to allow full recovery of any Availability Zone issue. We are already in the process of making a few changes to reduce the interdependency between ELB and EBS to avoid correlated failure in future events and allow ELB recovery even when\nthere are EBS issues within an Availability Zone. Finally, we are also in the process of a few additional improvements to our recovery workflows that will be released in the coming weeks that will further improve the recovery time of ELB load balancers during any similar event. \nFor customers using an ELB load balancer with an application running in\nmultiple Availability Zones, ELB will provision load balancer instances in every Availability Zone in which the application is running. For these multiple Availability Zone applications, ELB can route traffic away from degraded Availability Zones to allow multiple Availability Zone applications to quickly recover. During this event, customers using ELB with applications running in multiple Availability Zones that included the affected Availability Zone may have experienced elevated error rates during the early parts of the primary event as load balancer instances or the EC2 instances running the customer's application were affected by the EBS issue. By 11:49AM PDT, the ELB service shifted customer traffic away from the impaired Availability Zone for most load balancers with multiple Availability Zone applications. This allowed applications behind these load balancers to serve traffic from their instances in other, unaffected Availability Zones. Unfortunately, a bug in the traffic shifting functionality incorrectly mapped a small number of the affected load balancers and therefore didn't shift traffic correctly. These load balancers continued to send a portion of the customer requests to the affected Availability Zone until the issue was identified and corrected at 12:45PM PDT. We have corrected the logic in\nthe ELB traffic shifting functionality so this error will not occur in\nthe future. We are also working to improve the sensitivity of the traffic shifting procedure so that traffic is more quickly failed away from a degraded Availability Zone in the future. Over time, we will also expose this traffic shifting functionality directly to ELB customers so that they have the ability to control the routing of their requests to the Availability Zones in which they run their applications. Finally, we will work on helping our customers understand and test the impact of this traffic shift so that they can be sure their applications can scale to handle the increased load caused by failing away from an Availability Zone. \n**Final Thoughts** \nWe apologize for the inconvenience and trouble this caused for affected customers. We know how critical our services are to our customers' businesses, and will work hard (and expeditiously) to apply the learning from this event to our services. While we saw that some of the changes that we previously made helped us mitigate some of the impact, we also learned about new failure modes. We will spend many hours over the coming days and weeks improving our understanding of the event and further investing in the resiliency of our services.\n\nAssistant:", "response": [["Inability to contact a data collection server triggered a latent memory leak bug in the reporting agent on the storage servers. And there is no graceful degradation handling, thus the reporting agent continuously contacted the collection server in a way that slowly consumed system memory. Also the monitoring system failed to alarm this EBS server's memory leak, also EBS servers generally make very dynamic use of all memory. By Monday morning, the rate of memory loss became quite high and confused enough memory on the affected storage servers which cannot keep with the request handling process. This error got further severed by the inability to do the failover, which resulted in the outage"]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the October 22, 2012 AWS Service Event in the US-East Region\n\nWe'd like to share more about the service event that occurred on Monday, October 22nd in the US- East Region. We have now completed the analysis of the events that affected AWS customers, and we want to describe what happened, our understanding of how customers were affected, and what we are doing to prevent a similar issue from occurring in the future. \n**The Primary Event and the Impact to Amazon Elastic Block Store (EBS) and Amazon Elastic Compute Cloud (EC2)** \nAt 10:00AM PDT Monday, a small number of Amazon Elastic Block Store (EBS) volumes in one of our five Availability Zones in the US-East Region began seeing degraded performance, and in some cases, became \"stuck\" (i.e. unable to process further I/O requests). The root cause of the problem was a latent bug in an operational data collection agent that runs on the EBS storage servers. Each EBS storage server has an\nagent that contacts a set of data collection servers and reports information that is used for fleet maintenance. The data collected with this system is important, but the collection is not time- sensitive and the system is designed to be tolerant of late or missing data. Last week, one of the data collection servers in the affected Availability Zone had a hardware failure and was replaced. As part of replacing that server, a DNS record was updated to remove the failed server and add the replacement server. While not noticed at the time, the DNS update did not successfully propagate to all of the internal DNS servers, and as a result, a fraction of the storage servers did not get the updated server address and continued to attempt to contact the failed data collection\nserver. Because of the design of the data collection service (which is tolerant to missing data), this did not cause any immediate issues or set off any alarms. However, this inability to contact a data collection\nserver triggered a latent memory leak bug in the reporting agent on the storage servers. Rather than gracefully deal with the failed connection, the reporting agent continued trying to contact the collection server in\na way that slowly consumed system memory. While we monitor aggregate memory consumption on each EBS Server, our monitoring failed to alarm on\nthis memory leak. EBS Servers generally make very dynamic use of all of their available memory for managing customer data, making it difficult to set accurate alarms on memory usage and free memory. By Monday morning, the rate of memory loss became quite high and consumed enough memory on the affected storage servers that they were unable to keep up with normal request handling processes. \nThe memory pressure on many of the EBS servers had reached a point where EBS servers began losing the ability to process customer requests and the number of stuck volumes increased quickly. This caused the system to begin to failover from the degraded servers to healthy servers. However, because many of the servers became memory-exhausted at the same time, the system was unable to find enough healthy servers to failover to, and more volumes became stuck. By approximately 11:00AM PDT, a large number of volumes in this Availability Zone were stuck. To remedy this, at 11:10AM PDT, the team made adjustments to reduce the failover rate. These adjustments removed load from the service, and by 11:35AM PDT, the system began automatically recovering many volumes. By 1:40PM PDT, about 60% of the affected volumes had recovered. The team continued to work to understand the issue and restore performance for the remaining volumes. The large surge in failover and recovery activity in the cluster made it difficult for the team to identify the root cause of the event. At 3:10PM PDT, the team identified the underlying issue and was able to begin restoring performance for the remaining volumes by freeing the excess memory consumed by the misbehaving collection agent. At this point, the system was able to recover most of the remaining stuck volumes; and by 4:15PM PDT, nearly all affected volumes were restored and performing normally. \nWe have deployed monitoring that will alarm if we see this specific memory leak again in any of our production EBS servers, and next week, we will begin deploying a fix for the memory leak issue. We are also modifying our system memory monitoring on the EBS storage servers to monitor and alarm on each process's memory consumption, and we will be deploying resource limits to prevent low priority processes from consuming excess resources on these hosts. We are also updating our internal DNS configuration to further ensure that DNS changes are propagated reliably, and as importantly, make sure that our monitoring and alarming surface issues more quickly should these changes not succeed. These actions will address the problems that triggered the event. In addition, we are evaluating how to change the EBS failover logic that led to the rapid deterioration early in this event. We believe we can make adjustments to reduce the impact of any similar correlated failure or degradation of EBS servers within an Availability Zone. \n**Impact on the EC2 and EBS APIs** \nThe primary event only affected EBS volumes in a single Availability Zone, so those customers running with adequate capacity in other Availability Zones in the US East Region were able to tolerate the event with limited impact to their applications. However, many customers reported difficulty using the service APIs to manage their resources during this event. We have invested heavily in making our service APIs resilient to failure during events affecting a single Availability Zone. And, other than a few short periods, our monitoring showed what looked to be a healthy level of launch and create activity throughout the event. However, we've heard from customers that they struggled to use the APIs for several hours. We now understand that our API throttling during the event disproportionately impacted some customers and affected their ability to use the APIs. \nWe use throttling to protect our services from being overwhelmed by internal and external callers that intentionally or unintentionally put excess load on our services. A simple example of the kind of issue throttling protects against is a runaway application that naively retries a request as fast as possible when it fails to get a positive result. Our systems are scaled to handle these sorts of client errors, but during a large operational event, it is not uncommon for many users to inadvertently increase load on the system. So, while we always have a base level of throttling in place, the team enabled a more aggressive throttling policy during this event to try to assure that the system remained stable during the period where customers and the system were trying to recover. Unfortunately, the throttling policy that was put in\nplace was too aggressive. \nAt 12:06PM PDT, the team implemented this aggressive API throttling policy to help assure stability of the system during the recovery. The team monitored the aggregate throttling rate as well as the overall activity (launches, volume creation, etc.) and did not at the time believe that customers were being substantially impacted. We now understand that this throttling policy, for a subset of our customers, was throttling a higher percentage of API calls than we realized during the event. The service APIs were still handling the vast majority of customer requests to launch and terminate instances and make other changes to their EC2 and EBS resources, but many customers experienced high levels of throttling on calls to describe their resources (e.g. DescribeInstances, DescribeImages, etc.). This made it difficult for these customers and their management applications to successfully use the service APIs during this period. It also affected users' ability to successfully manage their EC2 and EBS resources from the AWS Management Console. This throttling policy was in effect until 2:33PM PDT, after which we reduced the level of throttling considerably. \nWe have changed our operational procedures to not use this more aggressive throttling policy during any future event. We believe that our other throttling policies will provide us with the necessary service protection while avoiding the impact that customers saw during this event. We are also modifying our operational dashboard to add per-customer throttling monitoring (rather than just aggregate throttling rates) so that we have better visibility into the number of customers seeing heavy throttling. This will allow us to quickly understand the impact throttling is having on individual customers, regardless of what the overall throttling rate is, and make appropriate adjustments more quickly. \nThrottling is a valuable tool for managing the health of our services, and we employ it regularly without significantly affecting customers' ability to use our services. While customers need to expect that they will encounter API throttling from time to time, we realize that the throttling policy we used for part of this event had a greater impact on\nmany customers than we understood or intended. While this did not meaningfully affect users running high-availability applications architected to run across multiple Availability Zones with adequate running capacity to failover during Availability Zone disruptions, it did lead to several hours of significant API degradation for many of our customers. This inhibited these customers' ability to use the APIs to recover from this event, and in some cases, get normal work done. Therefore, AWS will be issuing a credit to any customer whose API calls were throttled by this aggressive throttling policy (i.e. any customer whose API access was throttled between 12:06PM PDT and 2:33PM PDT) for 100% of their EC2, EBS and ELB usage for three hours of their Monday usage (to cover the period the aggressive throttling policy was in\nplace). Affected customers do not need to take any action; the credits will be automatically applied to their AWS account prior to their October 31 bill being calculated. \n**Impact on Amazon Relational Database Service (RDS)** \nThis event also had an impact on the Amazon Relational Database Service (\"RDS\"). RDS uses EBS for database and log storage, and as a result, a portion of the RDS databases hosted in the affected Availability Zone became inaccessible. Throughout the course of the event, customers were able to create new RDS instances and access existing RDS instances in\nthe unaffected Availability Zones in the region. \nAmazon RDS provides two modes of operation: Single Availability Zone (Single-AZ), where a single database instance operates in one Availability Zone; and Multi Availability Zone (Multi-AZ), where two database instances are synchronously operated in two different Availability Zones. For Multi-AZ RDS, one of the two database instances is the \"primary\" and the other is a \"standby.\" The primary handles all database requests and replicates to the standby. In the case where a primary fails, the standby is promoted to be the new primary and is available to handle database requests after integrity checks are completed. \nSingle-AZ database instances are exposed to disruptions in an\nAvailability Zone. In this case, a Single-AZ database instance would have been affected if one of the EBS volumes it was relying on got stuck. During this event, a significant number of the Single-AZ databases in the affected zone became stuck as the EBS volumes used by them were affected by the primary EBS event described above. In the case of these Single-AZ databases, recovery depended on waiting for the underlying EBS volumes to have their performance restored. By 1:30PM PDT, a significant number of the impaired Single-AZ RDS instances were restored as the volumes they depended on became unstuck. By 3:30PM PDT, the majority of the affected database instances were restored, and by 6:35PM PDT, almost all of the affected Single-AZ RDS instances were restored. \nDuring the course of the event, almost all of the Multi-AZ instances were promoted to their standby in a healthy Availability Zone, and were available to handle database requests after integrity checks were completed. However, a single digit percentage of Multi-AZ RDS instances in the affected Availability Zone did not failover automatically due to two different software bugs. The first group of RDS instances that did not failover as expected encountered an uncommon stuck I/O condition, which the automatic failover logic did not handle correctly. These instances required operator action and were fully restored by 11:30 AM PDT. We have developed a fix for this bug and are in the process of rolling it out. The second group of Multi-AZ instances did not failover automatically because the master database instances were disconnected from their standby for a brief time interval immediately before these master database instances' volumes became stuck. Normally these events are simultaneous. Between the period of time the masters were disconnected from their standbys and the point where volumes became stuck, the masters continued to process transactions without being able to replicate to their standbys. When these masters subsequently became stuck, the system blocked automatic failover to the out-of-date standbys. We have already been working on a fix for this issue which will allow the standby to be favored immediately when its master is in\nan impaired Availability Zone. Due to the subtle nature of the issues involved, we are still in the process of completing this fix and carefully testing it, but are on track to deploy it fully by December. Database instances affected by this condition were restored once the associated EBS volumes had performance restored. While we are disappointed with the impact to these Multi-AZ instances, we are confident that when we complete the deployment of these two bug fixes, the root cause of the Multi-AZ failures we observed during this event will be addressed. It is the top priority of the team to complete these fixes and get them deployed to the fleet. \nCustomers affected by the Multi-AZ RDS issues did not get the availability they or we expected. If an application is Multi-AZ and has enough resources running to continue operating if one Availability Zone is lost, then that application component should remain available (with minimal service disruption). Accordingly, AWS will issue service credits to customers whose RDS Multi-AZ instances took longer than 20 minutes to fail over to their secondary copies, equal to 10 days of charges for those affected Multi-AZ instances. Affected customers do not need to take any action; the credits will be automatically applied to their AWS account prior to their October 31 bill being calculated. \n**Impact on Amazon Elastic Load Balancing (ELB)** \nThis event also affected the Amazon Elastic Load Balancing (ELB) service. Each ELB load balancer uses one or more load balancer instances to route traffic to customers' EC2 instances. These ELB load balancer instances use EBS for storing configuration and monitoring information, and when the EBS volumes on these load balancer instances hung, some of the ELB load balancers became degraded and the ELB service began\nexecuting recovery workflows to either restore or replace the affected load balancer instances. Customers can use ELB with applications that the run in either single or multiple Availability Zones. \nFor customers using an ELB load balancer with an application running in\na single Available Zone, ELB provisions load balancer instances in the Availability Zone in which the application is running (effectively creating a Single-AZ load balancer). During this event, a number of Single-AZ load balancers in the affected Availability Zone became impaired when some or all of the load balancer instances used by the load balancer became inaccessible due to the primary EBS issue. These affected load balancers recovered as soon as the ELB system was able to provision additional EBS volumes in the affected Availability Zone, or in some cases, when the EBS volumes on which particular load balancers relied, were restored. By 1:10PM PDT, the majority of affected Single-AZ load balancers had recovered, and by 3:30PM PDT, most of the remaining load balancers had also been recovered. Recovery of the last remaining load balancers was then slowed by an issue encountered by the ELB recovery workflows. ELB uses Elastic IP addresses (EIPs) to reliably route traffic to load balancer instances. EIPs are consumed as new load balancers are created and as existing load balancers are scaled. The increased demand for EIPs from the ELB recovery workflows (and the overall increase of customer activity during this period) caused ELB to consume all of the EIPs that were available to it. This stalled the recovery workflows and delayed recovery of the final affected load balancers. The team continued to manually recover the remaining impaired load balancers and was able to remediate the EIP shortage at 9:50PM PDT. \nWe are working on a number of improvements to shorten the recovery time of ELB for all customers. We will ensure that we have additional EIP capacity available to the ELB system at all times to allow full recovery of any Availability Zone issue. We are already in the process of making a few changes to reduce the interdependency between ELB and EBS to avoid correlated failure in future events and allow ELB recovery even when\nthere are EBS issues within an Availability Zone. Finally, we are also in the process of a few additional improvements to our recovery workflows that will be released in the coming weeks that will further improve the recovery time of ELB load balancers during any similar event. \nFor customers using an ELB load balancer with an application running in\nmultiple Availability Zones, ELB will provision load balancer instances in every Availability Zone in which the application is running. For these multiple Availability Zone applications, ELB can route traffic away from degraded Availability Zones to allow multiple Availability Zone applications to quickly recover. During this event, customers using ELB with applications running in multiple Availability Zones that included the affected Availability Zone may have experienced elevated error rates during the early parts of the primary event as load balancer instances or the EC2 instances running the customer's application were affected by the EBS issue. By 11:49AM PDT, the ELB service shifted customer traffic away from the impaired Availability Zone for most load balancers with multiple Availability Zone applications. This allowed applications behind these load balancers to serve traffic from their instances in other, unaffected Availability Zones. Unfortunately, a bug in the traffic shifting functionality incorrectly mapped a small number of the affected load balancers and therefore didn't shift traffic correctly. These load balancers continued to send a portion of the customer requests to the affected Availability Zone until the issue was identified and corrected at 12:45PM PDT. We have corrected the logic in\nthe ELB traffic shifting functionality so this error will not occur in\nthe future. We are also working to improve the sensitivity of the traffic shifting procedure so that traffic is more quickly failed away from a degraded Availability Zone in the future. Over time, we will also expose this traffic shifting functionality directly to ELB customers so that they have the ability to control the routing of their requests to the Availability Zones in which they run their applications. Finally, we will work on helping our customers understand and test the impact of this traffic shift so that they can be sure their applications can scale to handle the increased load caused by failing away from an Availability Zone. \n**Final Thoughts** \nWe apologize for the inconvenience and trouble this caused for affected customers. We know how critical our services are to our customers' businesses, and will work hard (and expeditiously) to apply the learning from this event to our services. While we saw that some of the changes that we previously made helped us mitigate some of the impact, we also learned about new failure modes. We will spend many hours over the coming days and weeks improving our understanding of the event and further investing in the resiliency of our services.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（logical）、桥接性（bridging）、可行性（actionable）、简洁性（simplicity）和流程完整性（process integrity）等几个角度来分析2012年10月22日在美国东部地区发生的AWS服务事件，并按照术语（terminology）、背景（background）、常见问题解答（faq）、流程（flow）和附录（appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **AWS**: 亚马逊提供的云计算平台服务。\n- **EBS (Elastic Block Store)**: AWS提供的一种存储服务，用于EC2实例的数据块存储。\n- **EC2 (Elastic Compute Cloud)**: AWS提供的一种计算服务，允许用户在云中运行虚拟服务器。\n- **API (Application Programming Interface)**: 应用程序编程接口，用于不同软件间的通信。\n- **DNS (Domain Name System)**: 域名系统，用于将域名转换为IP地址。\n- **RDS (Relational Database Service)**: AWS提供的托管数据库服务。\n- **ELB (Elastic Load Balancing)**: AWS提供的负载均衡服务，用于自动分配应用程序的流量。\n\n### 背景（Background）\n\n2012年10月22日，AWS美国东部地区的服务发生了一起影响EBS和EC2性能的事件。事件起因是EBS存储服务器上运行的数据收集代理存在一个潜在的bug，导致部分EBS卷性能下降，甚至无法处理进一步的I/O请求。此外，事件还影响了EC2和EBS的API使用，RDS数据库服务以及ELB服务。\n\n### 常见问题解答（FAQ）\n\n**Q: 事件是如何开始的？**\nA: 事件始于一个数据收集服务器的硬件故障和更换，DNS更新未能成功传播到所有内部DNS服务器，导致部分存储服务器试图联系已失败的数据收集服务器，触发了一个潜在的内存泄漏bug。\n\n**Q: AWS采取了哪些措施来恢复服务？**\nA: AWS团队调整了故障转移率，部署了监控以警报内存泄漏，修改了系统内存监控，并部署资源限制以防止低优先级进程消耗过多资源。\n\n**Q: 对于API使用有哪些影响？**\nA: 由于事件期间API的节流策略过于激进，一些客户在几小时内经历了高比例的API调用节流，影响了他们使用API管理资源的能力。\n\n**Q: RDS和ELB服务受到了怎样的影响？**\nA: RDS部分数据库变得不可访问，而ELB服务在恢复受影响的负载均衡器实例时遇到了延迟。\n\n### 流程（Flow）\n\n1. **事件发生**: 由于数据收集服务器的硬件故障和DNS更新问题，EBS卷开始出现性能下降。\n2. **影响扩散**: 内存泄漏问题导致EBS服务器无法处理请求，EC2和EBS API使用困难，RDS和ELB服务受影响。\n3. **AWS响应**: 团队进行故障转移，部署监控和修复措施，调整API节流策略。\n4. **服务恢复**: 受影响的卷和数据库实例逐步恢复，ELB服务也逐步恢复正常。\n5. **后续改进**: AWS采取措施改进服务，包括增加EIP容量，减少EBS和ELB的相互依赖，以及改进恢复工作流程。\n\n### 附录（Appendix）\n\n- **未来计划**: AWS计划继续改进服务的弹性，确保DNS更改的可靠性，并加快对新故障模式的理解。"]]}]
[{"prompt": "User: 为这篇报告中的故障做个简洁的总结作为标题： ## Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region\n\nWe would like to share more details with our customers about the event that occurred with the Amazon Elastic Load Balancing Service (\"ELB\") earlier this week in the US-East Region. While the service disruption\nonly affected applications using the ELB service (and only a fraction of the ELB load balancers were affected), the impacted load balancers saw significant impact for a prolonged period of time. \nThe service disruption began at 12:24 PM PST on December 24th when a portion of the ELB state data was logically deleted. This data is used and maintained by the ELB control plane to manage the configuration of the ELB load balancers in the region (for example tracking all the backend hosts to which traffic should be routed by each load balancer). The data was deleted by a maintenance process that was inadvertently run\nagainst the production ELB state data. This process was run by one of a very small number of developers who have access to this production\nenvironment. Unfortunately, the developer did not realize the mistake at the time. After this data was deleted, the ELB control plane began\nexperiencing high latency and error rates for API calls to manage ELB load balancers. In this initial part of the service disruption, there was no impact to the request handling functionality of running ELB load balancers because the missing ELB state data was not integral to the basic operation of running load balancers. \nOver the next couple hours, our technical teams focused on the API errors. The team was puzzled as many APIs were succeeding (customers were able to create and manage new load balancers but not manage existing load balancers) and others were failing. As this continued, some customers began to experience performance issues with their running load balancers. These issues only occurred after the ELB control plane attempted to make changes to a running load balancer. When a user modifies a load balancer configuration or a load balancer needs to scale up or down, the ELB control plane makes changes to the load balancer configuration. During this event, because the ELB control plane lacked some of the necessary ELB state data to successfully make these changes, load balancers that were modified were improperly configured by the control plane. This resulted in degraded performance and errors for customer applications using these modified load balancers. It was when\nthe ELB technical team started digging deeply into these degraded load balancers that the team identified the missing ELB state data as the root cause of the service disruption. At this point, the focus shifted to preventing additional service impact and recovering the missing ELB state data. \nAt 5:02 PM PST, the team disabled several of the ELB control plane workflows (including the scaling and descaling workflows) to prevent additional running load balancers from being affected by the missing ELB state data. At the peak of the event, 6.8% of running ELB load balancers were impacted. The rest of the load balancers in the system were unable to scale or be modified by customers, but were operating correctly. The team was able to manually recover some of the affected running load balancers on Monday night, and worked through the night to try to restore the missing ELB state data to allow the rest of the affected load balancers to recover (and to open all of the ELB APIs back up). \nThe team attempted to restore the ELB state data to a point-in-time just before 12:24 PM PST on December 24th (just before the event began). By restoring the data to this time, we would be able to merge in events that happened after this point to create an accurate state for each ELB load balancer. Unfortunately, the initial method used by the team to restore the ELB state data consumed several hours and failed to provide a usable snapshot of the data. This delayed recovery until an alternate recovery process was found. At 2:45 AM PST on December 25th, the team successfully restored a snapshot of the ELB state data to a time just before the data was deleted. The team then began merging this restored data with the system state changes that happened between this snapshot and the current time. By 5:40 AM PST, this data merge had been completed and the new ELB state data had been verified. The team then began slowly re-enabling the ELB service workflows and APIs. This process was done carefully to ensure that no impact was made to unaffected running load balancers and to ensure that each affected load balancer was correctly recovered. The system began recovering the remaining affected load balancers, and by 8:15 AM PST, the team had re-enabled the majority of APIs and backend workflows. By 10:30 AM PST, almost all affected load balancers had been restored to full operation. While the service was substantially recovered at this time, the team continued to closely monitor the service before communicating broadly that it was operating normally at 12:05 PM PST. \nWe have made a number of changes to protect the ELB service from this sort of disruption in the future. First, we have modified the access controls on our production ELB state data to prevent inadvertent modification without specific Change Management (CM) approval. Normally, we protect our production service data with non-permissive access control policies that prevent all access to production data. The ELB service had authorized additional access for a small number of developers to allow them to execute operational processes that are currently being automated. This access was incorrectly set to be persistent rather than requiring a per access approval. We have reverted this incorrect configuration and all access to production ELB data will require a per-incident CM approval. This would have prevented the ELB state data from being deleted in this event. This is a protection that we use across all of our services that has prevented this sort of problem in the past, but was not appropriately enabled for this ELB state data. We have also modified our data recovery process to reflect the learning we went through in this event. We are confident that we could recover ELB state data in a similar event significantly faster (if necessary) for any future operational event. We will also incorporate our learning from this event into our service architecture. We believe that we can reprogram our ELB control plane workflows to more thoughtfully reconcile the central service data with the current load balancer state. This would allow the service to recover automatically from logical data loss or corruption without needing manual data restoration. \nLast, but certainly not least, we want to apologize. We know how critical our services are to our customers' businesses, and we know this disruption came at an inopportune time for some of our customers. We will do everything we can to learn from this event and use it to drive further improvement in the ELB service.\n\nAssistant:", "response": [["Elastic Load Balancer ran into problems when \"a maintenance process that was inadvertently run against the production ELB state data\""]]}]
[{"prompt": "User: 下面是一篇故障报告： ## Summary of the December 24, 2012 Amazon ELB Service Event in the US-East Region\n\nWe would like to share more details with our customers about the event that occurred with the Amazon Elastic Load Balancing Service (\"ELB\") earlier this week in the US-East Region. While the service disruption\nonly affected applications using the ELB service (and only a fraction of the ELB load balancers were affected), the impacted load balancers saw significant impact for a prolonged period of time. \nThe service disruption began at 12:24 PM PST on December 24th when a portion of the ELB state data was logically deleted. This data is used and maintained by the ELB control plane to manage the configuration of the ELB load balancers in the region (for example tracking all the backend hosts to which traffic should be routed by each load balancer). The data was deleted by a maintenance process that was inadvertently run\nagainst the production ELB state data. This process was run by one of a very small number of developers who have access to this production\nenvironment. Unfortunately, the developer did not realize the mistake at the time. After this data was deleted, the ELB control plane began\nexperiencing high latency and error rates for API calls to manage ELB load balancers. In this initial part of the service disruption, there was no impact to the request handling functionality of running ELB load balancers because the missing ELB state data was not integral to the basic operation of running load balancers. \nOver the next couple hours, our technical teams focused on the API errors. The team was puzzled as many APIs were succeeding (customers were able to create and manage new load balancers but not manage existing load balancers) and others were failing. As this continued, some customers began to experience performance issues with their running load balancers. These issues only occurred after the ELB control plane attempted to make changes to a running load balancer. When a user modifies a load balancer configuration or a load balancer needs to scale up or down, the ELB control plane makes changes to the load balancer configuration. During this event, because the ELB control plane lacked some of the necessary ELB state data to successfully make these changes, load balancers that were modified were improperly configured by the control plane. This resulted in degraded performance and errors for customer applications using these modified load balancers. It was when\nthe ELB technical team started digging deeply into these degraded load balancers that the team identified the missing ELB state data as the root cause of the service disruption. At this point, the focus shifted to preventing additional service impact and recovering the missing ELB state data. \nAt 5:02 PM PST, the team disabled several of the ELB control plane workflows (including the scaling and descaling workflows) to prevent additional running load balancers from being affected by the missing ELB state data. At the peak of the event, 6.8% of running ELB load balancers were impacted. The rest of the load balancers in the system were unable to scale or be modified by customers, but were operating correctly. The team was able to manually recover some of the affected running load balancers on Monday night, and worked through the night to try to restore the missing ELB state data to allow the rest of the affected load balancers to recover (and to open all of the ELB APIs back up). \nThe team attempted to restore the ELB state data to a point-in-time just before 12:24 PM PST on December 24th (just before the event began). By restoring the data to this time, we would be able to merge in events that happened after this point to create an accurate state for each ELB load balancer. Unfortunately, the initial method used by the team to restore the ELB state data consumed several hours and failed to provide a usable snapshot of the data. This delayed recovery until an alternate recovery process was found. At 2:45 AM PST on December 25th, the team successfully restored a snapshot of the ELB state data to a time just before the data was deleted. The team then began merging this restored data with the system state changes that happened between this snapshot and the current time. By 5:40 AM PST, this data merge had been completed and the new ELB state data had been verified. The team then began slowly re-enabling the ELB service workflows and APIs. This process was done carefully to ensure that no impact was made to unaffected running load balancers and to ensure that each affected load balancer was correctly recovered. The system began recovering the remaining affected load balancers, and by 8:15 AM PST, the team had re-enabled the majority of APIs and backend workflows. By 10:30 AM PST, almost all affected load balancers had been restored to full operation. While the service was substantially recovered at this time, the team continued to closely monitor the service before communicating broadly that it was operating normally at 12:05 PM PST. \nWe have made a number of changes to protect the ELB service from this sort of disruption in the future. First, we have modified the access controls on our production ELB state data to prevent inadvertent modification without specific Change Management (CM) approval. Normally, we protect our production service data with non-permissive access control policies that prevent all access to production data. The ELB service had authorized additional access for a small number of developers to allow them to execute operational processes that are currently being automated. This access was incorrectly set to be persistent rather than requiring a per access approval. We have reverted this incorrect configuration and all access to production ELB data will require a per-incident CM approval. This would have prevented the ELB state data from being deleted in this event. This is a protection that we use across all of our services that has prevented this sort of problem in the past, but was not appropriately enabled for this ELB state data. We have also modified our data recovery process to reflect the learning we went through in this event. We are confident that we could recover ELB state data in a similar event significantly faster (if necessary) for any future operational event. We will also incorporate our learning from this event into our service architecture. We believe that we can reprogram our ELB control plane workflows to more thoughtfully reconcile the central service data with the current load balancer state. This would allow the service to recover automatically from logical data loss or corruption without needing manual data restoration. \nLast, but certainly not least, we want to apologize. We know how critical our services are to our customers' businesses, and we know this disruption came at an inopportune time for some of our customers. We will do everything we can to learn from this event and use it to drive further improvement in the ELB service.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑（logical）、桥接（bridging）、可操作性（actionable）、简洁性（simplicity）和流程完整性（process integrity）等几个角度来分析2012年12月24日在美国东部地区发生的亚马逊弹性负载均衡服务（ELB）事件，并按照术语（terminology）、背景（background）、常见问题解答（faq）、流程（flow）和附录（appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **逻辑（Logical）**：在此事件中，逻辑错误指的是维护过程中意外删除了ELB状态数据，导致ELB控制平面无法正常管理负载均衡器的配置。\n- **桥接（Bridging）**：桥接在此上下文中可以理解为技术团队在发现问题后，采取措施连接和恢复丢失的数据与系统状态之间的联系。\n- **可操作性（Actionable）**：事件中提到的可操作性措施包括禁用某些ELB控制平面工作流程，手动恢复受影响的负载均衡器，以及修改访问控制策略以防止未来的数据丢失。\n- **简洁性（Simplicity）**：简洁性体现在对事件处理流程的简化，例如通过禁用特定工作流程来防止进一步的服务影响。\n- **流程完整性（Process Integrity）**：流程完整性是指在整个事件响应和恢复过程中，确保所有操作和步骤都是准确无误的，以及在事后对流程进行改进以提高未来的服务质量。\n\n### 背景（Background）\n\n- 事件始于2012年12月24日12:24 PM PST，当时ELB服务的部分地区数据被逻辑删除。\n- 受影响的负载均衡器在长时间内遭受了显著影响。\n- 技术团队在几小时内专注于解决API错误，同时一些客户开始遇到运行中的负载均衡器性能问题。\n\n### 常见问题解答（FAQ）\n\n- **Q: 事件是如何开始的？**\n  - A: 事件开始于一个维护过程意外地对生产ELB状态数据执行了删除操作。\n\n- **Q: 受影响的负载均衡器出现了哪些问题？**\n  - A: 受影响的负载均衡器出现了高延迟和API调用错误率，以及由于配置不当导致的性能下降和错误。\n\n- **Q: 亚马逊是如何应对这一事件的？**\n  - A: 亚马逊通过禁用某些工作流程、手动恢复受影响的负载均衡器、尝试不同的数据恢复方法，并最终成功恢复了ELB状态数据。\n\n### 流程（Flow）\n\n1. **事件发现**：ELB控制平面开始经历API调用的高延迟和错误率。\n2. **问题定位**：技术团队发现丢失的ELB状态数据是服务中断的根本原因。\n3. **初步响应**：禁用ELB控制平面的某些工作流程以防止进一步影响。\n4. **数据恢复尝试**：尝试恢复ELB状态数据，但初次尝试失败。\n5. **成功恢复**：在12月25日凌晨成功恢复了ELB状态数据的快照。\n6. **服务恢复**：逐步重新启用ELB服务工作流程和API，最终在上午10:30 PST恢复了几乎所有受影响的负载均衡器。\n\n### 附录（Appendix）\n\n- 亚马逊对ELB服务进行了多项改进，包括修改生产数据的访问控制策略，改进数据恢复流程，并计划重新编程ELB控制平面工作流程以自动从逻辑数据丢失或损坏中恢复。\n- 亚马逊对客户的业务至关重要，他们承诺将从这次事件中学习并推动ELB服务的进一步改进。\n- 互联网探索者（Internet Explorer）的支持将于2022年7月31日结束，推荐使用Chrome、Firefox、Edge和Safari等浏览器。"]]}]
[{"prompt": "User: 下面是一篇故障报告： ### Summary of the Amazon EC2 DNS Resolution Issues in the Asia Pacific (Seoul) Region (AP-NORTHEAST-2)  \nWe'd like to give you some additional information about the service disruption that occurred in the Seoul (AP-NORTHEAST-2) Region on\nNovember 22, 2018. Between 8:19 AM and 9:43 AM KST, EC2 instances experienced DNS resolution issues in the AP-NORTHEAST-2 region. This was caused by a reduction in the number of healthy hosts that were part of the EC2 DNS resolver fleet, which provides a recursive DNS service to EC2 instances. Service was restored when the number of healthy hosts was restored to previous levels. EC2 network connectivity and DNS resolution\noutside of EC2 instances were not affected by this event. \nThe root cause of DNS resolution issues was a configuration update which incorrectly removed the setting that specifies the minimum healthy hosts for the EC2 DNS resolver fleet in the AP-NORTHEAST-2 Region. This resulted in the minimum healthy hosts configuration setting being interpreted as a very low default value that resulted in fewer in-service healthy hosts. With the reduced healthy host capacity for the EC2 DNS resolver fleet, DNS queries from within EC2 instances began to fail. At 8:21 AM KST, the engineering team was alerted to the DNS resolution issue within the AP-NORTHEAST-2 Region and immediately began\nworking on resolution. We identified root cause at 8:48 AM KST and we first ensured that there was no further impact by preventing additional healthy hosts from being removed from service; this took an additional 15 minutes. We then started restoring capacity to previous levels which took the bulk of the recovery time. At 9:43 AM KST, DNS queries from within EC2 instances saw full recovery. \nWe are taking multiple steps to prevent recurrence of this issue, some of which are already complete. We have immediately validated and ensured that every AWS region has the correct capacity settings for the EC2 DNS resolver service. We are implementing semantic configuration validation\nfor all EC2 DNS resolver configuration updates, to ensure every region\nalways has sufficient minimum healthy hosts. We are also adding throttling to ensure that only a limited amount of healthy host capacity can be removed from service each hour. This will prevent the downscaling of the EC2 DNS resolver fleet in the event of an invalid configuration\nparameter. \nFinally, we want to apologize for the impact this event caused for our customers. While we've had a strong track record of availability with EC2 DNS, we know how critical this service is to our customers, their applications and end users, and their businesses. We will do everything we can to learn from this event and use it to improve our availability even further.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。 \nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（logical）、桥接性（bridging）、可操作性（actionable）、简洁性（simplicity）和流程完整性（process integrity）几个角度来分析，并按照术语（terminology）、背景（background）、常见问题解答（faq）、流程（flow）和附录（appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **EC2 DNS 解析器舰队（EC2 DNS resolver fleet）**：为 EC2 实例提供递归 DNS 服务的一组服务器。\n- **健康主机（Healthy hosts）**：在 DNS 解析器舰队中正常运行的服务器。\n- **配置更新（Configuration update）**：对系统设置的更改，可能导致服务行为的变化。\n- **最小健康主机（Minimum healthy hosts）**：确保服务正常运行所需的最少健康主机数量。\n\n### 背景（Background）\n\n在2018年11月22日，AWS 的首尔地区（AP-NORTHEAST-2）发生了服务中断事件。在当地时间上午8:19至9:43之间，EC2 实例出现了 DNS 解析问题。问题的根本原因是配置更新错误地移除了指定 EC2 DNS 解析器舰队最小健康主机的设置，导致服务中的健康主机数量减少。\n\n### 常见问题解答（FAQ）\n\n**Q: DNS 解析问题是如何发生的？**\nA: 由于配置更新错误地移除了最小健康主机的设置，导致健康主机数量减少，从而引发了 DNS 解析问题。\n\n**Q: 这次服务中断对客户有什么影响？**\nA: 客户在 EC2 实例内发起的 DNS 查询失败，但 EC2 实例的网络连接和 EC2 外部的 DNS 解析过程未受影响。\n\n**Q: AWS 如何解决了这个问题？**\nA: AWS 工程团队在得知问题后立即开始解决。首先防止了更多健康主机从服务中移除，然后恢复了服务容量至之前的水平。\n\n### 流程（Flow）\n\n1. **问题发生**：在首尔地区（AP-NORTHEAST-2）的 EC2 实例出现 DNS 解析问题。\n2. **问题识别**：AWS 工程团队在8:21 AM KST被通知，并在8:48 AM KST确定了问题根源。\n3. **问题解决**：通过防止更多健康主机移除并恢复服务容量，DNS 查询在9:43 AM KST完全恢复。\n4. **预防措施**：AWS 采取了多项措施以防止此类问题再次发生，包括对所有 AWS 地区的 EC2 DNS 解析服务进行容量设置验证，实施配置验证，并添加了限制措施。\n\n### 附录（Appendix）\n\n- **AWS 支持 Internet Explorer 的结束**：AWS 将于2022年7月31日终止对 Internet Explorer 的支持。推荐使用 Chrome、Firefox、Edge 和 Safari 等浏览器。\n- **了解更多信息**：用户可以通过提供的链接了解更多关于 AWS 支持浏览器的信息。\n\n通过上述分析和总结，我们可以清晰地理解2018年11月22日在 AWS 首尔地区发生的 DNS 解析问题，以及 AWS 如何响应和解决该问题，并采取措施防止未来类似问题的发生。"]]}]
[{"prompt": "User: 下面是一篇故障报告： # Postmortem of database outage of January 31 \nPostmortem on the database outage of January 31 2017 with the lessons we learned. \nOn January 31st 2017, we experienced a major service outage for one of our products, the online service GitLab.com. The outage was caused by an\naccidental removal of data from our primary database server. \nThis incident caused the GitLab.com service to be unavailable for many hours. We also lost some production data that we were eventually unable to recover. Specifically, we lost modifications to database data such as projects, comments, user accounts, issues and snippets, that took place between 17:20 and 00:00 UTC on January 31. Our best estimate is that it affected roughly 5,000 projects, 5,000 comments and 700 new user accounts. Code repositories or wikis hosted on GitLab.com were unavailable during the outage, but were not affected by the data loss. [GitLab Enterprise](/enterprise/) customers, GitHost customers, and self-managed GitLab CE users were not affected by the outage, or the data loss. \nLosing production data is unacceptable. To ensure this does not happen\nagain we're working on multiple improvements to our operations & recovery procedures for GitLab.com. In this article we'll look at what went wrong, what we did to recover, and what we'll do to prevent this from happening in the future. \nTo the GitLab.com users whose data we lost and to the people affected by the outage: we're sorry. I apologize personally, as GitLab's CEO, and on behalf of everyone at GitLab. \n\nGitLab.com currently uses a single primary and a single secondary in\nhot-standby mode. The standby is only used for failover purposes. In\nthis setup a single database has to handle all the load, which is not ideal. The primary's hostname is `db1.cluster.gitlab.com`, while the secondary's hostname is `db2.cluster.gitlab.com`. \nIn the past we've had various other issues with this particular setup due to `db1.cluster.gitlab.com` being a single point of failure. For example: \n-   [A database outage on November 28th, 2016 due to     project_authorizations having too much     bloat](https://gitlab.com/gitlab-com/infrastructure/issues/791) -   [CI distributed heavy polling and exclusive row locking for seconds     takes GitLab.com     down](https://gitlab.com/gitlab-com/infrastructure/issues/514) -   [Scary DB     spikes](https://gitlab.com/gitlab-com/infrastructure/issues/364) \n\nOn January 31st an engineer started setting up multiple PostgreSQL servers in our staging environment. The plan was to try out [pgpool-II](http://www.pgpool.net/mediawiki/index.php/Main_Page) to see if it would reduce the load on our database by load balancing queries between the available hosts. Here is the issue for that plan: [infrastructure#259](https://gitlab.com/gitlab-com/infrastructure/issues/259). \n**± 17:20 UTC:** prior to starting this work, our engineer took an LVM snapshot of the production database and loaded this into the staging environment. This was necessary to ensure the staging database was up to date, allowing for more accurate load testing. This procedure normally happens automatically once every 24 hours (at 01:00 UTC), but they wanted a more up to date copy of the database. \n**± 19:00 UTC:** GitLab.com starts experiencing an increase in database load due to what we suspect was spam. In the week leading up to this event GitLab.com had been experiencing similar problems, but not this severe. One of the problems this load caused was that many users were not able to post comments on issues and merge requests. Getting the load under control took several hours. \nWe would later find out that part of the load was caused by a background job trying to remove a GitLab employee and their associated data. This was the result of their account being flagged for abuse and accidentally scheduled for removal. More information regarding this particular problem can be found in the issue [\"Removal of users by spam should not hard delete\"](https://gitlab.com/gitlab-org/gitlab-ce/issues/27581). \n**± 23:00 UTC:** Due to the increased load, our PostgreSQL secondary's replication process started to lag behind. The replication failed as WAL segments needed by the secondary were already removed from the primary. As GitLab.com was not using WAL archiving, the secondary had to be re-synchronised manually. This involves removing the existing data directory on the secondary, and running [pg_basebackup](https://www.postgresql.org/docs/9.6/static/app-pgbasebackup.html) to copy over the database from the primary to the secondary. \nOne of the engineers went to the secondary and wiped the data directory, then ran `pg_basebackup`. Unfortunately `pg_basebackup` would hang, producing no meaningful output, despite the `--verbose` option being set. After a few tries `pg_basebackup` mentioned that it could not connect due to the master not having enough available replication\nconnections (as controlled by the `max_wal_senders` option). \nTo resolve this our engineers decided to temporarily increase `max_wal_senders` from the default value of `3` to `32`. When applying the settings, PostgreSQL refused to restart, claiming too many semaphores were being created. This can happen when, for example, `max_connections` is set too high. In our case this was set to `8000`. Such a value is way too high, yet it had been applied almost a year ago and was working fine until that point. To resolve this the setting's value was reduced to `2000`, resulting in PostgreSQL restarting without issues. \nUnfortunately this did not resolve the problem of `pg_basebackup` not starting replication immediately. One of the engineers decided to run it with `strace` to see what it was blocking on. `strace` showed that `pg_basebackup` was hanging in a `poll` call, but that did not provide any other meaningful information that might have explained why. \n**± 23:30 UTC:** one of the engineers thinks that perhaps `pg_basebackup` created some files in the PostgreSQL data directory of the secondary during the previous attempts to run it. While normally `pg_basebackup` prints an error when this is the case, the engineer in\nquestion wasn't too sure what was going on. It would later be revealed by another engineer (who wasn't around at the time) that this is normal behaviour: `pg_basebackup` will wait for the primary to start sending over replication data and it will sit and wait silently until that time. Unfortunately this was not clearly documented in our [engineering runbooks](https://gitlab.com/gitlab-com/runbooks) nor in the official `pg_basebackup` document. \nTrying to restore the replication process, an engineer proceeds to wipe the PostgreSQL database directory, errantly thinking they were doing so on the secondary. Unfortunately this process was executed on the primary instead. The engineer terminated the process a second or two after noticing their mistake, but at this point around 300 GB of data had already been removed. \nHoping they could restore the database the engineers involved went to look for the database backups, and asked for help on Slack. Unfortunately the process of both finding and using backups failed completely. \n\nThis brings us to the recovery procedures. Normally in an event like this, one should be able to restore a database in relatively little time using a recent backup, though some form of data loss can not always be prevented. For GitLab.com we have the following procedures in place: \n1.  Every 24 hours a backup is generated using `pg_dump`, this backup is     uploaded to Amazon S3. Old backups are automatically removed after     some time. 2.  Every 24 hours we generate an LVM snapshot of the disk storing the     production database data. This snapshot is then loaded into the     staging environment, allowing us to more safely test changes without     impacting our production environment. Direct access to the staging     database is restricted, similar to our production database. 3.  For various servers (e.g. the NFS servers storing Git data) we use     Azure disk snapshots. These snapshots are taken once per 24 hours. 4.  Replication between PostgreSQL hosts, primarily used for failover     purposes and not for disaster recovery. \nAt this point the replication process was broken and data had already been wiped from both the primary and secondary, meaning we could not restore from either host. \n\nWhen we went to look for the `pg_dump` backups we found out they were not there. The S3 bucket was empty, and there was no recent backup to be found anywhere. Upon closer inspection we found out that the backup procedure was using `pg_dump` 9.2, while our database is running PostgreSQL 9.6 (for Postgres, 9.x releases are considered major). A difference in major versions results in `pg_dump` producing an error, terminating the backup procedure. \nThe difference is the result of how our Omnibus package works. We currently support both PostgreSQL 9.2 and 9.6, allowing users to upgrade (either manually or using commands provided by the package). To determine the correct version to use the Omnibus package looks at the PostgreSQL version of the database cluster (as determined by `$PGDIR/PG_VERSION`, with `$PGDIR` being the path to the data directory). When PostgreSQL 9.6 is detected Omnibus ensures all binaries use PostgreSQL 9.6, otherwise it defaults to PostgreSQL 9.2. \nThe `pg_dump` procedure was executed on a regular application server, not the database server. As a result there is no PostgreSQL data directory present on these servers, thus Omnibus defaults to PostgreSQL 9.2. This in turn resulted in `pg_dump` terminating with an error. \nWhile notifications are enabled for any cronjobs that error, these notifications are sent by email. For GitLab.com we use [DMARC](https://dmarc.org/). Unfortunately DMARC was not enabled for the cronjob emails, resulting in them being rejected by the receiver. This means we were never aware of the backups failing, until it was too late. \n\nAzure disk snapshots are used to generate a snapshot of an entire disk. These snapshots don't make it easy to restore individual chunks of data (e.g. a lost user account), though it's possible. The primary purpose is to restore entire disks in case of disk failure. \nIn Azure a snapshot belongs to a storage account, and a storage account in turn is linked to one or more hosts. Each storage account has a limit of roughly 30 TB. When restoring a snapshot using a host in the same storage account, the procedure usually completes very quickly. However, when using a host in a different storage account the procedure can take hours if not days to complete. For example, in one such case it took over a week to restore a snapshot. As a result we try not to rely on\nthis system too much. \nWhile enabled for the NFS servers, these snapshots were not enabled for any of the database servers as we assumed that our other backup procedures were sufficient enough. \n\nThe LVM snapshots are primarily used to easily copy data from our production environment to our staging environment. While this process was working as intended, the produced snapshots are not really meant to be used for disaster recovery. At the time of the outage we had two snapshots available: \n1.  A snapshot created for our staging environment every 24 hours,     almost 24 hours before the outage happened. 2.  A snapshot created manually by one of the engineers roughly 6 hours     before the outage. \nWhen we generate a snapshot the following steps are taken: \n1.  Generate a snapshot of production. 2.  Copy the snapshot to staging. 3.  Create a new disk using this snapshot. 4.  Remove all webhooks from the resulting database, to prevent them     from being triggered by accident. \n\nTo recover GitLab.com we decided to use the LVM snapshot created 6 hours before the outage, as it was our only option to reduce data loss as much as possible (the alternative was to lose almost 24 hours of data). This process would involve the following steps: \n1.  Copy the existing staging database to production, which would not     contain any webhooks. 2.  In parallel, copy the snapshot used to set up the database as this     snapshot might still contain the webhooks (we weren't entirely     sure). 3.  Set up a production database using the snapshot from step 1. 4.  Set up a separate database using the snapshot from step 2. 5.  Restore webhooks using the database set up in the previous step. 6.  Increment all database sequences by 100,000 so one can't re-use IDs     that might have been used before the outage. 7.  Gradually re-enable GitLab.com. \nFor our staging environment we were using Azure classic, without Premium Storage. This is primarily done to save costs as premium storage is quite expensive. As a result the disks are very slow, resulting in them being the main bottleneck in the restoration process. Because LVM snapshots are stored on the hosts they are taken for we had two options to restore data: \n1.  Copy over the LVM snapshot 2.  Copy over the PostgreSQL data directory \nIn both cases the amount of data to copy would be roughly the same. Since copying over and restoring the data directory would be easier we decided to go with this solution. \nCopying the data from the staging to the production host took around 18 hours. These disks are network disks and are throttled to a really low number (around 60Mbps), there is no way to move from cheap storage to premium, so this was the performance we would get out of it. There was no network or processor bottleneck, the bottleneck was in the drives. Once copied we were able to restore the database (including webhooks) to the state it was at January 31st, 17:20 UTC. \nOn February 1st at 17:00 UTC we managed to restore the GitLab.com database without webhooks. Restoring webhooks was done by creating a separate staging database using the LVM snapshot, but without triggering the removal of webhooks. This allowed us to generate a SQL dump of the table and import this into the restored GitLab.com database. \nAround 18:00 UTC we finished the final restoration procedures such as restoring the webhooks and confirming everything was operating as expected. \n\nIn the spirit of transparency we kept track of progress and notes in a [publicly visible Google document](https://docs.google.com/document/d/1GCK53YDcBWQveod9kfzW-VCxIABGiryG7_z_6jHdVik/pub). We also streamed the recovery procedure on YouTube, with a peak viewer count of around 5000 (resulting in the stream being the #2 live stream on YouTube for several hours). The stream was used to give our users live updates about the recovery procedure. Finally we used Twitter (<https://twitter.com/gitlabstatus>) to inform those that might not be watching the stream. \nThe document in question was initially private to GitLab employees and contained name of the engineer who accidentally removed the data. While the name was added by the engineer themselves (and they had no problem with this being public), we will redact names in future cases as other engineers may not be comfortable with their name being published. \n\nDatabase data such as projects, issues, snippets, etc. created between\nJanuary 31st 17:20 UTC and 23:30 UTC has been lost. Git repositories and Wikis were not removed as they are stored separately. \nIt's hard to estimate how much data has been lost exactly, but we estimate we have lost at least 5000 projects, 5000 comments, and roughly 700 users. This only affected users of GitLab.com, self-managed instances or GitHost instances were not affected. \n\nSince GitLab uses GitLab.com to develop GitLab the outage meant that for some it was harder to get work done. Most developers could continue working using their local Git repositories, but creating issues and such had to be delayed. To publish the blog post [\"GitLab.com Database Incident\"](/blog/2017/02/01/gitlab-dot-com-database-incident/) we used a private GitLab instance we normally use for private/sensitive workflows (e.g. security releases). This allowed us to build and deploy a new version of the website while GitLab.com was unavailable. \nWe also have a public monitoring website located at <https://dashboards.gitlab.com/>. Unfortunately the current setup for this website was not able to handle the load produced by users using this service during the outage. Fortunately our internal monitoring systems (which dashboards.gitlab.com is based on) were not affected. \n\nTo analyse the root cause of these problems we'll use a technique called [\"The 5 Whys\"](https://en.wikipedia.org/wiki/5_Whys). We'll break up the incident into 2 main problems: GitLab.com being down, and it taking a long time to restore GitLab.com. \n**Problem 1:** GitLab.com was down for about 18 hours. \n1.  **Why was GitLab.com down?** - The database directory of the primary     database was removed by accident, instead of removing the database     directory of the secondary. 2.  **Why was the database directory removed?** - Database replication\n    stopped, requiring the secondary to be reset/rebuilt. This in turn\n    requires that the PostgreSQL data directory is empty. Restoring this     required manual work as this was not automated, nor was it     documented properly. 3.  **Why did replication stop?** - A spike in database load caused the     database replication process to stop. This was due to the primary     removing WAL segments before the secondary could replicate them. 4.  **Why did the database load increase?** - This was caused by two     events happening at the same time: an increase in spam, and a     process trying to remove a GitLab employee and their associated     data. 5.  **Why was a GitLab employee scheduled for removal?** - The employee     was reported for abuse by a troll. The current system used for     responding to abuse reports makes it too easy to overlook the     details of those reported. As a result the employee was accidentally     scheduled for removal. \n**Problem 2:** restoring GitLab.com took over 18 hours. \n1.  **Why did restoring GitLab.com take so long?** - GitLab.com had to     be restored using a copy of the staging database. This was hosted on\n    slower Azure VMs in a different region. 2.  **Why was the staging database needed for restoring GitLab.com?** -     Azure disk snapshots were not enabled for the database servers, and     the periodic database backups using `pg_dump` were not working. 3.  **Why could we not fail over to the secondary database host?** - The     secondary database's data was wiped as part of restoring database     replication. As such it could not be used for disaster recovery. 4.  **Why could we not use the standard backup procedure?** - The     standard backup procedure uses `pg_dump` to perform a logical backup     of the database. This procedure failed silently because it was using     PostgreSQL 9.2, while GitLab.com runs on PostgreSQL 9.6. 5.  **Why did the backup procedure fail silently?** - Notifications were     sent upon failure, but because of the Emails being rejected there     was no indication of failure. The sender was an automated process     with no other means to report any errors. 6.  **Why were the Emails rejected?** - Emails were rejected by the     receiving mail server due to the Emails not being signed using     DMARC. 7.  **Why were Azure disk snapshots not enabled?** - We assumed our     other backup procedures were sufficient. Furthermore, restoring     these snapshots can take days. 8.  **Why was the backup procedure not tested on a regular basis?** -     Because there was no ownership, as a result nobody was responsible     for testing this procedure. \n\nWe are currently working on fixing and improving our various recovery procedures. Work is split across the following issues: \n1.  [Overview of status of all issues listed in this blog post     (#1684)](https://gitlab.com/gitlab-com/infrastructure/issues/1684) 2.  [Update PS1 across all hosts to more clearly differentiate between\n    hosts and environments     (#1094)](https://gitlab.com/gitlab-com/infrastructure/issues/1094) 3.  [Prometheus monitoring for backups     (#1095)](https://gitlab.com/gitlab-com/infrastructure/issues/1095) 4.  [Set PostgreSQL's max_connections to a sane value     (#1096)](https://gitlab.com/gitlab-com/infrastructure/issues/1096) 5.  [Investigate Point in time recovery & continuous archiving for     PostgreSQL     (#1097)](https://gitlab.com/gitlab-com/infrastructure/issues/1097) 6.  [Hourly LVM snapshots of the production databases     (#1098)](https://gitlab.com/gitlab-com/infrastructure/issues/1098) 7.  [Azure disk snapshots of production databases     (#1099)](https://gitlab.com/gitlab-com/infrastructure/issues/1099) 8.  [Move staging to the ARM environment     (#1100)](https://gitlab.com/gitlab-com/infrastructure/issues/1100) 9.  [Recover production replica(s)     (#1101)](https://gitlab.com/gitlab-com/infrastructure/issues/1101) 10. [Automated testing of recovering PostgreSQL database backups     (#1102)](https://gitlab.com/gitlab-com/infrastructure/issues/1102) 11. [Improve PostgreSQL replication documentation/runbooks     (#1103)](https://gitlab.com/gitlab-com/infrastructure/issues/1103) 12. [Investigate pgbarman for creating PostgreSQL backups     (#1105)](https://gitlab.com/gitlab-com/infrastructure/issues/1105) 13. [Investigate using WAL-E as a means of Database Backup and Realtime     Replication\n    (#494)](https://gitlab.com/gitlab-com/infrastructure/issues/494) 14. [Build Streaming Database     Restore](https://gitlab.com/gitlab-com/infrastructure/issues/1152) 15. [Assign an owner for data     durability](https://gitlab.com/gitlab-com/infrastructure/issues/1163) \nWe are also working on setting up multiple secondaries and balancing the load amongst these hosts. More information on this can be found at: \n-   [Bundle pgpool-II 3.6.1     (!1251)](https://gitlab.com/gitlab-org/omnibus-gitlab/merge_requests/1251) -   [Connection pooling/load balancing for PostgreSQL     (#259)](https://gitlab.com/gitlab-com/infrastructure/issues/259) \nOur main focus is to improve disaster recovery, and making it more obvious as to what host you're using; instead of preventing production\nengineers from running certain commands. For example, one could alias `rm` to something safer but in doing so would only protect themselves against accidentally running `rm -rf /important-data`, not against disk corruption or any of the many other ways you can lose data. \nAn ideal environment is one in which you *can* make mistakes but easily and quickly recover from them with minimal to no impact. This in turn\nrequires you to be able to perform these procedures on a regular basis, and make it easy to test and roll back any changes. For example, we are in the process of setting up procedures that allow developers to test their database migrations. More information on this can be found in the issue [\"Tool for executing and reverting Rails migrations on\nstaging\"](https://gitlab.com/gitlab-com/infrastructure/issues/811). \nWe're also looking into ways to build better recovery procedures for the entire GitLab.com infrastructure, and not just the database; and to ensure there is ownership of these procedures. The issue for this is [\"Disaster recovery for everything that is not the database\"](https://gitlab.com/gitlab-com/infrastructure/issues/1161). \nMonitoring wise we also started working on a public backup monitoring dashboard, which can be found at <https://dashboards.gitlab.com/dashboard/db/postgresql-backups>. Currently this dashboard only contains data of our `pg_dump` backup procedure, but we aim to add more data over time. \nOne might notice that at the moment our `pg_dump` backups are 3 days old. We perform these backups on a secondary as `pg_dump` can put quite a bit of pressure on a database. Since we are in the process of rebuilding our secondaries the `pg_dump` backup procedure is suspended for the time being. Fear not however, as LVM snapshots are now taken\nevery hour instead of once per 24 hours. Enabling Azure disk snapshots is something we're still looking into. \nFinally, we're looking into improving our abuse reporting and response system. More information regarding this can be found in the issue [\"Removal of users by spam should not hard delete\"](https://gitlab.com/gitlab-org/gitlab-ce/issues/27581). \nIf you think there are additional measures we can take to prevent incidents like this please let us know in the comments. \n\n\nPushing to the default branch will automatically update the merge request so that it's aware of there not being any differences between\nthe source and target branch. At this point you can safely close the merge request. \n\nThere are 3 options to resolve this: \n1.  Close the MR and create a new one 2.  Push new changes to the merge request's source branch 3.  Rebase/amend, and force push to the merge request's source branch \n\nGo to your project, then \"Pipelines\", \"New Pipeline\", use \"master\" as the branch, then create the pipeline. This will create and start a new pipeline using your master branch, which should result in your website being updated. \n\nMost likely they were, but the database is not aware of this. To solve this, create a new pipeline using the right branch and run it. \n\nPushing new commits should automatically solve this. Alternatively you can try force pushing to the target branch. \n\nProject details are stored in the database. This meant that this data was lost for projects created after 17:20. We ran a procedure to restore these projects based on their Git repositories that were still stored in\nour NFS cluster. This procedure however was only able to restore projects in their most basic form, without associated data such as issues and merge requests.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从逻辑性（Logical）、桥接性（Bridging）、可行性（Actionable）、简洁性（Simplicity）和流程完整性（Process Integrity）几个角度来分析GitLab.com在2017年1月31日数据库服务中断的事后分析报告，并按照术语（Terminology）、背景（Background）、常见问题解答（FAQ）、流程（Flow）和附录（Appendix）进行总结整理。\n\n### 术语（Terminology）\n\n- **逻辑性（Logical）**：报告中的逻辑性体现在对事件原因和影响的清晰阐述，以及对解决方案的合理推导。例如，报告详细解释了数据库负载增加的原因，以及这如何导致复制过程失败和数据丢失。\n- **桥接性（Bridging）**：报告通过连接不同的技术组件和流程，展示了整个系统的运作方式。例如，它解释了如何使用LVM快照和Azure磁盘快照来恢复服务。\n- **可行性（Actionable）**：报告提供了具体的改进措施和建议，这些措施旨在解决已识别的问题并防止未来的服务中断。\n- **简洁性（Simplicity）**：尽管报告涉及复杂的技术问题，但它尽量以简单易懂的方式呈现信息，使得非技术用户也能理解事件的基本情况。\n- **流程完整性（Process Integrity）**：报告详细描述了恢复过程和预防措施，展示了对流程完整性的重视。\n\n### 背景（Background）\n\n- **服务中断**：2017年1月31日，GitLab.com因数据库主服务器数据意外删除而经历了重大服务中断。\n- **数据丢失**：在UTC时间17:20至00:00之间，约有5000个项目、5000条评论和700个新用户账户的数据丢失。\n- **影响范围**：代码仓库和Wiki在中断期间不可用，但未受影响。GitLab企业客户、GitHost客户和自托管的GitLab CE用户未受影响。\n\n### 常见问题解答（FAQ）\n\n- **如何恢复丢失的数据？**：使用事故发生前6小时的LVM快照来最小化数据丢失，并逐步恢复GitLab.com。\n- **为什么备份没有成功？**：由于pg_dump版本与PostgreSQL版本不匹配，备份程序失败，且由于DMARC未启用，失败通知未能送达。\n- **如何防止未来类似的服务中断？**：改进备份和恢复流程，增加监控，以及测试和优化数据库复制和负载均衡。\n\n### 流程（Flow）\n\n1. **事故发生**：工程师在尝试设置多个PostgreSQL服务器时，意外删除了主数据库服务器的数据。\n2. **数据恢复尝试**：工程师尝试使用备份恢复数据，但由于备份程序的问题，这一过程失败。\n3. **服务恢复**：使用LVM快照恢复数据库，并逐步重新启用GitLab.com。\n4. **公开透明**：GitLab通过公开的Google文档和YouTube直播，向用户实时更新恢复进度。\n5. **根本原因分析**：使用“五个为什么”技术来分析问题的根本原因，并提出改进措施。\n\n### 附录（Appendix）\n\n- **改进措施**：包括更新监控系统、优化数据库复制、启用Azure磁盘快照、建立更好的恢复流程等。\n- **监控仪表板**：GitLab建立了一个公共备份监控仪表板，以实时跟踪备份状态。\n- **未来计划**：GitLab计划继续改进其数据库和整个基础设施的恢复流程，并确保这些流程有明确的责任人。"]]}]
[{"prompt": "User: 下面是一篇故障报告： # Why we spent the last month eliminating PostgreSQL subtransactions \nHow a mysterious stall in database queries uncovered a performance limitation with PostgreSQL. \n[Stan Hu and Grzegorz Bizon](/blog/authors/stanhu-and-grzesiek/) \nSince last June, we noticed the database on GitLab.com would mysteriously stall for minutes, which would lead to users seeing 500 errors during this time. Through a painstaking investigation over several weeks, we finally uncovered the cause of this: initiating a subtransaction via the [`SAVEPOINT` SQL query](https://www.postgresql.org/docs/current/sql-savepoint.html) while a long transaction is in progress can wreak havoc on database replicas. Thus launched a race, which we recently completed, to eliminate all `SAVEPOINT` queries from our code. Here's what happened, how we discovered the problem, and what we did to fix it. \n\nOn June 24th, we noticed that our CI/CD runners service reported a high error rate: \n![runners errors](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/ci-runners-errors.png) \nA quick investigation revealed that database queries used to retrieve CI/CD builds data were timing out and that the unprocessed builds backlog grew at a high rate: \n![builds queue](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/builds-queue.png) \nOur monitoring also showed that some of the SQL queries were waiting for PostgreSQL lightweight locks (`LWLocks`): \n![aggregated lwlocks](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/aggregated-lwlocks.png) \nIn the following weeks we had experienced a few incidents like this. We were surprised to see how sudden these performance degradations were, and how quickly things could go back to normal: \n![ci queries latency](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/ci-queries-latency.png) \n\nIn order to learn more, we extended our observability tooling [to sample more data from `pg_stat_activity`](https://gitlab.com/gitlab-cookbooks/gitlab-exporters/-/merge_requests/231). In PostgreSQL, the `pg_stat_activity` virtual table contains the list of all database connections in the system as well as what they are waiting for, such as a SQL query from the client. We observed a consistent pattern: the queries were waiting on `SubtransControlLock`. Below shows a graph of the URLs or jobs that were stalled: \n![endpoints locked](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/endpoints-locked.png) \nThe purple line shows the sampled number of transactions locked by `SubtransControlLock` for the `POST /api/v4/jobs/request` endpoint that we use for internal communication between GitLab and GitLab Runners processing CI/CD jobs. \nAlthough this endpoint was impacted the most, the whole database cluster appeared to be affected as many other, unrelated queries timed out. \nThis same pattern would rear its head on random days. A week would pass by without incident, and then it would show up for 15 minutes and disappear for days. Were we chasing the Loch Ness Monster? \nLet's call these stalled queries Nessie for fun and profit. \n\nTo understand `SubtransControlLock` ([PostgreSQL 13](https://www.postgresql.org/docs/13/monitoring-stats.html#MONITORING-PG-STAT-ACTIVITY-VIEW) renamed this to `SubtransSLRU`), we first must understand how subtransactions work in PostgreSQL. In PostgreSQL, a transaction can\nstart via a `BEGIN` statement, and a subtransaction can be started with a subsequent `SAVEPOINT` query. PostgreSQL assigns each of these a transaction ID (XID for short) [when a transaction or a subtransaction\nneeds one, usually before a client modifies data](https://gitlab.com/postgres/postgres/blob/a00c138b78521b9bc68b480490a8d601ecdeb816/src/backend/access/transam/README#L193-L198). \n\nFor example, let's say you were running an online store and a customer placed an order. Before the order is fullfilled, the system needs to ensure a credit card account exists for that user. In Rails, a common\npattern is to start a transaction for the order and call [`find_or_create_by`](https://apidock.com/rails/v5.2.3/ActiveRecord/Relation/find_or_create_by). For example: \n``` ruby Order.transaction do   begin\n    CreditAccount.transaction(requires_new: true) do       CreditAccount.find_or_create_by(customer_id: customer.id)   rescue ActiveRecord::RecordNotUnique     retry   end   # Fulfill the order   # ... end ``` \nIf two orders were placed around the same time, you wouldn't want the creation of a duplicate account to fail one of the orders. Instead, you would want the system to say, \"Oh, an account was just created; let me use that.\" \nThat's where subtransactions come in handy: the `requires_new: true` tells Rails to start a new subtransaction if the application already is in a transaction. The code above translates into several SQL calls that look something like: \n``` sql --- Start a transaction\nBEGIN SAVEPOINT active_record_1 --- Look up the account SELECT * FROM credit_accounts WHERE customer_id = 1 --- Insert the account; this may fail due to a duplicate constraint INSERT INTO credit_accounts (customer_id) VALUES (1) --- Abort this by rolling back ROLLBACK TO active_record_1 --- Retry here: Start a new subtransaction\nSAVEPOINT active_record_2 --- Find the newly-created account SELECT * FROM credit_accounts WHERE customer_id = 1 --- Save the data RELEASE SAVEPOINT active_record_2 COMMIT ``` \nOn line 7 above, the `INSERT` might fail if the customer account was already created, and the database unique constraint would prevent a duplicate entry. Without the first `SAVEPOINT` and `ROLLBACK` block, the whole transaction would have failed. With that subtransaction, the transaction can retry gracefully and look up the existing account. \n\nAs we mentioned earlier, Nessie returned at random times with queries waiting for `SubtransControlLock`. `SubtransControlLock` indicates that the query is waiting for PostgreSQL to load subtransaction data from disk into shared memory. \nWhy is this data needed? When a client runs a `SELECT`, for example, PostgreSQL needs to decide whether each version of a row, known as a tuple, is actually visible within the current transaction. It's possible that a tuple has been deleted or has yet to be committed by another transaction. Since only a top-level transaction can actually commit data, PostgreSQL needs to map a subtransaction ID (subXID) to its parent XID. \nThis mapping of subXID to parent XID is stored on disk in the `pg_subtrans` directory. Since reading from disk is slow, PostgreSQL adds a simple least-recently used (SLRU) cache in front for each backend process. The lookup is fast if the desired page is already cached. However, as [Laurenz Albe discussed in his blog post](https://www.cybertec-postgresql.com/en/subtransactions-and-performance-in-postgresql/), PostgreSQL may need to read from disk if the number of active subtransactions exceeds 64 in a given transaction, a condition\nPostgreSQL terms `suboverflow`. Think of it as the feeling you might get if you ate too many Subway sandwiches. \nSuboverflowing (is that a word?) can bog down performance because as Laurenz said, \"Other transactions have to update `pg_subtrans` to register subtransactions, and you can see in the perf output how they vie for lightweight locks with the readers.\" \n\nLaurenz's blog post suggested that we might be using too many subtransactions in one transaction. At first, we suspected we might be doing this in some of our expensive background jobs, such as project export or import. However, while we did see numerous `SAVEPOINT` calls in these jobs, we didn't see an unusual degree of nesting in local testing. \nTo isolate the cause, we started by [adding Prometheus metrics to track subtransactions as a Prometheus metric by model](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/66477). This led to nice graphs as the following: \n![subtransactions plot](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/subtransactions-plot.png) \nWhile this was helpful in seeing the rate of subtransactions over time, we didn't see any obvious spikes that occurred around the time of the database stalls. Still, it was possible that suboverflow was happening. \nTo see if that was happening, we [instrumented our application to track subtransactions and log a message whenever we detected more than 32 `SAVEPOINT` calls in a given\ntransaction](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/67918). Rails makes it possible for the application to subscribe to all of its SQL queries via `ActiveSupport` notifications. Our instrumentation\nlooked something like this, simplified for the purposes of discussion: \n``` ruby ActiveSupport::Notifications.subscribe('sql.active_record') do |event|   sql = event.payload.dig(:sql).to_s   connection = event.payload[:connection]   manager = connection&.transaction_manager \n  context = manager.transaction_context   return if context.nil? \n  if sql.start_with?('BEGIN')     context.set_depth(0)   elsif cmd.start_with?('SAVEPOINT', 'EXCEPTION')     context.increment_savepoints   elsif cmd.start_with?('ROLLBACK TO SAVEPOINT')     context.increment_rollbacks   elsif cmd.start_with?('RELEASE SAVEPOINT')     context.increment_releases   elsif sql.start_with?('COMMIT', 'ROLLBACK')     context.finish_transaction\n  end end ``` \nThis code looks for the key SQL commands that initiate transactions and subtransactions and increments counters when they occurred. After a `COMMIT,` we log a JSON message that contained the backtrace and the number of `SAVEPOINT` and `RELEASES` calls. For example: \n``` json\n{   \"sql\": \"/*application:web,correlation_id:01FEBFH1YTMSFEEHS57FA8C6JX,endpoint_id:POST /api/:version/projects/:id/merge_requests/:merge_request_iid/approve*/ BEGIN\",   \"savepoints_count\": 1,   \"savepoint_backtraces\": [     [       \"app/models/application_record.rb:75:in `block in safe_find_or_create_by'\",       \"app/models/application_record.rb:75:in `safe_find_or_create_by'\",       \"app/models/merge_request.rb:1859:in `ensure_metrics'\",       \"ee/lib/analytics/merge_request_metrics_refresh.rb:11:in `block in execute'\",       \"ee/lib/analytics/merge_request_metrics_refresh.rb:10:in `each'\",       \"ee/lib/analytics/merge_request_metrics_refresh.rb:10:in `execute'\",       \"ee/app/services/ee/merge_requests/approval_service.rb:57:in `calculate_approvals_metrics'\",       \"ee/app/services/ee/merge_requests/approval_service.rb:45:in `block in create_event'\",       \"ee/app/services/ee/merge_requests/approval_service.rb:43:in `create_event'\",       \"app/services/merge_requests/approval_service.rb:13:in `execute'\",       \"ee/app/services/ee/merge_requests/approval_service.rb:14:in `execute'\",       \"lib/api/merge_request_approvals.rb:58:in `block (3 levels) in <class:MergeRequestApprovals>'\",     ]   \"rollbacks_count\": 0,   \"releases_count\": 1 } ``` \nThis log message contains not only the number of subtransactions via `savepoints_count`, but it also contains a handy backtrace that identifies the exact source of the problem. The `sql` field also contains [Marginalia comments](https://github.com/basecamp/marginalia) that we tack onto every SQL query. These comments make it possible to identify what HTTP request initiated the SQL query. \n\nThe new instrumentation showed that while the application regularly used subtransactions, it never exceeded 10 nested `SAVEPOINT` calls. \nMeanwhile, [Nikolay Samokhvalov](https://gitlab.com/NikolayS), founder of [Postgres.ai](https://postgres.ai/), performed a battery of tests [trying to replicate the problem](https://gitlab.com/postgres-ai/postgresql-consulting/tests-and-benchmarks/-/issues/20). He replicated Laurenz's results when a single transaction exceeded 64 subtransactions, but that wasn't happening here. \nWhen the database stalls occurred, we observed a number of patterns: \n1.  Only the replicas were affected; the primary remained unaffected. 2.  There was a long-running transaction, usually relating to     PostgreSQL's autovacuuming, during the time. The stalls stopped     quickly after the transaction ended. \nWhy would this matter? Analyzing the PostgreSQL source code, Senior Support Engineer [Catalin Irimie](https://gitlab.com/cat) [posed an\nintriguing question that led to a breakthrough in our understanding](https://gitlab.com/gitlab-org/gitlab/-/issues/338410#note_652056284): \n> Does this mean that, having subtransactions spanning more than 32 > cache pages, concurrently, would trigger the exclusive > SubtransControlLock because we still end up reading them from the > disk? \n\nTo answer this, Nikolay immediately modified his test [to involve replicas and long-running transactions](https://gitlab.com/postgres-ai/postgresql-consulting/tests-and-benchmarks/-/issues/21#note_653453774). Within a day, he reproduced the problem: \n![Nikolay experiment](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/nikolay-experiment.png) \nThe image above shows that transaction rates remain steady around 360,000 transactions per second (TPS). Everything was proceeding fine until the long-running transaction started on the primary. Then suddenly the transaction rates plummeted to 50,000 TPS on the replicas. Canceling the long transaction immediately caused the transaction rate to return. \n\nIn his blog post, Nikolay called the problem [Subtrans SLRU overflow](https://v2.postgres.ai/blog/20210831-postgresql-subtransactions-considered-harmful#problem-4-subtrans-slru-overflow). In a busy database, it's possible for the size of the subtransaction\nlog to grow so large that the working set no longer fits into memory. This results in a lot of cache misses, which in turn causes a high amount of disk I/O and CPU as PostgreSQL furiously tries to load data from disk to keep up with all the lookups. \nAs mentioned earlier, the subtransaction cache holds a mapping of the subXID to the parent XID. When PostgreSQL needs to look up the subXID, it calculates in which memory page this ID would live, and then does a linear search to find in the memory page. If the page is not in the cache, it evicts one page and loads the desired one into memory. The diagram below shows the memory layout of the subtransaction SLRU. \n![Subtrans SLRU](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/subtrans-slru.png) \nBy default, each SLRU page is an 8K buffer holding 4-byte parent XIDs. This means 8192/4 = 2048 transaction IDs can be stored in each page. \nNote that there may be gaps in each page. PostgreSQL will cache XIDs as needed, so a single XID can occupy an entire page. \nThere are 32 (`NUM_SUBTRANS_BUFFERS`) pages, which means up to 65K transaction IDs can be stored in memory. Nikolay demonstrated that in a busy system, it took about 18 seconds to fill up all 65K entries. Then\nperformance dropped off a cliff, making the database replicas unusable. \nTo our surprise, our experiments also demonstrated that a single `SAVEPOINT` during a long-transaction [could initiate this problem if many writes also occurred simultaneously](https://gitlab.com/gitlab-org/gitlab/-/issues/338865#note_655312474). That is, it wasn't enough just to reduce the frequency of `SAVEPOINT`; we had to eliminate them completely. \n\nTo answer this question, we need to understand what happens when a `SAVEPOINT` occurs in one query while a long-running transaction is running. \nWe mentioned earlier that PostgreSQL needs to decide whether a given row is visible to support a feature called [multi-version concurrency control](https://www.postgresql.org/docs/current/mvcc.html), or MVCC for short. It does this by storing hidden columns, `xmin` and `xmax`, in\neach tuple. \n`xmin` holds the XID of when the tuple was created, and `xmax` holds the XID when it was marked as dead (0 if the row is still present). In\naddition, at the beginning of a transaction, PostgreSQL records metadata in a database snapshot. Among other items, this snapshot records the oldest XID and the newest XID in its own `xmin` and `xmax` values. \nThis metadata helps [PostgreSQL determine whether a tuple is visible](https://www.interdb.jp/pg/pgsql05.html). For example, a committed XID that started before `xmin` is definitely visible, while anything after `xmax` is invisible. \n\nLong transactions are bad in general because they can tie up connections, but they can cause a subtly different problem on a replica. On the replica, a single `SAVEPOINT` during a long transaction causes a snapshot to suboverflow. Remember that dragged down performance in the case where we had more than 64 subtransactions. \nFundamentally, the problem happens because a replica behaves differently from a primary when creating snapshots and checking for tuple visibility. The diagram below illustrates an example with some of the data structures used in PostgreSQL: \n![Diagram of subtransaction handling in\nreplicas](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/pg-replica-subtransaction-diagram.png) \nOn the top of this diagram, we can see the XIDs increase at the beginning of a subtransaction: the `INSERT` after the `BEGIN` gets 1, and the subsequent `INSERT` in `SAVEPOINT` gets 2. Another client comes along and performs a `INSERT` and `SELECT` at XID 3. \nOn the primary, PostgreSQL stores the transactions in progress in a shared memory segment. The process array (`procarray`) stores XID 1 with the first connection, and the database also writes that information to the `pg_xact` directory. XID 2 gets stored in the `pg_subtrans` directory, mapped to its parent, XID 1. \nIf a read happens on the primary, the snapshot generated contains `xmin` as 1, and `xmax` as 3. `txip` holds a list of transactions in progress, and `subxip` holds a list of subtransactions in progress. \nHowever, neither the `procarray` nor the snapshot are shared directly with the replica. The replica receives all the data it needs from the write-ahead log (WAL). \nPlaying the WAL back one entry at time, the replica populates a shared data structure called `KnownAssignedIds`. It contains all the transactions in progress on the primary. Since this structure can only hold a limited number of IDs, a busy database with a lot of active subtransactions could easily fill this buffer. PostgreSQL made a design\nchoice to kick out all subXIDs from this list and store them in the `pg_subtrans` directory. \nWhen a snapshot is generated on the replica, notice how `txip` is blank. A PostgreSQL replica treats **all** XIDs as though they are subtransactions and throws them into the `subxip` bucket. That works because if a XID has a parent XID, then it's a subtransaction. Otherwise, it's a normal transaction. [The code comments explain the rationale](https://gitlab.com/postgres/postgres/blob/9f540f840665936132dd30bd8e58e9a67e648f22/src/backend/storage/ipc/procarray.c#L1665-L1681). \nHowever, this means the snapshot is missing subXIDs, and that could be bad for MVCC. To deal with that, the [replica also updates `lastOverflowedXID`](https://gitlab.com/postgres/postgres/blob/9f540f840665936132dd30bd8e58e9a67e648f22/src/backend/storage/ipc/procarray.c#L3176-L3182): \n``` c  * When we throw away subXIDs from KnownAssignedXids, we need to keep track of  * that, similarly to tracking overflow of a PGPROC's subxids array.  We do  * that by remembering the lastOverflowedXID, ie the last thrown-away subXID.  * As long as that is within the range of interesting XIDs, we have to assume  * that subXIDs are missing from snapshots.  (Note that subXID overflow occurs  * on primary when 65th subXID arrives, whereas on standby it occurs when 64th  * subXID arrives - that is not an error.) ``` \nWhat is this \"range of interesting XIDs\"? We can see this in [the code below](https://gitlab.com/postgres/postgres/blob/4bf0bce161097869be5a56706b31388ba15e0113/src/backend/storage/ipc/procarray.c#L1702-L1703): \n``` c if (TransactionIdPrecedesOrEquals(xmin, procArray->lastOverflowedXid))     suboverflowed = true; ``` \nIf `lastOverflowedXid` is smaller than our snapshot's `xmin`, it means that all subtransactions have completed, so we don't need to check for subtransactions. However, in our example: \n1.  `xmin` is 1 because of the transaction. 2.  `lastOverflowXid` is 2 because of the `SAVEPOINT`. \nThis means `suboverflowed` is set to `true` here, which tells PostgreSQL that whenever a XID needs to be checked, check to see if it has a parent XID. Remember that this causes PostgreSQL to: \n1.  Look up the subXID for the parent XID in the SLRU cache. 2.  If this doesn't exist in the cache, fetch the data from `pg_trans`. \nIn a busy system, the requested XIDs could span an ever-growing range of values, which could easily exhaust the 64K entries in the SLRU cache. This range will continue to grow as long as the transaction runs; the rate of increase depends on how many updates are happening on the prmary. As soon as the transaction terminates, the `suboverflowed` state gets set to `false`. \nIn other words, we've replicated the same conditions as we saw with 64 subtransactions, only with a single `SAVEPOINT` and a long transaction. \n\nThere are three options: \n1.  Eliminate `SAVEPOINT` calls completely. 2.  Eliminate all long-running transactions. 3.  Apply [Andrey Borodin's patches to PostgreSQL and increase the     subtransaction\n    cache](https://www.postgresql.org/message-id/flat/494C5E7F-E410-48FA-A93E-F7723D859561%40yandex-team.ru#18c79477bf7fc44a3ac3d1ce55e4c169). \nWe chose the first option because most uses of subtransaction could be removed fairly easily. There were a [number of approaches](https://gitlab.com/groups/gitlab-org/-/epics/6540) we took: \n1.  Perform updates outside of a subtransaction. Examples:     [1](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/68471),     [2](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/68690) 2.  Rewrite a query to use a `INSERT` or an `UPDATE` with an\n    `ON CONFLICT` clause to deal with duplicate constraint violations.     Examples:     [1](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/68433),     [2](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/69240),     [3](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/68509) 3.  Live with a non-atomic `find_or_create_by`. We used this approach     sparingly. Example:     [1](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/68649) \nIn addition, we added [an alert whenever the application used a a single `SAVEPOINT`](https://gitlab.com/gitlab-com/runbooks/-/merge_requests/3881): \n![subtransaction\nalert](https://about.gitlab.com/images/blogimages/postgresql-subtransactions/subtransactions-alert-example.png) \nThis had the side benefit of flagging a [minor bug](https://gitlab.com/gitlab-org/gitlab/-/merge_requests/70889). \n\nIn our database, it wasn't practical to eliminate all long-running transactions because we think many of them happened via [database autovacuuming](https://www.postgresql.org/docs/current/runtime-config-autovacuum.html), but [we're not able to reproduce this yet](https://gitlab.com/postgres-ai/postgresql-consulting/tests-and-benchmarks/-/issues/21#note_669698320). We are working on partitioning the tables and sharding the database, but this is a much more time-consuming problem than removing all subtransactions. \n\nAlthough we tested Andrey's PostgreSQL patches, we did not feel comfortable deviating from the official PostgreSQL releases. Plus, maintaining a custom patched release over upgrades would add a significant maintenance burden for our infrastructure team. Our self-managed customers would also not benefit unless they used a patched database. \nAndrey's patches do two main things: \n1.  Allow administrators to change the SLRU size to any value. 2.  Adds an [associative     cache](https://www.youtube.com/watch?v=A0vR-ks3hsQ). to make it     performant to use a large cache value. \nRemember that the SLRU cache does a linear search for the desired page. That works fine when there are only 32 pages to search, but if you increase the cache size to 100 MB the search becomes much more expensive. The associative cache makes the lookup fast by indexing pages with a bitmask and looking up the entry with offsets from the remaining bits. This mitigates the problem because a transaction would need to be several magnitudes longer to cause a problem. \nNikolay demonstrated that the `SAVEPOINT` problem disappeared as soon as we increased the SLRU size to 100 MB with those patches. With a 100 MB cache, PostgreSQL can cache 26.2 million IDs (104857600/4), far more than the measely 65K. \nThese [patches are currently awaiting review](https://postgres.ai/blog/20210831-postgresql-subtransactions-considered-harmful#ideas-for-postgresql-development), but in our opinion they should be given high priority for PostgreSQL 15. \n\nSince removing all `SAVEPOINT` queries, we have not seen Nessie rear her head again. If you are running PostgreSQL with read replicas, we strongly recommend that you also remove *all* subtransactions until further notice. \nPostgreSQL is a fantastic database, and its well-commented code makes it possible to understand its limitations under different configurations. \nWe would like to thank the GitLab community for bearing with us while we iron out this production issue. \nWe are also grateful for the support from [Nikolay Samokhvalov](https://gitlab.com/NikolayS) and [Catalin\nIrimie](https://gitlab.com/cat), who contributed to understanding where our Loch Ness Monster was hiding.\n\n从logical、bridging、actionable、simplicity、process integrity几个角度来分析文档，然后按照固定的 terminology、background、faq、flow、appendix 分类做总结整理。\n\nAssistant:", "response": [["根据您提供的内容，我们可以从logical（逻辑性）、bridging（桥接性）、actionable（可行性）、simplicity（简洁性）和process integrity（流程完整性）等几个角度来分析文档，并按照terminology（术语）、background（背景）、faq（常见问题解答）、flow（流程）、appendix（附录）进行总结整理。\n\n### Terminology（术语）\n\n- **Subtransactions**: 在PostgreSQL中，子事务是嵌套在主事务中的一个事务，可以通过SAVEPOINT来创建。\n- **SAVEPOINT**: 在事务中设置的一个点，允许事务回滚到该点而不是整个事务的开始。\n- **SubtransControlLock**: PostgreSQL中的一个锁，用于控制对子事务数据的访问。\n- **SLRU (Simple Least Recently Used) cache**: 一个简单的最近最少使用缓存，用于缓存子事务ID到父事务ID的映射。\n- **MVCC (Multi-Version Concurrency Control)**: 多版本并发控制，PostgreSQL用于处理数据的并发访问。\n\n### Background（背景）\n\nGitLab.com的数据库在过去几个月中出现了神秘的延迟现象，导致用户在这段时间内看到500错误。经过数周的调查，发现问题的根本原因是在长时间运行的事务中通过SAVEPOINT SQL查询启动子事务会对数据库副本造成严重影响。这种现象被称为“Nessie”，即停滞的数据库查询。\n\n### FAQ（常见问题解答）\n\n**Q: 为什么使用SAVEPOINT会导致性能问题？**\nA: 当在长时间运行的事务中使用SAVEPOINT时，可能会导致子事务的数量超出SLRU缓存的容量，从而引发性能问题。\n\n**Q: 如何解决子事务导致的性能问题？**\nA: 可以通过消除所有SAVEPOINT查询、消除所有长时间运行的事务或应用PostgreSQL的补丁来增加子事务缓存来解决。\n\n**Q: 为什么选择消除SAVEPOINT而不是其他解决方案？**\nA: 消除SAVEPOINT相对容易实现，而且在GitLab的数据库中，大多数使用子事务的场景都可以被移除。\n\n### Flow（流程）\n\n1. **问题发现**: GitLab.com的数据库出现延迟，用户遇到500错误。\n2. **初步调查**: 发现数据库查询超时，SQL查询等待PostgreSQL的轻量级锁。\n3. **深入分析**: 通过扩展监控工具，发现查询等待SubtransControlLock。\n4. **问题定位**: 确定问题是由于子事务在长时间运行的事务中使用SAVEPOINT引起的。\n5. **解决方案探索**: 测试不同的解决方案，包括消除子事务、消除长时间运行的事务和应用PostgreSQL补丁。\n6. **实施解决方案**: 选择并实施消除所有SAVEPOINT查询的方案。\n7. **结果验证**: 移除所有SAVEPOINT后，未再出现性能问题。\n\n### Appendix（附录）\n\n- **Nessie现象**: 描述了GitLab.com数据库延迟现象的详细情况和影响。\n- **子事务和SAVEPOINT的使用场景**: 举例说明了在Rails框架中如何使用子事务和SAVEPOINT来处理潜在的数据冲突。\n- **PostgreSQL补丁详情**: 介绍了由Andrey Borodin提出的PostgreSQL补丁，包括允许更改SLRU大小和添加关联缓存的功能。\n- **性能测试结果**: 展示了在应用补丁后，如何通过增加SLRU缓存大小来消除性能问题。"]]}]
